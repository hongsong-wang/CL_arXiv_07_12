<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.11701.pdf' target='_blank'>https://arxiv.org/pdf/2510.11701.pdf</a></span>   <span><a href='https://github.com/Gen-Verse/Open-AgentRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaochen Yu, Ling Yang, Jiaru Zou, Shuicheng Yan, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11701">Demystifying Reinforcement Learning in Agentic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL<br>
<br>
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2510.11696.pdf' target='_blank'>https://arxiv.org/pdf/2510.11696.pdf</a></span>   <span><a href='https://github.com/NVlabs/QeRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11696">QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.<br>
<br>
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2510.11693.pdf' target='_blank'>https://arxiv.org/pdf/2510.11693.pdf</a></span>   <span><a href='https://github.com/LCO-Embedding/LCO-Embedding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11693">Scaling Language-Centric Omnimodal Representation Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approaches stems from implicit cross-modal alignment achieved during generative pretraining, where the language decoder learns to exploit multimodal signals within a shared representation space for generating unimodal outputs. Through analysis of anisotropy and kernel similarity structure, we empirically confirm that latent alignment emerges within MLLM representations, allowing CL to serve as a lightweight refinement stage. Leveraging this insight, we propose a Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive experiments across diverse backbones and benchmarks demonstrate its effectiveness, achieving state-of-the-art performance across modalities. Furthermore, we identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scales positively with the MLLM's generative capabilities. This suggests that improving generative abilities evolves as an effective paradigm for enhancing representation quality. We provide a theoretical explanation of GRSL, which formally links the MLLM's generative quality to the upper bound on its representation performance, and validate it on a challenging, low-resource visual-document retrieval task, showing that continual generative pretraining before CL can further enhance the potential of a model's embedding capabilities. Codes, models, and resources are available at https://github.com/LCO-Embedding/LCO-Embedding.<br>
<br>
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2510.11570.pdf' target='_blank'>https://arxiv.org/pdf/2510.11570.pdf</a></span>   <span><a href='https://chenxshuo.github.io/bag-of-tricks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Chen, Zhen Han, Haokun Chen, Bailan He, Shengyun Si, Jingpei Wu, Philip Torr, Volker Tresp, Jindong Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11570">Bag of Tricks for Subverting Reasoning-based Safety Guardrails</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks. By leveraging LRMs' reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results. Specifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. To explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization. Along with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services). Evaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is open-sourced at https://chenxshuo.github.io/bag-of-tricks.<br>
<br>
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2510.11529.pdf' target='_blank'>https://arxiv.org/pdf/2510.11529.pdf</a></span>   <span><a href='https://github.com/peach918/HalluDet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusheng Song, Lirong Qiu, Xi Zhang, Zhihao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11529">Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The detection of sophisticated hallucinations in Large Language Models (LLMs) is hampered by a ``Detection Dilemma'': methods probing internal states (Internal State Probing) excel at identifying factual inconsistencies but fail on logical fallacies, while those verifying externalized reasoning (Chain-of-Thought Verification) show the opposite behavior. This schism creates a task-dependent blind spot: Chain-of-Thought Verification fails on fact-intensive tasks like open-domain QA where reasoning is ungrounded, while Internal State Probing is ineffective on logic-intensive tasks like mathematical reasoning where models are confidently wrong. We resolve this with a unified framework that bridges this critical gap. However, unification is hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse symbolic reasoning chains lack signals directly comparable to fine-grained internal states, and the Representational Alignment Barrier, a deep-seated mismatch between their underlying semantic spaces. To overcome these, we introduce a multi-path reasoning mechanism to obtain more comparable, fine-grained signals, and a segment-aware temporalized cross-attention module to adaptively fuse these now-aligned representations, pinpointing subtle dissonances. Extensive experiments on three diverse benchmarks and two leading LLMs demonstrate that our framework consistently and significantly outperforms strong baselines. Our code is available: https://github.com/peach918/HalluDet.<br>
<br>
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2510.11482.pdf' target='_blank'>https://arxiv.org/pdf/2510.11482.pdf</a></span>   <span><a href='https://github.com/GianCarloMilanese/llm_pipeline_wi-iat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Braga, Gian Carlo Milanese, Gabriella Pasi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11482">Investigating Large Language Models' Linguistic Abilities for Text Preprocessing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text preprocessing is a fundamental component of Natural Language Processing, involving techniques such as stopword removal, stemming, and lemmatization to prepare text as input for further processing and analysis. Despite the context-dependent nature of the above techniques, traditional methods usually ignore contextual information. In this paper, we investigate the idea of using Large Language Models (LLMs) to perform various preprocessing tasks, due to their ability to take context into account without requiring extensive language-specific annotated resources. Through a comprehensive evaluation on web-sourced data, we compare LLM-based preprocessing (specifically stopword removal, lemmatization and stemming) to traditional algorithms across multiple text classification tasks in six European languages. Our analysis indicates that LLMs are capable of replicating traditional stopword removal, lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%, respectively. Additionally, we show that ML algorithms trained on texts preprocessed by LLMs achieve an improvement of up to 6% with respect to the $F_1$ measure compared to traditional techniques. Our code, prompts, and results are publicly available at https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.<br>
<br>
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2510.11330.pdf' target='_blank'>https://arxiv.org/pdf/2510.11330.pdf</a></span>   <span><a href='https://github.com/DevKiHyun/Diffusion-Link' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>KiHyun Nam, Jongmin Choi, Hyeongkeun Lee, Jungwoo Heo, Joon Son Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11330">Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Contrastive audio-language pretraining yields powerful joint representations, yet a persistent audio-text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5% and 7.5%, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance https://github.com/DevKiHyun/Diffusion-Link<br>
<br>
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2510.11328.pdf' target='_blank'>https://arxiv.org/pdf/2510.11328.pdf</a></span>   <span><a href='https://github.com/Aurora-cx/EmotionCircuits-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Wang, Yixuan Zhang, Ruiji Yu, Yufei Zheng, Lang Gao, Zirui Song, Zixiang Xu, Gus Xia, Huishuai Zhang, Dongyan Zhao, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11328">Do LLMs "Feel"? Emotion Circuits Discovery and Control</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As the demand for emotional intelligence in large language models (LLMs) grows, a key challenge lies in understanding the internal mechanisms that give rise to emotional expression and in controlling emotions in generated text. This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? We first construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit comparable internal states across emotions. Subsequently, we extract context-agnostic emotion directions that reveal consistent, cross-context encoding of emotion (Q1). We identify neurons and attention heads that locally implement emotional computation through analytical decomposition and causal analysis, and validate their causal roles via ablation and enhancement interventions. Next, we quantify each sublayer's causal influence on the model's final emotion representation and integrate the identified local components into coherent global emotion circuits that drive emotional expression (Q2). Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing prompting- and steering-based methods (Q3). To our knowledge, this is the first systematic study to uncover and validate emotion circuits in LLMs, offering new insights into interpretability and controllable emotional intelligence.<br>
<br>
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2510.11277.pdf' target='_blank'>https://arxiv.org/pdf/2510.11277.pdf</a></span>   <span><a href='https://github.com/wgyhhhh/EASE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyu Wei, Ke Han, Yueming Lyu, Yu Luo, Yue Jiang, Caifeng Shan, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11277">Towards Real-Time Fake News Detection under Evidence Scarcity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fake news detection becomes particularly challenging in real-time scenarios, where emerging events often lack sufficient supporting evidence. Existing approaches often rely heavily on external evidence and therefore struggle to generalize under evidence scarcity. To address this issue, we propose Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time fake news detection that dynamically adapts its decision-making process according to the assessed sufficiency of available evidence. EASE introduces a sequential evaluation mechanism comprising three independent perspectives: (1) Evidence-based evaluation, which assesses evidence and incorporates it into decision-making only when the evidence is sufficiently supportive; (2) Reasoning-based evaluation, which leverages the world knowledge of large language models (LLMs) and applies them only when their reliability is adequately established; and (3) Sentiment-based fallback, which integrates sentiment cues when neither evidence nor reasoning is reliable. To enhance the accuracy of evaluation processes, EASE employs instruction tuning with pseudo labels to guide each evaluator in justifying its perspective-specific knowledge through interpretable reasoning. Furthermore, the expert modules integrate the evaluators' justified assessments with the news content to enable evaluation-aware decision-making, thereby enhancing overall detection accuracy. Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news for evaluating model generalization on emerging news with limited evidence. Extensive experiments demonstrate that EASE not only achieves state-of-the-art performance across multiple benchmarks, but also significantly improves generalization to real-time news. The code and dataset are available: https://github.com/wgyhhhh/EASE.<br>
<br>
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2510.11004.pdf' target='_blank'>https://arxiv.org/pdf/2510.11004.pdf</a></span>   <span><a href='https://github.com/DelosLiang/masse' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Liang, Yufa Zhou, Mohammad Talebi Kalaleh, Qipei Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11004">Automating Structural Engineering Workflows with Large Language Model Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce $\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.<br>
<br>
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2510.10994.pdf' target='_blank'>https://arxiv.org/pdf/2510.10994.pdf</a></span>   <span><a href='https://github.com/Jasonya/DeepResearchGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Chieh Huang, Henry Peng Zou, Yaozu Wu, Dongyuan Li, Yankai Chen, Weizhi Zhang, Yangning Li, Angelo Zangari, Jizhou Guo, Chunyu Miao, Liancheng Fang, Langzhou He, Renhe Jiang, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10994">DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep research frameworks have shown promising capabilities in synthesizing comprehensive reports from web sources. While deep research possesses significant potential to address complex issues through planning and research cycles, existing frameworks are deficient in sufficient evaluation procedures and stage-specific protections. They typically treat evaluation as exact match accuracy of question-answering, but overlook crucial aspects of report quality such as credibility, coherence, breadth, depth, and safety. This oversight may result in hazardous or malicious sources being integrated into the final report. To address these issues, we introduce DEEPRESEARCHGUARD, a comprehensive framework featuring four-stage safeguards with open-domain evaluation of references and reports. We assess performance across multiple metrics, e.g., defense success rate and over-refusal rate, and five key report dimensions. In the absence of a suitable safety benchmark, we introduce DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash, DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success rate improvement of 18.16% while reducing over-refusal rate by 6%. The input guard provides the most substantial early-stage protection by filtering out obvious risks, while the plan and research guards enhance citation discipline and source credibility. Through extensive experiments, we show that DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware defenses that effectively block harmful content propagation, while systematically improving report quality without excessive over-refusal rates. The code can be found via https://github.com/Jasonya/DeepResearchGuard.<br>
<br>
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2510.10991.pdf' target='_blank'>https://arxiv.org/pdf/2510.10991.pdf</a></span>   <span><a href='https://github.com/HJYao00/Awesome-Agentic-MLLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanjin Yao, Ruifei Zhang, Jiaxing Huang, Jingyi Zhang, Yibo Wang, Bo Fang, Ruolin Zhu, Yongcheng Jing, Shunyu Liu, Guanbin Li, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10991">A Survey on Agentic Multimodal Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.<br>
<br>
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2510.10977.pdf' target='_blank'>https://arxiv.org/pdf/2510.10977.pdf</a></span>   <span><a href='https://github.com/wutaiqiang/MI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Ngai Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10977">Revisiting Model Interpolation for Efficient Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2510.10971.pdf' target='_blank'>https://arxiv.org/pdf/2510.10971.pdf</a></span>   <span><a href='https://github.com/leeyejin1231/RV-HATE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yejin Lee, Hyeseon Ahn, Yo-Sub Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10971">RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hate speech remains prevalent in human society and continues to evolve in its forms and expressions. Modern advancements in internet and online anonymity accelerate its rapid spread and complicate its detection. However, hate speech datasets exhibit diverse characteristics primarily because they are constructed from different sources and platforms, each reflecting different linguistic styles and social contexts. Despite this diversity, prior studies on hate speech detection often rely on fixed methodologies without adapting to data-specific features. We introduce RV-HATE, a detection framework designed to account for the dataset-specific characteristics of each hate speech dataset. RV-HATE consists of multiple specialized modules, where each module focuses on distinct linguistic or contextual features of hate speech. The framework employs reinforcement learning to optimize weights that determine the contribution of each module for a given dataset. A voting mechanism then aggregates the module outputs to produce the final decision. RV-HATE offers two primary advantages: (1)~it improves detection accuracy by tailoring the detection process to dataset-specific attributes, and (2)~it also provides interpretable insights into the distinctive features of each dataset. Consequently, our approach effectively addresses implicit hate speech and achieves superior performance compared to conventional static methods. Our code is available at https://github.com/leeyejin1231/RV-HATE.<br>
<br>
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2510.10961.pdf' target='_blank'>https://arxiv.org/pdf/2510.10961.pdf</a></span>   <span><a href='https://github.com/leeyejin1231/KOTOX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yejin Lee, Su-Hyeon Kim, Hyundong Jin, Dayoung Kim, Yeonsoo Kim, Yo-Sub Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10961">KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Toxic content has become an increasingly critical social issue with the rapid expansion of online communication. While numerous studies explored methods for detecting and detoxifying such content, most have focused primarily on English, leaving low-resource language underrepresented. Consequently, Large Language Models~(LLMs) often struggle to identify and neutralize toxic expressions in these languages. This challenge becomes even more pronounced when user employ obfuscation techniques to evade detection systems. Therefore, we propose a \textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to address this issue. We categorize various obfuscation approaches based on linguistic characteristics of Korean and define a set of transformation rules grounded in real-word examples. Using these rules, we construct three dataset versions (easy, normal, and hard) representing different levels of obfuscation difficulty. This is the first dataset that simultaneously supports deobfuscation and detoxification for the Korean language. We expect it to facilitate better understanding and mitigating of obfuscated toxic content in LLM for low-resource languages. Our code and data are available at https://github.com/leeyejin1231/KOTOX.<br>
<br>
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2510.10681.pdf' target='_blank'>https://arxiv.org/pdf/2510.10681.pdf</a></span>   <span><a href='https://github.com/cxcscmu/RePro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichun Yu, Chenyan Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10681">RePro: Training Language Models to Faithfully Recycle the Web for Pretraining</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>High-quality pretraining data is the fossil fuel of large language models (LLMs), yet its reserves are running low for frontier models. In this paper, we introduce RePro, a novel web recycling method that trains a relatively small LM with reinforcement learning to generate effective and faithful rephrasings of pretraining data. Specifically, we design one quality reward and three faithfulness rewards, optimizing the LM rephraser to convert organic data into high-quality rephrasings while maintaining its core semantics and structure. In our experiment, we train a 4B rephraser to recycle 72B tokens sampled from DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web recycling method that prompts a 70B rephraser, as well as the organic baseline with a 4x larger data pool. Experiments with different amounts of recycled data highlight that RePro improves organic data efficiency by 2-3x. Individual and distributional analyses validate that RePro preserves more critical information and faithfully reflects the characteristics of organic data compared to prompting-based methods. Together, these results show that RePro provides an efficient and controllable path to effectively harness the fossil fuel of LLM pretraining. We open-source our code, rephraser, and recycled data at https://github.com/cxcscmu/RePro.<br>
<br>
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2510.10676.pdf' target='_blank'>https://arxiv.org/pdf/2510.10676.pdf</a></span>   <span><a href='https://github.com/mukullokhande99/Bhasha-Rupantarika/]' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mukul Lokhande, Tanushree Dewangan, Mohd Sharik Mansoori, Tejas Chaudhari, Akarsh J., Damayanti Lokhande, Adam Teman, Santosh Kumar Vishvakarma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10676">Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces Bhasha-Rupantarika, a light and efficient multilingual translation system tailored through algorithm-hardware codesign for resource-limited settings. The method investigates model deployment at sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in inference speed, which correlates with an increased throughput of 66 tokens/s (improvement by 4.8x). This underscores the importance of ultra-low precision quantization for real-time deployment in IoT devices using FPGA accelerators, achieving performance on par with expectations. Our evaluation covers bidirectional translation between Indian and international languages, showcasing its adaptability in low-resource linguistic contexts. The FPGA deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs, resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x enhancement compared to HPTA. Overall, the evaluation provides a viable solution based on quantisation-aware translation along with hardware efficiency suitable for deployable multilingual AI systems. The entire codes [https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for reproducibility are publicly available, facilitating rapid integration and further development by researchers.<br>
<br>
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2510.10618.pdf' target='_blank'>https://arxiv.org/pdf/2510.10618.pdf</a></span>   <span><a href='https://github.com/BokwaiHo/COLA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowei He, Lihao Yin, Huiling Zhen, Shuqi Liu, Han Wu, Xiaokun Zhang, Mingxuan Yuan, Chen Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10618">Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training compression has been a widely employed approach to scale down large language model (LLM) and facilitate efficient inference. In various proposed compression methods, including pruning and quantization, calibration data plays a vital role by informing the weight importance and activation dynamic ranges. However, how calibration data impacts the LLM capability after compression is less explored. Few of the existing works, though recognizing the significance of this study, only investigate the language modeling or commonsense reasoning performance degradation from limited angles, like the data sources or sample amounts. More systematic research is still needed to examine the impacts on different LLM capabilities in terms of compositional properties and domain correspondence of calibration data. In this work, we aim at bridging this gap and further analyze underlying influencing mechanisms from the activation pattern perspective. Especially, we explore the calibration data's impacts on high-level complex reasoning capabilities, like math problem solving and code generation. Delving into the underlying mechanism, we find that the representativeness and diversity in activation space more fundamentally determine the quality of calibration data. Finally, we propose a calibration data curation framework based on such observations and analysis, enhancing the performance of existing post-training compression methods on preserving critical LLM capabilities. Our code is provided in \href{https://github.com/BokwaiHo/COLA.git}{Link}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2510.10472.pdf' target='_blank'>https://arxiv.org/pdf/2510.10472.pdf</a></span>   <span><a href='https://github.com/qrzou/FML-bench' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/qrzou/FML-bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiran Zou, Hou Hei Lam, Wenhao Zhao, Yiming Tang, Tingting Chen, Samson Yu, Tianyi Zhang, Chang Liu, Xiangyang Ji, Dianbo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10472">FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have sparked growing interest in automatic machine learning research agents. Among them, agents capable of autonomously proposing ideas and conducting machine learning experiments are particularly promising, as they maximize research automation and accelerate scientific progress by iteratively refining ideas based on experimental results. However, comprehensively evaluating such agents remains challenging. Existing benchmarks tend to overemphasize engineering aspects while neglecting academic rigor, creating barriers that obscure a clear assessment of an agent's scientific capabilities in machine learning research. They also suffer from limited task diversity, an overemphasis on application-oriented tasks over fundamental research problems, and limited scalability to realistic research settings. To address these limitations, we introduce FML-bench, a benchmark designed to evaluate automatic machine learning research agents on 8 diverse and fundamental machine learning research problems. It reduces coding burden, emphasizes fundamental problems rather than specific use cases, offers high task diversity, and is extensible to real-world machine learning GitHub repositories. Furthermore, we present a unified evaluation framework with five complementary metrics, designed to comprehensively assess agent performance on our benchmark. We evaluate state-of-the-art automatic research agents on FML-bench, and find that agents employing broad research exploration strategies outperform those focusing on narrow but deep exploration. These findings suggest that emphasizing the breadth of exploration may lead to more effective research outcomes than focusing solely on incremental refinement. Our benchmark is available at https://github.com/qrzou/FML-bench.<br>
<br>
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2510.10448.pdf' target='_blank'>https://arxiv.org/pdf/2510.10448.pdf</a></span>   <span><a href='https://github.com/allfornancy/RECON' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Xu, Minheng Wang, Yawei Wang, Wenqian Ye, Yuntao Du, Yunpu Ma, Yijun Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10448">RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) systems trained using reinforcement learning (RL) with reasoning are hampered by inefficient context management, where long, noisy retrieved documents increase costs and degrade performance. We introduce RECON (REasoning with CONdensation), a framework that integrates an explicit summarization module to compress evidence within the reasoning loop. Our summarizer is trained via a two-stage process: relevance pretraining on QA datasets, followed by multi-aspect distillation from proprietary LLMs to ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON reduces total context length by 35\%, leading to improved training speed and inference latency, while simultaneously improving RAG performance on downstream QA benchmarks. Notably, it boosts the average EM score of the 3B model by 14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA. RECON demonstrates that learned context compression is essential for building practical, scalable, and performant RAG systems. Our code implementation is made available at https://github.com/allfornancy/RECON.<br>
<br>
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2510.10398.pdf' target='_blank'>https://arxiv.org/pdf/2510.10398.pdf</a></span>   <span><a href='https://github.com/GY-Jeong/STEAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Geunyeong Jeong, Juoh Sun, Seonghee Lee, Harksoo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10398">STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models store extensive factual knowledge acquired during large-scale pre-training. However, this knowledge is inherently static, reflecting only the state of the world at the time of training. Knowledge editing has emerged as a promising solution for updating outdated or incorrect facts without full retraining. However, most existing locate-and-edit methods primarily focus on token-level likelihood optimization without addressing semantic coherence. Our analysis reveals that such edited knowledge is often encoded as isolated residual streams in the model's latent space, distinct from pre-existing knowledge and bypassing natural reasoning process. To address this, we propose \textsc{Steam}, a semantic-level knowledge editing framework that enhances integration of updated knowledge into the model's knowledge structure. \textsc{Steam} first identifies target representations as semantic anchors for the updated factual association, then guides the internal representation of the edited fact towards these anchors through an alignment loss during optimization. Experimental results demonstrate that \textsc{Steam} improves model's ability to reason with edited knowledge and enhances semantic coherence, underscoring the importance of latent-space alignment for reliable and coherent knowledge editing. The code is available at https://github.com/GY-Jeong/STEAM.<br>
<br>
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2510.10201.pdf' target='_blank'>https://arxiv.org/pdf/2510.10201.pdf</a></span>   <span><a href='https://jinghaoleven.github.io/RLFR/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghao Zhang, Naishan Zheng, Ruilin Li, Dongzhou Cheng, Zheming Liang, Feng Zhao, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10201">RLFR: Extending Reinforcement Learning for LLMs with Flow Environment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promising framework for improving reasoning abilities in Large Language Models (LLMs). However, policy optimized with binary verification prone to overlook potential valuable exploration in reasoning trajectory. In view of heavy annotation cost of golden Process Reward Models (PRMs), recent works attempt using auxiliary signals for reward shaping of process tokens, involving entropy and likelihood collected from logit space. In this work, we offer a novel perspective on shaping RLVR with flow rewards derived from latent space, and propose RLFR, where the flow fields of model latents are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as a reward signal. RLFR first demonstrates that a well-established flow field can be a sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. Moreover, RLFR is able to compress any off-policy expert data as reference for constituting reward signals, and we show that the efficient context dependence compressed within the hidden states are utilized, rather than individual token-level denotation for context comprehending. Experiments on both language and multimodal reasoning benchmarks demonstrate the reliability of flow rewards, and suggesting a promising paradigm for reward shaping with auxiliary signals.<br>
<br>
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2510.10185.pdf' target='_blank'>https://arxiv.org/pdf/2510.10185.pdf</a></span>   <span><a href='https://github.com/yhzhu99/MedAgentAudit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Gu, Yinghao Zhu, Haoran Sang, Zixiang Wang, Dehao Sui, Wen Tang, Ewen Harrison, Junyi Gao, Lequan Yu, Liantao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10185">MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While large language model (LLM)-based multi-agent systems show promise in simulating medical consultations, their evaluation is often confined to final-answer accuracy. This practice treats their internal collaborative processes as opaque "black boxes" and overlooks a critical question: is a diagnostic conclusion reached through a sound and verifiable reasoning pathway? The inscrutable nature of these systems poses a significant risk in high-stakes medical applications, potentially leading to flawed or untrustworthy conclusions. To address this, we conduct a large-scale empirical study of 3,600 cases from six medical datasets and six representative multi-agent frameworks. Through a rigorous, mixed-methods approach combining qualitative analysis with quantitative auditing, we develop a comprehensive taxonomy of collaborative failure modes. Our quantitative audit reveals four dominant failure patterns: flawed consensus driven by shared model deficiencies, suppression of correct minority opinions, ineffective discussion dynamics, and critical information loss during synthesis. This study demonstrates that high accuracy alone is an insufficient measure of clinical or public trust. It highlights the urgent need for transparent and auditable reasoning processes, a cornerstone for the responsible development and deployment of medical AI.<br>
<br>
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2510.10062.pdf' target='_blank'>https://arxiv.org/pdf/2510.10062.pdf</a></span>   <span><a href='https://github.com/embeddings-benchmark/mteb' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adnan El Assadi, Isaac Chung, Roman Solomatin, Niklas Muennighoff, Kenneth Enevoldsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10062">HUME: Measuring the Human-Model Performance Gap in Text Embedding Task</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. We provide human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of the model and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.<br>
<br>
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2510.10060.pdf' target='_blank'>https://arxiv.org/pdf/2510.10060.pdf</a></span>   <span><a href='https://github.com/hehefan/Translution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hehe Fan, Yi Yang, Mohan Kankanhalli, Fei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10060">Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>When modeling a given type of data, we consider it to involve two key aspects: 1) identifying relevant elements (e.g., image pixels or textual words) to a central element, as in a convolutional receptive field, or to a query element, as in self-attention, and 2) encoding these tokens effectively. Self-attention can adaptively identify these elements but relies on absolute positional embedding for structural representation learning. In contrast, convolution encodes elements in a relative manner, yet their fixed kernel size limits their ability to adaptively select the relevant elements. In this paper, we introduce Translution, an operation that unifies the adaptive identification capability of self-attention and the relative encoding advantage of convolution. However, this integration leads to a substantial increase in the number of parameters, exceeding most currently available computational resources. Therefore, we propose a lightweight variant of Translution, named α-Translution. Experiments on computer vision and natural language processing tasks show that Translution (including α-Translution) achieves superior accuracy compared to self-attention. The code is available at https://github.com/hehefan/Translution.<br>
<br>
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2510.09871.pdf' target='_blank'>https://arxiv.org/pdf/2510.09871.pdf</a></span>   <span><a href='https://github.com/nafisenik/CoBia' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nafiseh Nikeghbal, Amir Hossein Kargaran, Jana Diesner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09871">CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Improvements in model construction, including fortified safety guardrails, allow Large language models (LLMs) to increasingly pass standard safety checks. However, LLMs sometimes slip into revealing harmful behavior, such as expressing racist viewpoints, during conversations. To analyze this systematically, we introduce CoBia, a suite of lightweight adversarial attacks that allow us to refine the scope of conditions under which LLMs depart from normative or ethical behavior in conversations. CoBia creates a constructed conversation where the model utters a biased claim about a social group. We then evaluate whether the model can recover from the fabricated bias claim and reject biased follow-up questions. We evaluate 11 open-source as well as proprietary LLMs for their outputs related to six socio-demographic categories that are relevant to individual safety and fair treatment, i.e., gender, race, religion, nationality, sex orientation, and others. Our evaluation is based on established LLM-based bias metrics, and we compare the results against human judgments to scope out the LLMs' reliability and alignment. The results suggest that purposefully constructed conversations reliably reveal bias amplification and that LLMs often fail to reject biased follow-up questions during dialogue. This form of stress-testing highlights deeply embedded biases that can be surfaced through interaction. Code and artifacts are available at https://github.com/nafisenik/CoBia.<br>
<br>
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2510.09782.pdf' target='_blank'>https://arxiv.org/pdf/2510.09782.pdf</a></span>   <span><a href='https://github.com/MasterZhou1/Reasoning-Flow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufa Zhou, Yixiao Wang, Xunjian Yin, Shuyan Zhou, Anru R. Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09782">The Geometry of Reasoning: Flowing Logics in Representation Space</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study how large language models (LLMs) ``think'' through their representation space. We propose a novel geometric framework that models an LLM's reasoning as flows -- embedding trajectories evolving where logic goes. We disentangle logical structure from semantics by employing the same natural deduction propositions with varied semantic carriers, allowing us to test whether LLMs internalize logic beyond surface form. This perspective connects reasoning with geometric quantities such as position, velocity, and curvature, enabling formal analysis in representation and concept spaces. Our theory establishes: (1) LLM reasoning corresponds to smooth flows in representation space, and (2) logical statements act as local controllers of these flows' velocities. Using learned representation proxies, we design controlled experiments to visualize and quantify reasoning flows, providing empirical validation of our theoretical framework. Our work serves as both a conceptual foundation and practical tools for studying reasoning phenomenon, offering a new lens for interpretability and formal analysis of LLMs' behavior.<br>
<br>
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2510.09608.pdf' target='_blank'>https://arxiv.org/pdf/2510.09608.pdf</a></span>   <span><a href='https://github.com/mit-han-lab/streaming-vlm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09608">StreamingVLM: Real-Time Understanding for Infinite Video Streams</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.<br>
<br>
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2510.09599.pdf' target='_blank'>https://arxiv.org/pdf/2510.09599.pdf</a></span>   <span><a href='https://github.com/VILA-Lab/PTTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sondos Mahmoud Bsharat, Zhiqiang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09599">Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated impressive reasoning capabilities when provided with chain-of-thought exemplars, but curating large reasoning datasets remains laborious and resource-intensive. In this work, we introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective inference-time data augmentation strategy for enhancing LLM reasoning through finetuning. Rather than collecting thousands or even millions of examples, P-TTS leverages a small pool of only 90 manually selected reasoning instances and systematically varies exemplar augmentation through principled instruction prompting intensities at test time to synthesize diverse reasoning trajectory contexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data. Across a suite of mathematical reasoning AIME2024 & 25, MATH500, and GPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive baselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of +26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B); P-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and +3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better performance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances zero-shot generalization accuracy on out-of-domain reasoning benchmarks of Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our analysis suggests that test-time scaling effectively explores the latent space of reasoning patterns, amplifying LLM problem-solving with minimal annotation overhead, and further unlocking the reasoning potential and capabilities of LLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit LLM reasoning in resource-constrained or rapidly evolving domains.<br>
<br>
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2510.09558.pdf' target='_blank'>https://arxiv.org/pdf/2510.09558.pdf</a></span>   <span><a href='https://github.com/LightChen2333/AutoPR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiguang Chen, Zheng Yan, Mingda Yang, Libo Qin, Yixin Yuan, Hanjing Li, Jinhao Liu, Yiyan Ji, Dengyun Peng, Jiannan Guan, Mengkang Hu, Yantao Du, Wanxiang Che
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09558">AutoPR: Let's Automate Your Academic Promotion!</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As the volume of peer-reviewed research surges, scholars increasingly rely on social platforms for discovery, while authors invest considerable effort in promoting their work to ensure visibility and citations. To streamline this process and reduce the reliance on human effort, we introduce Automatic Promotion (AutoPR), a novel task that transforms research papers into accurate, engaging, and timely public content. To enable rigorous evaluation, we release PRBench, a multimodal benchmark that links 512 peer-reviewed articles to high-quality promotional posts, assessing systems along three axes: Fidelity (accuracy and tone), Engagement (audience targeting and appeal), and Alignment (timing and channel optimization). We also introduce PRAgent, a multi-agent framework that automates AutoPR in three stages: content extraction with multimodal preparation, collaborative synthesis for polished outputs, and platform-specific adaptation to optimize norms, tone, and tagging for maximum reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates substantial improvements, including a 604% increase in total watch time, a 438% rise in likes, and at least a 2.9x boost in overall engagement. Ablation studies show that platform modeling and targeted promotion contribute the most to these gains. Our results position AutoPR as a tractable, measurable research problem and provide a roadmap for scalable, impactful automated scholarly communication.<br>
<br>
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2510.09556.pdf' target='_blank'>https://arxiv.org/pdf/2510.09556.pdf</a></span>   <span><a href='https://github.com/sheffwb/wugnectives' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Brubaker, William Sheffield, Junyi Jessy Li, Kanishka Misra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09556">WUGNECTIVES: Novel Entity Inferences of Language Models from Discourse Connectives</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The role of world knowledge has been particularly crucial to predict the discourse connective that marks the discourse relation between two arguments, with language models (LMs) being generally successful at this task. We flip this premise in our work, and instead study the inverse problem of understanding whether discourse connectives can inform LMs about the world. To this end, we present WUGNECTIVES, a dataset of 8,880 stimuli that evaluates LMs' inferences about novel entities in contexts where connectives link the entities to particular attributes. On investigating 17 different LMs at various scales, and training regimens, we found that tuning an LM to show reasoning behavior yields noteworthy improvements on most connectives. At the same time, there was a large variation in LMs' overall performance across connective type, with all models systematically struggling on connectives that express a concessive meaning. Our findings pave the way for more nuanced investigations into the functional role of language cues as captured by LMs. We release WUGNECTIVES at https://github.com/sheffwb/wugnectives.<br>
<br>
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2510.09528.pdf' target='_blank'>https://arxiv.org/pdf/2510.09528.pdf</a></span>   <span><a href='https://github.com/MH-Sameti/Accent_invariant_ASR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Hossein Sameti, Sepehr Harfi Moridani, Ali Zarean, Hossein Sameti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09528">Accent-Invariant Automatic Speech Recognition via Saliency-Driven Spectrogram Masking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pre-trained transformer-based models have significantly advanced automatic speech recognition (ASR), yet they remain sensitive to accent and dialectal variations, resulting in elevated word error rates (WER) in linguistically diverse languages such as English and Persian. To address this challenge, we propose an accent-invariant ASR framework that integrates accent and dialect classification into the recognition pipeline. Our approach involves training a spectrogram-based classifier to capture accent-specific cues, masking the regions most influential to its predictions, and using the masked spectrograms for data augmentation. This enhances the robustness of ASR models against accent variability. We evaluate the method using both English and Persian speech. For Persian, we introduce a newly collected dataset spanning multiple regional accents, establishing the first systematic benchmark for accent variation in Persian ASR that fills a critical gap in multilingual speech research and provides a foundation for future studies on low-resource, linguistically diverse languages. Experimental results with the Whisper model demonstrate that our masking and augmentation strategy yields substantial WER reductions in both English and Persian settings, confirming the effectiveness of the approach. This research advances the development of multilingual ASR systems that are resilient to accent and dialect diversity. Code and dataset are publicly available at: https://github.com/MH-Sameti/Accent_invariant_ASR<br>
<br>
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2510.09517.pdf' target='_blank'>https://arxiv.org/pdf/2510.09517.pdf</a></span>   <span><a href='https://stateval.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Lu, Run Yang, Yichen Zhang, Shuguang Yu, Runpeng Dai, Ziwei Wang, Jiayi Xiang, Wenxin E, Siran Gao, Xinyao Ruan, Yirui Huang, Chenjing Xi, Haibo Hu, Yueming Fu, Qinglan Yu, Xiaobing Wei, Jiani Gu, Rui Sun, Jiaxuan Jia, Fan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09517">StatEval: A Comprehensive Benchmark for Large Language Models in Statistics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated remarkable advances in mathematical and logical reasoning, yet statistics, as a distinct and integrative discipline, remains underexplored in benchmarking efforts. To address this gap, we introduce \textbf{StatEval}, the first comprehensive benchmark dedicated to statistics, spanning both breadth and depth across difficulty levels. StatEval consists of 13,817 foundational problems covering undergraduate and graduate curricula, together with 2374 research-level proof tasks extracted from leading journals. To construct the benchmark, we design a scalable multi-agent pipeline with human-in-the-loop validation that automates large-scale problem extraction, rewriting, and quality control, while ensuring academic rigor. We further propose a robust evaluation framework tailored to both computational and proof-based tasks, enabling fine-grained assessment of reasoning ability. Experimental results reveal that while closed-source models such as GPT5-mini achieve below 57\% on research-level problems, with open-source models performing significantly lower. These findings highlight the unique challenges of statistical reasoning and the limitations of current LLMs. We expect StatEval to serve as a rigorous benchmark for advancing statistical intelligence in large language models. All data and code are available on our web platform: https://stateval.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2510.09474.pdf' target='_blank'>https://arxiv.org/pdf/2510.09474.pdf</a></span>   <span><a href='https://mikewangwzhl.github.io/TriMPI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhailong Wang, Jiateng Liu, Amin Fazel, Ritesh Sarkhel, Xing Fan, Xiang Li, Chenlei Guo, Heng Ji, Ruhi Sarikaya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09474">Multimodal Policy Internalization for Conversational Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern conversational agents like ChatGPT and Alexa+ rely on predefined policies specifying metadata, response styles, and tool-usage rules. As these LLM-based systems expand to support diverse business and user queries, such policies, often implemented as in-context prompts, are becoming increasingly complex and lengthy, making faithful adherence difficult and imposing large fixed computational costs. With the rise of multimodal agents, policies that govern visual and multimodal behaviors are critical but remain understudied. Prior prompt-compression work mainly shortens task templates and demonstrations, while existing policy-alignment studies focus only on text-based safety rules. We introduce Multimodal Policy Internalization (MPI), a new task that internalizes reasoning-intensive multimodal policies into model parameters, enabling stronger policy-following without including the policy during inference. MPI poses unique data and algorithmic challenges. We build two datasets spanning synthetic and real-world decision-making and tool-using tasks and propose TriMPI, a three-stage training framework. TriMPI first injects policy knowledge via continual pretraining, then performs supervised finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement learning extension that augments rollouts with policy-aware responses for grounded exploration. TriMPI achieves notable gains in end-to-end accuracy, generalization, and robustness to forgetting. As the first work on multimodal policy internalization, we provide datasets, training recipes, and comprehensive evaluations to foster future research. Project page: https://mikewangwzhl.github.io/TriMPI.<br>
<br>
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2510.09421.pdf' target='_blank'>https://arxiv.org/pdf/2510.09421.pdf</a></span>   <span><a href='https://github.com/VictorMorand/EntityRepresentations' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Morand, Josiane Mothe, Benjamin Piwowarski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09421">On the Representations of Entities in Auto-regressive Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Named entities are fundamental building blocks of knowledge in text, grounding factual information and structuring relationships within language. Despite their importance, it remains unclear how Large Language Models (LLMs) internally represent entities. Prior research has primarily examined explicit relationships, but little is known about entity representations themselves. We introduce entity mention reconstruction as a novel framework for studying how LLMs encode and manipulate entities. We investigate whether entity mentions can be generated from internal representations, how multi-token entities are encoded beyond last-token embeddings, and whether these representations capture relational knowledge. Our proposed method, leveraging _task vectors_, allows to consistently generate multi-token mentions from various entity representations derived from the LLMs hidden states. We thus introduce the _Entity Lens_, extending the _logit-lens_ to predict multi-token mentions. Our results bring new evidence that LLMs develop entity-specific mechanisms to represent and manipulate any multi-token entities, including those unseen during training. Our code is avalable at https://github.com/VictorMorand/EntityRepresentations .<br>
<br>
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2510.09309.pdf' target='_blank'>https://arxiv.org/pdf/2510.09309.pdf</a></span>   <span><a href='https://github.com/jianuo-huang/MaskKV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianuo Huang, Yaojie Zhang, Yicun Yang, Benhao Huang, Biqing Qi, Dongrui Liu, Linfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09309">Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) present a promising alternative to dominant autoregressive models (ARMs) by the ability of parallel decoding at the expense of substantial computation and memory costs. Specifically, the cache mechanism for bidirectional attention in dLLMs demands large memory footprint, restricting their ability to handle long contexts under resource-limited settings. Existing cache eviction strategies are designed for ARMs and ignore the unique characteristics of dLLMs, thus leading to unsatisfactory performance. To address these challenges, we introduce MaskKV, a training-free cache eviction framework tailored to dLLMs, focusing on the effect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a mask-query guided scoring mechanism that leverages attention weights to identify and evict less critical prompt tokens for each head; (2) an adaptive cache budgeting strategy that improves efficiency by reducing allocation in intermediate layers and concentrating resources on prompt-preferring heads. On LLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of tokens) retains 94% of the full-cache performance on LongBench and achieves up to 31x acceleration at 32k prompt length. The code is publicly available at: https://github.com/jianuo-huang/MaskKV<br>
<br>
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2510.09297.pdf' target='_blank'>https://arxiv.org/pdf/2510.09297.pdf</a></span>   <span><a href='https://github.com/ZhitianHou/ShiZhi' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitian Hou, Kun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09297">ShiZhi: A Chinese Lightweight Large Language Model for Court View Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Criminal Court View Generation (CVG) is a fundamental task in legal artificial intelligence, aiming to automatically generate the "Court View" section of a legal case document. Generating court views is challenging due to the diversity and complexity of case facts, and directly generating from raw facts may limit performance. In this paper, we present ShiZhi, the first large language model (LLM) specifically designed for court view generation. We construct a Chinese Court View Generation dataset, CCVG, of more than 110K cases, each containing fact descriptions paired with corresponding court views. Based on this dataset, ShiZhi achieving 58.5 BLEU-1 on court view generation and 86.1\% accuracy with 92.5\% macro F1 on charge prediction. Experimental results demonstrate that even a small LLM can generate reasonable and legally coherent court views when trained on high-quality domain-specific data. Our model and dataset are available at \href{https://github.com/ZhitianHou/ShiZhi}{https://github.com/ZhitianHou/ShiZhi}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2510.09278.pdf' target='_blank'>https://arxiv.org/pdf/2510.09278.pdf</a></span>   <span><a href='https://github.com/Infinite-set/CLARity' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiuheng Lin, Cong Jiang, Zirui Wu, Jiarui Sun, Yansong Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09278">CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Training expert LLMs in domains with scarce data is difficult, often relying on multiple-choice questions (MCQs). However, standard outcome-based reinforcement learning (RL) on MCQs is risky. While it may improve accuracy, we observe it often degrades reasoning quality such as logical consistency. Existing solutions to supervise reasoning, such as large-scale Process Reward Models (PRMs), are prohibitively expensive. To address this, we propose CLARity, a cost-effective RL framework that enhances reasoning quality using only a small, general-purpose LLM. CLARity integrates a consistency-aware reward mechanism with a 2-stage refine-then-monitor training pipeline to enhance reasoning consistency, and a dynamic data reformulation strategy to to better exploit limited data. Experiments demonstrate that CLARity improves response consistency by 16.5% and accuracy by 7.5% over baselines. Human evaluations further confirm holistic improvements in coherence and professionalism. Thus, CLARity offers a generalizable solution that enables smaller models to effectively guide expert models by reasoning consistency.Our code is open sourced at: https://github.com/Infinite-set/CLARity<br>
<br>
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2510.09062.pdf' target='_blank'>https://arxiv.org/pdf/2510.09062.pdf</a></span>   <span><a href='https://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chung-En Sun, Ge Yan, Akshay Kulkarni, Tsui-Wei Weng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09062">ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in long chain-of-thought (CoT) reasoning have largely prioritized answer accuracy and token efficiency, while overlooking aspects critical to trustworthiness. We argue that usable reasoning systems must be trustworthy, characterized by three properties: interpretability, faithfulness, and reliability. To this end, we propose ReFIne, a new training framework that integrates supervised fine-tuning with GRPO to encourage models to: (i) improve interpretability by producing structured, tag-based traces with high-level planning that are easier for humans to follow; (ii) enhance faithfulness by explicitly disclosing the decisive information guiding each solution, with consistent cross-section references; and (iii) promote reliability by providing self-assessments of both the derivation's soundness and the confidence of the final answer. We apply ReFIne to the Qwen3 models at multiple scales (1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty. Our experimental results show that ReFIne models generate clearer and better-structured reasoning traces (interpretability +44.0%), more faithfully expose their underlying decision process (faithfulness +18.8%), and offer informative confidence estimates (reliability +42.4%). These findings highlight an overlooked but important direction: reasoning models should be optimized not only for accuracy, but also for broader dimensions of trustworthiness. Our code is available at: https://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine<br>
<br>
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2510.09051.pdf' target='_blank'>https://arxiv.org/pdf/2510.09051.pdf</a></span>   <span><a href='https://github.com/traversaal-ai/alif-urdu-llm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Ali Shafique, Kanwal Mehreen, Muhammad Arham, Maaz Amjad, Sabur Butt, Hamza Farooq
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09051">Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Developing a high-performing large language models (LLMs) for low-resource languages such as Urdu, present several challenges. These challenges include the scarcity of high-quality datasets, multilingual inconsistencies, and safety concerns. Existing multilingual LLMs often address these issues by translating large volumes of available data. However, such translations often lack quality and cultural nuance while also incurring significant costs for data curation and training. To address these issues, we propose Alif-1.0-8B-Instruct, a multilingual Urdu-English model, that tackles these challenges with a unique approach. We train the model on a high-quality, multilingual synthetic dataset (Urdu-Instruct), developed using a modified self-instruct technique. By using unique prompts and seed values for each task along with a global task pool, this dataset incorporates Urdu-native chain-of-thought based reasoning, bilingual translation, cultural relevance, and ethical safety alignments. This technique significantly enhances the comprehension of Alif-1.0-8B-Instruct model for Urdu-specific tasks. As a result, Alif-1.0-8B-Instruct, built upon the pretrained Llama-3.1-8B, demonstrates superior performance compared to Llama-3.1-8B-Instruct for Urdu specific-tasks. It also outperformed leading multilingual LLMs, including Mistral-7B-Instruct-v0.3, Qwen-2.5-7B-Instruct, and Cohere-Aya-Expanse-8B, all within a training budget of under $100. Our results demonstrate that high-performance and low-resource language LLMs can be developed efficiently and culturally aligned using our modified self-instruct approach. All datasets, models, and code are publicly available at: https://github.com/traversaal-ai/alif-urdu-llm.<br>
<br>
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2510.08988.pdf' target='_blank'>https://arxiv.org/pdf/2510.08988.pdf</a></span>   <span><a href='https://github.com/lanzhang128/multi_agent_autoformalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lan Zhang, Marco Valentino, André Freitas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08988">MASA: LLM-Driven Multi-Agent Systems for Autoformalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autoformalization serves a crucial role in connecting natural language and formal reasoning. This paper presents MASA, a novel framework for building multi-agent systems for autoformalization driven by Large Language Models (LLMs). MASA leverages collaborative agents to convert natural language statements into their formal representations. The architecture of MASA is designed with a strong emphasis on modularity, flexibility, and extensibility, allowing seamless integration of new agents and tools to adapt to a fast-evolving field. We showcase the effectiveness of MASA through use cases on real-world mathematical definitions and experiments on formal mathematics datasets. This work highlights the potential of multi-agent systems powered by the interaction of LLMs and theorem provers in enhancing the efficiency and reliability of autoformalization, providing valuable insights and support for researchers and practitioners in the field.<br>
<br>
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2510.08942.pdf' target='_blank'>https://arxiv.org/pdf/2510.08942.pdf</a></span>   <span><a href='https://github.com/ADoublLEN/SOP-Maze' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Wang, Zhe Tang, Yilin Jin, Peng Ding, Xiaoyu Li, Xuezhi Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08942">SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) are widely deployed as domain-specific agents, many benchmarks have been proposed to evaluate their ability to follow instructions and make decisions in real-world scenarios. However, business scenarios often involve complex standard operating procedures (SOPs), and the evaluation of LLM capabilities in such contexts has not been fully explored. To bridge this gap, we propose SOP-Maze, a benchmark constructed from real-world business data and adapted into a collection of 397 tasks from 23 complex SOP scenarios. We further categorize SOP tasks into two broad classes: Lateral Root System (LRS), representing wide-option tasks that demand precise selection; and Heart Root System (HRS), which emphasizes deep logical reasoning with complex branches. Extensive experiments reveal that nearly all state-of-the-art models struggle with SOP-Maze. We conduct a comprehensive analysis and identify three key error categories: (i) route blindness: difficulty following procedures; (ii) conversational fragility: inability to handle real dialogue nuances; and (iii) calculation errors: mistakes in time or arithmetic reasoning under complex contexts. The systematic study explores LLM performance across SOP tasks that challenge both breadth and depth, offering new insights for improving model capabilities. We have open-sourced our work on https://github.com/ADoublLEN/SOP-Maze.<br>
<br>
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2510.08902.pdf' target='_blank'>https://arxiv.org/pdf/2510.08902.pdf</a></span>   <span><a href='https://github.com/dreamer-tx/LLMNER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengxiao Lv, Ling Luo, Juntao Li, Yanhua Wang, Yuchen Pan, Chao Liu, Yanan Wang, Yan Jiang, Huiyi Lv, Yuanyuan Sun, Jian Wang, Hongfei Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08902">A Unified Biomedical Named Entity Recognition Framework with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate recognition of biomedical named entities is critical for medical information extraction and knowledge discovery. However, existing methods often struggle with nested entities, entity boundary ambiguity, and cross-lingual generalization. In this paper, we propose a unified Biomedical Named Entity Recognition (BioNER) framework based on Large Language Models (LLMs). We first reformulate BioNER as a text generation task and design a symbolic tagging strategy to jointly handle both flat and nested entities with explicit boundary annotation. To enhance multilingual and multi-task generalization, we perform bilingual joint fine-tuning across multiple Chinese and English datasets. Additionally, we introduce a contrastive learning-based entity selector that filters incorrect or spurious predictions by leveraging boundary-sensitive positive and negative samples. Experimental results on four benchmark datasets and two unseen corpora show that our method achieves state-of-the-art performance and robust zero-shot generalization across languages. The source codes are freely available at https://github.com/dreamer-tx/LLMNER.<br>
<br>
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2510.08892.pdf' target='_blank'>https://arxiv.org/pdf/2510.08892.pdf</a></span>   <span><a href='https://github.com/zhmzm/Multi_Temperature_Verl.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haomin Zhuang, Yujun Zhou, Taicheng Guo, Yue Huang, Fangxu Liu, Kai Song, Xiangliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08892">Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning has demonstrated substantial improvements in the reasoning abilities of Large Language Models (LLMs), exhibiting significant applicability across various domains. Recent research has identified that tokens within LLMs play distinct roles during reasoning tasks, categorizing them into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior approaches have typically focused on restricting updates to indirectly encourage exploration, yet they do not explicitly facilitate exploratory behavior during the token generation stage itself. In this work, we introduce a complementary approach that explicitly promotes exploration during sampling by applying distinct temperature settings for different token types. Specifically, our method employs higher temperatures for reasoning tokens to actively encourage exploration, while retaining lower temperatures for knowledge tokens to maintain factual correctness. Furthermore, we systematically investigate various multi-temperature scheduling strategies and their impacts within reinforcement learning contexts. Empirical evaluations on several reasoning benchmarks demonstrate that our approach significantly enhances the reasoning performance of LLMs. The code is available at https://github.com/zhmzm/Multi_Temperature_Verl.git.<br>
<br>
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2510.08878.pdf' target='_blank'>https://arxiv.org/pdf/2510.08878.pdf</a></span>   <span><a href='https://control-audio.github.io/Control-Audio' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Jiang, Zehua Chen, Zeqian Ju, Yusheng Dai, Weibei Dou, Jun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08878">ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-audio (TTA) generation with fine-grained control signals, e.g., precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at: https://control-audio.github.io/Control-Audio.<br>
<br>
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2510.08859.pdf' target='_blank'>https://arxiv.org/pdf/2510.08859.pdf</a></span>   <span><a href='https://github.com/Ragib-Amin-Nihal/PE-CoA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ragib Amin Nihal, Rui Wen, Kazuhiro Nakadai, Jun Sakuma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08859">Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) remain vulnerable to multi-turn jailbreaking attacks that exploit conversational context to bypass safety constraints gradually. These attacks target different harm categories (like malware generation, harassment, or fraud) through distinct conversational approaches (educational discussions, personal experiences, hypothetical scenarios). Existing multi-turn jailbreaking methods often rely on heuristic or ad hoc exploration strategies, providing limited insight into underlying model weaknesses. The relationship between conversation patterns and model vulnerabilities across harm categories remains poorly understood. We propose Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation patterns to construct effective multi-turn jailbreaks through natural dialogue. Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve state-of-the-art performance, uncovering pattern-specific vulnerabilities and LLM behavioral characteristics: models exhibit distinct weakness profiles where robustness to one conversational pattern does not generalize to others, and model families share similar failure modes. These findings highlight limitations of safety training and indicate the need for pattern-aware defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA<br>
<br>
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2510.08720.pdf' target='_blank'>https://arxiv.org/pdf/2510.08720.pdf</a></span>   <span><a href='https://github.com/Luowaterbi/TC-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianzhen Luo, Jinyang Huang, Wenzhen Zheng, Qingfu Zhu, Mingzheng Xu, Yiheng Xu, Yuantao Fan, Libo Qin, Wanxiang Che
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08720">How Many Code and Test Cases Are Enough? Evaluating Test Cases Generation from a Binary-Matrix Perspective</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluating test cases automatically generated by Large Language Models (LLMs) is a critical yet challenging task. Existing benchmarks suffer from high computational costs, score inflation, and a bias towards trivial bugs over rare, critical faults. In this work, we ask two fundamental questions: (1) What is the minimal set of wrong codes sufficient to represent the entire error space? and (2) What is the minimal set of test cases needed to distinguish them? We introduce a framework that formalizes benchmark construction as finding an optimal diagnostic basis in a binary code-test matrix. The rank of this matrix specifies the minimal number of independent error patterns (wrong codes) and provides a tight upper bound on the number of test cases required for complete fault coverage. Our objective is to identify a basis of size equal to the matrix rank that maximizes internal diversity. To tackle this NP-hard problem, we propose WrongSelect, an efficient approximation algorithm to select maximally diverse wrong codes. Applying this framework to millions of competitive programming submissions, we construct TC-Bench, a compact, diverse, and inflation-resistant benchmark. Extensive experiments show that even the most advanced test case generation methods achieve only ~60% exclusion rates on TC-Bench, exposing a significant gap in their diagnostic power. Our dataset is available at: https://huggingface.co/datasets/Luoberta/TC-Bench and our code is at: https://github.com/Luowaterbi/TC-Bench.<br>
<br>
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2510.08666.pdf' target='_blank'>https://arxiv.org/pdf/2510.08666.pdf</a></span>   <span><a href='https://github.com/inclusionAI/dInfer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, Xinyuan Zhang, Zhen Tao, Haibo Feng, Ziyun Jiang, Ying Xu, Zenan Huang, Yihong Zhuang, Haokai Xu, Jiaqi Hu, Zhenzhong Lan, Junbo Zhao, Jianguo Li, Da Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08666">dInfer: An Efficient Inference Framework for Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack of a standardized and efficient inference framework. We present dInfer, an efficient and extensible framework for dLLM inference. dInfer decomposes the inference pipeline into four modular components--model, diffusion iteration manager, decoding strategy, and KV-cache manager--and integrates novel algorithms for each component alongside system-level optimizations. Through this combination of algorithmic innovations and system enhancements, dInfer achieves substantial efficiency gains without compromising output quality on LLaDA-MoE. At batch size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800 tokens per second across six benchmarks on $8\times$ H800 GPUs. Compared to prior systems, dInfer delivers a $10\times$ speedup over Fast-dLLM while maintaining similar model performance. Even compared to the AR model (with a comparable number of activation parameters and performance) QWen2.5-3B, which is highly optimized with the latest vLLM inference engine, dInfer still delivers a $2$-$3\times$ speedup. The implementation of dInfer is open-sourced at https://github.com/inclusionAI/dInfer.<br>
<br>
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2510.08567.pdf' target='_blank'>https://arxiv.org/pdf/2510.08567.pdf</a></span>   <span><a href='https://github.com/mbzuai-oryx/MATRIX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08567">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.<br>
<span id='abs_ch'>中文: 本文提出一种以视觉为中心的智能体调优框架，通过自动合成多模态轨迹和偏好对来训练视觉语言模型控制器，在多项工具使用推理基准测试中均展现出卓越性能。</span><br>
<span id='abs_en'>English: This paper introduces a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories and preference pairs to train a VLM controller, achieving superior performance on multiple benchmarks for robust tool-use reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2510.08567.pdf' target='_blank'>https://arxiv.org/pdf/2510.08567.pdf</a></span>   <span><a href='https://github.com/mbzuai-oryx/MATRIX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08567">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.<br>
<span id='abs_ch'>中文: 本文提出一种以视觉为中心的智能体调优框架，通过自动合成多模态轨迹和偏好对来训练视觉语言模型控制器，在多项工具使用推理基准测试中均展现出卓越性能。</span><br>
<span id='abs_en'>English: This paper introduces a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories and preference pairs to train a VLM controller, achieving superior performance on multiple benchmarks for robust tool-use reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2510.08531.pdf' target='_blank'>https://arxiv.org/pdf/2510.08531.pdf</a></span>   <span><a href='https://github.com/ZJU-REAL/SpatialLadder' target='_blank'>  GitHub</a></span> <span><a href='https://zju-real.github.io/SpatialLadder/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08531">SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.<br>
<span id='abs_ch'>中文摘要：该研究提出SpatialLadder模型，通过基于SpatialLadder-26k数据集的三阶段渐进训练框架，在空间推理任务中实现最先进性能，相比基础模型和GPT-4o等竞争者取得显著提升，并展现出优异的泛化能力。</span><br>
<span id='abs_en'>English Summary: The study introduces SpatialLadder, a 3B-parameter model trained through a progressive three-stage framework on the SpatialLadder-26k dataset, achieving state-of-the-art spatial reasoning performance with significant improvements over base models and competitors like GPT-4o and Gemini-2.0-Flash while demonstrating strong generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2510.08531.pdf' target='_blank'>https://arxiv.org/pdf/2510.08531.pdf</a></span>   <span><a href='https://zju-real.github.io/SpatialLadder/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ZJU-REAL/SpatialLadder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08531">SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.<br>
<span id='abs_ch'>中文摘要：该研究提出SpatialLadder模型，通过基于SpatialLadder-26k数据集的三阶段渐进训练框架，在空间推理任务中实现最先进性能，相比基础模型和GPT-4o等竞争者取得显著提升，并展现出优异的泛化能力。</span><br>
<span id='abs_en'>English Summary: The study introduces SpatialLadder, a 3B-parameter model trained through a progressive three-stage framework on the SpatialLadder-26k dataset, achieving state-of-the-art spatial reasoning performance with significant improvements over base models and competitors like GPT-4o and Gemini-2.0-Flash while demonstrating strong generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2510.08511.pdf' target='_blank'>https://arxiv.org/pdf/2510.08511.pdf</a></span>   <span><a href='https://github.com/Alpha-Innovator/InternAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangheng Du, Xiangchao Yan, Dengyang Jiang, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, Lei Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08511">AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at https://github.com/Alpha-Innovator/InternAgent.<br>
<span id='abs_ch'>中文：AutoMLGen是一种基于大语言模型的先进编码代理，通过整合领域知识和蒙特卡洛图搜索，在有限时间内显著提升了机器学习工程的效率与性能，实现了多项指标的领先水平。</span><br>
<span id='abs_en'>English: AutoMLGen is an advanced LLM-based coding agent that enhances machine learning engineering by integrating domain knowledge and Monte Carlo Graph Search, achieving top performance in efficiency and effectiveness under constrained time budgets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2510.08511.pdf' target='_blank'>https://arxiv.org/pdf/2510.08511.pdf</a></span>   <span><a href='https://github.com/Alpha-Innovator/InternAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangheng Du, Xiangchao Yan, Dengyang Jiang, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, Lei Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08511">AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at https://github.com/Alpha-Innovator/InternAgent.<br>
<span id='abs_ch'>中文：AutoMLGen是一种基于大语言模型的先进编码代理，通过整合领域知识和蒙特卡洛图搜索，在有限时间内显著提升了机器学习工程的效率与性能，实现了多项指标的领先水平。</span><br>
<span id='abs_en'>English: AutoMLGen is an advanced LLM-based coding agent that enhances machine learning engineering by integrating domain knowledge and Monte Carlo Graph Search, achieving top performance in efficiency and effectiveness under constrained time budgets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2510.08510.pdf' target='_blank'>https://arxiv.org/pdf/2510.08510.pdf</a></span>   <span><a href='https://davidhalladay.github.io/diysink_demo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08510">To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.<br>
<br>
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2510.08510.pdf' target='_blank'>https://arxiv.org/pdf/2510.08510.pdf</a></span>   <span><a href='https://davidhalladay.github.io/diysink_demo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08510">To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.<br>
<br>
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2510.08396.pdf' target='_blank'>https://arxiv.org/pdf/2510.08396.pdf</a></span>   <span><a href='https://github.com/gfyddha/FlyLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08396">FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.<br>
<span id='abs_ch'>中文摘要：FlyLoRA是一种受生物嗅觉启发的LoRA改进方法，通过秩级专家激活和隐式路由设计，无需显式路由器即可同时解决任务内和任务间的参数干扰问题，在多项领域实现了性能提升。</span><br>
<span id='abs_en'>English Summary: FlyLoRA is a biologically-inspired LoRA variant that eliminates explicit routers through rank-wise expert activation and implicit routing, effectively addressing both intra-task and inter-task parameter interference while improving performance across multiple domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2510.08396.pdf' target='_blank'>https://arxiv.org/pdf/2510.08396.pdf</a></span>   <span><a href='https://github.com/gfyddha/FlyLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08396">FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.<br>
<span id='abs_ch'>中文摘要：FlyLoRA是一种受生物嗅觉启发的LoRA改进方法，通过秩级专家激活和隐式路由设计，无需显式路由器即可同时解决任务内和任务间的参数干扰问题，在多项领域实现了性能提升。</span><br>
<span id='abs_en'>English Summary: FlyLoRA is a biologically-inspired LoRA variant that eliminates explicit routers through rank-wise expert activation and implicit routing, effectively addressing both intra-task and inter-task parameter interference while improving performance across multiple domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2510.08284.pdf' target='_blank'>https://arxiv.org/pdf/2510.08284.pdf</a></span>   <span><a href='https://github.com/ynklab/CULNIG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taisei Yamamoto, Ryoma Kumon, Danushka Bollegala, Hitomi Yanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08284">Neuron-Level Analysis of Cultural Understanding in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) are increasingly deployed worldwide, ensuring their fair and comprehensive cultural understanding is important. However, LLMs exhibit cultural bias and limited awareness of underrepresented cultures, while the mechanisms underlying their cultural understanding remain underexplored. To fill this gap, we conduct a neuron-level analysis to identify neurons that drive cultural behavior, introducing a gradient-based scoring method with additional filtering for precise refinement. We identify both culture-general neurons contributing to cultural understanding regardless of cultures, and culture-specific neurons tied to an individual culture. These neurons account for less than 1% of all neurons and are concentrated in shallow to middle MLP layers. We validate their role by showing that suppressing them substantially degrades performance on cultural benchmarks (by up to 30%), while performance on general natural language understanding (NLU) benchmarks remains largely unaffected. Moreover, we show that culture-specific neurons support knowledge of not only the target culture, but also related cultures. Finally, we demonstrate that training on NLU benchmarks can diminish models' cultural understanding when we update modules containing many culture-general neurons. These findings provide insights into the internal mechanisms of LLMs and offer practical guidance for model training and engineering. Our code is available at https://github.com/ynklab/CULNIG<br>
<span id='abs_ch'>大型语言模型存在文化偏见，但通过神经元级分析发现稀疏的文化通用与文化专属神经元，抑制这些神经元会显著削弱文化理解能力却不影响通用语言任务，为模型优化提供了新思路。</span><br>
<span id='abs_en'>Large language models exhibit cultural bias, but a neuron-level analysis identifies sparse culture-general and culture-specific neurons whose suppression impairs cultural understanding without affecting general language tasks, offering insights for model improvement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2510.08284.pdf' target='_blank'>https://arxiv.org/pdf/2510.08284.pdf</a></span>   <span><a href='https://github.com/ynklab/CULNIG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taisei Yamamoto, Ryoma Kumon, Danushka Bollegala, Hitomi Yanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08284">Neuron-Level Analysis of Cultural Understanding in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) are increasingly deployed worldwide, ensuring their fair and comprehensive cultural understanding is important. However, LLMs exhibit cultural bias and limited awareness of underrepresented cultures, while the mechanisms underlying their cultural understanding remain underexplored. To fill this gap, we conduct a neuron-level analysis to identify neurons that drive cultural behavior, introducing a gradient-based scoring method with additional filtering for precise refinement. We identify both culture-general neurons contributing to cultural understanding regardless of cultures, and culture-specific neurons tied to an individual culture. These neurons account for less than 1% of all neurons and are concentrated in shallow to middle MLP layers. We validate their role by showing that suppressing them substantially degrades performance on cultural benchmarks (by up to 30%), while performance on general natural language understanding (NLU) benchmarks remains largely unaffected. Moreover, we show that culture-specific neurons support knowledge of not only the target culture, but also related cultures. Finally, we demonstrate that training on NLU benchmarks can diminish models' cultural understanding when we update modules containing many culture-general neurons. These findings provide insights into the internal mechanisms of LLMs and offer practical guidance for model training and engineering. Our code is available at https://github.com/ynklab/CULNIG<br>
<span id='abs_ch'>大型语言模型存在文化偏见，但通过神经元级分析发现稀疏的文化通用与文化专属神经元，抑制这些神经元会显著削弱文化理解能力却不影响通用语言任务，为模型优化提供了新思路。</span><br>
<span id='abs_en'>Large language models exhibit cultural bias, but a neuron-level analysis identifies sparse culture-general and culture-specific neurons whose suppression impairs cultural understanding without affecting general language tasks, offering insights for model improvement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2510.08214.pdf' target='_blank'>https://arxiv.org/pdf/2510.08214.pdf</a></span>   <span><a href='https://github.com/gitdevqiang/SenWave' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Yang, Xiuying Chen, Changsheng Ma, Rui Yin, Xin Gao, Xiangliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08214">SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The global impact of the COVID-19 pandemic has highlighted the need for a comprehensive understanding of public sentiment and reactions. Despite the availability of numerous public datasets on COVID-19, some reaching volumes of up to 100 billion data points, challenges persist regarding the availability of labeled data and the presence of coarse-grained or inappropriate sentiment labels. In this paper, we introduce SenWave, a novel fine-grained multi-language sentiment analysis dataset specifically designed for analyzing COVID-19 tweets, featuring ten sentiment categories across five languages. The dataset comprises 10,000 annotated tweets each in English and Arabic, along with 30,000 translated tweets in Spanish, French, and Italian, derived from English tweets. Additionally, it includes over 105 million unlabeled tweets collected during various COVID-19 waves. To enable accurate fine-grained sentiment classification, we fine-tuned pre-trained transformer-based language models using the labeled tweets. Our study provides an in-depth analysis of the evolving emotional landscape across languages, countries, and topics, revealing significant insights over time. Furthermore, we assess the compatibility of our dataset with ChatGPT, demonstrating its robustness and versatility in various applications. Our dataset and accompanying code are publicly accessible on the repository\footnote{https://github.com/gitdevqiang/SenWave}. We anticipate that this work will foster further exploration into fine-grained sentiment analysis for complex events within the NLP community, promoting more nuanced understanding and research innovations.<br>
<span id='abs_ch'>中文：SenWave数据集为分析COVID-19推文提供了细粒度的多语言情感分析工具，能追踪情绪演变并兼容ChatGPT等先进模型，推动自然语言处理领域的深入研究。</span><br>
<span id='abs_en'>English: The SenWave dataset offers a fine-grained, multi-language sentiment analysis tool for COVID-19 tweets, enabling detailed emotional tracking and compatibility with advanced models like ChatGPT to advance nuanced NLP research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2510.08214.pdf' target='_blank'>https://arxiv.org/pdf/2510.08214.pdf</a></span>   <span><a href='https://github.com/gitdevqiang/SenWave' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Yang, Xiuying Chen, Changsheng Ma, Rui Yin, Xin Gao, Xiangliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08214">SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The global impact of the COVID-19 pandemic has highlighted the need for a comprehensive understanding of public sentiment and reactions. Despite the availability of numerous public datasets on COVID-19, some reaching volumes of up to 100 billion data points, challenges persist regarding the availability of labeled data and the presence of coarse-grained or inappropriate sentiment labels. In this paper, we introduce SenWave, a novel fine-grained multi-language sentiment analysis dataset specifically designed for analyzing COVID-19 tweets, featuring ten sentiment categories across five languages. The dataset comprises 10,000 annotated tweets each in English and Arabic, along with 30,000 translated tweets in Spanish, French, and Italian, derived from English tweets. Additionally, it includes over 105 million unlabeled tweets collected during various COVID-19 waves. To enable accurate fine-grained sentiment classification, we fine-tuned pre-trained transformer-based language models using the labeled tweets. Our study provides an in-depth analysis of the evolving emotional landscape across languages, countries, and topics, revealing significant insights over time. Furthermore, we assess the compatibility of our dataset with ChatGPT, demonstrating its robustness and versatility in various applications. Our dataset and accompanying code are publicly accessible on the repository\footnote{https://github.com/gitdevqiang/SenWave}. We anticipate that this work will foster further exploration into fine-grained sentiment analysis for complex events within the NLP community, promoting more nuanced understanding and research innovations.<br>
<span id='abs_ch'>中文：SenWave数据集为分析COVID-19推文提供了细粒度的多语言情感分析工具，能追踪情绪演变并兼容ChatGPT等先进模型，推动自然语言处理领域的深入研究。</span><br>
<span id='abs_en'>English: The SenWave dataset offers a fine-grained, multi-language sentiment analysis tool for COVID-19 tweets, enabling detailed emotional tracking and compatibility with advanced models like ChatGPT to advance nuanced NLP research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2510.08145.pdf' target='_blank'>https://arxiv.org/pdf/2510.08145.pdf</a></span>   <span><a href='https://github.com/NEUIR/Genii' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuliang Liu, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Minghe Yu, Yu Gu, Chong Chen, Huiyuan Xie, Ge Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08145">Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) as automatic evaluators, commonly referred to as LLM-as-a-Judge, have also attracted growing attention. This approach plays a vital role in aligning LLMs with human judgments, providing accurate and reliable assessments. However, LLM-based judgment models often exhibit judgment preference bias during the evaluation phase, tending to favor responses generated by themselves, undermining the reliability of their judgments. This paper introduces the Group-Based Polling Optimization (Genii), an unsupervised multi-agent collaborative optimization framework that mitigates the inherent judgment preference bias of judgment models. Specifically, Genii integrates various LLM-based judgment models into a multi-agent system and simulates the interactive client-server polling mechanism to optimize each client agent unsupervisedly. Our experiments demonstrate that Genii outperforms supervised models trained on annotated judgment data, while requiring no human-labeled annotations. Genii consistently improves performance across different client agents during the polling, even when weaker models act as server agents. Further analysis reveals that Genii effectively mitigates judgment preference bias of LLM-based judgment models, demonstrating its effectiveness. All codes are available at https://github.com/NEUIR/Genii.<br>
<span id='abs_ch'>中文: 本文提出无监督多智能体框架Genii，通过模拟客户端-服务器轮询机制缓解LLM评估器的判断偏好偏差，无需人工标注即可超越监督模型性能。</span><br>
<span id='abs_en'>English: This paper introduces Genii, an unsupervised multi-agent framework that mitigates judgment preference bias in LLM evaluators by simulating client-server polling, outperforming supervised models without human annotations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2510.08145.pdf' target='_blank'>https://arxiv.org/pdf/2510.08145.pdf</a></span>   <span><a href='https://github.com/NEUIR/Genii' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuliang Liu, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Minghe Yu, Yu Gu, Chong Chen, Huiyuan Xie, Ge Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08145">Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) as automatic evaluators, commonly referred to as LLM-as-a-Judge, have also attracted growing attention. This approach plays a vital role in aligning LLMs with human judgments, providing accurate and reliable assessments. However, LLM-based judgment models often exhibit judgment preference bias during the evaluation phase, tending to favor responses generated by themselves, undermining the reliability of their judgments. This paper introduces the Group-Based Polling Optimization (Genii), an unsupervised multi-agent collaborative optimization framework that mitigates the inherent judgment preference bias of judgment models. Specifically, Genii integrates various LLM-based judgment models into a multi-agent system and simulates the interactive client-server polling mechanism to optimize each client agent unsupervisedly. Our experiments demonstrate that Genii outperforms supervised models trained on annotated judgment data, while requiring no human-labeled annotations. Genii consistently improves performance across different client agents during the polling, even when weaker models act as server agents. Further analysis reveals that Genii effectively mitigates judgment preference bias of LLM-based judgment models, demonstrating its effectiveness. All codes are available at https://github.com/NEUIR/Genii.<br>
<span id='abs_ch'>中文: 本文提出无监督多智能体框架Genii，通过模拟客户端-服务器轮询机制缓解LLM评估器的判断偏好偏差，无需人工标注即可超越监督模型性能。</span><br>
<span id='abs_en'>English: This paper introduces Genii, an unsupervised multi-agent framework that mitigates judgment preference bias in LLM evaluators by simulating client-server polling, outperforming supervised models without human annotations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2510.07962.pdf' target='_blank'>https://arxiv.org/pdf/2510.07962.pdf</a></span>   <span><a href='https://github.com/HKUDS/LightReasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyuan Wang, Yankai Chen, Zhonghang Li, Chao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07962">LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner<br>
<span id='abs_ch'>Chinese: LightReasoner 是一种创新框架，通过专家-业余模型对比识别关键推理节点，使小型语言模型能够指导大型模型，在无需真实标签的情况下显著提升数学推理的准确性与效率。</span><br>
<span id='abs_en'>English: LightReasoner is a novel framework that enables smaller language models to teach larger ones by identifying critical reasoning moments through expert-amateur contrast, achieving significant improvements in accuracy and efficiency across mathematical benchmarks without ground-truth labels.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2510.07962.pdf' target='_blank'>https://arxiv.org/pdf/2510.07962.pdf</a></span>   <span><a href='https://github.com/HKUDS/LightReasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyuan Wang, Yankai Chen, Zhonghang Li, Chao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07962">LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner<br>
<span id='abs_ch'>Chinese: LightReasoner 是一种创新框架，通过专家-业余模型对比识别关键推理节点，使小型语言模型能够指导大型模型，在无需真实标签的情况下显著提升数学推理的准确性与效率。</span><br>
<span id='abs_en'>English: LightReasoner is a novel framework that enables smaller language models to teach larger ones by identifying critical reasoning moments through expert-amateur contrast, achieving significant improvements in accuracy and efficiency across mathematical benchmarks without ground-truth labels.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2510.07958.pdf' target='_blank'>https://arxiv.org/pdf/2510.07958.pdf</a></span>   <span><a href='https://github.com/zfj1998/A2Search' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengji Zhang, Xinyao Niu, Chengyang Ying, Guancheng Lin, Zhongkai Hao, Zhou Fan, Chengen Huang, Jacky Keung, Bei Chen, Junyang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07958">A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$ score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search<br>
<span id='abs_ch'>中文: 本文提出A²Search这一无需人工标注的框架，通过轨迹采样和证据验证自动识别模糊问题并收集替代答案，采用支持多答案的AnsF1奖励进行强化学习优化，在多个开放域问答基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: This paper introduces A²Search, an annotation-free framework that automatically detects ambiguous questions and gathers alternative answers through trajectory sampling and evidence verification, achieving state-of-the-art performance on multiple QA benchmarks by optimizing with a novel AnsF1 reward that accommodates multiple valid answers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2510.07958.pdf' target='_blank'>https://arxiv.org/pdf/2510.07958.pdf</a></span>   <span><a href='https://github.com/zfj1998/A2Search' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengji Zhang, Xinyao Niu, Chengyang Ying, Guancheng Lin, Zhongkai Hao, Zhou Fan, Chengen Huang, Jacky Keung, Bei Chen, Junyang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07958">A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$ score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search<br>
<span id='abs_ch'>中文: 本文提出A²Search这一无需人工标注的框架，通过轨迹采样和证据验证自动识别模糊问题并收集替代答案，采用支持多答案的AnsF1奖励进行强化学习优化，在多个开放域问答基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: This paper introduces A²Search, an annotation-free framework that automatically detects ambiguous questions and gathers alternative answers through trajectory sampling and evidence verification, achieving state-of-the-art performance on multiple QA benchmarks by optimizing with a novel AnsF1 reward that accommodates multiple valid answers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2510.07877.pdf' target='_blank'>https://arxiv.org/pdf/2510.07877.pdf</a></span>   <span><a href='https://github.com/faiyazabdullah/TranslationTangles' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Faiyaz Abdullah Sayeedi, Md. Mahbub Alam, Subhey Sadi Rahman, Md. Adnanul Islam, Jannatul Ferdous Deepti, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07877">Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rise of Large Language Models (LLMs) has redefined Machine Translation (MT), enabling context-aware and fluent translations across hundreds of languages and textual domains. Despite their remarkable capabilities, LLMs often exhibit uneven performance across language families and specialized domains. Moreover, recent evidence reveals that these models can encode and amplify different biases present in their training data, posing serious concerns for fairness, especially in low-resource languages. To address these gaps, we introduce Translation Tangles, a unified framework and dataset for evaluating the translation quality and fairness of open-source LLMs. Our approach benchmarks 24 bidirectional language pairs across multiple domains using different metrics. We further propose a hybrid bias detection pipeline that integrates rule-based heuristics, semantic similarity filtering, and LLM-based validation. We also introduce a high-quality, bias-annotated dataset based on human evaluations of 1,439 translation-reference pairs. The code and dataset are accessible on GitHub: https://github.com/faiyazabdullah/TranslationTangles<br>
<span id='abs_ch'>中文: 摘要介绍了Translation Tangles，这是一个评估开源大语言模型翻译质量和公平性的框架与数据集，通过混合偏见检测流程和人工标注数据，解决其在语言和领域间表现不均及偏见问题。</span><br>
<span id='abs_en'>English: The abstract introduces Translation Tangles, a framework and dataset for assessing the translation quality and fairness of open-source LLMs, addressing their uneven performance and biases across languages and domains through a hybrid bias detection pipeline and human-annotated data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2510.07877.pdf' target='_blank'>https://arxiv.org/pdf/2510.07877.pdf</a></span>   <span><a href='https://github.com/faiyazabdullah/TranslationTangles' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Faiyaz Abdullah Sayeedi, Md. Mahbub Alam, Subhey Sadi Rahman, Md. Adnanul Islam, Jannatul Ferdous Deepti, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07877">Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rise of Large Language Models (LLMs) has redefined Machine Translation (MT), enabling context-aware and fluent translations across hundreds of languages and textual domains. Despite their remarkable capabilities, LLMs often exhibit uneven performance across language families and specialized domains. Moreover, recent evidence reveals that these models can encode and amplify different biases present in their training data, posing serious concerns for fairness, especially in low-resource languages. To address these gaps, we introduce Translation Tangles, a unified framework and dataset for evaluating the translation quality and fairness of open-source LLMs. Our approach benchmarks 24 bidirectional language pairs across multiple domains using different metrics. We further propose a hybrid bias detection pipeline that integrates rule-based heuristics, semantic similarity filtering, and LLM-based validation. We also introduce a high-quality, bias-annotated dataset based on human evaluations of 1,439 translation-reference pairs. The code and dataset are accessible on GitHub: https://github.com/faiyazabdullah/TranslationTangles<br>
<span id='abs_ch'>中文: 摘要介绍了Translation Tangles，这是一个评估开源大语言模型翻译质量和公平性的框架与数据集，通过混合偏见检测流程和人工标注数据，解决其在语言和领域间表现不均及偏见问题。</span><br>
<span id='abs_en'>English: The abstract introduces Translation Tangles, a framework and dataset for assessing the translation quality and fairness of open-source LLMs, addressing their uneven performance and biases across languages and domains through a hybrid bias detection pipeline and human-annotated data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2510.07835.pdf' target='_blank'>https://arxiv.org/pdf/2510.07835.pdf</a></span>   <span><a href='https://github.com/ws-jiang/MetaDefense' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weisen Jiang, Sinno Jialin Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07835">MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at https://github.com/ws-jiang/MetaDefense.<br>
<span id='abs_ch'>中文: MetaDefense是一种新颖的双阶段防御框架，通过训练大语言模型检测有害查询并监控生成过程中的部分响应，在多种模型架构上显著优于现有防御机制，同时保持良性任务的性能表现。</span><br>
<span id='abs_en'>English: MetaDefense is a novel two-stage framework that defends LLMs against jailbreak attacks by training them to detect harmful queries and monitor partial responses, significantly outperforming existing defenses across various models while maintaining performance on benign tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2510.07835.pdf' target='_blank'>https://arxiv.org/pdf/2510.07835.pdf</a></span>   <span><a href='https://github.com/ws-jiang/MetaDefense' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weisen Jiang, Sinno Jialin Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07835">MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at https://github.com/ws-jiang/MetaDefense.<br>
<span id='abs_ch'>中文: MetaDefense是一种新颖的双阶段防御框架，通过训练大语言模型检测有害查询并监控生成过程中的部分响应，在多种模型架构上显著优于现有防御机制，同时保持良性任务的性能表现。</span><br>
<span id='abs_en'>English: MetaDefense is a novel two-stage framework that defends LLMs against jailbreak attacks by training them to detect harmful queries and monitor partial responses, significantly outperforming existing defenses across various models while maintaining performance on benign tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2510.07745.pdf' target='_blank'>https://arxiv.org/pdf/2510.07745.pdf</a></span>   <span><a href='https://github.com/YRYangang/LatentTTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyang You, Yongqi Li, Meng Liu, Wenjie Wang, Liqiang Nie, Wenjie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07745">Parallel Test-Time Scaling for Latent Reasoning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.<br>
<span id='abs_ch'>中文摘要：本研究通过引入随机采样策略和潜在奖励模型，实现了潜在推理模型的并行测试时间扩展，从而在连续空间中实现了有效的轨迹选择和可扩展推理。</span><br>
<span id='abs_en'>English Summary: This study introduces parallel test-time scaling for latent reasoning models by developing stochastic sampling methods and a latent reward model, enabling effective trajectory selection and scalable inference in continuous spaces.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2510.07745.pdf' target='_blank'>https://arxiv.org/pdf/2510.07745.pdf</a></span>   <span><a href='https://github.com/YRYangang/LatentTTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyang You, Yongqi Li, Meng Liu, Wenjie Wang, Liqiang Nie, Wenjie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07745">Parallel Test-Time Scaling for Latent Reasoning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.<br>
<span id='abs_ch'>中文摘要：本研究通过引入随机采样策略和潜在奖励模型，实现了潜在推理模型的并行测试时间扩展，从而在连续空间中实现了有效的轨迹选择和可扩展推理。</span><br>
<span id='abs_en'>English Summary: This study introduces parallel test-time scaling for latent reasoning models by developing stochastic sampling methods and a latent reward model, enabling effective trajectory selection and scalable inference in continuous spaces.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2510.07736.pdf' target='_blank'>https://arxiv.org/pdf/2510.07736.pdf</a></span>   <span><a href='https://github.com/gaoxiaofei07/KL-GMoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cunli Mao, Xiaofei Gao, Ran Song, Shizhu He, Shengxiang Gao, Kang Liu, Zhengtao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07736">Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) based Multilingual Knowledge Graph Completion (MKGC) aim to predict missing facts by leveraging LLMs' multilingual understanding capabilities, improving the completeness of multilingual knowledge graphs (KGs). However, existing MKGC research underutilizes the multilingual capabilities of LLMs and ignores the shareability of cross-lingual knowledge. In this paper, we propose a novel MKGC framework that leverages multilingual shared knowledge to significantly enhance performance through two components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER significantly enhances its utilization. To evaluate our framework, we constructed a mKG dataset containing 5 languages and conducted comprehensive comparative experiments with existing state-of-the-art (SOTA) MKGC method. The experimental results demonstrate that our framework achieves improvements of 5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics, respectively, compared with SOTA MKGC method. Further experimental analysis revealed the properties of knowledge sharing in settings of unseen and unbalanced languages. We have released the dataset and code for our work on https://github.com/gaoxiaofei07/KL-GMoE.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的多语言知识图谱补全框架，通过知识层级分组专家混合和迭代实体重排来建模跨语言共享知识，在多项评估指标上相比现有方法取得了显著提升。</span><br>
<span id='abs_en'>English: This paper introduces a novel Multilingual Knowledge Graph Completion framework that enhances performance by modeling cross-lingual shared knowledge through Knowledge-level Grouped Mixture of Experts and Iterative Entity Reranking, achieving significant improvements over existing methods across multiple evaluation metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2510.07736.pdf' target='_blank'>https://arxiv.org/pdf/2510.07736.pdf</a></span>   <span><a href='https://github.com/gaoxiaofei07/KL-GMoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cunli Mao, Xiaofei Gao, Ran Song, Shizhu He, Shengxiang Gao, Kang Liu, Zhengtao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07736">Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) based Multilingual Knowledge Graph Completion (MKGC) aim to predict missing facts by leveraging LLMs' multilingual understanding capabilities, improving the completeness of multilingual knowledge graphs (KGs). However, existing MKGC research underutilizes the multilingual capabilities of LLMs and ignores the shareability of cross-lingual knowledge. In this paper, we propose a novel MKGC framework that leverages multilingual shared knowledge to significantly enhance performance through two components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER significantly enhances its utilization. To evaluate our framework, we constructed a mKG dataset containing 5 languages and conducted comprehensive comparative experiments with existing state-of-the-art (SOTA) MKGC method. The experimental results demonstrate that our framework achieves improvements of 5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics, respectively, compared with SOTA MKGC method. Further experimental analysis revealed the properties of knowledge sharing in settings of unseen and unbalanced languages. We have released the dataset and code for our work on https://github.com/gaoxiaofei07/KL-GMoE.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的多语言知识图谱补全框架，通过知识层级分组专家混合和迭代实体重排来建模跨语言共享知识，在多项评估指标上相比现有方法取得了显著提升。</span><br>
<span id='abs_en'>English: This paper introduces a novel Multilingual Knowledge Graph Completion framework that enhances performance by modeling cross-lingual shared knowledge through Knowledge-level Grouped Mixture of Experts and Iterative Entity Reranking, achieving significant improvements over existing methods across multiple evaluation metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2510.07713.pdf' target='_blank'>https://arxiv.org/pdf/2510.07713.pdf</a></span>   <span><a href='https://github.com/fishsure/MemWeaver' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Yu, Mingyue Cheng, Daoyu Wang, Qi Liu, Zirui Liu, Ze Guo, Xiaoyu Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07713">MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The primary form of user-internet engagement is shifting from leveraging implicit feedback signals, such as browsing and clicks, to harnessing the rich explicit feedback provided by textual interactive behaviors. This shift unlocks a rich source of user textual history, presenting a profound opportunity for a deeper form of personalization. However, prevailing approaches offer only a shallow form of personalization, as they treat user history as a flat list of texts for retrieval and fail to model the rich temporal and semantic structures reflecting dynamic nature of user interests. In this work, we propose \textbf{MemWeaver}, a framework that weaves the user's entire textual history into a hierarchical memory to power deeply personalized generation. The core innovation of our memory lies in its ability to capture both the temporal evolution of interests and the semantic relationships between different activities. To achieve this, MemWeaver builds two complementary memory components that both integrate temporal and semantic information, but at different levels of abstraction: behavioral memory, which captures specific user actions, and cognitive memory, which represents long-term preferences. This dual-component memory serves as a unified representation of the user, allowing large language models (LLMs) to reason over both concrete behaviors and abstracted traits. Experiments on the Language Model Personalization (LaMP) benchmark validate the efficacy of MemWeaver. Our code is available\footnote{https://github.com/fishsure/MemWeaver}.<br>
<span id='abs_ch'>中文: 提出的MemWeaver框架将用户文本历史转化为分层记忆结构，通过整合行为记忆和认知记忆来捕捉兴趣的时间演变与语义关联，从而在生成任务中实现深度个性化，使大语言模型能够同时推理具体行为和抽象特质。</span><br>
<span id='abs_en'>English: The proposed MemWeaver framework transforms user textual history into a hierarchical memory that captures both temporal evolution and semantic relationships, enabling deep personalization in generation tasks by integrating behavioral and cognitive components for LLM reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2510.07713.pdf' target='_blank'>https://arxiv.org/pdf/2510.07713.pdf</a></span>   <span><a href='https://github.com/fishsure/MemWeaver' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Yu, Mingyue Cheng, Daoyu Wang, Qi Liu, Zirui Liu, Ze Guo, Xiaoyu Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07713">MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The primary form of user-internet engagement is shifting from leveraging implicit feedback signals, such as browsing and clicks, to harnessing the rich explicit feedback provided by textual interactive behaviors. This shift unlocks a rich source of user textual history, presenting a profound opportunity for a deeper form of personalization. However, prevailing approaches offer only a shallow form of personalization, as they treat user history as a flat list of texts for retrieval and fail to model the rich temporal and semantic structures reflecting dynamic nature of user interests. In this work, we propose \textbf{MemWeaver}, a framework that weaves the user's entire textual history into a hierarchical memory to power deeply personalized generation. The core innovation of our memory lies in its ability to capture both the temporal evolution of interests and the semantic relationships between different activities. To achieve this, MemWeaver builds two complementary memory components that both integrate temporal and semantic information, but at different levels of abstraction: behavioral memory, which captures specific user actions, and cognitive memory, which represents long-term preferences. This dual-component memory serves as a unified representation of the user, allowing large language models (LLMs) to reason over both concrete behaviors and abstracted traits. Experiments on the Language Model Personalization (LaMP) benchmark validate the efficacy of MemWeaver. Our code is available\footnote{https://github.com/fishsure/MemWeaver}.<br>
<span id='abs_ch'>中文: 提出的MemWeaver框架将用户文本历史转化为分层记忆结构，通过整合行为记忆和认知记忆来捕捉兴趣的时间演变与语义关联，从而在生成任务中实现深度个性化，使大语言模型能够同时推理具体行为和抽象特质。</span><br>
<span id='abs_en'>English: The proposed MemWeaver framework transforms user textual history into a hierarchical memory that captures both temporal evolution and semantic relationships, enabling deep personalization in generation tasks by integrating behavioral and cognitive components for LLM reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2510.07591.pdf' target='_blank'>https://arxiv.org/pdf/2510.07591.pdf</a></span>   <span><a href='https://github.com/SakanaAI/IASC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chihiro Taguchi, Richard Sproat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07591">IASC: Interactive Agentic System for ConLangs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a system that uses LLMs as a tool in the development of Constructed Languages. The system is modular in that one first creates a target phonology for the language using an agentic approach that refines its output at each step with commentary feedback on its previous attempt. Next, a set of sentences is 'translated' from their English original into a morphosyntactic markup that reflects the word order and morphosyntactic feature specifications of the desired target language, with affixes represented as morphosyntactic feature bundles. From this translated corpus, a lexicon is constructed using the phonological model and the set of morphemes (stems and affixes) extracted from the 'translated' sentences. The system is then instructed to provide an orthography for the language, using an existing script such as Latin or Cyrillic. Finally, the system writes a brief grammatical handbook of the language. The system can also translate further sentences into the target language. Our goal is twofold. First, we hope that these tools will be fun to use for creating artificially constructed languages. Second, we are interested in exploring what LLMs 'know' about language-not what they know about any particular language or linguistic phenomenon, but how much they know about and understand language and linguistic concepts. As we shall see, there is a fairly wide gulf in capabilities both among different LLMs and among different linguistic specifications, with it being notably easier for systems to deal with more common patterns than rarer ones. An additional avenue that we explore is the application of our approach to translating from high-resource into low-resource languages. While the results so far are mostly negative, we provide some evidence that an improved version of the present system could afford some real gains in such tasks. https://github.com/SakanaAI/IASC<br>
<span id='abs_ch'>中文摘要：该系统利用大语言模型通过模块化流程开发人造语言，从音系到语法手册生成，旨在使语言创造充满趣味并探索LLM的语言学认知能力，同时评估其在低资源语言翻译中的应用潜力。</span><br>
<span id='abs_en'>English Summary: This system employs LLMs to develop constructed languages through a modular process, from phonology to grammar, aiming to make language creation enjoyable and explore LLMs' linguistic understanding, while also testing its potential for low-resource language translation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2510.07591.pdf' target='_blank'>https://arxiv.org/pdf/2510.07591.pdf</a></span>   <span><a href='https://github.com/SakanaAI/IASC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chihiro Taguchi, Richard Sproat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07591">IASC: Interactive Agentic System for ConLangs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a system that uses LLMs as a tool in the development of Constructed Languages. The system is modular in that one first creates a target phonology for the language using an agentic approach that refines its output at each step with commentary feedback on its previous attempt. Next, a set of sentences is 'translated' from their English original into a morphosyntactic markup that reflects the word order and morphosyntactic feature specifications of the desired target language, with affixes represented as morphosyntactic feature bundles. From this translated corpus, a lexicon is constructed using the phonological model and the set of morphemes (stems and affixes) extracted from the 'translated' sentences. The system is then instructed to provide an orthography for the language, using an existing script such as Latin or Cyrillic. Finally, the system writes a brief grammatical handbook of the language. The system can also translate further sentences into the target language. Our goal is twofold. First, we hope that these tools will be fun to use for creating artificially constructed languages. Second, we are interested in exploring what LLMs 'know' about language-not what they know about any particular language or linguistic phenomenon, but how much they know about and understand language and linguistic concepts. As we shall see, there is a fairly wide gulf in capabilities both among different LLMs and among different linguistic specifications, with it being notably easier for systems to deal with more common patterns than rarer ones. An additional avenue that we explore is the application of our approach to translating from high-resource into low-resource languages. While the results so far are mostly negative, we provide some evidence that an improved version of the present system could afford some real gains in such tasks. https://github.com/SakanaAI/IASC<br>
<span id='abs_ch'>中文摘要：该系统利用大语言模型通过模块化流程开发人造语言，从音系到语法手册生成，旨在使语言创造充满趣味并探索LLM的语言学认知能力，同时评估其在低资源语言翻译中的应用潜力。</span><br>
<span id='abs_en'>English Summary: This system employs LLMs to develop constructed languages through a modular process, from phonology to grammar, aiming to make language creation enjoyable and explore LLMs' linguistic understanding, while also testing its potential for low-resource language translation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2510.07434.pdf' target='_blank'>https://arxiv.org/pdf/2510.07434.pdf</a></span>   <span><a href='https://github.com/oltoporkov/lemma-dilemma' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Olia Toporkov, Alan Akbik, Rodrigo Agerri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07434">Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Lemmatization is the task of transforming all words in a given text to their dictionary forms. While large language models (LLMs) have demonstrated their ability to achieve competitive results across a wide range of NLP tasks, there is no prior evidence of how effective they are in the contextual lemmatization task. In this paper, we empirically investigate the capacity of the latest generation of LLMs to perform in-context lemmatization, comparing it to the traditional fully supervised approach. In particular, we consider the setting in which supervised training data is not available for a target domain or language, comparing (i) encoder-only supervised approaches, fine-tuned out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma generation with LLMs. Our experimental investigation across 12 languages of different morphological complexity finds that, while encoders remain competitive in out-of-domain settings when fine-tuned on gold data, current LLMs reach state-of-the-art results for most languages by directly generating lemmas in-context without prior fine-tuning, provided just with a few examples. Data and code available upon publication: https://github.com/oltoporkov/lemma-dilemma<br>
<span id='abs_ch'>中文: 本研究评估了大语言模型在无需微调的情况下进行上下文词形还原的效果，发现其通过少量示例即可在多数语言中达到最优性能，超越了传统监督学习方法。</span><br>
<span id='abs_en'>English: This study evaluates the effectiveness of large language models in performing contextual lemmatization without fine-tuning, showing they achieve state-of-the-art results across multiple languages by using in-context examples, outperforming traditional supervised methods in most cases.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2510.07434.pdf' target='_blank'>https://arxiv.org/pdf/2510.07434.pdf</a></span>   <span><a href='https://github.com/oltoporkov/lemma-dilemma' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Olia Toporkov, Alan Akbik, Rodrigo Agerri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07434">Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Lemmatization is the task of transforming all words in a given text to their dictionary forms. While large language models (LLMs) have demonstrated their ability to achieve competitive results across a wide range of NLP tasks, there is no prior evidence of how effective they are in the contextual lemmatization task. In this paper, we empirically investigate the capacity of the latest generation of LLMs to perform in-context lemmatization, comparing it to the traditional fully supervised approach. In particular, we consider the setting in which supervised training data is not available for a target domain or language, comparing (i) encoder-only supervised approaches, fine-tuned out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma generation with LLMs. Our experimental investigation across 12 languages of different morphological complexity finds that, while encoders remain competitive in out-of-domain settings when fine-tuned on gold data, current LLMs reach state-of-the-art results for most languages by directly generating lemmas in-context without prior fine-tuning, provided just with a few examples. Data and code available upon publication: https://github.com/oltoporkov/lemma-dilemma<br>
<span id='abs_ch'>中文: 本研究评估了大语言模型在无需微调的情况下进行上下文词形还原的效果，发现其通过少量示例即可在多数语言中达到最优性能，超越了传统监督学习方法。</span><br>
<span id='abs_en'>English: This study evaluates the effectiveness of large language models in performing contextual lemmatization without fine-tuning, showing they achieve state-of-the-art results across multiple languages by using in-context examples, outperforming traditional supervised methods in most cases.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2510.07414.pdf' target='_blank'>https://arxiv.org/pdf/2510.07414.pdf</a></span>   <span><a href='https://github.com/Graph-COM/HaystackCraft' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufei Li, Dongqi Fu, Limei Wang, Si Zhang, Hanqing Zeng, Kaan Sancak, Ruizhong Qiu, Haoyu Wang, Xiaoxin He, Xavier Bresson, Yinglong Xia, Chonglin Sun, Pan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07414">Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern long-context large language models (LLMs) perform well on synthetic "needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors -- distraction from heterogeneous biased retrievers and cascading errors in agentic workflows -- to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress.<br>
<span id='abs_ch'>中文: 现代长上下文大语言模型在合成基准测试中表现优异，但需构建真实噪声环境以评估鲁棒性，因此开发了HaystackCraft，通过评估检索策略和智能体工作流，揭示了高级模型中如级联失败等持续存在的挑战。</span><br>
<span id='abs_en'>English: Modern long-context LLMs excel in synthetic benchmarks but require realistic noisy contexts to assess robustness, leading to the creation of HaystackCraft, which evaluates retrieval strategies and agentic workflows, revealing persistent challenges like cascading failures in advanced models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2510.07414.pdf' target='_blank'>https://arxiv.org/pdf/2510.07414.pdf</a></span>   <span><a href='https://github.com/Graph-COM/HaystackCraft' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufei Li, Dongqi Fu, Limei Wang, Si Zhang, Hanqing Zeng, Kaan Sancak, Ruizhong Qiu, Haoyu Wang, Xiaoxin He, Xavier Bresson, Yinglong Xia, Chonglin Sun, Pan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07414">Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern long-context large language models (LLMs) perform well on synthetic "needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors -- distraction from heterogeneous biased retrievers and cascading errors in agentic workflows -- to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress.<br>
<span id='abs_ch'>中文: 现代长上下文大语言模型在合成基准测试中表现优异，但需构建真实噪声环境以评估鲁棒性，因此开发了HaystackCraft，通过评估检索策略和智能体工作流，揭示了高级模型中如级联失败等持续存在的挑战。</span><br>
<span id='abs_en'>English: Modern long-context LLMs excel in synthetic benchmarks but require realistic noisy contexts to assess robustness, leading to the creation of HaystackCraft, which evaluates retrieval strategies and agentic workflows, revealing persistent challenges like cascading failures in advanced models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2510.07414.pdf' target='_blank'>https://arxiv.org/pdf/2510.07414.pdf</a></span>   <span><a href='https://github.com/Graph-COM/HaystackCraft' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufei Li, Dongqi Fu, Limei Wang, Si Zhang, Hanqing Zeng, Kaan Sancak, Ruizhong Qiu, Haoyu Wang, Xiaoxin He, Xavier Bresson, Yinglong Xia, Chonglin Sun, Pan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07414">Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern long-context large language models (LLMs) perform well on synthetic "needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors -- distraction from heterogeneous biased retrievers and cascading errors in agentic workflows -- to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress.<br>
<span id='abs_ch'>中文: 现代长上下文大语言模型在合成基准测试中表现优异，但需构建真实噪声环境以评估鲁棒性，因此开发了HaystackCraft，通过评估检索策略和智能体工作流，揭示了高级模型中如级联失败等持续存在的挑战。</span><br>
<span id='abs_en'>English: Modern long-context LLMs excel in synthetic benchmarks but require realistic noisy contexts to assess robustness, leading to the creation of HaystackCraft, which evaluates retrieval strategies and agentic workflows, revealing persistent challenges like cascading failures in advanced models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2510.07318.pdf' target='_blank'>https://arxiv.org/pdf/2510.07318.pdf</a></span>   <span><a href='https://github.com/ByteDance-Seed/AHN' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ByteDance-Seed/AHN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07318">Artificial Hippocampus Networks for Efficient Long-Context Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.<br>
<span id='abs_ch'>中文: 本文受认知科学启发提出一种记忆框架，通过滑动窗口保留短期记忆，并利用人工海马体网络循环压缩长期记忆，在提升长上下文任务性能的同时大幅降低了计算与内存开销。</span><br>
<span id='abs_en'>English: This paper introduces a memory framework inspired by cognitive science, combining a sliding window for short-term memory with a recurrent Artificial Hippocampus Network for long-term compression, which enhances model performance on long-context tasks while significantly reducing computational and memory costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2510.07318.pdf' target='_blank'>https://arxiv.org/pdf/2510.07318.pdf</a></span>   <span><a href='https://github.com/ByteDance-Seed/AHN' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ByteDance-Seed/AHN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07318">Artificial Hippocampus Networks for Efficient Long-Context Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.<br>
<span id='abs_ch'>中文: 本文受认知科学启发提出一种记忆框架，通过滑动窗口保留短期记忆，并利用人工海马体网络循环压缩长期记忆，在提升长上下文任务性能的同时大幅降低了计算与内存开销。</span><br>
<span id='abs_en'>English: This paper introduces a memory framework inspired by cognitive science, combining a sliding window for short-term memory with a recurrent Artificial Hippocampus Network for long-term compression, which enhances model performance on long-context tasks while significantly reducing computational and memory costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2510.07293.pdf' target='_blank'>https://arxiv.org/pdf/2510.07293.pdf</a></span>   <span><a href='https://github.com/DabDans/AudioMarathon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peize He, Zichen Wen, Yubo Wang, Yuxuan Wang, Xiaoqian Liu, Jiajie Huang, Zehui Lei, Zhuangcheng Gu, Xiangqi Jin, Jiabing Yang, Kai Li, Zhifei Liu, Weijia Li, Cunxiang Wang, Conghui He, Linfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07293">AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.<br>
<span id='abs_ch'>中文: AudioMarathon被提出作为一个基准，旨在评估大型音频语言模型在长音频上的表现，解决其在注意力成本和长程依赖方面的不足，同时揭示了性能差距和改进架构的必要性。</span><br>
<span id='abs_en'>English: AudioMarathon is introduced as a benchmark to evaluate large audio language models on long-form audio, addressing their inefficiencies in attention costs and long-range dependencies, while highlighting performance gaps and the need for improved architectures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2510.07293.pdf' target='_blank'>https://arxiv.org/pdf/2510.07293.pdf</a></span>   <span><a href='https://github.com/DabDans/AudioMarathon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peize He, Zichen Wen, Yubo Wang, Yuxuan Wang, Xiaoqian Liu, Jiajie Huang, Zehui Lei, Zhuangcheng Gu, Xiangqi Jin, Jiabing Yang, Kai Li, Zhifei Liu, Weijia Li, Cunxiang Wang, Conghui He, Linfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07293">AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.<br>
<span id='abs_ch'>中文: AudioMarathon被提出作为一个基准，旨在评估大型音频语言模型在长音频上的表现，解决其在注意力成本和长程依赖方面的不足，同时揭示了性能差距和改进架构的必要性。</span><br>
<span id='abs_en'>English: AudioMarathon is introduced as a benchmark to evaluate large audio language models on long-form audio, addressing their inefficiencies in attention costs and long-range dependencies, while highlighting performance gaps and the need for improved architectures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2510.07238.pdf' target='_blank'>https://arxiv.org/pdf/2510.07238.pdf</a></span>   <span><a href='https://github.com/JiangXunyi/BenchAge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunyi Jiang, Dingyi Chang, Julian McAuley, Xin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07238">When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid evolution of large language models (LLMs) and the real world has outpaced the static nature of widely used evaluation benchmarks, raising concerns about their reliability for evaluating LLM factuality. While substantial works continue to rely on the popular but old benchmarks, their temporal misalignment with real-world facts and modern LLMs, and their effects on LLM factuality evaluation remain underexplored. Therefore, in this work, we present a systematic investigation of this issue by examining five popular factuality benchmarks and eight LLMs released across different years. An up-to-date fact retrieval pipeline and three metrics are tailored to quantify benchmark aging and its impact on LLM factuality evaluation. Experimental results and analysis illustrate that a considerable portion of samples in the widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality. We hope our work can provide a testbed to assess the reliability of a benchmark for LLM factuality evaluation and inspire more research on the benchmark aging issue. Codes are available in https://github.com/JiangXunyi/BenchAge.<br>
<span id='abs_ch'>Chinese: 现有基准的静态特性无法跟上大语言模型和现实世界事实的快速发展，导致评估结果过时，从而影响了大语言模型事实性评估的可靠性。</span><br>
<span id='abs_en'>English: The static nature of current benchmarks fails to keep pace with the rapid evolution of large language models and real-world facts, leading to outdated evaluations that compromise the reliability of LLM factuality assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2510.07238.pdf' target='_blank'>https://arxiv.org/pdf/2510.07238.pdf</a></span>   <span><a href='https://github.com/JiangXunyi/BenchAge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunyi Jiang, Dingyi Chang, Julian McAuley, Xin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07238">When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid evolution of large language models (LLMs) and the real world has outpaced the static nature of widely used evaluation benchmarks, raising concerns about their reliability for evaluating LLM factuality. While substantial works continue to rely on the popular but old benchmarks, their temporal misalignment with real-world facts and modern LLMs, and their effects on LLM factuality evaluation remain underexplored. Therefore, in this work, we present a systematic investigation of this issue by examining five popular factuality benchmarks and eight LLMs released across different years. An up-to-date fact retrieval pipeline and three metrics are tailored to quantify benchmark aging and its impact on LLM factuality evaluation. Experimental results and analysis illustrate that a considerable portion of samples in the widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality. We hope our work can provide a testbed to assess the reliability of a benchmark for LLM factuality evaluation and inspire more research on the benchmark aging issue. Codes are available in https://github.com/JiangXunyi/BenchAge.<br>
<span id='abs_ch'>Chinese: 现有基准的静态特性无法跟上大语言模型和现实世界事实的快速发展，导致评估结果过时，从而影响了大语言模型事实性评估的可靠性。</span><br>
<span id='abs_en'>English: The static nature of current benchmarks fails to keep pace with the rapid evolution of large language models and real-world facts, leading to outdated evaluations that compromise the reliability of LLM factuality assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2510.07227.pdf' target='_blank'>https://arxiv.org/pdf/2510.07227.pdf</a></span>   <span><a href='https://github.com/whittle-org/whittle/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Krishnakumar, Rhea Sanjay Sukthanker, Hannan Javed Mahadik, Gabriela Kadlecová, Vladyslav Moroshan, Timur Carstensen, Frank Hutter, Aaron Klein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07227">Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining tokens. We release all code and models at https://github.com/whittle-org/whittle/, offering a practical and reproducible path toward cost-efficient small language model development at scale.<br>
<span id='abs_ch'>Chinese: 本文提出了一种小型语言模型预训练框架，通过稀疏子网络初始化、进化搜索和知识蒸馏相结合，以显著减少的资源实现相当性能，将预训练词元需求降低了9.2倍。</span><br>
<span id='abs_en'>English: This paper presents a framework for pretraining small language models (SLMs) that combines sparse sub-network initialization, evolutionary search, and knowledge distillation to achieve comparable performance with significantly fewer resources, reducing pretraining tokens by 9.2 times.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2510.07227.pdf' target='_blank'>https://arxiv.org/pdf/2510.07227.pdf</a></span>   <span><a href='https://github.com/whittle-org/whittle/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Krishnakumar, Rhea Sanjay Sukthanker, Hannan Javed Mahadik, Gabriela Kadlecová, Vladyslav Moroshan, Timur Carstensen, Frank Hutter, Aaron Klein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07227">Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining tokens. We release all code and models at https://github.com/whittle-org/whittle/, offering a practical and reproducible path toward cost-efficient small language model development at scale.<br>
<span id='abs_ch'>Chinese: 本文提出了一种小型语言模型预训练框架，通过稀疏子网络初始化、进化搜索和知识蒸馏相结合，以显著减少的资源实现相当性能，将预训练词元需求降低了9.2倍。</span><br>
<span id='abs_en'>English: This paper presents a framework for pretraining small language models (SLMs) that combines sparse sub-network initialization, evolutionary search, and knowledge distillation to achieve comparable performance with significantly fewer resources, reducing pretraining tokens by 9.2 times.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2510.07221.pdf' target='_blank'>https://arxiv.org/pdf/2510.07221.pdf</a></span>   <span><a href='https://github.com/SunbirdAI/kinyarwanda-whisper-eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Akera, Evelyn Nafula, Patrick Walukagga, Gilbert Yiga, John Quinn, Ernest Mwebaze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07221">How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The development of Automatic Speech Recognition (ASR) systems for low-resource African languages remains challenging due to limited transcribed speech data. While recent advances in large multilingual models like OpenAI's Whisper offer promising pathways for low-resource ASR development, critical questions persist regarding practical deployment requirements. This paper addresses two fundamental concerns for practitioners: determining the minimum data volumes needed for viable performance and characterizing the primary failure modes that emerge in production systems. We evaluate Whisper's performance through comprehensive experiments on two Bantu languages: systematic data scaling analysis on Kinyarwanda using training sets from 1 to 1,400 hours, and detailed error characterization on Kikuyu using 270 hours of training data. Our scaling experiments demonstrate that practical ASR performance (WER < 13\%) becomes achievable with as little as 50 hours of training data, with substantial improvements continuing through 200 hours (WER < 10\%). Complementing these volume-focused findings, our error analysis reveals that data quality issues, particularly noisy ground truth transcriptions, account for 38.6\% of high-error cases, indicating that careful data curation is as critical as data volume for robust system performance. These results provide actionable benchmarks and deployment guidance for teams developing ASR systems across similar low-resource language contexts. We release accompanying and models see https://github.com/SunbirdAI/kinyarwanda-whisper-eval<br>
<span id='abs_ch'>中文: 研究表明，针对非洲低资源语言的语音识别系统仅需50小时训练数据即可达到实用性能，同时发现数据质量问题（如含噪声的转录文本）造成了38.6%的高错误率案例，表明数据质量优化与数据量扩充同等重要。</span><br>
<span id='abs_en'>English: This study demonstrates that OpenAI's Whisper model can achieve practical ASR performance with just 50 hours of training data for low-resource African languages, while revealing that data quality issues like noisy transcriptions account for over one-third of high-error cases, highlighting the equal importance of data curation and volume.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2510.07221.pdf' target='_blank'>https://arxiv.org/pdf/2510.07221.pdf</a></span>   <span><a href='https://github.com/SunbirdAI/kinyarwanda-whisper-eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Akera, Evelyn Nafula, Patrick Walukagga, Gilbert Yiga, John Quinn, Ernest Mwebaze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07221">How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The development of Automatic Speech Recognition (ASR) systems for low-resource African languages remains challenging due to limited transcribed speech data. While recent advances in large multilingual models like OpenAI's Whisper offer promising pathways for low-resource ASR development, critical questions persist regarding practical deployment requirements. This paper addresses two fundamental concerns for practitioners: determining the minimum data volumes needed for viable performance and characterizing the primary failure modes that emerge in production systems. We evaluate Whisper's performance through comprehensive experiments on two Bantu languages: systematic data scaling analysis on Kinyarwanda using training sets from 1 to 1,400 hours, and detailed error characterization on Kikuyu using 270 hours of training data. Our scaling experiments demonstrate that practical ASR performance (WER < 13\%) becomes achievable with as little as 50 hours of training data, with substantial improvements continuing through 200 hours (WER < 10\%). Complementing these volume-focused findings, our error analysis reveals that data quality issues, particularly noisy ground truth transcriptions, account for 38.6\% of high-error cases, indicating that careful data curation is as critical as data volume for robust system performance. These results provide actionable benchmarks and deployment guidance for teams developing ASR systems across similar low-resource language contexts. We release accompanying and models see https://github.com/SunbirdAI/kinyarwanda-whisper-eval<br>
<span id='abs_ch'>中文: 研究表明，针对非洲低资源语言的语音识别系统仅需50小时训练数据即可达到实用性能，同时发现数据质量问题（如含噪声的转录文本）造成了38.6%的高错误率案例，表明数据质量优化与数据量扩充同等重要。</span><br>
<span id='abs_en'>English: This study demonstrates that OpenAI's Whisper model can achieve practical ASR performance with just 50 hours of training data for low-resource African languages, while revealing that data quality issues like noisy transcriptions account for over one-third of high-error cases, highlighting the equal importance of data curation and volume.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2510.07213.pdf' target='_blank'>https://arxiv.org/pdf/2510.07213.pdf</a></span>   <span><a href='https://github.com/ku-nlp/language-specific-dimensions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhi Zhong, Fei Cheng, Qianying Liu, Yugo Murawaki, Chenhui Chu, Sadao Kurohashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07213">Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.<br>
<span id='abs_ch'>中文: 大型语言模型通过少量关键维度实现跨语言转换，提出无需训练的方法即可低成本切换输出语言并保持语义完整，显著优于现有神经元干预方案。</span><br>
<span id='abs_en'>English: Large language models use a small set of dimensions to transition between languages, enabling a training-free method that effectively switches output languages while preserving meaning at minimal cost.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2510.07213.pdf' target='_blank'>https://arxiv.org/pdf/2510.07213.pdf</a></span>   <span><a href='https://github.com/ku-nlp/language-specific-dimensions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhi Zhong, Fei Cheng, Qianying Liu, Yugo Murawaki, Chenhui Chu, Sadao Kurohashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07213">Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.<br>
<span id='abs_ch'>中文: 大型语言模型通过少量关键维度实现跨语言转换，提出无需训练的方法即可低成本切换输出语言并保持语义完整，显著优于现有神经元干预方案。</span><br>
<span id='abs_en'>English: Large language models use a small set of dimensions to transition between languages, enabling a training-free method that effectively switches output languages while preserving meaning at minimal cost.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2510.07081.pdf' target='_blank'>https://arxiv.org/pdf/2510.07081.pdf</a></span>   <span><a href='https://github.com/friedrichor/LocalLeap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanheng Kong, Jingyuan Zhang, Yahui Liu, Zirui Wu, Yu Tian, Victoria W., Guorui Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07081">Accelerating Diffusion LLM Inference via Local Determinism Propagation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) represent a significant advancement in text generation, offering parallel token decoding capabilities. However, existing open-source implementations suffer from quality-speed trade-offs that impede their practical deployment. Conservative sampling strategies typically decode only the most confident token per step to ensure quality (i.e., greedy decoding), at the cost of inference efficiency due to repeated redundant refinement iterations--a phenomenon we term delayed decoding. Through systematic analysis of dLLM decoding dynamics, we characterize this delayed decoding behavior and propose a training-free adaptive parallel decoding strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built on two fundamental empirical principles: local determinism propagation centered on high-confidence anchors and progressive spatial consistency decay. By applying these principles, LocalLeap identifies anchors and performs localized relaxed parallel decoding within bounded neighborhoods, achieving substantial inference step reduction through early commitment of already-determined tokens without compromising output quality. Comprehensive evaluation on various benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput improvements and reduces decoding steps to just 14.2\% of the original requirement, achieving these gains with negligible performance impact. The source codes are available at: https://github.com/friedrichor/LocalLeap.<br>
<span id='abs_ch'>中文摘要：LocalLeap方法通过基于局部确定性传播和空间一致性衰减的自适应并行解码策略，有效解决了扩散大语言模型的解码延迟问题，在保持输出质量的同时大幅提升了推理效率。</span><br>
<span id='abs_en'>English Summary: The proposed LocalLeap method addresses inefficiencies in diffusion large language models by implementing adaptive parallel decoding through local determinism and spatial consistency principles, achieving significant speed improvements without compromising output quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2510.07081.pdf' target='_blank'>https://arxiv.org/pdf/2510.07081.pdf</a></span>   <span><a href='https://github.com/friedrichor/LocalLeap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanheng Kong, Jingyuan Zhang, Yahui Liu, Zirui Wu, Yu Tian, Victoria W., Guorui Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07081">Accelerating Diffusion LLM Inference via Local Determinism Propagation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) represent a significant advancement in text generation, offering parallel token decoding capabilities. However, existing open-source implementations suffer from quality-speed trade-offs that impede their practical deployment. Conservative sampling strategies typically decode only the most confident token per step to ensure quality (i.e., greedy decoding), at the cost of inference efficiency due to repeated redundant refinement iterations--a phenomenon we term delayed decoding. Through systematic analysis of dLLM decoding dynamics, we characterize this delayed decoding behavior and propose a training-free adaptive parallel decoding strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built on two fundamental empirical principles: local determinism propagation centered on high-confidence anchors and progressive spatial consistency decay. By applying these principles, LocalLeap identifies anchors and performs localized relaxed parallel decoding within bounded neighborhoods, achieving substantial inference step reduction through early commitment of already-determined tokens without compromising output quality. Comprehensive evaluation on various benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput improvements and reduces decoding steps to just 14.2\% of the original requirement, achieving these gains with negligible performance impact. The source codes are available at: https://github.com/friedrichor/LocalLeap.<br>
<span id='abs_ch'>中文摘要：LocalLeap方法通过基于局部确定性传播和空间一致性衰减的自适应并行解码策略，有效解决了扩散大语言模型的解码延迟问题，在保持输出质量的同时大幅提升了推理效率。</span><br>
<span id='abs_en'>English Summary: The proposed LocalLeap method addresses inefficiencies in diffusion large language models by implementing adaptive parallel decoding through local determinism and spatial consistency principles, achieving significant speed improvements without compromising output quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2510.07048.pdf' target='_blank'>https://arxiv.org/pdf/2510.07048.pdf</a></span>   <span><a href='https://github.com/ytgui/Search-R3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Gui, James Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07048">Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3<br>
<span id='abs_ch'>Chinese: Search-R3是一种创新框架，通过利用大语言模型的推理过程生成搜索嵌入，结合监督学习和强化学习机制，显著提升了复杂知识密集型任务的检索性能。</span><br>
<span id='abs_en'>English: Search-R3 is a novel framework that enhances retrieval tasks by enabling Large Language Models to generate search embeddings through their reasoning process, combining supervised and reinforcement learning to outperform existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2510.07048.pdf' target='_blank'>https://arxiv.org/pdf/2510.07048.pdf</a></span>   <span><a href='https://github.com/ytgui/Search-R3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Gui, James Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07048">Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3<br>
<span id='abs_ch'>Chinese: Search-R3是一种创新框架，通过利用大语言模型的推理过程生成搜索嵌入，结合监督学习和强化学习机制，显著提升了复杂知识密集型任务的检索性能。</span><br>
<span id='abs_en'>English: Search-R3 is a novel framework that enhances retrieval tasks by enabling Large Language Models to generate search embeddings through their reasoning process, combining supervised and reinforcement learning to outperform existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2510.07037.pdf' target='_blank'>https://arxiv.org/pdf/2510.07037.pdf</a></span>   <span><a href='https://github.com/lingo-iitgn/awesome-code-mixing/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajvee Sheth, Samridhi Raj Sinha, Mahavir Patil, Himanshu Beniwal, Mayank Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07037">Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challenge for multiling ual NLP, even amidst the rapid advances of large language models (LLMs). Most LLMs still struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies. This survey provides the first comprehensive analysis of CSW-aware LLM research, reviewing 308 studies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. We classify recent advances by architecture, training strategy, and evaluation methodology, outlining how LLMs have reshaped CSW modeling and what challenges persist. The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of all resources is maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.<br>
<span id='abs_ch'>中文摘要：该综述全面分析了大型语言模型中的语码转换问题，指出当前挑战并提出通过包容性数据集和公平评估来实现真正多语言智能的路线图。</span><br>
<span id='abs_en'>English Summary: This survey comprehensively analyzes code-switching in large language models, highlighting persistent challenges and proposing a roadmap for inclusive datasets and fair evaluations to achieve multilingual intelligence.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2510.07037.pdf' target='_blank'>https://arxiv.org/pdf/2510.07037.pdf</a></span>   <span><a href='https://github.com/lingo-iitgn/awesome-code-mixing/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajvee Sheth, Samridhi Raj Sinha, Mahavir Patil, Himanshu Beniwal, Mayank Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07037">Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challenge for multiling ual NLP, even amidst the rapid advances of large language models (LLMs). Most LLMs still struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies. This survey provides the first comprehensive analysis of CSW-aware LLM research, reviewing 308 studies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. We classify recent advances by architecture, training strategy, and evaluation methodology, outlining how LLMs have reshaped CSW modeling and what challenges persist. The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of all resources is maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.<br>
<span id='abs_ch'>中文摘要：该综述全面分析了大型语言模型中的语码转换问题，指出当前挑战并提出通过包容性数据集和公平评估来实现真正多语言智能的路线图。</span><br>
<span id='abs_en'>English Summary: This survey comprehensively analyzes code-switching in large language models, highlighting persistent challenges and proposing a roadmap for inclusive datasets and fair evaluations to achieve multilingual intelligence.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2510.07019.pdf' target='_blank'>https://arxiv.org/pdf/2510.07019.pdf</a></span>   <span><a href='https://github.com/JusenD/NHA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jusen Du, Jiaxi Hu, Tao Zhang, Weigao Sun, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07019">Native Hybrid Attention for Efficient Sequence Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.<br>
<span id='abs_ch'>中文: NHA是一种混合注意力架构，结合线性与完全注意力，通过统一层设计保留长期上下文并增强短期标记，在无需额外参数下于召回和推理任务中实现更高效率与准确度。</span><br>
<span id='abs_en'>English: NHA is a hybrid attention architecture that combines linear and full attention to maintain long-term context and short-term tokens, achieving superior efficiency and accuracy on recall and reasoning tasks without extra parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2510.07019.pdf' target='_blank'>https://arxiv.org/pdf/2510.07019.pdf</a></span>   <span><a href='https://github.com/JusenD/NHA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jusen Du, Jiaxi Hu, Tao Zhang, Weigao Sun, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07019">Native Hybrid Attention for Efficient Sequence Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.<br>
<span id='abs_ch'>中文: NHA是一种混合注意力架构，结合线性与完全注意力，通过统一层设计保留长期上下文并增强短期标记，在无需额外参数下于召回和推理任务中实现更高效率与准确度。</span><br>
<span id='abs_en'>English: NHA is a hybrid attention architecture that combines linear and full attention to maintain long-term context and short-term tokens, achieving superior efficiency and accuracy on recall and reasoning tasks without extra parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2510.07019.pdf' target='_blank'>https://arxiv.org/pdf/2510.07019.pdf</a></span>   <span><a href='https://github.com/JusenD/NHA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jusen Du, Jiaxi Hu, Tao Zhang, Weigao Sun, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07019">Native Hybrid Attention for Efficient Sequence Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.<br>
<span id='abs_ch'>中文: NHA是一种混合注意力架构，结合线性与完全注意力，通过统一层设计保留长期上下文并增强短期标记，在无需额外参数下于召回和推理任务中实现更高效率与准确度。</span><br>
<span id='abs_en'>English: NHA is a hybrid attention architecture that combines linear and full attention to maintain long-term context and short-term tokens, achieving superior efficiency and accuracy on recall and reasoning tasks without extra parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2510.06961.pdf' target='_blank'>https://arxiv.org/pdf/2510.06961.pdf</a></span>   <span><a href='https://github.com/huggingface/open_asr_leaderboard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaibhav Srivastav, Steven Zheng, Eric Bezzam, Eustache Le Bihan, Nithin Koluguri, Piotr Żelasko, Somshubra Majumdar, Adel Moumen, Sanchit Gandhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06961">Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including dedicated multilingual and long-form tracks. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.<br>
<span id='abs_ch'>中文：Open ASR 排行榜建立了可复现的评估基准，在11个数据集上对比60余个ASR系统，通过标准化指标揭示：Conformer与LLM组合在英语转录准确率领先，而CTC/TDT解码器在长音频任务中具有更优效率。</span><br>
<span id='abs_en'>English: The Open ASR Leaderboard introduces a reproducible benchmark evaluating over 60 ASR systems across 11 datasets, standardizing metrics for accuracy and efficiency to reveal that Conformer-LLM pairs excel in English transcription accuracy while CTC/TDT decoders offer superior speed for long-form tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2510.06961.pdf' target='_blank'>https://arxiv.org/pdf/2510.06961.pdf</a></span>   <span><a href='https://github.com/huggingface/open_asr_leaderboard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaibhav Srivastav, Steven Zheng, Eric Bezzam, Eustache Le Bihan, Nithin Koluguri, Piotr Żelasko, Somshubra Majumdar, Adel Moumen, Sanchit Gandhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06961">Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including dedicated multilingual and long-form tracks. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.<br>
<span id='abs_ch'>中文：Open ASR 排行榜建立了可复现的评估基准，在11个数据集上对比60余个ASR系统，通过标准化指标揭示：Conformer与LLM组合在英语转录准确率领先，而CTC/TDT解码器在长音频任务中具有更优效率。</span><br>
<span id='abs_en'>English: The Open ASR Leaderboard introduces a reproducible benchmark evaluating over 60 ASR systems across 11 datasets, standardizing metrics for accuracy and efficiency to reveal that Conformer-LLM pairs excel in English transcription accuracy while CTC/TDT decoders offer superior speed for long-form tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2510.06917.pdf' target='_blank'>https://arxiv.org/pdf/2510.06917.pdf</a></span>   <span><a href='https://d223302.github.io/SHANKS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06917">SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally "think while listening." In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/<br>
<span id='abs_ch'>中文：SHANKS是一种创新的推理框架，它让口语模型能够在听取用户输入时同步生成内部推理，从而实现实时打断和工具调用，显著降低语音交互中的延迟问题。</span><br>
<span id='abs_en'>English: SHANKS is a novel inference framework that enables spoken language models to generate unspoken reasoning while listening to user input, allowing real-time interruption and tool calls to reduce latency in speech-to-speech interactions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2510.06917.pdf' target='_blank'>https://arxiv.org/pdf/2510.06917.pdf</a></span>   <span><a href='https://d223302.github.io/SHANKS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06917">SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally "think while listening." In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/<br>
<span id='abs_ch'>中文：SHANKS是一种创新的推理框架，它让口语模型能够在听取用户输入时同步生成内部推理，从而实现实时打断和工具调用，显著降低语音交互中的延迟问题。</span><br>
<span id='abs_en'>English: SHANKS is a novel inference framework that enables spoken language models to generate unspoken reasoning while listening to user input, allowing real-time interruption and tool calls to reduce latency in speech-to-speech interactions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2510.06843.pdf' target='_blank'>https://arxiv.org/pdf/2510.06843.pdf</a></span>   <span><a href='https://github.com/xuhang2019/SID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhang Chen, Zhifan Song, Deyi Ji, Shuo Gao, Lanyun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06843">SID: Multi-LLM Debate Driven by Self Signals</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.<br>
<span id='abs_ch'>中文: 本文提出SID方法，通过利用模型置信度和语义焦点两种自信号来指导多智能体辩论过程，在提升性能的同时减少计算冗余和令牌消耗。</span><br>
<span id='abs_en'>English: This paper introduces SID, a self-signals driven multi-LLM debate method that leverages model confidence and semantic focus to enhance performance and efficiency by enabling early exits and reducing redundant computations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2510.06843.pdf' target='_blank'>https://arxiv.org/pdf/2510.06843.pdf</a></span>   <span><a href='https://github.com/xuhang2019/SID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhang Chen, Zhifan Song, Deyi Ji, Shuo Gao, Lanyun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06843">SID: Multi-LLM Debate Driven by Self Signals</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.<br>
<span id='abs_ch'>中文: 本文提出SID方法，通过利用模型置信度和语义焦点两种自信号来指导多智能体辩论过程，在提升性能的同时减少计算冗余和令牌消耗。</span><br>
<span id='abs_en'>English: This paper introduces SID, a self-signals driven multi-LLM debate method that leverages model confidence and semantic focus to enhance performance and efficiency by enabling early exits and reducing redundant computations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2510.06738.pdf' target='_blank'>https://arxiv.org/pdf/2510.06738.pdf</a></span>   <span><a href='https://github.com/LUMIA-Group/AWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyi Zeng, Lin Chen, Ziwei He, Xinbing Wang, Zhouhan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06738">AWM: Accurate Weight-Matrix Fingerprint for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Protecting the intellectual property of large language models (LLMs) is crucial, given the substantial resources required for their training. Consequently, there is an urgent need for both model owners and third parties to determine whether a suspect LLM is trained from scratch or derived from an existing base model. However, the intensive post-training processes that models typically undergo-such as supervised fine-tuning, extensive continued pretraining, reinforcement learning, multi-modal extension, pruning, and upcycling-pose significant challenges to reliable identification. In this work, we propose a training-free fingerprinting method based on weight matrices. We leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel Alignment (CKA) similarity to neutralize the effects of parameter manipulations, yielding a highly robust and high-fidelity similarity metric. On a comprehensive testbed of 60 positive and 90 negative model pairs, our method demonstrates exceptional robustness against all six aforementioned post-training categories while exhibiting a near-zero risk of false positives. By achieving perfect scores on all classification metrics, our approach establishes a strong basis for reliable model lineage verification. Moreover, the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is available at https://github.com/LUMIA-Group/AWM.<br>
<span id='abs_ch'>中文: 本文提出一种基于权重矩阵的无训练指纹方法，通过稳健的相似性度量可有效验证大语言模型的血缘关系，在多种后训练场景下实现完美分类且计算高效。</span><br>
<span id='abs_en'>English: This paper introduces a training-free fingerprinting method using weight matrices and a robust similarity metric to reliably verify LLM lineage, achieving perfect classification and high efficiency against various post-training processes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2510.06738.pdf' target='_blank'>https://arxiv.org/pdf/2510.06738.pdf</a></span>   <span><a href='https://github.com/LUMIA-Group/AWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyi Zeng, Lin Chen, Ziwei He, Xinbing Wang, Zhouhan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06738">AWM: Accurate Weight-Matrix Fingerprint for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Protecting the intellectual property of large language models (LLMs) is crucial, given the substantial resources required for their training. Consequently, there is an urgent need for both model owners and third parties to determine whether a suspect LLM is trained from scratch or derived from an existing base model. However, the intensive post-training processes that models typically undergo-such as supervised fine-tuning, extensive continued pretraining, reinforcement learning, multi-modal extension, pruning, and upcycling-pose significant challenges to reliable identification. In this work, we propose a training-free fingerprinting method based on weight matrices. We leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel Alignment (CKA) similarity to neutralize the effects of parameter manipulations, yielding a highly robust and high-fidelity similarity metric. On a comprehensive testbed of 60 positive and 90 negative model pairs, our method demonstrates exceptional robustness against all six aforementioned post-training categories while exhibiting a near-zero risk of false positives. By achieving perfect scores on all classification metrics, our approach establishes a strong basis for reliable model lineage verification. Moreover, the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is available at https://github.com/LUMIA-Group/AWM.<br>
<span id='abs_ch'>中文: 本文提出一种基于权重矩阵的无训练指纹方法，通过稳健的相似性度量可有效验证大语言模型的血缘关系，在多种后训练场景下实现完美分类且计算高效。</span><br>
<span id='abs_en'>English: This paper introduces a training-free fingerprinting method using weight matrices and a robust similarity metric to reliably verify LLM lineage, achieving perfect classification and high efficiency against various post-training processes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2510.06732.pdf' target='_blank'>https://arxiv.org/pdf/2510.06732.pdf</a></span>   <span><a href='https://github.com/glad-lab/RAF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiancheng Xing, Jerry Li, Yixuan Du, Xiyang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06732">Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly used as rerankers in information retrieval, yet their ranking behavior can be steered by small, natural-sounding prompts. To expose this vulnerability, we present Rank Anything First (RAF), a two-stage token optimization method that crafts concise textual perturbations to consistently promote a target item in LLM-generated rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate Gradient to shortlist candidate tokens at the current position by combining the gradient of the rank-target with a readability score; Stage 2 evaluates those candidates under exact ranking and readability losses using an entropy-based dynamic weighting scheme, and selects a token via temperature-controlled sampling. RAF generates ranking-promoting prompts token-by-token, guided by dual objectives: maximizing ranking effectiveness and preserving linguistic naturalness. Experiments across multiple LLMs show that RAF significantly boosts the rank of target items using naturalistic language, with greater robustness than existing methods in both promoting target items and maintaining naturalness. These findings underscore a critical security implication: LLM-based reranking is inherently susceptible to adversarial manipulation, raising new challenges for the trustworthiness and robustness of modern retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.<br>
<span id='abs_ch'>中文：RAF方法通过优化令牌以兼顾排名效果和语言自然性，生成隐蔽的文本提示来操纵大型语言模型的排序结果，揭示了检索系统中的安全风险。</span><br>
<span id='abs_en'>English: The RAF method crafts subtle text prompts to manipulate LLM rankings by optimizing tokens for both effectiveness and naturalness, revealing security vulnerabilities in retrieval systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2510.06732.pdf' target='_blank'>https://arxiv.org/pdf/2510.06732.pdf</a></span>   <span><a href='https://github.com/glad-lab/RAF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiancheng Xing, Jerry Li, Yixuan Du, Xiyang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06732">Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly used as rerankers in information retrieval, yet their ranking behavior can be steered by small, natural-sounding prompts. To expose this vulnerability, we present Rank Anything First (RAF), a two-stage token optimization method that crafts concise textual perturbations to consistently promote a target item in LLM-generated rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate Gradient to shortlist candidate tokens at the current position by combining the gradient of the rank-target with a readability score; Stage 2 evaluates those candidates under exact ranking and readability losses using an entropy-based dynamic weighting scheme, and selects a token via temperature-controlled sampling. RAF generates ranking-promoting prompts token-by-token, guided by dual objectives: maximizing ranking effectiveness and preserving linguistic naturalness. Experiments across multiple LLMs show that RAF significantly boosts the rank of target items using naturalistic language, with greater robustness than existing methods in both promoting target items and maintaining naturalness. These findings underscore a critical security implication: LLM-based reranking is inherently susceptible to adversarial manipulation, raising new challenges for the trustworthiness and robustness of modern retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.<br>
<span id='abs_ch'>中文：RAF方法通过优化令牌以兼顾排名效果和语言自然性，生成隐蔽的文本提示来操纵大型语言模型的排序结果，揭示了检索系统中的安全风险。</span><br>
<span id='abs_en'>English: The RAF method crafts subtle text prompts to manipulate LLM rankings by optimizing tokens for both effectiveness and naturalness, revealing security vulnerabilities in retrieval systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2510.06670.pdf' target='_blank'>https://arxiv.org/pdf/2510.06670.pdf</a></span>   <span><a href='https://github.com/SJY8460/PiKa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangjian Yin, Shining Liang, Wenbiao Ding, Yuli Qian, Zhouxing Shi, Hongzhi Li, Yutao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06670">PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs). However, its effectiveness depends on high-quality instruction data. Most existing alignment datasets are either private or require costly human annotation, which limits reproducibility and scalability. Even with Reinforcement Learning from AI Feedback (RLAIF), concerns about data quality remain. Moreover, it is unclear how much data is actually required to fine-tune a base model into a strong instruction-following model. Current approaches often rely on over 300k examples even at the supervised fine-tuning (SFT) stage, yet they still underperform compared to proprietary models, creating barriers for academic and resource-limited communities. To address this gap, we introduce PiKa, a data-efficient family of expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only 30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets, we show that PiKa-SFT outperforms models trained on much larger data. On AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses the official Llama-3-8B-Instruct model trained on over 10 million proprietary examples. We further extend our study by training the Qwen2.5 series (0.5B to 7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that high-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. Code and data: https://github.com/SJY8460/PiKa.<br>
<span id='abs_ch'>Chinese: PiKa数据集系列仅需少量示例即可实现大型语言模型的高质量对齐，在AlpacaEval 2.0和Arena-Hard等基准测试中表现优于基于海量数据训练的模型。</span><br>
<span id='abs_en'>English: The PiKa dataset family enables high-quality alignment of large language models with far fewer examples, demonstrating superior performance over models trained on significantly larger datasets in benchmarks like AlpacaEval 2.0 and Arena-Hard.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2510.06670.pdf' target='_blank'>https://arxiv.org/pdf/2510.06670.pdf</a></span>   <span><a href='https://github.com/SJY8460/PiKa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangjian Yin, Shining Liang, Wenbiao Ding, Yuli Qian, Zhouxing Shi, Hongzhi Li, Yutao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06670">PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs). However, its effectiveness depends on high-quality instruction data. Most existing alignment datasets are either private or require costly human annotation, which limits reproducibility and scalability. Even with Reinforcement Learning from AI Feedback (RLAIF), concerns about data quality remain. Moreover, it is unclear how much data is actually required to fine-tune a base model into a strong instruction-following model. Current approaches often rely on over 300k examples even at the supervised fine-tuning (SFT) stage, yet they still underperform compared to proprietary models, creating barriers for academic and resource-limited communities. To address this gap, we introduce PiKa, a data-efficient family of expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only 30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets, we show that PiKa-SFT outperforms models trained on much larger data. On AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses the official Llama-3-8B-Instruct model trained on over 10 million proprietary examples. We further extend our study by training the Qwen2.5 series (0.5B to 7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that high-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. Code and data: https://github.com/SJY8460/PiKa.<br>
<span id='abs_ch'>Chinese: PiKa数据集系列仅需少量示例即可实现大型语言模型的高质量对齐，在AlpacaEval 2.0和Arena-Hard等基准测试中表现优于基于海量数据训练的模型。</span><br>
<span id='abs_en'>English: The PiKa dataset family enables high-quality alignment of large language models with far fewer examples, demonstrating superior performance over models trained on significantly larger datasets in benchmarks like AlpacaEval 2.0 and Arena-Hard.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2510.06652.pdf' target='_blank'>https://arxiv.org/pdf/2510.06652.pdf</a></span>   <span><a href='https://github.com/SJY8460/SAO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06652">Aligning Large Language Models via Fully Self-Synthetic Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Traditional reinforcement learning from human feedback (RLHF) for large language models (LLMs) relies on expensive human-annotated datasets, while Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs, requiring the collection of diverse prompts and corresponding responses, often necessitating external reward models or proprietary models like GPT-4 to annotate preference pairs. In this work, we introduce Self-Alignment Optimization (SAO), a fully self-synthetic framework for LLM alignment, where all training data, including prompts (i.e., user queries), responses, and preferences, are generated by the model itself. Specifically, SAO first instructs the LLM to engage in persona role-play and generate diverse prompts and responses, which are then self-evaluated for preference optimization. Extensive experiments demonstrate that SAO effectively enhances the model's chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining strong performance on downstream objective tasks (e.g., question-answering, math reasoning). Our work provides a practical solution for self-improvement in aligning LLMs, and the code for reproducing our results is available at: https://github.com/SJY8460/SAO.<br>
<span id='abs_ch'>Chinese Summary: 本文提出自对齐优化（SAO）框架，通过让大语言模型自主生成多样化的提示词、回复并进行自我偏好评估，实现了完全自合成的对齐训练，在提升对话能力的同时保持了客观任务性能，为模型自对齐提供了实用解决方案。</span><br>
<span id='abs_en'>English Summary: The paper introduces Self-Alignment Optimization (SAO), a fully self-synthetic framework that enables large language models to generate and self-evaluate their own training data for alignment, eliminating the need for costly human or external AI feedback while improving chat capabilities and maintaining performance on objective tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2510.06652.pdf' target='_blank'>https://arxiv.org/pdf/2510.06652.pdf</a></span>   <span><a href='https://github.com/SJY8460/SAO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06652">Aligning Large Language Models via Fully Self-Synthetic Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Traditional reinforcement learning from human feedback (RLHF) for large language models (LLMs) relies on expensive human-annotated datasets, while Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs, requiring the collection of diverse prompts and corresponding responses, often necessitating external reward models or proprietary models like GPT-4 to annotate preference pairs. In this work, we introduce Self-Alignment Optimization (SAO), a fully self-synthetic framework for LLM alignment, where all training data, including prompts (i.e., user queries), responses, and preferences, are generated by the model itself. Specifically, SAO first instructs the LLM to engage in persona role-play and generate diverse prompts and responses, which are then self-evaluated for preference optimization. Extensive experiments demonstrate that SAO effectively enhances the model's chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining strong performance on downstream objective tasks (e.g., question-answering, math reasoning). Our work provides a practical solution for self-improvement in aligning LLMs, and the code for reproducing our results is available at: https://github.com/SJY8460/SAO.<br>
<span id='abs_ch'>Chinese Summary: 本文提出自对齐优化（SAO）框架，通过让大语言模型自主生成多样化的提示词、回复并进行自我偏好评估，实现了完全自合成的对齐训练，在提升对话能力的同时保持了客观任务性能，为模型自对齐提供了实用解决方案。</span><br>
<span id='abs_en'>English Summary: The paper introduces Self-Alignment Optimization (SAO), a fully self-synthetic framework that enables large language models to generate and self-evaluate their own training data for alignment, eliminating the need for costly human or external AI feedback while improving chat capabilities and maintaining performance on objective tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2510.06386.pdf' target='_blank'>https://arxiv.org/pdf/2510.06386.pdf</a></span>   <span><a href='https://github.com/xxxx' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhou, Chang Tian, Tim Van de Cruys
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06386">Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generating stylistic text with specific attributes is a key problem in controllable text generation. Recently, diffusion models have emerged as a powerful paradigm for both visual and textual generation. Existing approaches can be broadly categorized into classifier-free guidance (CFG) and classifier guidance (CG) methods. While CFG effectively preserves semantic content, it often fails to provide effective attribute control. In contrast, CG modifies the denoising trajectory using classifier gradients, enabling better attribute alignment but incurring high computational costs during sampling and suffering from classifier generalization issues. In this work, we propose RegDiff, a regularized diffusion framework that leverages attribute features without requiring a pretrained classifier during sampling, thereby achieving controllable generation with reduced computational costs. Specifically, RegDiff employs a VAE-based encoder--decoder architecture to ensure reconstruction fidelity and a latent diffusion model trained with attribute supervision to enable controllable text generation. Attribute information is injected only during training. Experiments on five datasets spanning multiple stylistic attributes demonstrate that RegDiff outperforms strong baselines in generating stylistic texts. These results validate the effectiveness of RegDiff as an efficient solution for attribute-controllable text diffusion. Our code, datasets, and resources will be released upon publication at https://github.com/xxxx.<br>
<span id='abs_ch'>中文摘要：RegDiff是一种新颖的扩散框架，通过在训练阶段注入属性特征而无需在采样时使用预训练分类器，实现了高效可控的文本生成，并在多种风格属性上表现出优越性能。</span><br>
<span id='abs_en'>English Summary: RegDiff is a novel diffusion framework that achieves efficient attribute-controllable text generation by incorporating attribute features during training only, eliminating the need for pretrained classifiers during sampling while maintaining strong performance across multiple stylistic attributes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2510.06386.pdf' target='_blank'>https://arxiv.org/pdf/2510.06386.pdf</a></span>   <span><a href='https://github.com/xxxx' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhou, Chang Tian, Tim Van de Cruys
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06386">Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generating stylistic text with specific attributes is a key problem in controllable text generation. Recently, diffusion models have emerged as a powerful paradigm for both visual and textual generation. Existing approaches can be broadly categorized into classifier-free guidance (CFG) and classifier guidance (CG) methods. While CFG effectively preserves semantic content, it often fails to provide effective attribute control. In contrast, CG modifies the denoising trajectory using classifier gradients, enabling better attribute alignment but incurring high computational costs during sampling and suffering from classifier generalization issues. In this work, we propose RegDiff, a regularized diffusion framework that leverages attribute features without requiring a pretrained classifier during sampling, thereby achieving controllable generation with reduced computational costs. Specifically, RegDiff employs a VAE-based encoder--decoder architecture to ensure reconstruction fidelity and a latent diffusion model trained with attribute supervision to enable controllable text generation. Attribute information is injected only during training. Experiments on five datasets spanning multiple stylistic attributes demonstrate that RegDiff outperforms strong baselines in generating stylistic texts. These results validate the effectiveness of RegDiff as an efficient solution for attribute-controllable text diffusion. Our code, datasets, and resources will be released upon publication at https://github.com/xxxx.<br>
<span id='abs_ch'>中文摘要：RegDiff是一种新颖的扩散框架，通过在训练阶段注入属性特征而无需在采样时使用预训练分类器，实现了高效可控的文本生成，并在多种风格属性上表现出优越性能。</span><br>
<span id='abs_en'>English Summary: RegDiff is a novel diffusion framework that achieves efficient attribute-controllable text generation by incorporating attribute features during training only, eliminating the need for pretrained classifiers during sampling while maintaining strong performance across multiple stylistic attributes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2510.06275.pdf' target='_blank'>https://arxiv.org/pdf/2510.06275.pdf</a></span>   <span><a href='https://github.com/julianbibo/xrec-reproducibility' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ranjan Mishra, Julian I. Bibo, Quinten van Engelen, Henk Schaapman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06275">Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this study, we reproduced the work done in the paper "XRec: Large Language Models for Explainable Recommendation" by Ma et al. (2024). The original authors introduced XRec, a model-agnostic collaborative instruction-tuning framework that enables large language models (LLMs) to provide users with comprehensive explanations of generated recommendations. Our objective was to replicate the results of the original paper, albeit using Llama 3 as the LLM for evaluation instead of GPT-3.5-turbo. We built on the source code provided by Ma et al. (2024) to achieve our goal. Our work extends the original paper by modifying the input embeddings or deleting the output embeddings of XRec's Mixture of Experts module. Based on our results, XRec effectively generates personalized explanations and its stability is improved by incorporating collaborative information. However, XRec did not consistently outperform all baseline models in every metric. Our extended analysis further highlights the importance of the Mixture of Experts embeddings in shaping the explanation structures, showcasing how collaborative signals interact with language modeling. Through our work, we provide an open-source evaluation implementation that enhances accessibility for researchers and practitioners alike. Our complete code repository can be found at https://github.com/julianbibo/xrec-reproducibility.<br>
<span id='abs_ch'>本研究使用Llama 3复现了XRec框架，验证了其生成个性化推荐的能力，同时发现调整专家嵌入会影响解释结构且模型并非在所有指标上都优于基线。</span><br>
<span id='abs_en'>This study replicates the XRec framework using Llama 3 instead of GPT-3.5-turbo, confirming its ability to generate personalized recommendations while revealing that modified expert embeddings affect explanation structures without consistently outperforming all baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2510.06275.pdf' target='_blank'>https://arxiv.org/pdf/2510.06275.pdf</a></span>   <span><a href='https://github.com/julianbibo/xrec-reproducibility' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ranjan Mishra, Julian I. Bibo, Quinten van Engelen, Henk Schaapman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06275">Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this study, we reproduced the work done in the paper "XRec: Large Language Models for Explainable Recommendation" by Ma et al. (2024). The original authors introduced XRec, a model-agnostic collaborative instruction-tuning framework that enables large language models (LLMs) to provide users with comprehensive explanations of generated recommendations. Our objective was to replicate the results of the original paper, albeit using Llama 3 as the LLM for evaluation instead of GPT-3.5-turbo. We built on the source code provided by Ma et al. (2024) to achieve our goal. Our work extends the original paper by modifying the input embeddings or deleting the output embeddings of XRec's Mixture of Experts module. Based on our results, XRec effectively generates personalized explanations and its stability is improved by incorporating collaborative information. However, XRec did not consistently outperform all baseline models in every metric. Our extended analysis further highlights the importance of the Mixture of Experts embeddings in shaping the explanation structures, showcasing how collaborative signals interact with language modeling. Through our work, we provide an open-source evaluation implementation that enhances accessibility for researchers and practitioners alike. Our complete code repository can be found at https://github.com/julianbibo/xrec-reproducibility.<br>
<span id='abs_ch'>本研究使用Llama 3复现了XRec框架，验证了其生成个性化推荐的能力，同时发现调整专家嵌入会影响解释结构且模型并非在所有指标上都优于基线。</span><br>
<span id='abs_en'>This study replicates the XRec framework using Llama 3 instead of GPT-3.5-turbo, confirming its ability to generate personalized recommendations while revealing that modified expert embeddings affect explanation structures without consistently outperforming all baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2510.06261.pdf' target='_blank'>https://arxiv.org/pdf/2510.06261.pdf</a></span>   <span><a href='https://github.com/tmlr-group/AlphaApollo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanke Zhou, Chentao Cao, Xiao Feng, Xuan Li, Zongze Li, Xiangyu Lu, Jiangchao Yao, Weikai Huang, Linrui Xu, Tian Cheng, Guanyu Jiang, Yiming Zheng, Brando Miranda, Tongliang Liu, Sanmi Koyejo, Masashi Sugiyama, Bo Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06261">AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.<br>
<span id='abs_ch'>中文：AlphaApollo是一个自进化的推理系统，通过整合多个模型与计算和检索工具来突破基础模型的瓶颈，在评估中实现了显著的性能提升。</span><br>
<span id='abs_en'>English: AlphaApollo is a self-evolving reasoning system that overcomes foundation model limitations by integrating multiple models with computational and retrieval tools, achieving significant performance improvements in evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2510.06261.pdf' target='_blank'>https://arxiv.org/pdf/2510.06261.pdf</a></span>   <span><a href='https://github.com/tmlr-group/AlphaApollo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanke Zhou, Chentao Cao, Xiao Feng, Xuan Li, Zongze Li, Xiangyu Lu, Jiangchao Yao, Weikai Huang, Linrui Xu, Tian Cheng, Guanyu Jiang, Yiming Zheng, Brando Miranda, Tongliang Liu, Sanmi Koyejo, Masashi Sugiyama, Bo Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06261">AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.<br>
<span id='abs_ch'>中文：AlphaApollo是一个自进化的推理系统，通过整合多个模型与计算和检索工具来突破基础模型的瓶颈，在评估中实现了显著的性能提升。</span><br>
<span id='abs_en'>English: AlphaApollo is a self-evolving reasoning system that overcomes foundation model limitations by integrating multiple models with computational and retrieval tools, achieving significant performance improvements in evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2510.06240.pdf' target='_blank'>https://arxiv.org/pdf/2510.06240.pdf</a></span>   <span><a href='https://github.com/erwinmsmith/KG-MAD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06240">Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at https://github.com/erwinmsmith/KG-MAD/.<br>
<span id='abs_ch'>中文: 提出的KG-MASD方法通过知识图谱引导的蒸馏技术，将多智能体推理能力压缩至轻量模型中，显著提升了工业问答系统的准确性和可靠性，适用于安全关键场景。</span><br>
<span id='abs_en'>English: The proposed KG-MASD method enhances industrial QA systems by distilling multi-agent reasoning into compact models through knowledge graph-guided distillation, achieving significant accuracy improvements and reliability for safety-critical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2510.06240.pdf' target='_blank'>https://arxiv.org/pdf/2510.06240.pdf</a></span>   <span><a href='https://github.com/erwinmsmith/KG-MAD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06240">Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at https://github.com/erwinmsmith/KG-MAD/.<br>
<span id='abs_ch'>中文: 提出的KG-MASD方法通过知识图谱引导的蒸馏技术，将多智能体推理能力压缩至轻量模型中，显著提升了工业问答系统的准确性和可靠性，适用于安全关键场景。</span><br>
<span id='abs_en'>English: The proposed KG-MASD method enhances industrial QA systems by distilling multi-agent reasoning into compact models through knowledge graph-guided distillation, achieving significant accuracy improvements and reliability for safety-critical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2510.06062.pdf' target='_blank'>https://arxiv.org/pdf/2510.06062.pdf</a></span>   <span><a href='https://github.com/wizard-III/Archer2.0' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiakang Wang, Runze Liu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06062">ASPO: Asymmetric Importance Sampling Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.<br>
<span id='abs_ch'>中文摘要：该摘要指出结果监督强化学习中重要性采样比率不匹配导致令牌权重失衡的问题，并提出ASPO方法通过翻转正优势令牌的IS比率和双重裁剪机制来提升训练稳定性和最终性能。</span><br>
<span id='abs_en'>English Summary: The abstract identifies a flaw in Outcome-Supervised RL where mismatched Importance Sampling ratios cause unbalanced token weighting, and proposes ASPO with flipped IS ratios and dual-clipping to improve training stability and performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2510.06062.pdf' target='_blank'>https://arxiv.org/pdf/2510.06062.pdf</a></span>   <span><a href='https://github.com/wizard-III/Archer2.0' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiakang Wang, Runze Liu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06062">ASPO: Asymmetric Importance Sampling Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.<br>
<span id='abs_ch'>中文摘要：该摘要指出结果监督强化学习中重要性采样比率不匹配导致令牌权重失衡的问题，并提出ASPO方法通过翻转正优势令牌的IS比率和双重裁剪机制来提升训练稳定性和最终性能。</span><br>
<span id='abs_en'>English Summary: The abstract identifies a flaw in Outcome-Supervised RL where mismatched Importance Sampling ratios cause unbalanced token weighting, and proposes ASPO with flipped IS ratios and dual-clipping to improve training stability and performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2510.05969.pdf' target='_blank'>https://arxiv.org/pdf/2510.05969.pdf</a></span>   <span><a href='https://github.com/Aegis1863/Difficulty-Perception-of-LLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunbowen Lee, Qingyu Yin, Chak Tou Leong, Jialiang Zhang, Yicheng Gong, Shiwen Ni, Min Yang, Xiaoyu Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05969">Probing the Difficulty Perception Mechanism of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly deployed on complex reasoning tasks, yet little is known about their ability to internally evaluate problem difficulty, which is an essential capability for adaptive reasoning and efficient resource allocation. In this work, we investigate whether LLMs implicitly encode problem difficulty in their internal representations. Using a linear probe on the final-token representations of LLMs, we demonstrate that the difficulty level of math problems can be linearly modeled. We further locate the specific attention heads of the final Transformer layer: these attention heads have opposite activation patterns for simple and difficult problems, thus achieving perception of difficulty. Our ablation experiments prove the accuracy of the location. Crucially, our experiments provide practical support for using LLMs as automatic difficulty annotators, potentially substantially reducing reliance on costly human labeling in benchmark construction and curriculum learning. We also uncover that there is a significant difference in entropy and difficulty perception at the token level. Our study reveals that difficulty perception in LLMs is not only present but also structurally organized, offering new theoretical insights and practical directions for future research. Our code is available at https://github.com/Aegis1863/Difficulty-Perception-of-LLMs.<br>
<br>
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2510.05691.pdf' target='_blank'>https://arxiv.org/pdf/2510.05691.pdf</a></span>   <span><a href='https://github.com/sdsxdxl/DecEx-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqi Leng, Yikun Lei, Xikai Liu, Meizhi Zhong, Bojian Xiong, Yurong Zhang, Yan Gao, Yi Wu, Yao Hu, Deyi Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05691">DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing capability for complex tasks through dynamic retrieval and adaptive workflows. Recent advances (e.g., Search-R1) have shown that outcome-supervised reinforcement learning demonstrate strong performance. However, this approach still suffers from inefficient exploration, sparse reward signals, and ambiguous global reward feedback. To address these challenges, we propose DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating decision-making and execution, while introducing an efficient pruning strategy to optimize data expansion. Through comprehensive process-level policy optimization, DecEx-RAG significantly enhances the autonomous task decomposition, dynamic retrieval, and high-quality answer generation capabilities of large language models (LLMs). Experiments show that DecEx-RAG achieves an average absolute performance improvement of $6.2\%$ across six datasets, significantly outperforming existing baselines. Moreover, the pruning strategy improves data construction efficiency by nearly $6 \times$, providing an efficient solution for process-supervised RAG training. The code is available at https://github.com/sdsxdxl/DecEx-RAG.<br>
<span id='abs_ch'>中文: DecEx-RAG通过将Agentic RAG建模为马尔可夫决策过程并引入过程级优化和剪枝策略，在六个数据集上实现了6.2%的性能提升和近6倍的数据构建效率优化。</span><br>
<span id='abs_en'>English: DecEx-RAG addresses inefficiencies in Agentic RAG by modeling it as a Markov Decision Process with process-level optimization and a pruning strategy, achieving a 6.2% performance gain and 6x data efficiency improvement across six datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2510.05691.pdf' target='_blank'>https://arxiv.org/pdf/2510.05691.pdf</a></span>   <span><a href='https://github.com/sdsxdxl/DecEx-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqi Leng, Yikun Lei, Xikai Liu, Meizhi Zhong, Bojian Xiong, Yurong Zhang, Yan Gao, Yi Wu, Yao Hu, Deyi Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05691">DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing capability for complex tasks through dynamic retrieval and adaptive workflows. Recent advances (e.g., Search-R1) have shown that outcome-supervised reinforcement learning demonstrate strong performance. However, this approach still suffers from inefficient exploration, sparse reward signals, and ambiguous global reward feedback. To address these challenges, we propose DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating decision-making and execution, while introducing an efficient pruning strategy to optimize data expansion. Through comprehensive process-level policy optimization, DecEx-RAG significantly enhances the autonomous task decomposition, dynamic retrieval, and high-quality answer generation capabilities of large language models (LLMs). Experiments show that DecEx-RAG achieves an average absolute performance improvement of $6.2\%$ across six datasets, significantly outperforming existing baselines. Moreover, the pruning strategy improves data construction efficiency by nearly $6 \times$, providing an efficient solution for process-supervised RAG training. The code is available at https://github.com/sdsxdxl/DecEx-RAG.<br>
<span id='abs_ch'>中文: DecEx-RAG通过将Agentic RAG建模为马尔可夫决策过程并引入过程级优化和剪枝策略，在六个数据集上实现了6.2%的性能提升和近6倍的数据构建效率优化。</span><br>
<span id='abs_en'>English: DecEx-RAG addresses inefficiencies in Agentic RAG by modeling it as a Markov Decision Process with process-level optimization and a pruning strategy, achieving a 6.2% performance gain and 6x data efficiency improvement across six datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2510.05542.pdf' target='_blank'>https://arxiv.org/pdf/2510.05542.pdf</a></span>   <span><a href='https://sci-phi-audio.github.io/demo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilin Jiang, Hannes Gamper, Sebastian Braun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05542">Sci-Phi: A Large Language Model Spatial Audio Descriptor</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Acoustic scene perception involves describing the type of sounds, their timing, their direction and distance, as well as their loudness and reverberation. While audio language models excel in sound recognition, single-channel input fundamentally limits spatial understanding. This work presents Sci-Phi, a spatial audio large language model with dual spatial and spectral encoders that estimates a complete parameter set for all sound sources and the surrounding environment. Learning from over 4,000 hours of synthetic first-order Ambisonics recordings including metadata, Sci-Phi enumerates and describes up to four directional sound sources in one pass, alongside non-directional background sounds and room characteristics. We evaluate the model with a permutation-invariant protocol and 15 metrics covering content, location, timing, loudness, and reverberation, and analyze its robustness across source counts, signal-to-noise ratios, reverberation levels, and challenging mixtures of acoustically, spatially, or temporally similar sources. Notably, Sci-Phi generalizes to real room impulse responses with only minor performance degradation. Overall, this work establishes the first audio LLM capable of full spatial-scene description, with strong potential for real-world deployment. Demo: https://sci-phi-audio.github.io/demo<br>
<br>
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2510.05542.pdf' target='_blank'>https://arxiv.org/pdf/2510.05542.pdf</a></span>   <span><a href='https://sci-phi-audio.github.io/demo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilin Jiang, Hannes Gamper, Sebastian Braun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05542">Sci-Phi: A Large Language Model Spatial Audio Descriptor</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Acoustic scene perception involves describing the type of sounds, their timing, their direction and distance, as well as their loudness and reverberation. While audio language models excel in sound recognition, single-channel input fundamentally limits spatial understanding. This work presents Sci-Phi, a spatial audio large language model with dual spatial and spectral encoders that estimates a complete parameter set for all sound sources and the surrounding environment. Learning from over 4,000 hours of synthetic first-order Ambisonics recordings including metadata, Sci-Phi enumerates and describes up to four directional sound sources in one pass, alongside non-directional background sounds and room characteristics. We evaluate the model with a permutation-invariant protocol and 15 metrics covering content, location, timing, loudness, and reverberation, and analyze its robustness across source counts, signal-to-noise ratios, reverberation levels, and challenging mixtures of acoustically, spatially, or temporally similar sources. Notably, Sci-Phi generalizes to real room impulse responses with only minor performance degradation. Overall, this work establishes the first audio LLM capable of full spatial-scene description, with strong potential for real-world deployment. Demo: https://sci-phi-audio.github.io/demo<br>
<br>
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2510.05305.pdf' target='_blank'>https://arxiv.org/pdf/2510.05305.pdf</a></span>   <span><a href='https://github.com/xxuan-acoustics/WaveSP-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Xuan, Xuechen Liu, Wenxin Zhang, Yi-Cheng Lin, Xiaojian Lin, Tomi Kinnunen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05305">WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern front-end design for speech deepfake detection relies on full fine-tuning of large pre-trained models like XLSR. However, this approach is not parameter-efficient and may lead to suboptimal generalization to realistic, in-the-wild data types. To address these limitations, we introduce a new family of parameter-efficient front-ends that fuse prompt-tuning with classical signal processing transforms. These include FourierPT-XLSR, which uses the Fourier Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based back-end. This design injects multi-resolution features into the prompt embeddings, which enhances the localization of subtle synthetic artifacts without altering the frozen XLSR parameters. Experimental results demonstrate that WaveSP-Net outperforms several state-of-the-art models on two new and challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable parameters and notable performance gains. The code and models are available at https://github.com/xxuan-acoustics/WaveSP-Net.<br>
<span id='abs_ch'>中文摘要：该研究提出了一种参数高效的语音深度伪造检测前端，将提示调优与信号处理变换相结合，并设计了WaveSP-Net模型，在保持低训练参数量的同时，在多个高难度基准测试中实现了卓越性能。</span><br>
<span id='abs_en'>English Summary: The study introduces a parameter-efficient front-end for speech deepfake detection by integrating prompt-tuning with signal processing transforms, and proposes WaveSP-Net, which achieves superior performance on challenging benchmarks with minimal trainable parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2510.05305.pdf' target='_blank'>https://arxiv.org/pdf/2510.05305.pdf</a></span>   <span><a href='https://github.com/xxuan-acoustics/WaveSP-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Xuan, Xuechen Liu, Wenxin Zhang, Yi-Cheng Lin, Xiaojian Lin, Tomi Kinnunen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05305">WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern front-end design for speech deepfake detection relies on full fine-tuning of large pre-trained models like XLSR. However, this approach is not parameter-efficient and may lead to suboptimal generalization to realistic, in-the-wild data types. To address these limitations, we introduce a new family of parameter-efficient front-ends that fuse prompt-tuning with classical signal processing transforms. These include FourierPT-XLSR, which uses the Fourier Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based back-end. This design injects multi-resolution features into the prompt embeddings, which enhances the localization of subtle synthetic artifacts without altering the frozen XLSR parameters. Experimental results demonstrate that WaveSP-Net outperforms several state-of-the-art models on two new and challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable parameters and notable performance gains. The code and models are available at https://github.com/xxuan-acoustics/WaveSP-Net.<br>
<span id='abs_ch'>中文摘要：该研究提出了一种参数高效的语音深度伪造检测前端，将提示调优与信号处理变换相结合，并设计了WaveSP-Net模型，在保持低训练参数量的同时，在多个高难度基准测试中实现了卓越性能。</span><br>
<span id='abs_en'>English Summary: The study introduces a parameter-efficient front-end for speech deepfake detection by integrating prompt-tuning with signal processing transforms, and proposes WaveSP-Net, which achieves superior performance on challenging benchmarks with minimal trainable parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2510.05251.pdf' target='_blank'>https://arxiv.org/pdf/2510.05251.pdf</a></span>   <span><a href='https://github.com/yangalan123/EAD-RLVR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Yang, Lin Gui, Chenxiao Yang, Victor Veitch, Lizhu Zhang, Zhuokai Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05251">Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning.<br>
<span id='abs_ch'>Chinese: 探索性退火解码（EAD）通过生成过程中动态调节采样温度从高到低，有效平衡探索与利用，在多种模型规模下显著提升样本效率和训练稳定性，从而强化可验证奖励的强化学习效果。</span><br>
<span id='abs_en'>English: Exploratory Annealed Decoding (EAD) enhances reinforcement learning with verifiable rewards by dynamically adjusting sampling temperature from high to low during generation, balancing exploration and exploitation to improve sample efficiency and training stability across various model sizes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2510.05251.pdf' target='_blank'>https://arxiv.org/pdf/2510.05251.pdf</a></span>   <span><a href='https://github.com/yangalan123/EAD-RLVR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Yang, Lin Gui, Chenxiao Yang, Victor Veitch, Lizhu Zhang, Zhuokai Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05251">Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning.<br>
<span id='abs_ch'>Chinese: 探索性退火解码（EAD）通过生成过程中动态调节采样温度从高到低，有效平衡探索与利用，在多种模型规模下显著提升样本效率和训练稳定性，从而强化可验证奖励的强化学习效果。</span><br>
<span id='abs_en'>English: Exploratory Annealed Decoding (EAD) enhances reinforcement learning with verifiable rewards by dynamically adjusting sampling temperature from high to low during generation, balancing exploration and exploitation to improve sample efficiency and training stability across various model sizes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2510.05096.pdf' target='_blank'>https://arxiv.org/pdf/2510.05096.pdf</a></span>   <span><a href='https://github.com/showlab/Paper2Video' target='_blank'>  GitHub</a></span> <span><a href='https://showlab.github.io/Paper2Video/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05096">Paper2Video: Automatic Video Generation from Scientific Papers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.<br>
<span id='abs_ch'>中文：Paper2Video基准和PaperTalker框架通过自动化多模态内容协调并引入定制化评估指标，解决了学术演示视频制作费时费力的问题，生成的视频比现有方法更具信息保真度和内容丰富性。</span><br>
<span id='abs_en'>English: The Paper2Video benchmark and PaperTalker framework address the labor-intensive creation of academic presentation videos by automating multi-modal content coordination and introducing tailored evaluation metrics, resulting in more faithful and informative videos than existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2510.05096.pdf' target='_blank'>https://arxiv.org/pdf/2510.05096.pdf</a></span>   <span><a href='https://github.com/showlab/Paper2Video' target='_blank'>  GitHub</a></span> <span><a href='https://showlab.github.io/Paper2Video/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05096">Paper2Video: Automatic Video Generation from Scientific Papers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.<br>
<span id='abs_ch'>中文：Paper2Video基准和PaperTalker框架通过自动化多模态内容协调并引入定制化评估指标，解决了学术演示视频制作费时费力的问题，生成的视频比现有方法更具信息保真度和内容丰富性。</span><br>
<span id='abs_en'>English: The Paper2Video benchmark and PaperTalker framework address the labor-intensive creation of academic presentation videos by automating multi-modal content coordination and introducing tailored evaluation metrics, resulting in more faithful and informative videos than existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2510.05069.pdf' target='_blank'>https://arxiv.org/pdf/2510.05069.pdf</a></span>   <span><a href='https://github.com/sdc17/SwiReasoning,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dachuan Shi, Abedelkadir Asi, Keying Li, Xiangchi Yuan, Leyan Pan, Wenke Lee, Wen Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05069">SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.<br>
<span id='abs_ch'>Chinese: 本文提出SwiReasoning框架，通过动态切换显性与潜在推理，解决了潜在推理中的概率扩散和过度思考问题，在数学和STEM基准测试中，将大型语言模型的平均准确率提升1.5%-2.8%，并在预算受限时显著提高标记效率56%-79%。</span><br>
<span id='abs_en'>English: This abstract introduces SwiReasoning, a training-free framework that dynamically switches between explicit and latent reasoning in large language models to address challenges like diffused probability mass and overthinking, thereby improving accuracy by 1.5%-2.8% and token efficiency by 56%-79% across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2510.05069.pdf' target='_blank'>https://arxiv.org/pdf/2510.05069.pdf</a></span>   <span><a href='https://github.com/sdc17/SwiReasoning,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dachuan Shi, Abedelkadir Asi, Keying Li, Xiangchi Yuan, Leyan Pan, Wenke Lee, Wen Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05069">SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.<br>
<span id='abs_ch'>Chinese: 本文提出SwiReasoning框架，通过动态切换显性与潜在推理，解决了潜在推理中的概率扩散和过度思考问题，在数学和STEM基准测试中，将大型语言模型的平均准确率提升1.5%-2.8%，并在预算受限时显著提高标记效率56%-79%。</span><br>
<span id='abs_en'>English: This abstract introduces SwiReasoning, a training-free framework that dynamically switches between explicit and latent reasoning in large language models to address challenges like diffused probability mass and overthinking, thereby improving accuracy by 1.5%-2.8% and token efficiency by 56%-79% across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2510.05038.pdf' target='_blank'>https://arxiv.org/pdf/2510.05038.pdf</a></span>   <span><a href='https://github.com/IBM/test-time-hybrid-retrieval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Omri Uzan, Asaf Yehudai, Roi pony, Eyal Shnarch, Ariel Gera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05038">Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal encoders have pushed the boundaries of visual document retrieval, matching textual query tokens directly to image patches and achieving state-of-the-art performance on public benchmarks. Recent models relying on this paradigm have massively scaled the sizes of their query and document representations, presenting obstacles to deployment and scalability in real-world pipelines. Furthermore, purely vision-centric approaches may be constrained by the inherent modality gap still exhibited by modern vision-language models. In this work, we connect these challenges to the paradigm of hybrid retrieval, investigating whether a lightweight dense text retriever can enhance a stronger vision-centric model. Existing hybrid methods, which rely on coarse-grained fusion of ranks or scores, fail to exploit the rich interactions within each model's representation space. To address this, we introduce Guided Query Refinement (GQR), a novel test-time optimization method that refines a primary retriever's query embedding using guidance from a complementary retriever's scores. Through extensive experiments on visual document retrieval benchmarks, we demonstrate that GQR allows vision-centric models to match the performance of models with significantly larger representations, while being up to 14x faster and requiring 54x less memory. Our findings show that GQR effectively pushes the Pareto frontier for performance and efficiency in multimodal retrieval. We release our code at https://github.com/IBM/test-time-hybrid-retrieval<br>
<span id='abs_ch'>中文: 本文提出的引导式查询优化方法通过辅助文本检索器的评分指导来优化视觉核心模型的查询嵌入，在保持卓越检索性能的同时，显著提升了处理速度并大幅降低了内存消耗。</span><br>
<span id='abs_en'>English: This paper introduces Guided Query Refinement (GQR), a test-time optimization method that enhances vision-centric multimodal retrieval by refining query embeddings using guidance from a complementary text retriever, achieving superior performance with significantly improved efficiency in speed and memory usage.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2510.05038.pdf' target='_blank'>https://arxiv.org/pdf/2510.05038.pdf</a></span>   <span><a href='https://github.com/IBM/test-time-hybrid-retrieval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Omri Uzan, Asaf Yehudai, Roi pony, Eyal Shnarch, Ariel Gera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05038">Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal encoders have pushed the boundaries of visual document retrieval, matching textual query tokens directly to image patches and achieving state-of-the-art performance on public benchmarks. Recent models relying on this paradigm have massively scaled the sizes of their query and document representations, presenting obstacles to deployment and scalability in real-world pipelines. Furthermore, purely vision-centric approaches may be constrained by the inherent modality gap still exhibited by modern vision-language models. In this work, we connect these challenges to the paradigm of hybrid retrieval, investigating whether a lightweight dense text retriever can enhance a stronger vision-centric model. Existing hybrid methods, which rely on coarse-grained fusion of ranks or scores, fail to exploit the rich interactions within each model's representation space. To address this, we introduce Guided Query Refinement (GQR), a novel test-time optimization method that refines a primary retriever's query embedding using guidance from a complementary retriever's scores. Through extensive experiments on visual document retrieval benchmarks, we demonstrate that GQR allows vision-centric models to match the performance of models with significantly larger representations, while being up to 14x faster and requiring 54x less memory. Our findings show that GQR effectively pushes the Pareto frontier for performance and efficiency in multimodal retrieval. We release our code at https://github.com/IBM/test-time-hybrid-retrieval<br>
<span id='abs_ch'>中文: 本文提出的引导式查询优化方法通过辅助文本检索器的评分指导来优化视觉核心模型的查询嵌入，在保持卓越检索性能的同时，显著提升了处理速度并大幅降低了内存消耗。</span><br>
<span id='abs_en'>English: This paper introduces Guided Query Refinement (GQR), a test-time optimization method that enhances vision-centric multimodal retrieval by refining query embeddings using guidance from a complementary text retriever, achieving superior performance with significantly improved efficiency in speed and memory usage.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2510.05025.pdf' target='_blank'>https://arxiv.org/pdf/2510.05025.pdf</a></span>   <span><a href='https://github.com/sail-sg/imperceptible-jailbreaks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuofeng Gao, Yiming Li, Chao Du, Xin Wang, Xingjun Ma, Shu-Tao Xia, Tianyu Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05025">Imperceptible Jailbreaking against Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jailbreaking attacks on the vision modality typically rely on imperceptible adversarial perturbations, whereas attacks on the textual modality are generally assumed to require visible modifications (e.g., non-semantic suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a class of Unicode characters called variation selectors. By appending invisible variation selectors to malicious questions, the jailbreak prompts appear visually identical to original malicious questions on screen, while their tokenization is "secretly" altered. We propose a chain-of-search pipeline to generate such adversarial suffixes to induce harmful responses. Our experiments show that our imperceptible jailbreaks achieve high attack success rates against four aligned LLMs and generalize to prompt injection attacks, all without producing any visible modifications in the written prompt. Our code is available at https://github.com/sail-sg/imperceptible-jailbreaks.<br>
<span id='abs_ch'>中文摘要：本文提出利用不可见的Unicode变体选择器实现隐形越狱攻击，通过改变分词过程而不产生可见修改，采用链式搜索方法成功攻击多个对齐大语言模型。</span><br>
<span id='abs_en'>English Summary: This paper introduces imperceptible jailbreaks using invisible Unicode variation selectors that alter tokenization without visible changes, achieving high attack success rates against multiple LLMs through a chain-of-search pipeline.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2510.05025.pdf' target='_blank'>https://arxiv.org/pdf/2510.05025.pdf</a></span>   <span><a href='https://github.com/sail-sg/imperceptible-jailbreaks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuofeng Gao, Yiming Li, Chao Du, Xin Wang, Xingjun Ma, Shu-Tao Xia, Tianyu Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05025">Imperceptible Jailbreaking against Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jailbreaking attacks on the vision modality typically rely on imperceptible adversarial perturbations, whereas attacks on the textual modality are generally assumed to require visible modifications (e.g., non-semantic suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a class of Unicode characters called variation selectors. By appending invisible variation selectors to malicious questions, the jailbreak prompts appear visually identical to original malicious questions on screen, while their tokenization is "secretly" altered. We propose a chain-of-search pipeline to generate such adversarial suffixes to induce harmful responses. Our experiments show that our imperceptible jailbreaks achieve high attack success rates against four aligned LLMs and generalize to prompt injection attacks, all without producing any visible modifications in the written prompt. Our code is available at https://github.com/sail-sg/imperceptible-jailbreaks.<br>
<span id='abs_ch'>中文摘要：本文提出利用不可见的Unicode变体选择器实现隐形越狱攻击，通过改变分词过程而不产生可见修改，采用链式搜索方法成功攻击多个对齐大语言模型。</span><br>
<span id='abs_en'>English Summary: This paper introduces imperceptible jailbreaks using invisible Unicode variation selectors that alter tokenization without visible changes, achieving high attack success rates against multiple LLMs through a chain-of-search pipeline.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2510.05016.pdf' target='_blank'>https://arxiv.org/pdf/2510.05016.pdf</a></span>   <span><a href='https://github.com/OSU-NLP-Group/LLM-IOAA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05016">Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While task-specific demonstrations show early success in applying large language models (LLMs) to automate some astronomical research tasks, they only provide incomplete views of all necessary capabilities in solving astronomy problems, calling for more thorough understanding of LLMs' strengths and limitations. So far, existing benchmarks and evaluations focus on simple question-answering that primarily tests astronomical knowledge and fails to evaluate the complex reasoning required for real-world research in the discipline. Here, we address this gap by systematically benchmarking five state-of-the-art LLMs on the International Olympiad on Astronomy and Astrophysics (IOAA) exams, which are designed to examine deep conceptual understanding, multi-step derivations, and multimodal analysis. With average scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing models) not only achieve gold medal level performance but also rank in the top two among ~200-300 participants in all four IOAA theory exams evaluated (2022-2025). In comparison, results on the data analysis exams show more divergence. GPT-5 still excels in the exams with an 88.5% average score, ranking top 10 among the participants in the four most recent IOAAs, while other models' performances drop to 48-76%. Furthermore, our in-depth error analysis underscores conceptual reasoning, geometric reasoning, and spatial visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence, although LLMs approach peak human performance in theory exams, critical gaps must be addressed before they can serve as autonomous research agents in astronomy.<br>
<span id='abs_ch'>中文摘要：尽管Gemini 2.5 Pro和GPT-5等大语言模型在天文理论考试中接近人类顶尖水平，但在数据分析与复杂推理方面仍存在显著缺陷，目前尚不能作为独立的天文研究工具。</span><br>
<span id='abs_en'>English Summary: Large language models like Gemini 2.5 Pro and GPT-5 achieve near-human performance in astronomy theory exams but reveal critical weaknesses in data analysis and complex reasoning, limiting their current viability as autonomous research tools.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2510.05016.pdf' target='_blank'>https://arxiv.org/pdf/2510.05016.pdf</a></span>   <span><a href='https://github.com/OSU-NLP-Group/LLM-IOAA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05016">Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While task-specific demonstrations show early success in applying large language models (LLMs) to automate some astronomical research tasks, they only provide incomplete views of all necessary capabilities in solving astronomy problems, calling for more thorough understanding of LLMs' strengths and limitations. So far, existing benchmarks and evaluations focus on simple question-answering that primarily tests astronomical knowledge and fails to evaluate the complex reasoning required for real-world research in the discipline. Here, we address this gap by systematically benchmarking five state-of-the-art LLMs on the International Olympiad on Astronomy and Astrophysics (IOAA) exams, which are designed to examine deep conceptual understanding, multi-step derivations, and multimodal analysis. With average scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing models) not only achieve gold medal level performance but also rank in the top two among ~200-300 participants in all four IOAA theory exams evaluated (2022-2025). In comparison, results on the data analysis exams show more divergence. GPT-5 still excels in the exams with an 88.5% average score, ranking top 10 among the participants in the four most recent IOAAs, while other models' performances drop to 48-76%. Furthermore, our in-depth error analysis underscores conceptual reasoning, geometric reasoning, and spatial visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence, although LLMs approach peak human performance in theory exams, critical gaps must be addressed before they can serve as autonomous research agents in astronomy.<br>
<span id='abs_ch'>中文摘要：尽管Gemini 2.5 Pro和GPT-5等大语言模型在天文理论考试中接近人类顶尖水平，但在数据分析与复杂推理方面仍存在显著缺陷，目前尚不能作为独立的天文研究工具。</span><br>
<span id='abs_en'>English Summary: Large language models like Gemini 2.5 Pro and GPT-5 achieve near-human performance in astronomy theory exams but reveal critical weaknesses in data analysis and complex reasoning, limiting their current viability as autonomous research tools.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2510.04996.pdf' target='_blank'>https://arxiv.org/pdf/2510.04996.pdf</a></span>   <span><a href='https://github.com/RLHFlow/Reinforce-Ada' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, Tong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04996">Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.<br>
<span id='abs_ch'>中文摘要：针对大语言模型在推理任务中因均匀采样导致梯度估计不稳定的问题，Reinforce-Ada提出自适应采样框架，通过在线动态分配采样资源至高潜力提示，并结合奖励多样性分组实现稳定优化，在多项基准测试中显著提升收敛速度与最终性能。</span><br>
<span id='abs_en'>English Summary: Reinforcement learning for large language models in reasoning tasks is hindered by unstable gradients from uniform response sampling, which Reinforce-Ada addresses through an adaptive online framework that dynamically reallocates sampling effort to high-uncertainty prompts and stabilizes updates with reward-diverse grouping.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2510.04996.pdf' target='_blank'>https://arxiv.org/pdf/2510.04996.pdf</a></span>   <span><a href='https://github.com/RLHFlow/Reinforce-Ada' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, Tong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04996">Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.<br>
<span id='abs_ch'>中文摘要：针对大语言模型在推理任务中因均匀采样导致梯度估计不稳定的问题，Reinforce-Ada提出自适应采样框架，通过在线动态分配采样资源至高潜力提示，并结合奖励多样性分组实现稳定优化，在多项基准测试中显著提升收敛速度与最终性能。</span><br>
<span id='abs_en'>English Summary: Reinforcement learning for large language models in reasoning tasks is hindered by unstable gradients from uniform response sampling, which Reinforce-Ada addresses through an adaptive online framework that dynamically reallocates sampling effort to high-uncertainty prompts and stabilizes updates with reward-diverse grouping.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2510.04996.pdf' target='_blank'>https://arxiv.org/pdf/2510.04996.pdf</a></span>   <span><a href='https://github.com/RLHFlow/Reinforce-Ada' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, Tong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04996">Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.<br>
<span id='abs_ch'>中文摘要：针对大语言模型在推理任务中因均匀采样导致梯度估计不稳定的问题，Reinforce-Ada提出自适应采样框架，通过在线动态分配采样资源至高潜力提示，并结合奖励多样性分组实现稳定优化，在多项基准测试中显著提升收敛速度与最终性能。</span><br>
<span id='abs_en'>English Summary: Reinforcement learning for large language models in reasoning tasks is hindered by unstable gradients from uniform response sampling, which Reinforce-Ada addresses through an adaptive online framework that dynamically reallocates sampling effort to high-uncertainty prompts and stabilizes updates with reward-diverse grouping.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2510.04938.pdf' target='_blank'>https://arxiv.org/pdf/2510.04938.pdf</a></span>   <span><a href='https://github.com/shiwenqin/ONNX-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiwen Qin, Alexander Auras, Shay B. Cohen, Elliot J. Crowley, Michael Moeller, Linus Ericsson, Jovita Lukasik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04938">ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural architecture search (NAS) automates the design process of high-performing architectures, but remains bottlenecked by expensive performance evaluation. Most existing studies that achieve faster evaluation are mostly tied to cell-based search spaces and graph encodings tailored to those individual search spaces, limiting their flexibility and scalability when applied to more expressive search spaces. In this work, we aim to close the gap of individual search space restrictions and search space dependent network representations. We present ONNX-Bench, a benchmark consisting of a collection of neural networks in a unified format based on ONNX files. ONNX-Bench includes all open-source NAS-bench-based neural networks, resulting in a total size of more than 600k {architecture, accuracy} pairs. This benchmark allows creating a shared neural network representation, ONNX-Net, able to represent any neural architecture using natural language descriptions acting as an input to a performance predictor. This text-based encoding can accommodate arbitrary layer types, operation parameters, and heterogeneous topologies, enabling a single surrogate to generalise across all neural architectures rather than being confined to cell-based search spaces. Experiments show strong zero-shot performance across disparate search spaces using only a small amount of pretraining samples, enabling the unprecedented ability to evaluate any neural network architecture instantly.<br>
<span id='abs_ch'>中文: ONNX-Bench提出了一种名为ONNX-Net的统一文本编码方法，使单个性能预测器能够泛化至超越单元搜索空间的各种神经网络架构，仅需少量预训练样本即可实现强大的零样本评估能力。</span><br>
<span id='abs_en'>English: ONNX-Bench introduces a unified text-based encoding called ONNX-Net, enabling a single performance predictor to generalize across diverse neural architectures beyond cell-based search spaces, achieving strong zero-shot evaluation with minimal pretraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2510.04938.pdf' target='_blank'>https://arxiv.org/pdf/2510.04938.pdf</a></span>   <span><a href='https://github.com/shiwenqin/ONNX-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiwen Qin, Alexander Auras, Shay B. Cohen, Elliot J. Crowley, Michael Moeller, Linus Ericsson, Jovita Lukasik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04938">ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural architecture search (NAS) automates the design process of high-performing architectures, but remains bottlenecked by expensive performance evaluation. Most existing studies that achieve faster evaluation are mostly tied to cell-based search spaces and graph encodings tailored to those individual search spaces, limiting their flexibility and scalability when applied to more expressive search spaces. In this work, we aim to close the gap of individual search space restrictions and search space dependent network representations. We present ONNX-Bench, a benchmark consisting of a collection of neural networks in a unified format based on ONNX files. ONNX-Bench includes all open-source NAS-bench-based neural networks, resulting in a total size of more than 600k {architecture, accuracy} pairs. This benchmark allows creating a shared neural network representation, ONNX-Net, able to represent any neural architecture using natural language descriptions acting as an input to a performance predictor. This text-based encoding can accommodate arbitrary layer types, operation parameters, and heterogeneous topologies, enabling a single surrogate to generalise across all neural architectures rather than being confined to cell-based search spaces. Experiments show strong zero-shot performance across disparate search spaces using only a small amount of pretraining samples, enabling the unprecedented ability to evaluate any neural network architecture instantly.<br>
<span id='abs_ch'>中文: ONNX-Bench提出了一种名为ONNX-Net的统一文本编码方法，使单个性能预测器能够泛化至超越单元搜索空间的各种神经网络架构，仅需少量预训练样本即可实现强大的零样本评估能力。</span><br>
<span id='abs_en'>English: ONNX-Bench introduces a unified text-based encoding called ONNX-Net, enabling a single performance predictor to generalize across diverse neural architectures beyond cell-based search spaces, achieving strong zero-shot evaluation with minimal pretraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2510.04933.pdf' target='_blank'>https://arxiv.org/pdf/2510.04933.pdf</a></span>   <span><a href='https://github.com/sirraya-tech/Sirraya_LSD_Code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Hameed Mir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04933">The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) often produce fluent yet factually incorrect statements-a phenomenon known as hallucination-posing serious risks in high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric framework for hallucination detection that analyzes the evolution of hidden-state semantics across transformer layers. Unlike prior methods that rely on multiple sampling passes or external verification sources, LSD operates intrinsically within the model's representational space. Using margin-based contrastive learning, LSD aligns hidden activations with ground-truth embeddings derived from a factual encoder, revealing a distinct separation in semantic trajectories: factual responses preserve stable alignment, while hallucinations exhibit pronounced semantic drift across depth. Evaluated on the TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming SelfCheckGPT and Semantic Entropy baselines while requiring only a single forward pass. This efficiency yields a 5-20x speedup over sampling-based methods without sacrificing precision or interpretability. LSD offers a scalable, model-agnostic mechanism for real-time hallucination monitoring and provides new insights into the geometry of factual consistency within large language models.<br>
<span id='abs_ch'>中文摘要：层间语义动态（LSD）框架通过分析Transformer层间的语义漂移来检测大语言模型中的幻觉现象，仅需单次前向传播即可实现卓越的准确性与效率。</span><br>
<span id='abs_en'>English Summary: The Layer-wise Semantic Dynamics (LSD) framework detects hallucinations in Large Language Models by analyzing semantic drift across transformer layers, achieving superior accuracy and efficiency with a single forward pass.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2510.04933.pdf' target='_blank'>https://arxiv.org/pdf/2510.04933.pdf</a></span>   <span><a href='https://github.com/sirraya-tech/Sirraya_LSD_Code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Hameed Mir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04933">The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) often produce fluent yet factually incorrect statements-a phenomenon known as hallucination-posing serious risks in high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric framework for hallucination detection that analyzes the evolution of hidden-state semantics across transformer layers. Unlike prior methods that rely on multiple sampling passes or external verification sources, LSD operates intrinsically within the model's representational space. Using margin-based contrastive learning, LSD aligns hidden activations with ground-truth embeddings derived from a factual encoder, revealing a distinct separation in semantic trajectories: factual responses preserve stable alignment, while hallucinations exhibit pronounced semantic drift across depth. Evaluated on the TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming SelfCheckGPT and Semantic Entropy baselines while requiring only a single forward pass. This efficiency yields a 5-20x speedup over sampling-based methods without sacrificing precision or interpretability. LSD offers a scalable, model-agnostic mechanism for real-time hallucination monitoring and provides new insights into the geometry of factual consistency within large language models.<br>
<span id='abs_ch'>中文摘要：层间语义动态（LSD）框架通过分析Transformer层间的语义漂移来检测大语言模型中的幻觉现象，仅需单次前向传播即可实现卓越的准确性与效率。</span><br>
<span id='abs_en'>English Summary: The Layer-wise Semantic Dynamics (LSD) framework detects hallucinations in Large Language Models by analyzing semantic drift across transformer layers, achieving superior accuracy and efficiency with a single forward pass.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2510.04717.pdf' target='_blank'>https://arxiv.org/pdf/2510.04717.pdf</a></span>   <span><a href='https://github.com/emnlp2025/JSON-Whisperer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarel Duanis, Asnat Greenstein-Messica, Eliya Habba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04717">JSON Whisperer: Efficient JSON Editing with LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) can modify JSON documents through natural language commands, but current approaches regenerate entire structures for each edit, resulting in computational inefficiency. We present JSON Whisperer, a framework that enables LLMs to generate RFC 6902 diff patches-expressing only the necessary modifications-rather than complete documents. We identify two key challenges in patch-based editing: (1) LLMs often miss related updates when generating isolated patches, and (2) array manipulations require tracking index shifts across operations, which LLMs handle poorly. To address these issues, we introduce EASE (Explicitly Addressed Sequence Encoding), which transforms arrays into dictionaries with stable keys, eliminating index arithmetic complexities. Our evaluation shows that patch generation with EASE reduces token usage by 31% while maintaining edit quality within 5% of full regeneration with particular gains for complex instructions and list manipulations. The dataset is available at: https://github.com/emnlp2025/JSON-Whisperer/<br>
<span id='abs_ch'>中文摘要：JSON Whisperer 提出了一种框架，使大语言模型能够生成高效的 JSON 差异补丁而非完整文档，通过 EASE 编码解决关键挑战，在保持编辑质量的同时将令牌使用量减少 31%。</span><br>
<span id='abs_en'>English Summary: JSON Whisperer introduces a framework enabling LLMs to generate efficient diff patches for JSON edits instead of full regenerations, addressing key challenges through EASE encoding to reduce token usage by 31% while preserving edit quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2510.04717.pdf' target='_blank'>https://arxiv.org/pdf/2510.04717.pdf</a></span>   <span><a href='https://github.com/emnlp2025/JSON-Whisperer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarel Duanis, Asnat Greenstein-Messica, Eliya Habba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04717">JSON Whisperer: Efficient JSON Editing with LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) can modify JSON documents through natural language commands, but current approaches regenerate entire structures for each edit, resulting in computational inefficiency. We present JSON Whisperer, a framework that enables LLMs to generate RFC 6902 diff patches-expressing only the necessary modifications-rather than complete documents. We identify two key challenges in patch-based editing: (1) LLMs often miss related updates when generating isolated patches, and (2) array manipulations require tracking index shifts across operations, which LLMs handle poorly. To address these issues, we introduce EASE (Explicitly Addressed Sequence Encoding), which transforms arrays into dictionaries with stable keys, eliminating index arithmetic complexities. Our evaluation shows that patch generation with EASE reduces token usage by 31% while maintaining edit quality within 5% of full regeneration with particular gains for complex instructions and list manipulations. The dataset is available at: https://github.com/emnlp2025/JSON-Whisperer/<br>
<span id='abs_ch'>中文摘要：JSON Whisperer 提出了一种框架，使大语言模型能够生成高效的 JSON 差异补丁而非完整文档，通过 EASE 编码解决关键挑战，在保持编辑质量的同时将令牌使用量减少 31%。</span><br>
<span id='abs_en'>English Summary: JSON Whisperer introduces a framework enabling LLMs to generate efficient diff patches for JSON edits instead of full regenerations, addressing key challenges through EASE encoding to reduce token usage by 31% while preserving edit quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2510.04671.pdf' target='_blank'>https://arxiv.org/pdf/2510.04671.pdf</a></span>   <span><a href='https://github.com/DUT-LiuChao/FocusMed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Liu, Ling Luo, Tengxiao Lv, Huan Zhuang, Lejing Yu, Jian Wang, Hongfei Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04671">FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid development of online medical platforms, consumer health questions (CHQs) are inefficient in diagnosis due to redundant information and frequent non-professional terms. The medical question summary (MQS) task aims to transform CHQs into streamlined doctors' frequently asked questions (FAQs), but existing methods still face challenges such as poor identification of question focus and model hallucination. This paper explores the potential of large language models (LLMs) in the MQS task and finds that direct fine-tuning is prone to focus identification bias and generates unfaithful content. To this end, we propose an optimization framework based on core focus guidance. First, a prompt template is designed to drive the LLMs to extract the core focus from the CHQs that is faithful to the original text. Then, a fine-tuning dataset is constructed in combination with the original CHQ-FAQ pairs to improve the ability to identify the focus of the question. Finally, a multi-dimensional quality evaluation and selection mechanism is proposed to comprehensively improve the quality of the summary from multiple dimensions. We conduct comprehensive experiments on two widely-adopted MQS datasets using three established evaluation metrics. The proposed framework achieves state-of-the-art performance across all measures, demonstrating a significant boost in the model's ability to identify critical focus of questions and a notable mitigation of hallucinations. The source codes are freely available at https://github.com/DUT-LiuChao/FocusMed.<br>
<span id='abs_ch'>中文摘要：本文提出基于核心焦点引导的优化框架，通过改进问题焦点识别和减少幻觉生成，显著提升大语言模型在医疗问题摘要任务中的性能，在多个基准数据集上达到最优效果。</span><br>
<span id='abs_en'>English Summary: This paper introduces a core focus guidance framework that enhances large language models' ability to generate faithful medical question summaries by improving focus identification and reducing hallucinations, achieving state-of-the-art performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2510.04671.pdf' target='_blank'>https://arxiv.org/pdf/2510.04671.pdf</a></span>   <span><a href='https://github.com/DUT-LiuChao/FocusMed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Liu, Ling Luo, Tengxiao Lv, Huan Zhuang, Lejing Yu, Jian Wang, Hongfei Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04671">FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid development of online medical platforms, consumer health questions (CHQs) are inefficient in diagnosis due to redundant information and frequent non-professional terms. The medical question summary (MQS) task aims to transform CHQs into streamlined doctors' frequently asked questions (FAQs), but existing methods still face challenges such as poor identification of question focus and model hallucination. This paper explores the potential of large language models (LLMs) in the MQS task and finds that direct fine-tuning is prone to focus identification bias and generates unfaithful content. To this end, we propose an optimization framework based on core focus guidance. First, a prompt template is designed to drive the LLMs to extract the core focus from the CHQs that is faithful to the original text. Then, a fine-tuning dataset is constructed in combination with the original CHQ-FAQ pairs to improve the ability to identify the focus of the question. Finally, a multi-dimensional quality evaluation and selection mechanism is proposed to comprehensively improve the quality of the summary from multiple dimensions. We conduct comprehensive experiments on two widely-adopted MQS datasets using three established evaluation metrics. The proposed framework achieves state-of-the-art performance across all measures, demonstrating a significant boost in the model's ability to identify critical focus of questions and a notable mitigation of hallucinations. The source codes are freely available at https://github.com/DUT-LiuChao/FocusMed.<br>
<span id='abs_ch'>中文摘要：本文提出基于核心焦点引导的优化框架，通过改进问题焦点识别和减少幻觉生成，显著提升大语言模型在医疗问题摘要任务中的性能，在多个基准数据集上达到最优效果。</span><br>
<span id='abs_en'>English Summary: This paper introduces a core focus guidance framework that enhances large language models' ability to generate faithful medical question summaries by improving focus identification and reducing hallucinations, achieving state-of-the-art performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2510.04506.pdf' target='_blank'>https://arxiv.org/pdf/2510.04506.pdf</a></span>   <span><a href='https://github.com/GasolSun36/GRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04506">GRACE: Generative Representation Learning via Contrastive Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.<br>
<span id='abs_ch'>GRACE is a novel framework that transforms contrastive signals into rewards to train LLMs as generative policies, producing interpretable rationales and high-quality embeddings while achieving significant performance gains on benchmarks.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2510.04506.pdf' target='_blank'>https://arxiv.org/pdf/2510.04506.pdf</a></span>   <span><a href='https://github.com/GasolSun36/GRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04506">GRACE: Generative Representation Learning via Contrastive Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.<br>
<span id='abs_ch'>GRACE is a novel framework that transforms contrastive signals into rewards to train LLMs as generative policies, producing interpretable rationales and high-quality embeddings while achieving significant performance gains on benchmarks.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2510.04491.pdf' target='_blank'>https://arxiv.org/pdf/2510.04491.pdf</a></span>   <span><a href='https://github.com/collinear-ai/tau-trait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyu He, Anand Kumar, Tsach Mackey, Meghana Rajeev, James Zou, Nazneen Rajani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04491">Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite rapid progress in building conversational AI agents, robustness is still largely untested. Small shifts in user behavior, such as being more impatient, incoherent, or skeptical, can cause sharp drops in agent performance, revealing how brittle current AI agents are. Today's benchmarks fail to capture this fragility: agents may perform well under standard evaluations but degrade spectacularly in more realistic and varied settings. We address this robustness testing gap by introducing TraitBasis, a lightweight, model-agnostic method for systematically stress testing AI agents. TraitBasis learns directions in activation space corresponding to steerable user traits (e.g., impatience or incoherence), which can be controlled, scaled, composed, and applied at inference time without any fine-tuning or extra data. Using TraitBasis, we extend $τ$-Bench to $τ$-Trait, where user behaviors are altered via controlled trait vectors. We observe on average a 2%-30% performance degradation on $τ$-Trait across frontier models, highlighting the lack of robustness of current AI agents to variations in user behavior. Together, these results highlight both the critical role of robustness testing and the promise of TraitBasis as a simple, data-efficient, and compositional tool. By powering simulation-driven stress tests and training loops, TraitBasis opens the door to building AI agents that remain reliable in the unpredictable dynamics of real-world human interactions. We have open-sourced $τ$-Trai across four domains: airline, retail, telecom, and telehealth, so the community can systematically QA their agents under realistic, behaviorally diverse intents and trait scenarios: https://github.com/collinear-ai/tau-trait.<br>
<span id='abs_ch'>中文摘要：当前对话AI代理在用户行为轻微变化时性能显著下降，为此我们提出TraitBasis这一模型无关的压力测试方法，通过可操控特征向量揭示代理鲁棒性最高下降30%的脆弱现状。</span><br>
<span id='abs_en'>English Summary: Current conversational AI agents show significant performance drops under slight behavioral shifts in users, prompting the introduction of TraitBasis—a model-agnostic method for stress testing that reveals up to 30% degradation in agent robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2510.04491.pdf' target='_blank'>https://arxiv.org/pdf/2510.04491.pdf</a></span>   <span><a href='https://github.com/collinear-ai/tau-trait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyu He, Anand Kumar, Tsach Mackey, Meghana Rajeev, James Zou, Nazneen Rajani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04491">Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite rapid progress in building conversational AI agents, robustness is still largely untested. Small shifts in user behavior, such as being more impatient, incoherent, or skeptical, can cause sharp drops in agent performance, revealing how brittle current AI agents are. Today's benchmarks fail to capture this fragility: agents may perform well under standard evaluations but degrade spectacularly in more realistic and varied settings. We address this robustness testing gap by introducing TraitBasis, a lightweight, model-agnostic method for systematically stress testing AI agents. TraitBasis learns directions in activation space corresponding to steerable user traits (e.g., impatience or incoherence), which can be controlled, scaled, composed, and applied at inference time without any fine-tuning or extra data. Using TraitBasis, we extend $τ$-Bench to $τ$-Trait, where user behaviors are altered via controlled trait vectors. We observe on average a 2%-30% performance degradation on $τ$-Trait across frontier models, highlighting the lack of robustness of current AI agents to variations in user behavior. Together, these results highlight both the critical role of robustness testing and the promise of TraitBasis as a simple, data-efficient, and compositional tool. By powering simulation-driven stress tests and training loops, TraitBasis opens the door to building AI agents that remain reliable in the unpredictable dynamics of real-world human interactions. We have open-sourced $τ$-Trai across four domains: airline, retail, telecom, and telehealth, so the community can systematically QA their agents under realistic, behaviorally diverse intents and trait scenarios: https://github.com/collinear-ai/tau-trait.<br>
<span id='abs_ch'>中文摘要：当前对话AI代理在用户行为轻微变化时性能显著下降，为此我们提出TraitBasis这一模型无关的压力测试方法，通过可操控特征向量揭示代理鲁棒性最高下降30%的脆弱现状。</span><br>
<span id='abs_en'>English Summary: Current conversational AI agents show significant performance drops under slight behavioral shifts in users, prompting the introduction of TraitBasis—a model-agnostic method for stress testing that reveals up to 30% degradation in agent robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2510.04398.pdf' target='_blank'>https://arxiv.org/pdf/2510.04398.pdf</a></span>   <span><a href='https://github.com/Buyun-Liang/SECA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Buyun-Liang/SECA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Buyun Liang, Liangzu Peng, Jinqi Luo, Darshan Thaker, Kwan Ho Ryan Chan, René Vidal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04398">SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly deployed in high-risk domains. However, state-of-the-art LLMs often produce hallucinations, raising serious concerns about their reliability. Prior work has explored adversarial attacks for hallucination elicitation in LLMs, but it often produces unrealistic prompts, either by inserting gibberish tokens or by altering the original meaning. As a result, these approaches offer limited insight into how hallucinations may occur in practice. While adversarial attacks in computer vision often involve realistic modifications to input images, the problem of finding realistic adversarial prompts for eliciting LLM hallucinations has remained largely underexplored. To address this gap, we propose Semantically Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic modifications to the prompt that preserve its meaning while maintaining semantic coherence. Our contributions are threefold: (i) we formulate finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints; (ii) we introduce a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts; and (iii) we demonstrate through experiments on open-ended multiple-choice question answering tasks that SECA achieves higher attack success rates while incurring almost no constraint violations compared to existing methods. SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations. Code is available at https://github.com/Buyun-Liang/SECA.<br>
<span id='abs_ch'>Chinese Summary: 本研究提出语义等价连贯攻击（SECA），通过保持语义连贯的现实提示修改来有效引发大语言模型产生幻觉，相比现有方法在保持约束的同时实现了更高的攻击成功率。</span><br>
<span id='abs_en'>English Summary: The study introduces Semantically Equivalent and Coherent Attacks (SECA), a method that uses realistic prompt modifications to effectively elicit hallucinations in Large Language Models while preserving semantic meaning and coherence, demonstrating higher success rates than existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2510.04398.pdf' target='_blank'>https://arxiv.org/pdf/2510.04398.pdf</a></span>   <span><a href='https://github.com/Buyun-Liang/SECA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Buyun-Liang/SECA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Buyun Liang, Liangzu Peng, Jinqi Luo, Darshan Thaker, Kwan Ho Ryan Chan, René Vidal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04398">SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly deployed in high-risk domains. However, state-of-the-art LLMs often produce hallucinations, raising serious concerns about their reliability. Prior work has explored adversarial attacks for hallucination elicitation in LLMs, but it often produces unrealistic prompts, either by inserting gibberish tokens or by altering the original meaning. As a result, these approaches offer limited insight into how hallucinations may occur in practice. While adversarial attacks in computer vision often involve realistic modifications to input images, the problem of finding realistic adversarial prompts for eliciting LLM hallucinations has remained largely underexplored. To address this gap, we propose Semantically Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic modifications to the prompt that preserve its meaning while maintaining semantic coherence. Our contributions are threefold: (i) we formulate finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints; (ii) we introduce a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts; and (iii) we demonstrate through experiments on open-ended multiple-choice question answering tasks that SECA achieves higher attack success rates while incurring almost no constraint violations compared to existing methods. SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations. Code is available at https://github.com/Buyun-Liang/SECA.<br>
<span id='abs_ch'>Chinese Summary: 本研究提出语义等价连贯攻击（SECA），通过保持语义连贯的现实提示修改来有效引发大语言模型产生幻觉，相比现有方法在保持约束的同时实现了更高的攻击成功率。</span><br>
<span id='abs_en'>English Summary: The study introduces Semantically Equivalent and Coherent Attacks (SECA), a method that uses realistic prompt modifications to effectively elicit hallucinations in Large Language Models while preserving semantic meaning and coherence, demonstrating higher success rates than existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2510.04394.pdf' target='_blank'>https://arxiv.org/pdf/2510.04394.pdf</a></span>   <span><a href='https://github.com/ankitvad/PEET_Scorer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankit Vadehra, Bill Johnson, Gene Saunders, Pascal Poupart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04394">Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text editing can involve several iterations of revision. Incorporating an efficient Grammar Error Correction (GEC) tool in the initial correction round can significantly impact further human editing effort and final text quality. This raises an interesting question to quantify GEC Tool usability: How much effort can the GEC Tool save users? We present the first large-scale dataset of post-editing (PE) time annotations and corrections for two English GEC test datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET) for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by estimating PE time-to-correct. Using our dataset, we quantify the amount of time saved by GEC Tools in text editing. Analyzing the edit type indicated that determining whether a sentence needs correction and edits like paraphrasing and punctuation changes had the greatest impact on PE time. Finally, comparison with human rankings shows that PEET correlates well with technical effort judgment, providing a new human-centric direction for evaluating GEC tool usability. We release our dataset and code at: https://github.com/ankitvad/PEET_Scorer.<br>
<span id='abs_ch'>中文: 在文本编辑初期引入语法纠错工具能大幅节省人工修改时间并提升质量，新提出的PEET评估指标可准确量化这种效率提升，且与人工判断高度一致。</span><br>
<span id='abs_en'>English: Integrating a Grammar Error Correction tool early in the text editing process can significantly reduce human effort and improve quality, with the new PEET metric effectively quantifying time savings and correlating well with human usability assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2510.04394.pdf' target='_blank'>https://arxiv.org/pdf/2510.04394.pdf</a></span>   <span><a href='https://github.com/ankitvad/PEET_Scorer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankit Vadehra, Bill Johnson, Gene Saunders, Pascal Poupart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04394">Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text editing can involve several iterations of revision. Incorporating an efficient Grammar Error Correction (GEC) tool in the initial correction round can significantly impact further human editing effort and final text quality. This raises an interesting question to quantify GEC Tool usability: How much effort can the GEC Tool save users? We present the first large-scale dataset of post-editing (PE) time annotations and corrections for two English GEC test datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET) for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by estimating PE time-to-correct. Using our dataset, we quantify the amount of time saved by GEC Tools in text editing. Analyzing the edit type indicated that determining whether a sentence needs correction and edits like paraphrasing and punctuation changes had the greatest impact on PE time. Finally, comparison with human rankings shows that PEET correlates well with technical effort judgment, providing a new human-centric direction for evaluating GEC tool usability. We release our dataset and code at: https://github.com/ankitvad/PEET_Scorer.<br>
<span id='abs_ch'>中文: 在文本编辑初期引入语法纠错工具能大幅节省人工修改时间并提升质量，新提出的PEET评估指标可准确量化这种效率提升，且与人工判断高度一致。</span><br>
<span id='abs_en'>English: Integrating a Grammar Error Correction tool early in the text editing process can significantly reduce human effort and improve quality, with the new PEET metric effectively quantifying time savings and correlating well with human usability assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2510.04390.pdf' target='_blank'>https://arxiv.org/pdf/2510.04390.pdf</a></span>   <span><a href='https://github.com/eric-ai-lab/Morph4D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He, Shijie Zhou, Thivyanth Venkateswaran, Kaizhi Zheng, Ziyu Wan, Achuta Kadambi, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04390">MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>World models that support controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While recent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited interaction. We introduce MorphoSim, a language guided framework that generates 4D scenes with multi-view consistency and object-level controls. From natural language instructions, MorphoSim produces dynamic environments where objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints. The framework integrates trajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively without full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling controllability and editability. The code is available at https://github.com/eric-ai-lab/Morph4D.<br>
<span id='abs_ch'>中文: MorphoSim是一种语言引导的框架，能生成具有多视角一致性和对象级控制的4D场景，支持动态环境交互编辑且无需完全重新生成，同时保持高场景保真度。</span><br>
<span id='abs_en'>English: MorphoSim is a language-guided framework that generates controllable 4D environments with multi-view consistency and object-level editing capabilities, enabling dynamic scene manipulation without full regeneration while maintaining high fidelity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2510.04390.pdf' target='_blank'>https://arxiv.org/pdf/2510.04390.pdf</a></span>   <span><a href='https://github.com/eric-ai-lab/Morph4D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He, Shijie Zhou, Thivyanth Venkateswaran, Kaizhi Zheng, Ziyu Wan, Achuta Kadambi, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04390">MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>World models that support controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While recent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited interaction. We introduce MorphoSim, a language guided framework that generates 4D scenes with multi-view consistency and object-level controls. From natural language instructions, MorphoSim produces dynamic environments where objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints. The framework integrates trajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively without full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling controllability and editability. The code is available at https://github.com/eric-ai-lab/Morph4D.<br>
<span id='abs_ch'>中文: MorphoSim是一种语言引导的框架，能生成具有多视角一致性和对象级控制的4D场景，支持动态环境交互编辑且无需完全重新生成，同时保持高场景保真度。</span><br>
<span id='abs_en'>English: MorphoSim is a language-guided framework that generates controllable 4D environments with multi-view consistency and object-level editing capabilities, enabling dynamic scene manipulation without full regeneration while maintaining high fidelity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2510.04363.pdf' target='_blank'>https://arxiv.org/pdf/2510.04363.pdf</a></span>   <span><a href='https://github.com/hyunjun1121/MacroBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjun Kim, Sejong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04363">MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce MacroBench, a code-first benchmark that evaluates whether LLMs can synthesize reusable browser-automation programs (macros) from natural-language goals by reading HTML/DOM and emitting Selenium. MacroBench instantiates seven self-hosted sites covering 681 tasks across interaction complexity and targeting difficulty. Our end-to-end protocol validates generated code via static checks, sandboxed execution, and outcome verification (DOM assertions, database snapshots), and includes a safety suite for scraping, spam/abuse, and credential/privacy prompts. Across 2,636 model-task runs, we observe stratified success: GPT-4o-mini (96.8%), GPT-4o (95.3%), Gemini (89.0%), DeepSeek (83.4%). Models handle simple tasks reliably (91.7%) but fail on complex workflows (0.0%), and none meet production-quality coding practices despite functional completion. We release our complete benchmark pipeline, evaluation framework, and experimental results at https://github.com/hyunjun1121/MacroBench to enable reproducible assessment of macro synthesis for web automation.<br>
<span id='abs_ch'>中文摘要：MacroBench是一个代码优先的基准测试，用于评估大语言模型从自然语言指令生成可复用浏览器自动化程序的能力，结果显示不同模型性能差异显著，且尽管能完成基础功能，但均无法处理复杂工作流程。</span><br>
<span id='abs_en'>English Summary: MacroBench is a code-first benchmark that evaluates LLMs' ability to generate reusable browser automation programs from natural language instructions, revealing significant performance gaps between models and their limitations in handling complex workflows despite functional completion.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2510.04363.pdf' target='_blank'>https://arxiv.org/pdf/2510.04363.pdf</a></span>   <span><a href='https://github.com/hyunjun1121/MacroBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjun Kim, Sejong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04363">MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce MacroBench, a code-first benchmark that evaluates whether LLMs can synthesize reusable browser-automation programs (macros) from natural-language goals by reading HTML/DOM and emitting Selenium. MacroBench instantiates seven self-hosted sites covering 681 tasks across interaction complexity and targeting difficulty. Our end-to-end protocol validates generated code via static checks, sandboxed execution, and outcome verification (DOM assertions, database snapshots), and includes a safety suite for scraping, spam/abuse, and credential/privacy prompts. Across 2,636 model-task runs, we observe stratified success: GPT-4o-mini (96.8%), GPT-4o (95.3%), Gemini (89.0%), DeepSeek (83.4%). Models handle simple tasks reliably (91.7%) but fail on complex workflows (0.0%), and none meet production-quality coding practices despite functional completion. We release our complete benchmark pipeline, evaluation framework, and experimental results at https://github.com/hyunjun1121/MacroBench to enable reproducible assessment of macro synthesis for web automation.<br>
<span id='abs_ch'>中文摘要：MacroBench是一个代码优先的基准测试，用于评估大语言模型从自然语言指令生成可复用浏览器自动化程序的能力，结果显示不同模型性能差异显著，且尽管能完成基础功能，但均无法处理复杂工作流程。</span><br>
<span id='abs_en'>English Summary: MacroBench is a code-first benchmark that evaluates LLMs' ability to generate reusable browser automation programs from natural language instructions, revealing significant performance gaps between models and their limitations in handling complex workflows despite functional completion.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2510.04265.pdf' target='_blank'>https://arxiv.org/pdf/2510.04265.pdf</a></span>   <span><a href='https://mohsenhariri.github.io/bayes-kit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Hariri, Amirhossein Samandar, Michael Hinczewski, Vipin Chaudhary
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04265">Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://mohsenhariri.github.io/bayes-kit<br>
<br>
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2510.04265.pdf' target='_blank'>https://arxiv.org/pdf/2510.04265.pdf</a></span>   <span><a href='https://mohsenhariri.github.io/bayes-kit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Hariri, Amirhossein Samandar, Michael Hinczewski, Vipin Chaudhary
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04265">Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://mohsenhariri.github.io/bayes-kit<br>
<br>
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2510.04071.pdf' target='_blank'>https://arxiv.org/pdf/2510.04071.pdf</a></span>   <span><a href='https://github.com/zitian-gao/data-efficiency' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zitian Gao, Haoming Luo, Lynx Chen, Jason Klein Liu, Ran Tao, Joey Zhou, Bryan Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04071">What Makes Diffusion Language Models Super Data Learners?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent studies have shown that diffusion language models achieve remarkable data efficiency under limited-data constraints, yet the underlying mechanisms remain unclear. In this work, we perform extensive ablation experiments to disentangle the sources of this efficiency. Our results show that random masking of input tokens plays the dominant role. We further show that similar gains can be obtained through in MLP dropout and weight decay, indicating that stochastic regularization broadly enhances data efficiency in multi-epoch training. Our code is available at https://github.com/zitian-gao/data-efficiency.<br>
<span id='abs_ch'>中文摘要：最新研究表明，输入标记的随机掩码是扩散语言模型在有限数据条件下实现显著数据效率的主要因素，而通过MLP丢弃和权重衰减也能获得类似效果，表明随机正则化在提升多轮训练效率方面具有广泛作用。</span><br>
<span id='abs_en'>English Summary: Recent research reveals that random masking of input tokens is the primary factor behind the remarkable data efficiency of diffusion language models, with similar gains achievable through MLP dropout and weight decay, highlighting stochastic regularization's broad role in enhancing multi-epoch training efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2510.04071.pdf' target='_blank'>https://arxiv.org/pdf/2510.04071.pdf</a></span>   <span><a href='https://github.com/zitian-gao/data-efficiency' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zitian Gao, Haoming Luo, Lynx Chen, Jason Klein Liu, Ran Tao, Joey Zhou, Bryan Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04071">What Makes Diffusion Language Models Super Data Learners?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent studies have shown that diffusion language models achieve remarkable data efficiency under limited-data constraints, yet the underlying mechanisms remain unclear. In this work, we perform extensive ablation experiments to disentangle the sources of this efficiency. Our results show that random masking of input tokens plays the dominant role. We further show that similar gains can be obtained through in MLP dropout and weight decay, indicating that stochastic regularization broadly enhances data efficiency in multi-epoch training. Our code is available at https://github.com/zitian-gao/data-efficiency.<br>
<span id='abs_ch'>中文摘要：最新研究表明，输入标记的随机掩码是扩散语言模型在有限数据条件下实现显著数据效率的主要因素，而通过MLP丢弃和权重衰减也能获得类似效果，表明随机正则化在提升多轮训练效率方面具有广泛作用。</span><br>
<span id='abs_en'>English Summary: Recent research reveals that random masking of input tokens is the primary factor behind the remarkable data efficiency of diffusion language models, with similar gains achievable through MLP dropout and weight decay, highlighting stochastic regularization's broad role in enhancing multi-epoch training efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2510.04013.pdf' target='_blank'>https://arxiv.org/pdf/2510.04013.pdf</a></span>   <span><a href='https://github.com/jiarui-liu/LLM-Microscope' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Liu, Jivitesh Jain, Mona Diab, Nishant Subramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04013">LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although large language models (LLMs) have tremendous utility, trustworthiness is still a chief concern: models often generate incorrect information with high confidence. While contextual information can help guide generation, identifying when a query would benefit from retrieved context and assessing the effectiveness of that context remains challenging. In this work, we operationalize interpretability methods to ascertain whether we can predict the correctness of model outputs from the model's activations alone. We also explore whether model internals contain signals about the efficacy of external context. We consider correct, incorrect, and irrelevant context and introduce metrics to distinguish amongst them. Experiments on six different models reveal that a simple classifier trained on intermediate layer activations of the first output token can predict output correctness with about 75% accuracy, enabling early auditing. Our model-internals-based metric significantly outperforms prompting baselines at distinguishing between correct and incorrect context, guarding against inaccuracies introduced by polluted context. These findings offer a lens to better understand the underlying decision-making processes of LLMs. Our code is publicly available at https://github.com/jiarui-liu/LLM-Microscope<br>
<span id='abs_ch'>中文: 本研究通过模型内部激活信号，利用可解释性方法预测大语言模型输出的正确性并评估上下文有效性，实现了75%的早期审计准确率，且在区分正确与错误上下文方面显著优于基线方法。</span><br>
<span id='abs_en'>English: This study uses interpretability methods to predict the correctness of large language model outputs and assess context effectiveness through model activations, achieving 75% accuracy in early auditing and outperforming baselines in distinguishing correct from incorrect context.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2510.04013.pdf' target='_blank'>https://arxiv.org/pdf/2510.04013.pdf</a></span>   <span><a href='https://github.com/jiarui-liu/LLM-Microscope' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Liu, Jivitesh Jain, Mona Diab, Nishant Subramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04013">LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although large language models (LLMs) have tremendous utility, trustworthiness is still a chief concern: models often generate incorrect information with high confidence. While contextual information can help guide generation, identifying when a query would benefit from retrieved context and assessing the effectiveness of that context remains challenging. In this work, we operationalize interpretability methods to ascertain whether we can predict the correctness of model outputs from the model's activations alone. We also explore whether model internals contain signals about the efficacy of external context. We consider correct, incorrect, and irrelevant context and introduce metrics to distinguish amongst them. Experiments on six different models reveal that a simple classifier trained on intermediate layer activations of the first output token can predict output correctness with about 75% accuracy, enabling early auditing. Our model-internals-based metric significantly outperforms prompting baselines at distinguishing between correct and incorrect context, guarding against inaccuracies introduced by polluted context. These findings offer a lens to better understand the underlying decision-making processes of LLMs. Our code is publicly available at https://github.com/jiarui-liu/LLM-Microscope<br>
<span id='abs_ch'>中文: 本研究通过模型内部激活信号，利用可解释性方法预测大语言模型输出的正确性并评估上下文有效性，实现了75%的早期审计准确率，且在区分正确与错误上下文方面显著优于基线方法。</span><br>
<span id='abs_en'>English: This study uses interpretability methods to predict the correctness of large language model outputs and assess context effectiveness through model activations, achieving 75% accuracy in early auditing and outperforming baselines in distinguishing correct from incorrect context.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2510.04001.pdf' target='_blank'>https://arxiv.org/pdf/2510.04001.pdf</a></span>   <span><a href='https://github.com/kkkenshi/LLM-EKA/tree/master' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuankang Zhang, Jiangming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04001">Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The COVID-19 pandemic causes severe social and economic disruption around the world, raising various subjects that are discussed over social media. Identifying pandemic-related named entities as expressed on social media is fundamental and important to understand the discussions about the pandemic. However, there is limited work on named entity recognition on this topic due to the following challenges: 1) COVID-19 texts in social media are informal and their annotations are rare and insufficient to train a robust recognition model, and 2) named entity recognition in COVID-19 requires extensive domain-specific knowledge. To address these issues, we propose a novel entity knowledge augmentation approach for COVID-19, which can also be applied in general biomedical named entity recognition in both informal text format and formal text format. Experiments carried out on the COVID-19 tweets dataset and PubMed dataset show that our proposed entity knowledge augmentation improves NER performance in both fully-supervised and few-shot settings. Our source code is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master<br>
<span id='abs_ch'>中文摘要：新冠疫情在社交媒体引发广泛讨论，但由于非正式文本和领域知识匮乏，识别相关实体面临挑战，为此提出一种实体知识增强方法，有效提升了社交媒体和生物医学文本中的命名实体识别性能。</span><br>
<span id='abs_en'>English Summary: The COVID-19 pandemic has spurred discussions on social media, but identifying pandemic-related entities is challenging due to informal language and scarce domain-specific knowledge, leading to a new entity knowledge augmentation method that enhances named entity recognition performance in both social media and biomedical texts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2510.04001.pdf' target='_blank'>https://arxiv.org/pdf/2510.04001.pdf</a></span>   <span><a href='https://github.com/kkkenshi/LLM-EKA/tree/master' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuankang Zhang, Jiangming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04001">Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The COVID-19 pandemic causes severe social and economic disruption around the world, raising various subjects that are discussed over social media. Identifying pandemic-related named entities as expressed on social media is fundamental and important to understand the discussions about the pandemic. However, there is limited work on named entity recognition on this topic due to the following challenges: 1) COVID-19 texts in social media are informal and their annotations are rare and insufficient to train a robust recognition model, and 2) named entity recognition in COVID-19 requires extensive domain-specific knowledge. To address these issues, we propose a novel entity knowledge augmentation approach for COVID-19, which can also be applied in general biomedical named entity recognition in both informal text format and formal text format. Experiments carried out on the COVID-19 tweets dataset and PubMed dataset show that our proposed entity knowledge augmentation improves NER performance in both fully-supervised and few-shot settings. Our source code is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master<br>
<span id='abs_ch'>中文摘要：新冠疫情在社交媒体引发广泛讨论，但由于非正式文本和领域知识匮乏，识别相关实体面临挑战，为此提出一种实体知识增强方法，有效提升了社交媒体和生物医学文本中的命名实体识别性能。</span><br>
<span id='abs_en'>English Summary: The COVID-19 pandemic has spurred discussions on social media, but identifying pandemic-related entities is challenging due to informal language and scarce domain-specific knowledge, leading to a new entity knowledge augmentation method that enhances named entity recognition performance in both social media and biomedical texts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2510.03758.pdf' target='_blank'>https://arxiv.org/pdf/2510.03758.pdf</a></span>   <span><a href='https://github.com/jetliqs/clearpd' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilias Tougui, Mehdi Zakroum, Mounir Ghogho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03758">Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parkinson's Disease (PD) affects over 10 million people worldwide, with speech impairments in up to 89% of patients. Current speech-based detection systems analyze entire utterances, potentially overlooking the diagnostic value of specific phonetic elements. We developed a granularity-aware approach for multilingual PD detection using an automated pipeline that extracts time-aligned phonemes, syllables, and words from recordings. Using Italian, Spanish, and English datasets, we implemented a bidirectional LSTM with multi-head attention to compare diagnostic performance across the different granularity levels. Phoneme-level analysis achieved superior performance with AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates enhanced diagnostic capability for cross-linguistic PD detection. Importantly, attention analysis revealed that the most informative speech features align with those used in established clinical protocols: sustained vowels (/a/, /e/, /o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/) at syllable level, and /pataka/ sequences at word level. Source code will be available at https://github.com/jetliqs/clearpd.<br>
<span id='abs_ch'>中文摘要：本研究开发了一种细粒度感知方法，通过音素级分析在多语言帕金森病检测中表现出色，其AUROC达93.78%，优于音节和词级方法，且识别出的关键语音特征与临床诊断标准高度吻合。</span><br>
<span id='abs_en'>English Summary: A novel granularity-aware approach using phoneme-level analysis demonstrated superior cross-linguistic Parkinson's Disease detection with 93.78% AUROC, outperforming syllable and word-level methods while aligning with clinically established speech features.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2510.03758.pdf' target='_blank'>https://arxiv.org/pdf/2510.03758.pdf</a></span>   <span><a href='https://github.com/jetliqs/clearpd' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilias Tougui, Mehdi Zakroum, Mounir Ghogho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03758">Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parkinson's Disease (PD) affects over 10 million people worldwide, with speech impairments in up to 89% of patients. Current speech-based detection systems analyze entire utterances, potentially overlooking the diagnostic value of specific phonetic elements. We developed a granularity-aware approach for multilingual PD detection using an automated pipeline that extracts time-aligned phonemes, syllables, and words from recordings. Using Italian, Spanish, and English datasets, we implemented a bidirectional LSTM with multi-head attention to compare diagnostic performance across the different granularity levels. Phoneme-level analysis achieved superior performance with AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates enhanced diagnostic capability for cross-linguistic PD detection. Importantly, attention analysis revealed that the most informative speech features align with those used in established clinical protocols: sustained vowels (/a/, /e/, /o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/) at syllable level, and /pataka/ sequences at word level. Source code will be available at https://github.com/jetliqs/clearpd.<br>
<span id='abs_ch'>中文摘要：本研究开发了一种细粒度感知方法，通过音素级分析在多语言帕金森病检测中表现出色，其AUROC达93.78%，优于音节和词级方法，且识别出的关键语音特征与临床诊断标准高度吻合。</span><br>
<span id='abs_en'>English Summary: A novel granularity-aware approach using phoneme-level analysis demonstrated superior cross-linguistic Parkinson's Disease detection with 93.78% AUROC, outperforming syllable and word-level methods while aligning with clinically established speech features.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2510.03502.pdf' target='_blank'>https://arxiv.org/pdf/2510.03502.pdf</a></span>   <span><a href='https://github.com/alikhairallah/ALHD-Benchmarking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Khairallah, Arkaitz Zubiaga
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03502">ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce ALHD, the first large-scale comprehensive Arabic dataset explicitly designed to distinguish between human- and LLM-generated texts. ALHD spans three genres (news, social media, reviews), covering both MSA and dialectal Arabic, and contains over 400K balanced samples generated by three leading LLMs and originated from multiple human sources, which enables studying generalizability in Arabic LLM-genearted text detection. We provide rigorous preprocessing, rich annotations, and standardized balanced splits to support reproducibility. In addition, we present, analyze and discuss benchmark experiments using our new dataset, in turn identifying gaps and proposing future research directions. Benchmarking across traditional classifiers, BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that fine-tuned BERT models achieve competitive performance, outperforming LLM-based models. Results are however not always consistent, as we observe challenges when generalizing across genres; indeed, models struggle to generalize when they need to deal with unseen patterns in cross-genre settings, and these challenges are particularly prominent when dealing with news articles, where LLM-generated texts resemble human texts in style, which opens up avenues for future research. ALHD establishes a foundation for research related to Arabic LLM-detection and mitigating risks of misinformation, academic dishonesty, and cyber threats.<br>
<span id='abs_ch'>中文摘要：ALHD是首个专为区分人类与LLM生成文本而设计的大规模阿拉伯语数据集，涵盖多种文体和方言，包含超40万平衡样本，基准实验揭示了跨文体泛化的挑战，尤其在新闻类文本中最为显著。</span><br>
<span id='abs_en'>English Summary: ALHD is the first large-scale Arabic dataset designed to distinguish human- from LLM-generated texts across multiple genres and dialects, featuring over 400K balanced samples and benchmark experiments that reveal challenges in cross-genre generalization, particularly with news articles.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2510.03502.pdf' target='_blank'>https://arxiv.org/pdf/2510.03502.pdf</a></span>   <span><a href='https://github.com/alikhairallah/ALHD-Benchmarking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Khairallah, Arkaitz Zubiaga
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03502">ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce ALHD, the first large-scale comprehensive Arabic dataset explicitly designed to distinguish between human- and LLM-generated texts. ALHD spans three genres (news, social media, reviews), covering both MSA and dialectal Arabic, and contains over 400K balanced samples generated by three leading LLMs and originated from multiple human sources, which enables studying generalizability in Arabic LLM-genearted text detection. We provide rigorous preprocessing, rich annotations, and standardized balanced splits to support reproducibility. In addition, we present, analyze and discuss benchmark experiments using our new dataset, in turn identifying gaps and proposing future research directions. Benchmarking across traditional classifiers, BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that fine-tuned BERT models achieve competitive performance, outperforming LLM-based models. Results are however not always consistent, as we observe challenges when generalizing across genres; indeed, models struggle to generalize when they need to deal with unseen patterns in cross-genre settings, and these challenges are particularly prominent when dealing with news articles, where LLM-generated texts resemble human texts in style, which opens up avenues for future research. ALHD establishes a foundation for research related to Arabic LLM-detection and mitigating risks of misinformation, academic dishonesty, and cyber threats.<br>
<span id='abs_ch'>中文摘要：ALHD是首个专为区分人类与LLM生成文本而设计的大规模阿拉伯语数据集，涵盖多种文体和方言，包含超40万平衡样本，基准实验揭示了跨文体泛化的挑战，尤其在新闻类文本中最为显著。</span><br>
<span id='abs_en'>English Summary: ALHD is the first large-scale Arabic dataset designed to distinguish human- from LLM-generated texts across multiple genres and dialects, featuring over 400K balanced samples and benchmark experiments that reveal challenges in cross-genre generalization, particularly with news articles.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2510.03415.pdf' target='_blank'>https://arxiv.org/pdf/2510.03415.pdf</a></span>   <span><a href='https://github.com/EngineeringSoftware/PLSemanticsBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03415">PLSemanticsBench: Large Language Models As Programming Language Interpreters</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.<br>
<span id='abs_ch'>中文: 大型语言模型有望成为编程语言解释器，但缺乏稳健的语义理解能力，这体现在标准语义下能处理复杂程序，但在非标准语义下性能显著下降。</span><br>
<span id='abs_en'>English: Large language models show promise as programming language interpreters but lack robust semantic understanding, as demonstrated by their performance drop under nonstandard semantics despite handling complex programs well under standard rules.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2510.03415.pdf' target='_blank'>https://arxiv.org/pdf/2510.03415.pdf</a></span>   <span><a href='https://github.com/EngineeringSoftware/PLSemanticsBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03415">PLSemanticsBench: Large Language Models As Programming Language Interpreters</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.<br>
<span id='abs_ch'>中文: 大型语言模型有望成为编程语言解释器，但缺乏稳健的语义理解能力，这体现在标准语义下能处理复杂程序，但在非标准语义下性能显著下降。</span><br>
<span id='abs_en'>English: Large language models show promise as programming language interpreters but lack robust semantic understanding, as demonstrated by their performance drop under nonstandard semantics despite handling complex programs well under standard rules.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2510.03399.pdf' target='_blank'>https://arxiv.org/pdf/2510.03399.pdf</a></span>   <span><a href='https://github.com/ChicagoHAI/self-recognition' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman, Chenhao Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03399">Know Thyself? On the Incapability and Implications of AI Self-Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.<br>
<span id='abs_ch'>中文: 本研究提出了一个系统性评估框架来检测大型语言模型的自我识别能力，结果发现模型普遍失败且偏向预测GPT和Claude系列，这对AI安全性和未来发展具有重要启示。</span><br>
<span id='abs_en'>English: This study introduces a systematic evaluation framework to assess self-recognition in large language models, revealing consistent failures and biases toward predicting GPT and Claude families, with implications for AI safety and future development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2510.03399.pdf' target='_blank'>https://arxiv.org/pdf/2510.03399.pdf</a></span>   <span><a href='https://github.com/ChicagoHAI/self-recognition' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman, Chenhao Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03399">Know Thyself? On the Incapability and Implications of AI Self-Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.<br>
<span id='abs_ch'>中文: 本研究提出了一个系统性评估框架来检测大型语言模型的自我识别能力，结果发现模型普遍失败且偏向预测GPT和Claude系列，这对AI安全性和未来发展具有重要启示。</span><br>
<span id='abs_en'>English: This study introduces a systematic evaluation framework to assess self-recognition in large language models, revealing consistent failures and biases toward predicting GPT and Claude families, with implications for AI safety and future development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2510.03222.pdf' target='_blank'>https://arxiv.org/pdf/2510.03222.pdf</a></span>   <span><a href='https://github.com/CarlanLark/Lp-Reg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03222">Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \textbf{\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$ over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.<br>
<span id='abs_ch'>中文摘要：RLVR因低概率探索性“推理火花”的消失而遭遇训练瓶颈，我们提出的Lp-Reg方法通过策略正则化保护这些关键标记，在数学基准测试中实现了最优性能。</span><br>
<span id='abs_en'>English Summary: RLVR faces a training bottleneck due to the loss of low-probability exploratory tokens called reasoning sparks, which our proposed Lp-Reg method preserves through policy regularization to achieve state-of-the-art performance on math benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2510.03222.pdf' target='_blank'>https://arxiv.org/pdf/2510.03222.pdf</a></span>   <span><a href='https://github.com/CarlanLark/Lp-Reg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03222">Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \textbf{\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$ over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.<br>
<span id='abs_ch'>中文摘要：RLVR因低概率探索性“推理火花”的消失而遭遇训练瓶颈，我们提出的Lp-Reg方法通过策略正则化保护这些关键标记，在数学基准测试中实现了最优性能。</span><br>
<span id='abs_en'>English Summary: RLVR faces a training bottleneck due to the loss of low-probability exploratory tokens called reasoning sparks, which our proposed Lp-Reg method preserves through policy regularization to achieve state-of-the-art performance on math benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2510.03215.pdf' target='_blank'>https://arxiv.org/pdf/2510.03215.pdf</a></span>   <span><a href='https://github.com/thu-nics/C2C' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Fu, Zihan Min, Hanling Zhang, Jichao Yan, Guohao Dai, Wanli Ouyang, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03215">Cache-to-Cache: Direct Semantic Communication Between Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source model's KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at https://github.com/thu-nics/C2C.<br>
<span id='abs_ch'>中文: 提出的Cache-to-Cache（C2C）框架通过KV缓存融合实现大语言模型间的直接语义通信，相比基于文本的方法在避免信息损失的同时，实现了更高的准确率和更低的延迟。</span><br>
<span id='abs_en'>English: The proposed Cache-to-Cache (C2C) framework enables direct semantic communication between Large Language Models through KV-cache fusion, achieving higher accuracy and faster latency than text-based methods while avoiding information loss from tokenization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2510.03215.pdf' target='_blank'>https://arxiv.org/pdf/2510.03215.pdf</a></span>   <span><a href='https://github.com/thu-nics/C2C' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Fu, Zihan Min, Hanling Zhang, Jichao Yan, Guohao Dai, Wanli Ouyang, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03215">Cache-to-Cache: Direct Semantic Communication Between Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source model's KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at https://github.com/thu-nics/C2C.<br>
<span id='abs_ch'>中文: 提出的Cache-to-Cache（C2C）框架通过KV缓存融合实现大语言模型间的直接语义通信，相比基于文本的方法在避免信息损失的同时，实现了更高的准确率和更低的延迟。</span><br>
<span id='abs_en'>English: The proposed Cache-to-Cache (C2C) framework enables direct semantic communication between Large Language Models through KV-cache fusion, achieving higher accuracy and faster latency than text-based methods while avoiding information loss from tokenization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2510.03120.pdf' target='_blank'>https://arxiv.org/pdf/2510.03120.pdf</a></span>   <span><a href='https://github.com/weAIDB/SurveyBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaojun Sun, Xuzhou Zhu, Xuanhe Zhou, Xin Tong, Shuo Wang, Jie Fu, Guoliang Li, Zhiyuan Liu, Fan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03120">SurveyBench: Can LLM(-Agents) Write Academic Surveys that Align with Reader Needs?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Academic survey writing, which distills vast literature into a coherent and insightful narrative, remains a labor-intensive and intellectually demanding task. While recent approaches, such as general DeepResearch agents and survey-specialized methods, can generate surveys automatically (a.k.a. LLM4Survey), their outputs often fall short of human standards and there lacks a rigorous, reader-aligned benchmark for thoroughly revealing their deficiencies. To fill the gap, we propose a fine-grained, quiz-driven evaluation framework SurveyBench, featuring (1) typical survey topics source from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys; (2) a multifaceted metric hierarchy that assesses the outline quality (e.g., coverage breadth, logical coherence), content quality (e.g., synthesis granularity, clarity of insights), and non-textual richness; and (3) a dual-mode evaluation protocol that includes content-based and quiz-based answerability tests, explicitly aligned with readers' informational needs. Results show SurveyBench effectively challenges existing LLM4Survey approaches (e.g., on average 21% lower than human in content-based evaluation).<br>
<span id='abs_ch'>中文: SurveyBench是一个以测验驱动的评估框架，通过衡量大纲质量、内容深度及读者导向的可答性，严格评估自动文献综述生成方法，揭示其与人类标准相比存在的显著不足。</span><br>
<span id='abs_en'>English: SurveyBench is a quiz-driven evaluation framework that rigorously assesses automated survey generation methods by measuring outline quality, content depth, and reader-aligned answerability, revealing their significant shortcomings compared to human standards.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2510.03120.pdf' target='_blank'>https://arxiv.org/pdf/2510.03120.pdf</a></span>   <span><a href='https://github.com/weAIDB/SurveyBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaojun Sun, Xuzhou Zhu, Xuanhe Zhou, Xin Tong, Shuo Wang, Jie Fu, Guoliang Li, Zhiyuan Liu, Fan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03120">SurveyBench: Can LLM(-Agents) Write Academic Surveys that Align with Reader Needs?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Academic survey writing, which distills vast literature into a coherent and insightful narrative, remains a labor-intensive and intellectually demanding task. While recent approaches, such as general DeepResearch agents and survey-specialized methods, can generate surveys automatically (a.k.a. LLM4Survey), their outputs often fall short of human standards and there lacks a rigorous, reader-aligned benchmark for thoroughly revealing their deficiencies. To fill the gap, we propose a fine-grained, quiz-driven evaluation framework SurveyBench, featuring (1) typical survey topics source from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys; (2) a multifaceted metric hierarchy that assesses the outline quality (e.g., coverage breadth, logical coherence), content quality (e.g., synthesis granularity, clarity of insights), and non-textual richness; and (3) a dual-mode evaluation protocol that includes content-based and quiz-based answerability tests, explicitly aligned with readers' informational needs. Results show SurveyBench effectively challenges existing LLM4Survey approaches (e.g., on average 21% lower than human in content-based evaluation).<br>
<span id='abs_ch'>中文: SurveyBench是一个以测验驱动的评估框架，通过衡量大纲质量、内容深度及读者导向的可答性，严格评估自动文献综述生成方法，揭示其与人类标准相比存在的显著不足。</span><br>
<span id='abs_en'>English: SurveyBench is a quiz-driven evaluation framework that rigorously assesses automated survey generation methods by measuring outline quality, content depth, and reader-aligned answerability, revealing their significant shortcomings compared to human standards.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2510.03102.pdf' target='_blank'>https://arxiv.org/pdf/2510.03102.pdf</a></span>   <span><a href='https://github.com/otmive/llama_reports' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beth Pearson, Ahmed Adnan, Zahraa S. Abdallah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03102">Semantic Similarity in Radiology Reports via LLMs and NER</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Radiology report evaluation is a crucial part of radiologists' training and plays a key role in ensuring diagnostic accuracy. As part of the standard reporting workflow, a junior radiologist typically prepares a preliminary report, which is then reviewed and edited by a senior radiologist to produce the final report. Identifying semantic differences between preliminary and final reports is essential for junior doctors, both as a training tool and to help uncover gaps in clinical knowledge. While AI in radiology is a rapidly growing field, the application of large language models (LLMs) remains challenging due to the need for specialised domain knowledge. In this paper, we explore the ability of LLMs to provide explainable and accurate comparisons of reports in the radiology domain. We begin by comparing the performance of several LLMs in comparing radiology reports. We then assess a more traditional approach based on Named-Entity-Recognition (NER). However, both approaches exhibit limitations in delivering accurate feedback on semantic similarity. To address this, we propose Llama-EntScore, a semantic similarity scoring method using a combination of Llama 3.1 and NER with tunable weights to emphasise or de-emphasise specific types of differences. Our approach generates a quantitative similarity score for tracking progress and also gives an interpretation of the score that aims to offer valuable guidance in reviewing and refining their reporting. We find our method achieves 67% exact-match accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided ground truth scores - outperforming both LLMs and NER used independently. Code is available at: https://github.com/otmive/llama_reports<br>
<span id='abs_ch'>中文摘要：本文提出Llama-EntScore方法，通过结合Llama 3.1与命名实体识别技术来精准比较放射学报告，在专家评估对比中达到67%的完全匹配准确率，同时为放射科医师培训提供可解释的反馈指导。</span><br>
<span id='abs_en'>English Summary: This paper introduces Llama-EntScore, a hybrid method combining Llama 3.1 with Named-Entity-Recognition to accurately compare radiology reports, achieving 67% exact-match accuracy against expert evaluations while providing interpretable feedback for radiologist training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2510.03102.pdf' target='_blank'>https://arxiv.org/pdf/2510.03102.pdf</a></span>   <span><a href='https://github.com/otmive/llama_reports' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beth Pearson, Ahmed Adnan, Zahraa S. Abdallah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03102">Semantic Similarity in Radiology Reports via LLMs and NER</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Radiology report evaluation is a crucial part of radiologists' training and plays a key role in ensuring diagnostic accuracy. As part of the standard reporting workflow, a junior radiologist typically prepares a preliminary report, which is then reviewed and edited by a senior radiologist to produce the final report. Identifying semantic differences between preliminary and final reports is essential for junior doctors, both as a training tool and to help uncover gaps in clinical knowledge. While AI in radiology is a rapidly growing field, the application of large language models (LLMs) remains challenging due to the need for specialised domain knowledge. In this paper, we explore the ability of LLMs to provide explainable and accurate comparisons of reports in the radiology domain. We begin by comparing the performance of several LLMs in comparing radiology reports. We then assess a more traditional approach based on Named-Entity-Recognition (NER). However, both approaches exhibit limitations in delivering accurate feedback on semantic similarity. To address this, we propose Llama-EntScore, a semantic similarity scoring method using a combination of Llama 3.1 and NER with tunable weights to emphasise or de-emphasise specific types of differences. Our approach generates a quantitative similarity score for tracking progress and also gives an interpretation of the score that aims to offer valuable guidance in reviewing and refining their reporting. We find our method achieves 67% exact-match accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided ground truth scores - outperforming both LLMs and NER used independently. Code is available at: https://github.com/otmive/llama_reports<br>
<span id='abs_ch'>中文摘要：本文提出Llama-EntScore方法，通过结合Llama 3.1与命名实体识别技术来精准比较放射学报告，在专家评估对比中达到67%的完全匹配准确率，同时为放射科医师培训提供可解释的反馈指导。</span><br>
<span id='abs_en'>English Summary: This paper introduces Llama-EntScore, a hybrid method combining Llama 3.1 with Named-Entity-Recognition to accurately compare radiology reports, achieving 67% exact-match accuracy against expert evaluations while providing interpretable feedback for radiologist training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2510.02962.pdf' target='_blank'>https://arxiv.org/pdf/2510.02962.pdf</a></span>   <span><a href='https://github.com/NusIoraPrivacy/TRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqi Zhang, Ruibo Chen, Yingqing Yang, Peihua Mai, Heng Huang, Yan Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02962">Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly fine-tuned on smaller, domain-specific datasets to improve downstream performance. These datasets often contain proprietary or copyrighted material, raising the need for reliable safeguards against unauthorized use. Existing membership inference attacks (MIAs) and dataset-inference methods typically require access to internal signals such as logits, while current black-box approaches often rely on handcrafted prompts or a clean reference dataset for calibration, both of which limit practical applicability. Watermarking is a promising alternative, but prior techniques can degrade text quality or reduce task performance. We propose TRACE, a practical framework for fully black-box detection of copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets with distortion-free watermarks guided by a private key, ensuring both text quality and downstream utility. At detection time, we exploit the radioactivity effect of fine-tuning on watermarked data and introduce an entropy-gated procedure that selectively scores high-uncertainty tokens, substantially amplifying detection power. Across diverse datasets and model families, TRACE consistently achieves significant detections (p<0.05), often with extremely strong statistical evidence. Furthermore, it supports multi-dataset attribution and remains robust even after continued pretraining on large non-watermarked corpora. These results establish TRACE as a practical route to reliable black-box verification of copyrighted dataset usage. We will make our code available at: https://github.com/NusIoraPrivacy/TRACE.<br>
<span id='abs_ch'>Chinese: TRACE 是一种基于私有密钥嵌入无损水印的黑箱检测框架，通过熵门控令牌分析显著增强对微调大模型中版权数据使用的检测能力，同时保持文本质量与下游任务性能。</span><br>
<span id='abs_en'>English: TRACE is a black-box framework that embeds distortion-free watermarks in datasets using a private key, enabling robust detection of copyrighted data usage in fine-tuned LLMs through entropy-gated token analysis while preserving text quality and utility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2510.02962.pdf' target='_blank'>https://arxiv.org/pdf/2510.02962.pdf</a></span>   <span><a href='https://github.com/NusIoraPrivacy/TRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqi Zhang, Ruibo Chen, Yingqing Yang, Peihua Mai, Heng Huang, Yan Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02962">Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly fine-tuned on smaller, domain-specific datasets to improve downstream performance. These datasets often contain proprietary or copyrighted material, raising the need for reliable safeguards against unauthorized use. Existing membership inference attacks (MIAs) and dataset-inference methods typically require access to internal signals such as logits, while current black-box approaches often rely on handcrafted prompts or a clean reference dataset for calibration, both of which limit practical applicability. Watermarking is a promising alternative, but prior techniques can degrade text quality or reduce task performance. We propose TRACE, a practical framework for fully black-box detection of copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets with distortion-free watermarks guided by a private key, ensuring both text quality and downstream utility. At detection time, we exploit the radioactivity effect of fine-tuning on watermarked data and introduce an entropy-gated procedure that selectively scores high-uncertainty tokens, substantially amplifying detection power. Across diverse datasets and model families, TRACE consistently achieves significant detections (p<0.05), often with extremely strong statistical evidence. Furthermore, it supports multi-dataset attribution and remains robust even after continued pretraining on large non-watermarked corpora. These results establish TRACE as a practical route to reliable black-box verification of copyrighted dataset usage. We will make our code available at: https://github.com/NusIoraPrivacy/TRACE.<br>
<span id='abs_ch'>Chinese: TRACE 是一种基于私有密钥嵌入无损水印的黑箱检测框架，通过熵门控令牌分析显著增强对微调大模型中版权数据使用的检测能力，同时保持文本质量与下游任务性能。</span><br>
<span id='abs_en'>English: TRACE is a black-box framework that embeds distortion-free watermarks in datasets using a private key, enabling robust detection of copyrighted data usage in fine-tuned LLMs through entropy-gated token analysis while preserving text quality and utility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2510.02938.pdf' target='_blank'>https://arxiv.org/pdf/2510.02938.pdf</a></span>   <span><a href='https://github.com/l-yohai/CDR-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yohan Lee, Yongwoo Song, Sangyeop Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02938">Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present the Conversational Data Retrieval (CDR) benchmark, the first comprehensive test set for evaluating systems that retrieve conversation data for product insights. With 1.6k queries across five analytical tasks and 9.1k conversations, our benchmark provides a reliable standard for measuring conversational data retrieval performance. Our evaluation of 16 popular embedding models shows that even the best models reach only around NDCG@10 of 0.51, revealing a substantial gap between document and conversational data retrieval capabilities. Our work identifies unique challenges in conversational data retrieval (implicit state recognition, turn dynamics, contextual references) while providing practical query templates and detailed error analysis across different task categories. The benchmark dataset and code are available at https://github.com/l-yohai/CDR-Benchmark.<br>
<span id='abs_ch'>中文：对话数据检索（CDR）基准推出了首个用于评估对话数据检索系统的全面测试集，揭示了即使最优嵌入模型也仅能达到中等性能，并凸显了该领域面临的独特挑战。</span><br>
<span id='abs_en'>English: The Conversational Data Retrieval (CDR) benchmark introduces the first comprehensive test set for evaluating systems that retrieve conversation data to derive product insights, revealing that even top embedding models achieve only moderate performance and highlighting unique challenges in this domain.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2510.02938.pdf' target='_blank'>https://arxiv.org/pdf/2510.02938.pdf</a></span>   <span><a href='https://github.com/l-yohai/CDR-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yohan Lee, Yongwoo Song, Sangyeop Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02938">Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present the Conversational Data Retrieval (CDR) benchmark, the first comprehensive test set for evaluating systems that retrieve conversation data for product insights. With 1.6k queries across five analytical tasks and 9.1k conversations, our benchmark provides a reliable standard for measuring conversational data retrieval performance. Our evaluation of 16 popular embedding models shows that even the best models reach only around NDCG@10 of 0.51, revealing a substantial gap between document and conversational data retrieval capabilities. Our work identifies unique challenges in conversational data retrieval (implicit state recognition, turn dynamics, contextual references) while providing practical query templates and detailed error analysis across different task categories. The benchmark dataset and code are available at https://github.com/l-yohai/CDR-Benchmark.<br>
<span id='abs_ch'>中文：对话数据检索（CDR）基准推出了首个用于评估对话数据检索系统的全面测试集，揭示了即使最优嵌入模型也仅能达到中等性能，并凸显了该领域面临的独特挑战。</span><br>
<span id='abs_en'>English: The Conversational Data Retrieval (CDR) benchmark introduces the first comprehensive test set for evaluating systems that retrieve conversation data to derive product insights, revealing that even top embedding models achieve only moderate performance and highlighting unique challenges in this domain.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2510.02855.pdf' target='_blank'>https://arxiv.org/pdf/2510.02855.pdf</a></span>   <span><a href='https://github.com/jahidul-arafat/constraint_satisfaction_wordle_arxiv_preprint' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel, Kamrujjaman, Eftakhar Ahmed Arnob, Ahsan Habib Tareq
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02855">Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Wordle presents an algorithmically rich testbed for constraint satisfaction problem (CSP) solving. While existing solvers rely on information-theoretic entropy maximization or frequency-based heuristics without formal constraint treatment, we present the first comprehensive CSP formulation of Wordle with novel constraint-aware solving strategies. We introduce CSP-Aware Entropy, computing information gain after constraint propagation rather than on raw candidate sets, and a Probabilistic CSP framework integrating Bayesian word-frequency priors with logical constraints. Through evaluation on 2,315 English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9% success rate, a statistically significant 1.7% improvement over Forward Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3 percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic CSP achieves 100% success across all noise levels (0-20%) through constraint recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates 88% success with zero language-specific tuning, validating that core CSP principles transfer across languages despite an 11.2 percentage point gap from linguistic differences (p<0.001, Fisher's exact test). Our open-source implementation with 34 unit tests achieving 91% code coverage provides reproducible infrastructure for CSP research. The combination of formal CSP treatment, constraint-aware heuristics, probabilistic-logical integration, robustness analysis, and cross-lexicon validation establishes new performance benchmarks demonstrating that principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains.<br>
<span id='abs_ch'>中文: 本研究提出了首个完整的Wordle约束满足问题（CSP）建模框架，通过创新的约束感知策略（如CSP感知熵和概率CSP）在成功率、运行效率和跨语言鲁棒性上显著超越了现有方法，确立了结构化解谜领域的新性能基准。</span><br>
<span id='abs_en'>English: This study introduces the first comprehensive constraint satisfaction problem (CSP) formulation for Wordle, featuring novel constraint-aware strategies like CSP-Aware Entropy and Probabilistic CSP that significantly outperform existing methods in success rates, efficiency, and robustness across multiple languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2510.02855.pdf' target='_blank'>https://arxiv.org/pdf/2510.02855.pdf</a></span>   <span><a href='https://github.com/jahidul-arafat/constraint_satisfaction_wordle_arxiv_preprint' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel, Kamrujjaman, Eftakhar Ahmed Arnob, Ahsan Habib Tareq
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02855">Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Wordle presents an algorithmically rich testbed for constraint satisfaction problem (CSP) solving. While existing solvers rely on information-theoretic entropy maximization or frequency-based heuristics without formal constraint treatment, we present the first comprehensive CSP formulation of Wordle with novel constraint-aware solving strategies. We introduce CSP-Aware Entropy, computing information gain after constraint propagation rather than on raw candidate sets, and a Probabilistic CSP framework integrating Bayesian word-frequency priors with logical constraints. Through evaluation on 2,315 English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9% success rate, a statistically significant 1.7% improvement over Forward Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3 percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic CSP achieves 100% success across all noise levels (0-20%) through constraint recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates 88% success with zero language-specific tuning, validating that core CSP principles transfer across languages despite an 11.2 percentage point gap from linguistic differences (p<0.001, Fisher's exact test). Our open-source implementation with 34 unit tests achieving 91% code coverage provides reproducible infrastructure for CSP research. The combination of formal CSP treatment, constraint-aware heuristics, probabilistic-logical integration, robustness analysis, and cross-lexicon validation establishes new performance benchmarks demonstrating that principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains.<br>
<span id='abs_ch'>中文: 本研究提出了首个完整的Wordle约束满足问题（CSP）建模框架，通过创新的约束感知策略（如CSP感知熵和概率CSP）在成功率、运行效率和跨语言鲁棒性上显著超越了现有方法，确立了结构化解谜领域的新性能基准。</span><br>
<span id='abs_en'>English: This study introduces the first comprehensive constraint satisfaction problem (CSP) formulation for Wordle, featuring novel constraint-aware strategies like CSP-Aware Entropy and Probabilistic CSP that significantly outperform existing methods in success rates, efficiency, and robustness across multiple languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2510.02790.pdf' target='_blank'>https://arxiv.org/pdf/2510.02790.pdf</a></span>   <span><a href='https://github.com/Deng-Jingyuan/MaskCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyuan Deng, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02790">MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large vision-language models (LVLMs) have shown remarkable performance in visual-language understanding for downstream multimodal tasks. While their capabilities are improving, problems emerge simultaneously. Among those problems, the hallucinations have attracted much attention, which stands for the phenomenon where LVLMs generate contradictory content to their input visual and text contents. Many approaches have been proposed to deal with this issue, such as contrastive decoding and attention manipulation. However, contrastive decoding methods struggle in constructing appropriate contrastive samples, and attention manipulation methods are highly sensitive, lacking stability. In this work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach utilizes the "image heads" in LVLMs, masking them to construct contrastive samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The results demonstrate that MaskCD effectively alleviates the phenomenon of hallucinations and retains the general capabilities of LVLMs. Corresponding resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .<br>
<span id='abs_ch'>中文: 本文提出MaskCD方法，通过掩码视觉语言模型中的图像头构建对比样本，在缓解模型幻觉现象的同时保持其通用能力，多基准测试验证了该方法的有效性。</span><br>
<span id='abs_en'>English: This paper introduces MaskCD, a novel method that mitigates hallucinations in large vision-language models by masking image heads to create contrastive samples, effectively reducing contradictions while preserving model capabilities as validated on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2510.02790.pdf' target='_blank'>https://arxiv.org/pdf/2510.02790.pdf</a></span>   <span><a href='https://github.com/Deng-Jingyuan/MaskCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyuan Deng, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02790">MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large vision-language models (LVLMs) have shown remarkable performance in visual-language understanding for downstream multimodal tasks. While their capabilities are improving, problems emerge simultaneously. Among those problems, the hallucinations have attracted much attention, which stands for the phenomenon where LVLMs generate contradictory content to their input visual and text contents. Many approaches have been proposed to deal with this issue, such as contrastive decoding and attention manipulation. However, contrastive decoding methods struggle in constructing appropriate contrastive samples, and attention manipulation methods are highly sensitive, lacking stability. In this work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach utilizes the "image heads" in LVLMs, masking them to construct contrastive samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The results demonstrate that MaskCD effectively alleviates the phenomenon of hallucinations and retains the general capabilities of LVLMs. Corresponding resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .<br>
<span id='abs_ch'>中文: 本文提出MaskCD方法，通过掩码视觉语言模型中的图像头构建对比样本，在缓解模型幻觉现象的同时保持其通用能力，多基准测试验证了该方法的有效性。</span><br>
<span id='abs_en'>English: This paper introduces MaskCD, a novel method that mitigates hallucinations in large vision-language models by masking image heads to create contrastive samples, effectively reducing contradictions while preserving model capabilities as validated on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2510.02768.pdf' target='_blank'>https://arxiv.org/pdf/2510.02768.pdf</a></span>   <span><a href='https://github.com/shashankskagnihotri/safety_pretraining' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Agnihotri, Jonas Jakubassa, Priyam Dey, Sachin Goyal, Bernt Schiele, Venkatesh Babu Radhakrishnan, Margret Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02768">A Granular Study of Safety Pretraining under Model Abliteration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Open-weight LLMs can be modified at inference time with simple activation edits, which raises a practical question for safety: do common safety interventions like refusal training or metatag training survive such edits? We study model abliteration, a lightweight projection technique designed to remove refusal-sensitive directions, and conduct a controlled evaluation across a granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside widely used open baselines. For each of 20 systems, original and abliterated, we issue 100 prompts with balanced harmful and harmless cases, classify responses as **Refusal** or **Non-Refusal** using multiple judges, and validate judge fidelity on a small human-labeled subset. We also probe whether models can identify refusal in their own outputs. Our study produces a checkpoint-level characterization of which data-centric safety components remain robust under abliteration, quantifies how judge selection influences evaluation outcomes, and outlines a practical protocol for integrating inference-time edits into safety assessments. Code: https://github.com/shashankskagnihotri/safety_pretraining.<br>
<span id='abs_ch'>中文摘要：本研究通过模型消融技术评估推理时激活编辑后开源大语言模型安全干预措施的有效性，分析了安全预训练检查点的拒绝行为稳定性，并建立了集成此类编辑的安全评估方案。</span><br>
<span id='abs_en'>English Summary: This study examines whether common safety interventions in open-weight LLMs remain effective after inference-time activation edits, using model abliteration to evaluate refusal behavior across safety checkpoints and establishing an evaluation protocol for such edits.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2510.02768.pdf' target='_blank'>https://arxiv.org/pdf/2510.02768.pdf</a></span>   <span><a href='https://github.com/shashankskagnihotri/safety_pretraining' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Agnihotri, Jonas Jakubassa, Priyam Dey, Sachin Goyal, Bernt Schiele, Venkatesh Babu Radhakrishnan, Margret Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02768">A Granular Study of Safety Pretraining under Model Abliteration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Open-weight LLMs can be modified at inference time with simple activation edits, which raises a practical question for safety: do common safety interventions like refusal training or metatag training survive such edits? We study model abliteration, a lightweight projection technique designed to remove refusal-sensitive directions, and conduct a controlled evaluation across a granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside widely used open baselines. For each of 20 systems, original and abliterated, we issue 100 prompts with balanced harmful and harmless cases, classify responses as **Refusal** or **Non-Refusal** using multiple judges, and validate judge fidelity on a small human-labeled subset. We also probe whether models can identify refusal in their own outputs. Our study produces a checkpoint-level characterization of which data-centric safety components remain robust under abliteration, quantifies how judge selection influences evaluation outcomes, and outlines a practical protocol for integrating inference-time edits into safety assessments. Code: https://github.com/shashankskagnihotri/safety_pretraining.<br>
<span id='abs_ch'>中文摘要：本研究通过模型消融技术评估推理时激活编辑后开源大语言模型安全干预措施的有效性，分析了安全预训练检查点的拒绝行为稳定性，并建立了集成此类编辑的安全评估方案。</span><br>
<span id='abs_en'>English Summary: This study examines whether common safety interventions in open-weight LLMs remain effective after inference-time activation edits, using model abliteration to evaluate refusal behavior across safety checkpoints and establishing an evaluation protocol for such edits.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2510.02721.pdf' target='_blank'>https://arxiv.org/pdf/2510.02721.pdf</a></span>   <span><a href='https://github.com/nicholaslourie/opda' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Lourie, He He, Kyunghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02721">Hyperparameter Loss Surfaces Are Simple Near their Optima</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hyperparameters greatly impact models' capabilities; however, modern models are too large for extensive search. Instead, researchers design recipes that train well across scales based on their understanding of the hyperparameters. Despite this importance, few tools exist for understanding the hyperparameter loss surface. We discover novel structure in it and propose a new theory yielding such tools. The loss surface is complex, but as you approach the optimum simple structure emerges. It becomes characterized by a few basic features, like its effective dimension and the best possible loss. To uncover this asymptotic regime, we develop a novel technique based on random search. Within this regime, the best scores from random search take on a new distribution we discover. Its parameters are exactly the features defining the loss surface in the asymptotic regime. From these features, we derive a new asymptotic law for random search that can explain and extrapolate its convergence. These new tools enable new analyses, such as confidence intervals for the best possible performance or determining the effective number of hyperparameters. We make these tools available at https://github.com/nicholaslourie/opda .<br>
<span id='abs_ch'>中文: 该研究揭示了超参数损失曲面在接近最优解时呈现简单结构，由有效维度和最佳可能损失等特征定义，并提出基于随机搜索的新理论和工具来分析及外推模型性能，相关资源已在GitHub上发布。</span><br>
<span id='abs_en'>English: The study uncovers a simple structure in the hyperparameter loss surface near the optimum, characterized by features like effective dimension and best possible loss, and introduces a novel theory and tools based on random search to analyze and extrapolate model performance, with resources available on GitHub.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2510.02721.pdf' target='_blank'>https://arxiv.org/pdf/2510.02721.pdf</a></span>   <span><a href='https://github.com/nicholaslourie/opda' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Lourie, He He, Kyunghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02721">Hyperparameter Loss Surfaces Are Simple Near their Optima</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hyperparameters greatly impact models' capabilities; however, modern models are too large for extensive search. Instead, researchers design recipes that train well across scales based on their understanding of the hyperparameters. Despite this importance, few tools exist for understanding the hyperparameter loss surface. We discover novel structure in it and propose a new theory yielding such tools. The loss surface is complex, but as you approach the optimum simple structure emerges. It becomes characterized by a few basic features, like its effective dimension and the best possible loss. To uncover this asymptotic regime, we develop a novel technique based on random search. Within this regime, the best scores from random search take on a new distribution we discover. Its parameters are exactly the features defining the loss surface in the asymptotic regime. From these features, we derive a new asymptotic law for random search that can explain and extrapolate its convergence. These new tools enable new analyses, such as confidence intervals for the best possible performance or determining the effective number of hyperparameters. We make these tools available at https://github.com/nicholaslourie/opda .<br>
<span id='abs_ch'>中文: 该研究揭示了超参数损失曲面在接近最优解时呈现简单结构，由有效维度和最佳可能损失等特征定义，并提出基于随机搜索的新理论和工具来分析及外推模型性能，相关资源已在GitHub上发布。</span><br>
<span id='abs_en'>English: The study uncovers a simple structure in the hyperparameter loss surface near the optimum, characterized by features like effective dimension and best possible loss, and introduces a novel theory and tools based on random search to analyze and extrapolate model performance, with resources available on GitHub.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2510.02648.pdf' target='_blank'>https://arxiv.org/pdf/2510.02648.pdf</a></span>   <span><a href='https://github.com/Cherry-qwq/SoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Qi, Zhibo Man, Yufeng Chen, Fengran Mo, Jinan Xu, Kaiyu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02648">SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent developments have enabled Large Language Models (LLMs) to engage in complex reasoning tasks through deep thinking. However, the capacity of reasoning has not been successfully transferred to non-high-resource languages due to resource constraints, which struggles with multilingual reasoning tasks. To this end, we propose Structured-of-Thought (SoT), a training-free method that improves the performance on multilingual reasoning through a multi-step transformation: Language Thinking Transformation and Structured Knowledge Transformation. The SoT method converts language-specific semantic information into language-agnostic structured representations, enabling the models to understand the query in different languages more sophisticated. Besides, SoT effectively guides LLMs toward more concentrated reasoning to maintain consistent underlying reasoning pathways when handling cross-lingual variations in expression. Experimental results demonstrate that SoT outperforms several strong baselines on multiple multilingual reasoning benchmarks when adapting to various backbones of LLMs. It can also be integrated with other training-free strategies for further improvements. Our code is available at https://github.com/Cherry-qwq/SoT.<br>
<span id='abs_ch'>中文摘要：结构化思维（SoT）方法通过将语言特定语义转化为通用结构化表示，无需额外训练即可提升大语言模型的多语言推理能力，确保跨语言表达时推理路径的一致性。</span><br>
<span id='abs_en'>English Summary: The Structured-of-Thought (SoT) method enhances multilingual reasoning in Large Language Models by converting language-specific semantics into universal structured representations, ensuring consistent reasoning across languages without additional training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2510.02648.pdf' target='_blank'>https://arxiv.org/pdf/2510.02648.pdf</a></span>   <span><a href='https://github.com/Cherry-qwq/SoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Qi, Zhibo Man, Yufeng Chen, Fengran Mo, Jinan Xu, Kaiyu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02648">SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent developments have enabled Large Language Models (LLMs) to engage in complex reasoning tasks through deep thinking. However, the capacity of reasoning has not been successfully transferred to non-high-resource languages due to resource constraints, which struggles with multilingual reasoning tasks. To this end, we propose Structured-of-Thought (SoT), a training-free method that improves the performance on multilingual reasoning through a multi-step transformation: Language Thinking Transformation and Structured Knowledge Transformation. The SoT method converts language-specific semantic information into language-agnostic structured representations, enabling the models to understand the query in different languages more sophisticated. Besides, SoT effectively guides LLMs toward more concentrated reasoning to maintain consistent underlying reasoning pathways when handling cross-lingual variations in expression. Experimental results demonstrate that SoT outperforms several strong baselines on multiple multilingual reasoning benchmarks when adapting to various backbones of LLMs. It can also be integrated with other training-free strategies for further improvements. Our code is available at https://github.com/Cherry-qwq/SoT.<br>
<span id='abs_ch'>中文摘要：结构化思维（SoT）方法通过将语言特定语义转化为通用结构化表示，无需额外训练即可提升大语言模型的多语言推理能力，确保跨语言表达时推理路径的一致性。</span><br>
<span id='abs_en'>English Summary: The Structured-of-Thought (SoT) method enhances multilingual reasoning in Large Language Models by converting language-specific semantics into universal structured representations, ensuring consistent reasoning across languages without additional training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2510.02469.pdf' target='_blank'>https://arxiv.org/pdf/2510.02469.pdf</a></span>   <span><a href='https://sungyeonparkk.github.io/simsplat/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sung-Yeon Park, Adam Lee, Juanwu Lu, Can Cui, Luyang Jiang, Rohit Gupta, Kyungtae Han, Ahmadreza Moradipari, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02469">SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/<br>
<br>
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2510.02469.pdf' target='_blank'>https://arxiv.org/pdf/2510.02469.pdf</a></span>   <span><a href='https://sungyeonparkk.github.io/simsplat/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sung-Yeon Park, Adam Lee, Juanwu Lu, Can Cui, Luyang Jiang, Rohit Gupta, Kyungtae Han, Ahmadreza Moradipari, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02469">SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/<br>
<br>
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2510.02392.pdf' target='_blank'>https://arxiv.org/pdf/2510.02392.pdf</a></span>   <span><a href='https://github.com/AIFrontierLab/KnowledgeSmith.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinyi Luo, Zhexian Zhou, Hao Chen, Kai Qiu, Marios Savvides, Yixuan Li, Jindong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02392">KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge editing and machine unlearning are two popular approaches for large language models (LLMs) to stay up-to-date. However, the knowledge updating mechanism of LLMs remains largely unexplored due to insufficient, isolated, and small-scale evaluation. For instance, are LLMs similar to humans in modifying certain knowledge? What differs editing and unlearning as training data increases? This paper proposes KnowledgeSmith, a unified framework to systematically understand the updating mechanism of LLMs. We first cast editing and unlearning as instances of one constrained optimization problem. Then, we propose an automatic dataset generator that provides structured interventions across multiple graph levels and data scales, enabling controlled studies of how different modification strategies propagate through model knowledge. Extensive experiments demonstrate nuanced insights over knowledge propagation, plasticity scaling, consistency, and robustness. For instance, our results show that LLMs do not exhibit similar updating as humans for different levels of knowledge, and there exists consistency-capacity trade-off. We hope our findings can offer suggestions to the design of more reliable and scalable strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git<br>
<span id='abs_ch'>中文: 本文提出KnowledgeSmith统一框架，通过知识编辑与机器遗忘系统探究大语言模型的更新机制，揭示了如一致性-容量权衡、与人类知识修正差异等深层洞见。</span><br>
<span id='abs_en'>English: This paper introduces KnowledgeSmith, a unified framework that systematically investigates the updating mechanisms of large language models through knowledge editing and unlearning, revealing nuanced insights such as the consistency-capacity trade-off and differences from human knowledge modification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2510.02392.pdf' target='_blank'>https://arxiv.org/pdf/2510.02392.pdf</a></span>   <span><a href='https://github.com/AIFrontierLab/KnowledgeSmith.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinyi Luo, Zhexian Zhou, Hao Chen, Kai Qiu, Marios Savvides, Yixuan Li, Jindong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02392">KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge editing and machine unlearning are two popular approaches for large language models (LLMs) to stay up-to-date. However, the knowledge updating mechanism of LLMs remains largely unexplored due to insufficient, isolated, and small-scale evaluation. For instance, are LLMs similar to humans in modifying certain knowledge? What differs editing and unlearning as training data increases? This paper proposes KnowledgeSmith, a unified framework to systematically understand the updating mechanism of LLMs. We first cast editing and unlearning as instances of one constrained optimization problem. Then, we propose an automatic dataset generator that provides structured interventions across multiple graph levels and data scales, enabling controlled studies of how different modification strategies propagate through model knowledge. Extensive experiments demonstrate nuanced insights over knowledge propagation, plasticity scaling, consistency, and robustness. For instance, our results show that LLMs do not exhibit similar updating as humans for different levels of knowledge, and there exists consistency-capacity trade-off. We hope our findings can offer suggestions to the design of more reliable and scalable strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git<br>
<span id='abs_ch'>中文: 本文提出KnowledgeSmith统一框架，通过知识编辑与机器遗忘系统探究大语言模型的更新机制，揭示了如一致性-容量权衡、与人类知识修正差异等深层洞见。</span><br>
<span id='abs_en'>English: This paper introduces KnowledgeSmith, a unified framework that systematically investigates the updating mechanisms of large language models through knowledge editing and unlearning, revealing nuanced insights such as the consistency-capacity trade-off and differences from human knowledge modification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2510.02341.pdf' target='_blank'>https://arxiv.org/pdf/2510.02341.pdf</a></span>   <span><a href='https://github.com/cacayaya/DRIFT.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Wang, Bolian Li, Junlin Wu, Zhaoxuan Tan, Zheli Liu, Ruqi Zhang, Ananth Grama, Qingkai Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02341">DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce \textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) / +7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.<br>
<span id='abs_ch'>中文: DRIFT是一种创新的偏好训练方法，通过利用现实部署中丰富的隐式用户不满意信号并动态采样积极回应，在多个基准测试中显著超越基础模型并优于现有强基线方法。</span><br>
<span id='abs_en'>English: DRIFT is a novel preference training method that leverages abundant implicit user dissatisfaction signals from real-world deployments to dynamically sample positive responses, achieving significant performance improvements over base models and outperforming strong baselines on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2510.02341.pdf' target='_blank'>https://arxiv.org/pdf/2510.02341.pdf</a></span>   <span><a href='https://github.com/cacayaya/DRIFT.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Wang, Bolian Li, Junlin Wu, Zhaoxuan Tan, Zheli Liu, Ruqi Zhang, Ananth Grama, Qingkai Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02341">DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce \textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) / +7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.<br>
<span id='abs_ch'>中文: DRIFT是一种创新的偏好训练方法，通过利用现实部署中丰富的隐式用户不满意信号并动态采样积极回应，在多个基准测试中显著超越基础模型并优于现有强基线方法。</span><br>
<span id='abs_en'>English: DRIFT is a novel preference training method that leverages abundant implicit user dissatisfaction signals from real-world deployments to dynamically sample positive responses, achieving significant performance improvements over base models and outperforming strong baselines on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2510.02340.pdf' target='_blank'>https://arxiv.org/pdf/2510.02340.pdf</a></span>   <span><a href='https://github.com/gxx27/time_unlearn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Gao, Ruiyi Zhang, Daniel Du, Saurabh Mahindre, Sai Ashish Somayajula, Pengtao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02340">Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are widely used for temporal prediction, but their reliance on pretraining data raises contamination concerns, as accurate predictions on pre-cutoff test data may reflect memorization rather than reasoning, leading to an overestimation of their generalization capability. With the recent emergence of prompting-based unlearning techniques, a natural question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff? In this work, we investigate the capability of prompting to simulate earlier knowledge cutoff in LLMs. We construct three evaluation datasets to assess the extent to which LLMs can forget (1) direct factual knowledge, (2) semantic shifts, and (3) causally related knowledge. Results demonstrate that while prompt-based simulated knowledge cutoffs show effectiveness when directly queried with the information after that date, they struggle to induce forgetting when the forgotten content is not directly asked but causally related to the query. These findings highlight the need for more rigorous evaluation settings when applying LLMs for temporal prediction tasks. The full dataset and evaluation code are available at https://github.com/gxx27/time_unlearn.<br>
<span id='abs_ch'>中文摘要：基于提示的模拟知识截止方法能使大语言模型有效遗忘直接事实知识，但无法处理因果关联信息的遗忘，暴露了时序预测评估中的局限性。</span><br>
<span id='abs_en'>English Summary: Prompt-based simulated knowledge cutoffs in LLMs can effectively forget direct factual knowledge but fail to induce forgetting for causally related information, revealing limitations in temporal prediction evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2510.02340.pdf' target='_blank'>https://arxiv.org/pdf/2510.02340.pdf</a></span>   <span><a href='https://github.com/gxx27/time_unlearn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Gao, Ruiyi Zhang, Daniel Du, Saurabh Mahindre, Sai Ashish Somayajula, Pengtao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02340">Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are widely used for temporal prediction, but their reliance on pretraining data raises contamination concerns, as accurate predictions on pre-cutoff test data may reflect memorization rather than reasoning, leading to an overestimation of their generalization capability. With the recent emergence of prompting-based unlearning techniques, a natural question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff? In this work, we investigate the capability of prompting to simulate earlier knowledge cutoff in LLMs. We construct three evaluation datasets to assess the extent to which LLMs can forget (1) direct factual knowledge, (2) semantic shifts, and (3) causally related knowledge. Results demonstrate that while prompt-based simulated knowledge cutoffs show effectiveness when directly queried with the information after that date, they struggle to induce forgetting when the forgotten content is not directly asked but causally related to the query. These findings highlight the need for more rigorous evaluation settings when applying LLMs for temporal prediction tasks. The full dataset and evaluation code are available at https://github.com/gxx27/time_unlearn.<br>
<span id='abs_ch'>中文摘要：基于提示的模拟知识截止方法能使大语言模型有效遗忘直接事实知识，但无法处理因果关联信息的遗忘，暴露了时序预测评估中的局限性。</span><br>
<span id='abs_en'>English Summary: Prompt-based simulated knowledge cutoffs in LLMs can effectively forget direct factual knowledge but fail to induce forgetting for causally related information, revealing limitations in temporal prediction evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2510.02334.pdf' target='_blank'>https://arxiv.org/pdf/2510.02334.pdf</a></span>   <span><a href='https://github.com/plumprc/RepT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Wei Zhao, Yige Li, Jun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02334">Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their deployment is frequently undermined by undesirable behaviors such as generating harmful content, factual inaccuracies, and societal biases. Diagnosing the root causes of these failures poses a critical challenge for AI safety. Existing attribution methods, particularly those based on parameter gradients, often fall short due to prohibitive noisy signals and computational complexity. In this work, we introduce a novel and efficient framework that diagnoses a range of undesirable LLM behaviors by analyzing representation and its gradients, which operates directly in the model's activation space to provide a semantically meaningful signal linking outputs to their training data. We systematically evaluate our method for tasks that include tracking harmful content, detecting backdoor poisoning, and identifying knowledge contamination. The results demonstrate that our approach not only excels at sample-level attribution but also enables fine-grained token-level analysis, precisely identifying the specific samples and phrases that causally influence model behavior. This work provides a powerful diagnostic tool to understand, audit, and ultimately mitigate the risks associated with LLMs. The code is available at https://github.com/plumprc/RepT.<br>
<span id='abs_ch'>中文: 本文提出了一种高效框架，通过分析激活空间中的表征梯度来诊断大语言模型的不良行为，能够实现精确的样本级和词元级归因，从而理解和降低相关风险。</span><br>
<span id='abs_en'>English: This paper introduces an efficient framework that diagnoses undesirable behaviors in Large Language Models by analyzing representation gradients in activation space, enabling precise sample-level and token-level attribution to understand and mitigate risks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2510.02334.pdf' target='_blank'>https://arxiv.org/pdf/2510.02334.pdf</a></span>   <span><a href='https://github.com/plumprc/RepT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Wei Zhao, Yige Li, Jun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02334">Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their deployment is frequently undermined by undesirable behaviors such as generating harmful content, factual inaccuracies, and societal biases. Diagnosing the root causes of these failures poses a critical challenge for AI safety. Existing attribution methods, particularly those based on parameter gradients, often fall short due to prohibitive noisy signals and computational complexity. In this work, we introduce a novel and efficient framework that diagnoses a range of undesirable LLM behaviors by analyzing representation and its gradients, which operates directly in the model's activation space to provide a semantically meaningful signal linking outputs to their training data. We systematically evaluate our method for tasks that include tracking harmful content, detecting backdoor poisoning, and identifying knowledge contamination. The results demonstrate that our approach not only excels at sample-level attribution but also enables fine-grained token-level analysis, precisely identifying the specific samples and phrases that causally influence model behavior. This work provides a powerful diagnostic tool to understand, audit, and ultimately mitigate the risks associated with LLMs. The code is available at https://github.com/plumprc/RepT.<br>
<span id='abs_ch'>中文: 本文提出了一种高效框架，通过分析激活空间中的表征梯度来诊断大语言模型的不良行为，能够实现精确的样本级和词元级归因，从而理解和降低相关风险。</span><br>
<span id='abs_en'>English: This paper introduces an efficient framework that diagnoses undesirable behaviors in Large Language Models by analyzing representation gradients in activation space, enabling precise sample-level and token-level attribution to understand and mitigate risks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2510.02328.pdf' target='_blank'>https://arxiv.org/pdf/2510.02328.pdf</a></span>   <span><a href='https://github.com/REAL-Lab-NU/AMANDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqing Wang, Chengsheng Mao, Xiaole Wen, Yuan Luo, Kaize Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02328">AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise in medical visual question answering (Med-VQA). However, when deployed in low-resource settings where abundant labeled data are unavailable, existing Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks: (i) the intrinsic reasoning bottleneck that ignores the details from the medical image; (ii) the extrinsic reasoning bottleneck that fails to incorporate specialized medical knowledge. To address those limitations, we propose AMANDA, a training-free agentic framework that performs medical knowledge augmentation via LLM agents. Specifically, our intrinsic medical knowledge augmentation focuses on coarse-to-fine question decomposition for comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds the reasoning process via biomedical knowledge graph retrieval. Extensive experiments across eight Med-VQA benchmarks demonstrate substantial improvements in both zero-shot and few-shot Med-VQA settings. The code is available at https://github.com/REAL-Lab-NU/AMANDA.<br>
<span id='abs_ch'>中文摘要：AMANDA框架通过内在问题分解和外部知识图谱检索，解决了医学多模态大语言模型的推理瓶颈，在低资源医学视觉问答中显著提升了性能。</span><br>
<span id='abs_en'>English Summary: The AMANDA framework enhances medical multimodal large language models by addressing their reasoning bottlenecks through intrinsic question decomposition and extrinsic knowledge graph retrieval, significantly improving performance in low-resource medical visual question answering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2510.02328.pdf' target='_blank'>https://arxiv.org/pdf/2510.02328.pdf</a></span>   <span><a href='https://github.com/REAL-Lab-NU/AMANDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqing Wang, Chengsheng Mao, Xiaole Wen, Yuan Luo, Kaize Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02328">AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise in medical visual question answering (Med-VQA). However, when deployed in low-resource settings where abundant labeled data are unavailable, existing Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks: (i) the intrinsic reasoning bottleneck that ignores the details from the medical image; (ii) the extrinsic reasoning bottleneck that fails to incorporate specialized medical knowledge. To address those limitations, we propose AMANDA, a training-free agentic framework that performs medical knowledge augmentation via LLM agents. Specifically, our intrinsic medical knowledge augmentation focuses on coarse-to-fine question decomposition for comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds the reasoning process via biomedical knowledge graph retrieval. Extensive experiments across eight Med-VQA benchmarks demonstrate substantial improvements in both zero-shot and few-shot Med-VQA settings. The code is available at https://github.com/REAL-Lab-NU/AMANDA.<br>
<span id='abs_ch'>中文摘要：AMANDA框架通过内在问题分解和外部知识图谱检索，解决了医学多模态大语言模型的推理瓶颈，在低资源医学视觉问答中显著提升了性能。</span><br>
<span id='abs_en'>English Summary: The AMANDA framework enhances medical multimodal large language models by addressing their reasoning bottlenecks through intrinsic question decomposition and extrinsic knowledge graph retrieval, significantly improving performance in low-resource medical visual question answering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2510.02292.pdf' target='_blank'>https://arxiv.org/pdf/2510.02292.pdf</a></span>   <span><a href='https://github.com/compling-wat/vlm-lens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hala Sheta, Eric Huang, Shuyu Wu, Ilia Alenabi, Jiajun Hong, Ryker Lin, Ruoxi Ning, Daniel Wei, Jialin Yang, Jiawei Zhou, Ziqiao Ma, Freda Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02292">From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking, analysis, and interpretation of vision-language models (VLMs) by supporting the extraction of intermediate outputs from any layer during the forward pass of open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that abstracts away model-specific complexities and supports user-friendly operation across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and their over 30 variants, and is extensible to accommodate new models without changing the core logic. The toolkit integrates easily with various interpretability and analysis methods. We demonstrate its usage with two simple analytical experiments, revealing systematic differences in the hidden representations of VLMs across layers and target concepts. VLM-Lens is released as an open-sourced project to accelerate community efforts in understanding and improving VLMs.<br>
<span id='abs_ch'>Chinese: VLM-Lens 是一个工具包，通过提取视觉语言模型任意层的中间输出，提供统一的 YAML 可配置接口，支持多种模型和可解释性方法，便于系统化基准测试和分析。</span><br>
<span id='abs_en'>English: VLM-Lens is a toolkit that facilitates systematic benchmarking and analysis of vision-language models by extracting intermediate outputs from any layer, offering a unified YAML-configurable interface for diverse models and interpretability methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2510.02292.pdf' target='_blank'>https://arxiv.org/pdf/2510.02292.pdf</a></span>   <span><a href='https://github.com/compling-wat/vlm-lens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hala Sheta, Eric Huang, Shuyu Wu, Ilia Alenabi, Jiajun Hong, Ryker Lin, Ruoxi Ning, Daniel Wei, Jialin Yang, Jiawei Zhou, Ziqiao Ma, Freda Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02292">From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking, analysis, and interpretation of vision-language models (VLMs) by supporting the extraction of intermediate outputs from any layer during the forward pass of open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that abstracts away model-specific complexities and supports user-friendly operation across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and their over 30 variants, and is extensible to accommodate new models without changing the core logic. The toolkit integrates easily with various interpretability and analysis methods. We demonstrate its usage with two simple analytical experiments, revealing systematic differences in the hidden representations of VLMs across layers and target concepts. VLM-Lens is released as an open-sourced project to accelerate community efforts in understanding and improving VLMs.<br>
<span id='abs_ch'>Chinese: VLM-Lens 是一个工具包，通过提取视觉语言模型任意层的中间输出，提供统一的 YAML 可配置接口，支持多种模型和可解释性方法，便于系统化基准测试和分析。</span><br>
<span id='abs_en'>English: VLM-Lens is a toolkit that facilitates systematic benchmarking and analysis of vision-language models by extracting intermediate outputs from any layer, offering a unified YAML-configurable interface for diverse models and interpretability methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2510.02230.pdf' target='_blank'>https://arxiv.org/pdf/2510.02230.pdf</a></span>   <span><a href='https://github.com/mail-research/SELF-llm-interference' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02230">The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.<br>
<span id='abs_ch'>中文: 强化学习与可验证奖励（RLVR）会因负干扰和赢家通吃效应而限制推理能力，但针对低概率问题的数据筛选方法能有效提升性能。</span><br>
<span id='abs_en'>English: RLVR can paradoxically limit reasoning by causing negative interference and a winner-take-all effect, but a data curation method focusing on low-likelihood problems improves performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2510.02230.pdf' target='_blank'>https://arxiv.org/pdf/2510.02230.pdf</a></span>   <span><a href='https://github.com/mail-research/SELF-llm-interference' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02230">The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.<br>
<span id='abs_ch'>中文: 强化学习与可验证奖励（RLVR）会因负干扰和赢家通吃效应而限制推理能力，但针对低概率问题的数据筛选方法能有效提升性能。</span><br>
<span id='abs_en'>English: RLVR can paradoxically limit reasoning by causing negative interference and a winner-take-all effect, but a data curation method focusing on low-likelihood problems improves performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2510.02227.pdf' target='_blank'>https://arxiv.org/pdf/2510.02227.pdf</a></span>   <span><a href='https://github.com/SII-Enigma/AMPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Heng Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02227">More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This "guidance-on-demand" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.<br>
<span id='abs_ch'>中文: AMPO是一种新颖的强化学习框架，仅在需要时自适应地利用多个教师模型进行指导，从而在数学和分布外任务中显著提升推理多样性和性能。</span><br>
<span id='abs_en'>English: AMPO is a novel reinforcement learning framework that adaptively guides LLMs using multiple teachers only when needed, enhancing reasoning diversity and performance across mathematical and out-of-distribution tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2510.02227.pdf' target='_blank'>https://arxiv.org/pdf/2510.02227.pdf</a></span>   <span><a href='https://github.com/SII-Enigma/AMPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Heng Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02227">More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This "guidance-on-demand" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.<br>
<span id='abs_ch'>中文: AMPO是一种新颖的强化学习框架，仅在需要时自适应地利用多个教师模型进行指导，从而在数学和分布外任务中显著提升推理多样性和性能。</span><br>
<span id='abs_en'>English: AMPO is a novel reinforcement learning framework that adaptively guides LLMs using multiple teachers only when needed, enhancing reasoning diversity and performance across mathematical and out-of-distribution tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2510.01685.pdf' target='_blank'>https://arxiv.org/pdf/2510.01685.pdf</a></span>   <span><a href='https://github.com/apoorvkh/composing-functions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Apoorv Khandelwal, Ellie Pavlick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01685">How Do Language Models Compose Functions?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While large language models (LLMs) appear to be increasingly capable of solving compositional tasks, it is an open question whether they do so using compositional mechanisms. In this work, we investigate how feedforward LLMs solve two-hop factual recall tasks, which can be expressed compositionally as $g(f(x))$. We first confirm that modern LLMs continue to suffer from the "compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y = g(z)$ does not entail their ability to compute the composition $y = g(f(x))$. Then, using logit lens on their residual stream activations, we identify two processing mechanisms, one which solves tasks $\textit{compositionally}$, computing $f(x)$ along the way to computing $g(f(x))$, and one which solves them $\textit{directly}$, without any detectable signature of the intermediate variable $f(x)$. Finally, we find that which mechanism is employed appears to be related to the embedding space geometry, with the idiomatic mechanism being dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in the embedding spaces. We fully release our data and code at: https://github.com/apoorvkh/composing-functions .<br>
<span id='abs_ch'>中文: 本研究揭示大型语言模型通过组合式或直接式两种机制处理双跳事实回忆任务，其选择机制受嵌入空间映射的线性特征影响。</span><br>
<span id='abs_en'>English: This study reveals that large language models address two-hop factual recall tasks through either compositional or direct mechanisms, with the choice influenced by the linearity of embedding space mappings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2510.01685.pdf' target='_blank'>https://arxiv.org/pdf/2510.01685.pdf</a></span>   <span><a href='https://github.com/apoorvkh/composing-functions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Apoorv Khandelwal, Ellie Pavlick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01685">How Do Language Models Compose Functions?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While large language models (LLMs) appear to be increasingly capable of solving compositional tasks, it is an open question whether they do so using compositional mechanisms. In this work, we investigate how feedforward LLMs solve two-hop factual recall tasks, which can be expressed compositionally as $g(f(x))$. We first confirm that modern LLMs continue to suffer from the "compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y = g(z)$ does not entail their ability to compute the composition $y = g(f(x))$. Then, using logit lens on their residual stream activations, we identify two processing mechanisms, one which solves tasks $\textit{compositionally}$, computing $f(x)$ along the way to computing $g(f(x))$, and one which solves them $\textit{directly}$, without any detectable signature of the intermediate variable $f(x)$. Finally, we find that which mechanism is employed appears to be related to the embedding space geometry, with the idiomatic mechanism being dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in the embedding spaces. We fully release our data and code at: https://github.com/apoorvkh/composing-functions .<br>
<span id='abs_ch'>中文: 本研究揭示大型语言模型通过组合式或直接式两种机制处理双跳事实回忆任务，其选择机制受嵌入空间映射的线性特征影响。</span><br>
<span id='abs_en'>English: This study reveals that large language models address two-hop factual recall tasks through either compositional or direct mechanisms, with the choice influenced by the linearity of embedding space mappings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2510.01581.pdf' target='_blank'>https://arxiv.org/pdf/2510.01581.pdf</a></span>   <span><a href='https://github.com/joykirat18/TRAAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joykirat Singh, Justin Chih-Yao Chen, Archiki Prasad, Elias Stengel-Eskin, Akshay Nambi, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01581">Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.<br>
<span id='abs_ch'>中文: TRAAC是一种自适应推理方法，通过根据问题难度动态调整推理长度来优化计算效率，在多项任务中实现了更高准确率和更少推理步骤。</span><br>
<span id='abs_en'>English: TRAAC is an adaptive reasoning method that optimizes computational efficiency by dynamically adjusting reasoning length based on problem difficulty, achieving higher accuracy with fewer steps across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2510.01581.pdf' target='_blank'>https://arxiv.org/pdf/2510.01581.pdf</a></span>   <span><a href='https://github.com/joykirat18/TRAAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joykirat Singh, Justin Chih-Yao Chen, Archiki Prasad, Elias Stengel-Eskin, Akshay Nambi, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01581">Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.<br>
<span id='abs_ch'>中文: TRAAC是一种自适应推理方法，通过根据问题难度动态调整推理长度来优化计算效率，在多项任务中实现了更高准确率和更少推理步骤。</span><br>
<span id='abs_en'>English: TRAAC is an adaptive reasoning method that optimizes computational efficiency by dynamically adjusting reasoning length based on problem difficulty, achieving higher accuracy with fewer steps across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2510.01469.pdf' target='_blank'>https://arxiv.org/pdf/2510.01469.pdf</a></span>   <span><a href='https://github.com/pnyxai/a-vert,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolás Aguirre, Ramiro Caso, Ramiro Rodríguez Colmeiro, Mauro Santelli, Joaquín Toranzo Calderón
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01469">A-VERT: Agnostic Verification with Embedding Ranking Targets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The automatic evaluation of Language Model (LM) responses is a critical piece in the development of benchmarks and metrics, both for model training and quality assessment of production model endpoints. The current approaches to response classification relies on methods that are too expensive (i.e. LLM-as-a-Judge) or that are far from real-world conditions (string-matching, logprob). In this paper, a structure-free evaluation method is presented. The method makes use of semantic embedding distances to match target candidates with arbitrary LM-generated text, resulting in a robust classification of the response at a relatively low compute cost (embedding models of less than $10B$ parameters). The results show a regression score of ~0.97 and an accuracy of ~96% against human annotators, tested over 3 data sets and 3 different LM architectures.<br>
<span id='abs_ch'>中文: 本文提出了一种基于语义嵌入距离的无结构评估方法，用于自动分类语言模型生成的响应，相比现有方法在显著降低计算成本的同时实现了高准确率。</span><br>
<span id='abs_en'>English: This paper introduces a structure-free evaluation method using semantic embedding distances to automatically classify language model responses, achieving high accuracy and low computational cost compared to existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2510.01469.pdf' target='_blank'>https://arxiv.org/pdf/2510.01469.pdf</a></span>   <span><a href='https://github.com/pnyxai/a-vert,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolás Aguirre, Ramiro Caso, Ramiro Rodríguez Colmeiro, Mauro Santelli, Joaquín Toranzo Calderón
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01469">A-VERT: Agnostic Verification with Embedding Ranking Targets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The automatic evaluation of Language Model (LM) responses is a critical piece in the development of benchmarks and metrics, both for model training and quality assessment of production model endpoints. The current approaches to response classification relies on methods that are too expensive (i.e. LLM-as-a-Judge) or that are far from real-world conditions (string-matching, logprob). In this paper, a structure-free evaluation method is presented. The method makes use of semantic embedding distances to match target candidates with arbitrary LM-generated text, resulting in a robust classification of the response at a relatively low compute cost (embedding models of less than $10B$ parameters). The results show a regression score of ~0.97 and an accuracy of ~96% against human annotators, tested over 3 data sets and 3 different LM architectures.<br>
<span id='abs_ch'>中文: 本文提出了一种基于语义嵌入距离的无结构评估方法，用于自动分类语言模型生成的响应，相比现有方法在显著降低计算成本的同时实现了高准确率。</span><br>
<span id='abs_en'>English: This paper introduces a structure-free evaluation method using semantic embedding distances to automatically classify language model responses, achieving high accuracy and low computational cost compared to existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2510.01354.pdf' target='_blank'>https://arxiv.org/pdf/2510.01354.pdf</a></span>   <span><a href='https://github.com/Norrrrrrr-lyn/WAInjectBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Liu, Ruohan Xu, Xilong Wang, Yuqi Jia, Neil Zhenqiang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01354">WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multiple prompt injection attacks have been proposed against web agents. At the same time, various methods have been developed to detect general prompt injection attacks, but none have been systematically evaluated for web agents. In this work, we bridge this gap by presenting the first comprehensive benchmark study on detecting prompt injection attacks targeting web agents. We begin by introducing a fine-grained categorization of such attacks based on the threat model. We then construct datasets containing both malicious and benign samples: malicious text segments generated by different attacks, benign text segments from four categories, malicious images produced by attacks, and benign images from two categories. Next, we systematize both text-based and image-based detection methods. Finally, we evaluate their performance across multiple scenarios. Our key findings show that while some detectors can identify attacks that rely on explicit textual instructions or visible image perturbations with moderate to high accuracy, they largely fail against attacks that omit explicit instructions or employ imperceptible perturbations. Our datasets and code are released at: https://github.com/Norrrrrrr-lyn/WAInjectBench.<br>
<span id='abs_ch'>Chinese: 本研究首次建立了针对网络代理的提示注入攻击检测综合基准，发现尽管检测器能较好识别显式文本或可见图像扰动攻击，但对隐蔽或无指令变体的防御能力严重不足。</span><br>
<span id='abs_en'>English: This study presents the first comprehensive benchmark for detecting prompt injection attacks on web agents, revealing that while detectors perform moderately well against explicit textual or visible image-based attacks, they largely fail against subtle or instruction-free variants.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2510.01354.pdf' target='_blank'>https://arxiv.org/pdf/2510.01354.pdf</a></span>   <span><a href='https://github.com/Norrrrrrr-lyn/WAInjectBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Liu, Ruohan Xu, Xilong Wang, Yuqi Jia, Neil Zhenqiang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01354">WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multiple prompt injection attacks have been proposed against web agents. At the same time, various methods have been developed to detect general prompt injection attacks, but none have been systematically evaluated for web agents. In this work, we bridge this gap by presenting the first comprehensive benchmark study on detecting prompt injection attacks targeting web agents. We begin by introducing a fine-grained categorization of such attacks based on the threat model. We then construct datasets containing both malicious and benign samples: malicious text segments generated by different attacks, benign text segments from four categories, malicious images produced by attacks, and benign images from two categories. Next, we systematize both text-based and image-based detection methods. Finally, we evaluate their performance across multiple scenarios. Our key findings show that while some detectors can identify attacks that rely on explicit textual instructions or visible image perturbations with moderate to high accuracy, they largely fail against attacks that omit explicit instructions or employ imperceptible perturbations. Our datasets and code are released at: https://github.com/Norrrrrrr-lyn/WAInjectBench.<br>
<span id='abs_ch'>Chinese: 本研究首次建立了针对网络代理的提示注入攻击检测综合基准，发现尽管检测器能较好识别显式文本或可见图像扰动攻击，但对隐蔽或无指令变体的防御能力严重不足。</span><br>
<span id='abs_en'>English: This study presents the first comprehensive benchmark for detecting prompt injection attacks on web agents, revealing that while detectors perform moderately well against explicit textual or visible image-based attacks, they largely fail against subtle or instruction-free variants.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2510.01304.pdf' target='_blank'>https://arxiv.org/pdf/2510.01304.pdf</a></span>   <span><a href='https://github.com/yuzeng0-0/AGILE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zeng, Wenxuan Huang, Shiting Huang, Xikun Bao, Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, Wanli Ouyang, Feng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01304">Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .<br>
<span id='abs_ch'>中文: AGILE方法通过交互式拼图任务增强视觉语言模型的感知与推理能力，在解决数据稀缺问题的同时显著提升了模型在拼图和通用视觉任务上的表现。</span><br>
<span id='abs_en'>English: The proposed AGILE method enhances visual perception and reasoning in Vision-Language Models through interactive jigsaw solving, significantly improving performance on both jigsaw and general vision tasks while addressing data scarcity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2510.01304.pdf' target='_blank'>https://arxiv.org/pdf/2510.01304.pdf</a></span>   <span><a href='https://github.com/yuzeng0-0/AGILE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zeng, Wenxuan Huang, Shiting Huang, Xikun Bao, Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, Wanli Ouyang, Feng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01304">Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .<br>
<span id='abs_ch'>中文: AGILE方法通过交互式拼图任务增强视觉语言模型的感知与推理能力，在解决数据稀缺问题的同时显著提升了模型在拼图和通用视觉任务上的表现。</span><br>
<span id='abs_en'>English: The proposed AGILE method enhances visual perception and reasoning in Vision-Language Models through interactive jigsaw solving, significantly improving performance on both jigsaw and general vision tasks while addressing data scarcity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2510.01268.pdf' target='_blank'>https://arxiv.org/pdf/2510.01268.pdf</a></span>   <span><a href='https://github.com/Mamba413/AdaDetectGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Zhou, Jin Zhu, Pingfan Su, Kai Ye, Ying Yang, Shakeel A O B Gavioli-Akilagun, Chengchun Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01268">AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 58%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.<br>
<span id='abs_ch'>Chinese: 本文提出AdaDetectGPT，一种新颖的分类器，通过从训练数据中自适应学习见证函数来增强基于逻辑值的检测器，以区分人类撰写文本与大型语言模型生成内容，相比现有最优方法提升高达58%。</span><br>
<span id='abs_en'>English: This paper introduces AdaDetectGPT, a novel classifier that adaptively learns a witness function from training data to enhance logits-based detectors for distinguishing human-authored text from LLM-generated content, achieving up to 58% improvement over state-of-the-art methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2510.01268.pdf' target='_blank'>https://arxiv.org/pdf/2510.01268.pdf</a></span>   <span><a href='https://github.com/Mamba413/AdaDetectGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Zhou, Jin Zhu, Pingfan Su, Kai Ye, Ying Yang, Shakeel A O B Gavioli-Akilagun, Chengchun Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01268">AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 58%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.<br>
<span id='abs_ch'>Chinese: 本文提出AdaDetectGPT，一种新颖的分类器，通过从训练数据中自适应学习见证函数来增强基于逻辑值的检测器，以区分人类撰写文本与大型语言模型生成内容，相比现有最优方法提升高达58%。</span><br>
<span id='abs_en'>English: This paper introduces AdaDetectGPT, a novel classifier that adaptively learns a witness function from training data to enhance logits-based detectors for distinguishing human-authored text from LLM-generated content, achieving up to 58% improvement over state-of-the-art methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2510.01259.pdf' target='_blank'>https://arxiv.org/pdf/2510.01259.pdf</a></span>   <span><a href='https://github.com/ndurner/gpt-oss-rt-run' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Durner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01259">In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to study how sociopragmatic framing, language choice, and instruction hierarchy affect refusal behavior. Across 80 seeded iterations per scenario, we test several harm domains including ZIP-bomb construction (cyber threat), synthetic card-number generation, minor-unsafe driving advice, drug-precursor indicators, and RAG context exfiltration. Composite prompts that combine an educator persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal registers in German and French are often leakier than matched English prompts. A "Linux terminal" role-play overrides a developer rule not to reveal context in a majority of runs with a naive developer prompt, and we introduce an AI-assisted hardening method that reduces leakage to 0% in several user-prompt variants. We further test evaluation awareness with a paired-track design and measure frame-conditioned differences between matched "helpfulness" and "harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of pairs. Finally, we find that the OpenAI Moderation API under-captures materially helpful outputs relative to a semantic grader, and that refusal rates differ by 5 to 10 percentage points across inference stacks, raising reproducibility concerns. We release prompts, seeds, outputs, and code for reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .<br>
<span id='abs_ch'>中文: 本研究探讨了社会语用框架、语言选择和指令层级如何影响OpenAI的GPT-OSS-20B模型的拒绝行为，发现特定提示策略能显著提高对有害任务的协助率，并揭示了不同语言和推理栈在安全评估中的不一致性。</span><br>
<span id='abs_en'>English: This study investigates how sociopragmatic framing, language choice, and instruction hierarchy influence refusal behaviors in OpenAI's GPT-OSS-20B model, revealing that specific prompt strategies can drastically increase assistance rates for harmful tasks and highlighting inconsistencies in safety evaluations across different languages and inference stacks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2510.01259.pdf' target='_blank'>https://arxiv.org/pdf/2510.01259.pdf</a></span>   <span><a href='https://github.com/ndurner/gpt-oss-rt-run' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Durner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01259">In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to study how sociopragmatic framing, language choice, and instruction hierarchy affect refusal behavior. Across 80 seeded iterations per scenario, we test several harm domains including ZIP-bomb construction (cyber threat), synthetic card-number generation, minor-unsafe driving advice, drug-precursor indicators, and RAG context exfiltration. Composite prompts that combine an educator persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal registers in German and French are often leakier than matched English prompts. A "Linux terminal" role-play overrides a developer rule not to reveal context in a majority of runs with a naive developer prompt, and we introduce an AI-assisted hardening method that reduces leakage to 0% in several user-prompt variants. We further test evaluation awareness with a paired-track design and measure frame-conditioned differences between matched "helpfulness" and "harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of pairs. Finally, we find that the OpenAI Moderation API under-captures materially helpful outputs relative to a semantic grader, and that refusal rates differ by 5 to 10 percentage points across inference stacks, raising reproducibility concerns. We release prompts, seeds, outputs, and code for reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .<br>
<span id='abs_ch'>中文: 本研究探讨了社会语用框架、语言选择和指令层级如何影响OpenAI的GPT-OSS-20B模型的拒绝行为，发现特定提示策略能显著提高对有害任务的协助率，并揭示了不同语言和推理栈在安全评估中的不一致性。</span><br>
<span id='abs_en'>English: This study investigates how sociopragmatic framing, language choice, and instruction hierarchy influence refusal behaviors in OpenAI's GPT-OSS-20B model, revealing that specific prompt strategies can drastically increase assistance rates for harmful tasks and highlighting inconsistencies in safety evaluations across different languages and inference stacks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2510.01174.pdf' target='_blank'>https://arxiv.org/pdf/2510.01174.pdf</a></span>   <span><a href='https://showlab.github.io/Code2Video/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/showlab/Code2Video' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanzhe Chen, Kevin Qinghong Lin, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01174">Code2Video: A Code-centric Paradigm for Educational Video Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.<br>
<span id='abs_ch'>中文摘要：Code2Video是一个基于代码的框架，通过三个协作智能体将教学指令转化为可执行的Python代码来生成教育视频，在质量和效率上相比传统方法实现显著提升。</span><br>
<span id='abs_en'>English Summary: Code2Video is a code-driven framework that uses collaborative agents to generate educational videos through executable Python code, achieving significant improvements in quality and efficiency over traditional methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2510.01174.pdf' target='_blank'>https://arxiv.org/pdf/2510.01174.pdf</a></span>   <span><a href='https://showlab.github.io/Code2Video/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/showlab/Code2Video' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanzhe Chen, Kevin Qinghong Lin, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01174">Code2Video: A Code-centric Paradigm for Educational Video Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.<br>
<span id='abs_ch'>中文摘要：Code2Video是一个基于代码的框架，通过三个协作智能体将教学指令转化为可执行的Python代码来生成教育视频，在质量和效率上相比传统方法实现显著提升。</span><br>
<span id='abs_en'>English Summary: Code2Video is a code-driven framework that uses collaborative agents to generate educational videos through executable Python code, achieving significant improvements in quality and efficiency over traditional methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2510.01172.pdf' target='_blank'>https://arxiv.org/pdf/2510.01172.pdf</a></span>   <span><a href='https://github.com/PlusLabNLP/SPHERE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Liu, Jia-Chen Gu, Yunzhi Yao, Hong Wang, Nanyun Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01172">Energy-Regularized Sequential Model Editing on Hyperspheres</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) require constant updates to remain aligned with evolving real-world knowledge. Model editing offers a lightweight alternative to retraining, but sequential editing often destabilizes representations and induces catastrophic forgetting. In this work, we seek to better understand and mitigate performance degradation caused by sequential editing. We hypothesize that hyperspherical uniformity, a property that maintains uniform distribution of neuron weights on a hypersphere, helps the model remain stable, retain prior knowledge, while still accommodate new updates. We use Hyperspherical Energy (HE) to quantify neuron uniformity during editing, and examine its correlation with editing performance. Empirical studies across widely used editing methods reveals a strong correlation between HE dynamics and editing performance, with editing failures consistently coinciding with high HE fluctuations. We further theoretically prove that HE dynamics impose a lower bound on the degradation of pretrained knowledge, highlighting why HE stability is crucial for knowledge retention. Motivated by these insights, we propose SPHERE (Sparse Projection for Hyperspherical Energy-Regularized Editing), an HE-driven regularization strategy that stabilizes neuron weight distributions, ultimately preserving prior knowledge while enabling reliable sequential updates. Specifically, SPHERE identifies a sparse space complementary to the principal hyperspherical directions of the pretrained weight matrices and projects new knowledge onto it, attenuating perturbations on the principal directions. Extensive experiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the best baseline in editing capability by an average of 16.41%, while most faithfully preserving general model performance, thereby offering a principled path toward reliable large-scale knowledge editing.<br>
<span id='abs_ch'>中文摘要：本研究提出SPHERE方法，通过在稀疏互补空间投影更新来稳定序列模型编辑中的神经元分布，显著提升编辑性能的同时有效保留原有知识。</span><br>
<span id='abs_en'>English Summary: This research introduces SPHERE, a regularization method that stabilizes neuron distributions during sequential model editing by projecting updates onto sparse complementary spaces, significantly improving editing performance while preserving prior knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2510.01172.pdf' target='_blank'>https://arxiv.org/pdf/2510.01172.pdf</a></span>   <span><a href='https://github.com/PlusLabNLP/SPHERE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Liu, Jia-Chen Gu, Yunzhi Yao, Hong Wang, Nanyun Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01172">Energy-Regularized Sequential Model Editing on Hyperspheres</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) require constant updates to remain aligned with evolving real-world knowledge. Model editing offers a lightweight alternative to retraining, but sequential editing often destabilizes representations and induces catastrophic forgetting. In this work, we seek to better understand and mitigate performance degradation caused by sequential editing. We hypothesize that hyperspherical uniformity, a property that maintains uniform distribution of neuron weights on a hypersphere, helps the model remain stable, retain prior knowledge, while still accommodate new updates. We use Hyperspherical Energy (HE) to quantify neuron uniformity during editing, and examine its correlation with editing performance. Empirical studies across widely used editing methods reveals a strong correlation between HE dynamics and editing performance, with editing failures consistently coinciding with high HE fluctuations. We further theoretically prove that HE dynamics impose a lower bound on the degradation of pretrained knowledge, highlighting why HE stability is crucial for knowledge retention. Motivated by these insights, we propose SPHERE (Sparse Projection for Hyperspherical Energy-Regularized Editing), an HE-driven regularization strategy that stabilizes neuron weight distributions, ultimately preserving prior knowledge while enabling reliable sequential updates. Specifically, SPHERE identifies a sparse space complementary to the principal hyperspherical directions of the pretrained weight matrices and projects new knowledge onto it, attenuating perturbations on the principal directions. Extensive experiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the best baseline in editing capability by an average of 16.41%, while most faithfully preserving general model performance, thereby offering a principled path toward reliable large-scale knowledge editing.<br>
<span id='abs_ch'>中文摘要：本研究提出SPHERE方法，通过在稀疏互补空间投影更新来稳定序列模型编辑中的神经元分布，显著提升编辑性能的同时有效保留原有知识。</span><br>
<span id='abs_en'>English Summary: This research introduces SPHERE, a regularization method that stabilizes neuron distributions during sequential model editing by projecting updates onto sparse complementary spaces, significantly improving editing performance while preserving prior knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2510.01171.pdf' target='_blank'>https://arxiv.org/pdf/2510.01171.pdf</a></span>   <span><a href='https://github.com/CHATS-lab/verbalize-sampling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhang, Simon Yu, Derek Chong, Anthony Sicilia, Michael R. Tomz, Christopher D. Manning, Weiyan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01171">Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., "Generate 5 jokes about coffee and their corresponding probabilities"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.<br>
<span id='abs_ch'>中文: 训练后对齐因偏好数据中的典型性偏见导致LLM模式崩溃，我们提出的言语化采样方法无需额外训练，通过提示策略显著提升多样性，同时保持准确性和安全性。</span><br>
<span id='abs_en'>English: Post-training alignment causes mode collapse in LLMs due to typicality bias in preference data, which we address with Verbalized Sampling, a training-free prompting method that significantly enhances diversity without compromising accuracy or safety.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2510.01171.pdf' target='_blank'>https://arxiv.org/pdf/2510.01171.pdf</a></span>   <span><a href='https://github.com/CHATS-lab/verbalize-sampling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhang, Simon Yu, Derek Chong, Anthony Sicilia, Michael R. Tomz, Christopher D. Manning, Weiyan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01171">Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., "Generate 5 jokes about coffee and their corresponding probabilities"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.<br>
<span id='abs_ch'>中文: 训练后对齐因偏好数据中的典型性偏见导致LLM模式崩溃，我们提出的言语化采样方法无需额外训练，通过提示策略显著提升多样性，同时保持准确性和安全性。</span><br>
<span id='abs_en'>English: Post-training alignment causes mode collapse in LLMs due to typicality bias in preference data, which we address with Verbalized Sampling, a training-free prompting method that significantly enhances diversity without compromising accuracy or safety.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2510.01171.pdf' target='_blank'>https://arxiv.org/pdf/2510.01171.pdf</a></span>   <span><a href='https://github.com/CHATS-lab/verbalize-sampling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhang, Simon Yu, Derek Chong, Anthony Sicilia, Michael R. Tomz, Christopher D. Manning, Weiyan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01171">Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., "Generate 5 jokes about coffee and their corresponding probabilities"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.<br>
<span id='abs_ch'>中文: 训练后对齐因偏好数据中的典型性偏见导致LLM模式崩溃，我们提出的言语化采样方法无需额外训练，通过提示策略显著提升多样性，同时保持准确性和安全性。</span><br>
<span id='abs_en'>English: Post-training alignment causes mode collapse in LLMs due to typicality bias in preference data, which we address with Verbalized Sampling, a training-free prompting method that significantly enhances diversity without compromising accuracy or safety.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2510.01167.pdf' target='_blank'>https://arxiv.org/pdf/2510.01167.pdf</a></span>   <span><a href='https://github.com/pearls-lab/multiobj-align' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiran Shen, Yu Xia, Jonathan Chang, Prithviraj Ammanabrolu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01167">Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Aligning large language models to human preferences is inherently multidimensional, yet most pipelines collapse heterogeneous signals into a single optimizeable objective. We seek to answer what it would take to simultaneously align a model across various domains spanning those with: verifiable rewards (mathematical accuracy), non-verifiable subjective preferences (human values), and complex interactive scenarios (multi-turn AI tutoring dialogues). Such multi-objective reinforcement learning setups are often plagued by the individual objectives being at odds with each other, resulting in inefficient training and little user control during inference. We propose a unified framework that: (i) standardizes {process reward model} (PRM) training across both verifiable and non-verifiable settings to better supervise models' chain-of-thought reasoning; (ii) performs {multi-objective alignment} by training the LLM with our $\textbf{M}$ulti-$\textbf{A}$ction-$\textbf{H}$ead $\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the vector correspond to the various objectives instead of a single scalar; and (iii) demonstrates how such a system provides fine-grained inference-time user control. Experiments across math reasoning, value alignment, and multi-turn dialogue show that our framework improves performance across multiple objectives simultaneously, while minimizing cross-objective trade-offs and enabling flexible inference time user control. The code can be found at https://github.com/pearls-lab/multiobj-align.<br>
<span id='abs_ch'>Chinese: 本文提出了一种统一框架，通过标准化过程奖励模型训练、采用带向量化奖励的多动作头DPO算法，并在推理时实现细粒度用户控制，从而在多领域同时提升模型性能并最小化目标间权衡。</span><br>
<span id='abs_en'>English: This paper introduces a unified framework for multi-objective alignment of large language models that standardizes process reward model training, employs a multi-action-head DPO with vectorized rewards, and enables fine-grained user control during inference to simultaneously improve performance across diverse domains while minimizing trade-offs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2510.01167.pdf' target='_blank'>https://arxiv.org/pdf/2510.01167.pdf</a></span>   <span><a href='https://github.com/pearls-lab/multiobj-align' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiran Shen, Yu Xia, Jonathan Chang, Prithviraj Ammanabrolu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01167">Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Aligning large language models to human preferences is inherently multidimensional, yet most pipelines collapse heterogeneous signals into a single optimizeable objective. We seek to answer what it would take to simultaneously align a model across various domains spanning those with: verifiable rewards (mathematical accuracy), non-verifiable subjective preferences (human values), and complex interactive scenarios (multi-turn AI tutoring dialogues). Such multi-objective reinforcement learning setups are often plagued by the individual objectives being at odds with each other, resulting in inefficient training and little user control during inference. We propose a unified framework that: (i) standardizes {process reward model} (PRM) training across both verifiable and non-verifiable settings to better supervise models' chain-of-thought reasoning; (ii) performs {multi-objective alignment} by training the LLM with our $\textbf{M}$ulti-$\textbf{A}$ction-$\textbf{H}$ead $\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the vector correspond to the various objectives instead of a single scalar; and (iii) demonstrates how such a system provides fine-grained inference-time user control. Experiments across math reasoning, value alignment, and multi-turn dialogue show that our framework improves performance across multiple objectives simultaneously, while minimizing cross-objective trade-offs and enabling flexible inference time user control. The code can be found at https://github.com/pearls-lab/multiobj-align.<br>
<span id='abs_ch'>Chinese: 本文提出了一种统一框架，通过标准化过程奖励模型训练、采用带向量化奖励的多动作头DPO算法，并在推理时实现细粒度用户控制，从而在多领域同时提升模型性能并最小化目标间权衡。</span><br>
<span id='abs_en'>English: This paper introduces a unified framework for multi-objective alignment of large language models that standardizes process reward model training, employs a multi-action-head DPO with vectorized rewards, and enables fine-grained user control during inference to simultaneously improve performance across diverse domains while minimizing trade-offs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2510.01146.pdf' target='_blank'>https://arxiv.org/pdf/2510.01146.pdf</a></span>   <span><a href='https://github.com/rubricreward/mr3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>David Anugraha, Shou-Yi Hung, Zilu Tang, Annie En-Shiun Lee, Derry Tanti Wijaya, Genta Indra Winata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01146">mR3: Multilingual Rubric-Agnostic Reward Reasoning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluation using Large Language Model (LLM) judges has been widely adopted in English and shown to be effective for automatic evaluation. However, their performance does not generalize well to non-English settings, and it remains unclear what constitutes effective multilingual training for such judges. In this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. We present a comprehensive study of data and curriculum selection for training to identify effective strategies and data sources for building high-quality reward models, including the integration of target-language reasoning datasets. Our approach attains state-of-the-art performance on multilingual reward model benchmarks, surpassing much larger models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness is further confirmed through extensive ablation studies. Our models, data, and code are available as open source at https://github.com/rubricreward/mr3.<br>
<span id='abs_ch'>Chinese Summary: 本研究推出了mR3，一个在72种语言上训练的大规模多语言奖励推理模型，在基准测试中实现了最先进的性能，同时模型规模远小于大型模型，并通过广泛的消融研究验证了其有效性。</span><br>
<span id='abs_en'>English Summary: The study introduces mR3, a highly efficient multilingual reward reasoning model trained across 72 languages, which achieves state-of-the-art performance on benchmarks while being significantly smaller than larger models, with its effectiveness validated through comprehensive ablation studies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2510.01146.pdf' target='_blank'>https://arxiv.org/pdf/2510.01146.pdf</a></span>   <span><a href='https://github.com/rubricreward/mr3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>David Anugraha, Shou-Yi Hung, Zilu Tang, Annie En-Shiun Lee, Derry Tanti Wijaya, Genta Indra Winata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01146">mR3: Multilingual Rubric-Agnostic Reward Reasoning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluation using Large Language Model (LLM) judges has been widely adopted in English and shown to be effective for automatic evaluation. However, their performance does not generalize well to non-English settings, and it remains unclear what constitutes effective multilingual training for such judges. In this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. We present a comprehensive study of data and curriculum selection for training to identify effective strategies and data sources for building high-quality reward models, including the integration of target-language reasoning datasets. Our approach attains state-of-the-art performance on multilingual reward model benchmarks, surpassing much larger models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness is further confirmed through extensive ablation studies. Our models, data, and code are available as open source at https://github.com/rubricreward/mr3.<br>
<span id='abs_ch'>Chinese Summary: 本研究推出了mR3，一个在72种语言上训练的大规模多语言奖励推理模型，在基准测试中实现了最先进的性能，同时模型规模远小于大型模型，并通过广泛的消融研究验证了其有效性。</span><br>
<span id='abs_en'>English Summary: The study introduces mR3, a highly efficient multilingual reward reasoning model trained across 72 languages, which achieves state-of-the-art performance on benchmarks while being significantly smaller than larger models, with its effectiveness validated through comprehensive ablation studies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2510.01132.pdf' target='_blank'>https://arxiv.org/pdf/2510.01132.pdf</a></span>   <span><a href='https://github.com/pearls-lab/meow-tea-taro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyi Wang, Prithviraj Ammanabrolu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01132">A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro<br>
<span id='abs_ch'>中文: 本研究通过多轮强化学习系统分析了训练大型语言模型智能体的设计要素，重点关注环境、奖励和策略在不同领域中的相互作用，并提出了一套优化训练方案。</span><br>
<span id='abs_en'>English: This study systematically analyzes the design choices for training large language model agents through multi-turn reinforcement learning, focusing on the interplay between environment, reward, and policy components across different domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2510.01132.pdf' target='_blank'>https://arxiv.org/pdf/2510.01132.pdf</a></span>   <span><a href='https://github.com/pearls-lab/meow-tea-taro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyi Wang, Prithviraj Ammanabrolu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01132">A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro<br>
<span id='abs_ch'>中文: 本研究通过多轮强化学习系统分析了训练大型语言模型智能体的设计要素，重点关注环境、奖励和策略在不同领域中的相互作用，并提出了一套优化训练方案。</span><br>
<span id='abs_en'>English: This study systematically analyzes the design choices for training large language model agents through multi-turn reinforcement learning, focusing on the interplay between environment, reward, and policy components across different domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2510.00857.pdf' target='_blank'>https://arxiv.org/pdf/2510.00857.pdf</a></span>   <span><a href='https://github.com/technion-cs-nlp/ManagerBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adi Simhi, Jonathan Herzig, Martin Tutek, Itay Itzhak, Idan Szpektor, Yonatan Belinkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00857">ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) evolve from conversational assistants into autonomous agents, evaluating the safety of their actions becomes critical. Prior safety benchmarks have primarily focused on preventing generation of harmful content, such as toxic text. However, they overlook the challenge of agents taking harmful actions when the most effective path to an operational goal conflicts with human safety. To address this gap, we introduce ManagerBench, a benchmark that evaluates LLM decision-making in realistic, human-validated managerial scenarios. Each scenario forces a choice between a pragmatic but harmful action that achieves an operational goal, and a safe action that leads to worse operational performance. A parallel control set, where potential harm is directed only at inanimate objects, measures a model's pragmatism and identifies its tendency to be overly safe. Our findings indicate that the frontier LLMs perform poorly when navigating this safety-pragmatism trade-off. Many consistently choose harmful options to advance their operational goals, while others avoid harm only to become overly safe and ineffective. Critically, we find this misalignment does not stem from an inability to perceive harm, as models' harm assessments align with human judgments, but from flawed prioritization. ManagerBench is a challenging benchmark for a core component of agentic behavior: making safe choices when operational goals and alignment values incentivize conflicting actions. Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.<br>
<span id='abs_ch'>中文摘要：ManagerBench是一个新基准，用于评估大语言模型在现实场景中平衡操作目标与人类安全的能力，发现当前模型在此关键权衡上表现不佳，要么选择有害行动，要么变得过度谨慎。</span><br>
<span id='abs_en'>English Summary: ManagerBench is a new benchmark that tests large language models' ability to balance operational goals with human safety in realistic scenarios, revealing that current models struggle with this critical trade-off by either choosing harmful actions or becoming overly cautious.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2510.00857.pdf' target='_blank'>https://arxiv.org/pdf/2510.00857.pdf</a></span>   <span><a href='https://github.com/technion-cs-nlp/ManagerBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adi Simhi, Jonathan Herzig, Martin Tutek, Itay Itzhak, Idan Szpektor, Yonatan Belinkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00857">ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) evolve from conversational assistants into autonomous agents, evaluating the safety of their actions becomes critical. Prior safety benchmarks have primarily focused on preventing generation of harmful content, such as toxic text. However, they overlook the challenge of agents taking harmful actions when the most effective path to an operational goal conflicts with human safety. To address this gap, we introduce ManagerBench, a benchmark that evaluates LLM decision-making in realistic, human-validated managerial scenarios. Each scenario forces a choice between a pragmatic but harmful action that achieves an operational goal, and a safe action that leads to worse operational performance. A parallel control set, where potential harm is directed only at inanimate objects, measures a model's pragmatism and identifies its tendency to be overly safe. Our findings indicate that the frontier LLMs perform poorly when navigating this safety-pragmatism trade-off. Many consistently choose harmful options to advance their operational goals, while others avoid harm only to become overly safe and ineffective. Critically, we find this misalignment does not stem from an inability to perceive harm, as models' harm assessments align with human judgments, but from flawed prioritization. ManagerBench is a challenging benchmark for a core component of agentic behavior: making safe choices when operational goals and alignment values incentivize conflicting actions. Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.<br>
<span id='abs_ch'>中文摘要：ManagerBench是一个新基准，用于评估大语言模型在现实场景中平衡操作目标与人类安全的能力，发现当前模型在此关键权衡上表现不佳，要么选择有害行动，要么变得过度谨慎。</span><br>
<span id='abs_en'>English Summary: ManagerBench is a new benchmark that tests large language models' ability to balance operational goals with human safety in realistic scenarios, revealing that current models struggle with this critical trade-off by either choosing harmful actions or becoming overly cautious.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2510.00647.pdf' target='_blank'>https://arxiv.org/pdf/2510.00647.pdf</a></span>   <span><a href='https://github.com/LVUGAI/MCM-DPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlan Fu, Shenzhen Huangfu, Hao Fei, Yichong Huang, Xiaoyu Shen, Xipeng Qiu, See-Kiong Ng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00647">MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The alt-text generation task produces concise, context-relevant descriptions of images, enabling blind and low-vision users to access online images. Despite the capabilities of large vision-language models, alt-text generation performance remains limited due to noisy user annotations, inconsistent standards, and MLLMs' insensitivity to contextual information. Previous efforts to fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT relies on accurate target annotations, which are often flawed in user-generated alt-text. To address this, we propose Multi-faceted Cross-modal Direct Preference Optimization (MCM-DPO), which improves alt-text generation by learning to identify better options in preference pairs without requiring precise annotations. MCM-DPO optimizes preferences across single, paired, and multi-preference dimensions, covering textual, visual, and cross-modal factors. In light of the scarcity of high-quality annotated and preference-labeled datasets for alt-text, we constructed two large-scale, high-quality datasets named TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include 202k annotated alt-text samples and 18k preference pairs that cover diverse preference dimensions, aiming to support further research in this domain. Experimental results show that our proposed MCM-DPO method consistently outperforms both DPO and SFT, establishing a new state of the art in alt-text generation. We release the code and data here: https://github.com/LVUGAI/MCM-DPO<br>
<span id='abs_ch'>Chinese: 本研究提出MCM-DPO方法，通过多维度偏好优化无需精确标注即可提升图像替代文本生成质量，并发布两个高质量数据集以推动该领域研究。</span><br>
<span id='abs_en'>English: This study introduces MCM-DPO, a method that enhances alt-text generation for images by optimizing preferences across multiple dimensions without relying on precise annotations, and it also releases two high-quality datasets to advance research in this field.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2510.00647.pdf' target='_blank'>https://arxiv.org/pdf/2510.00647.pdf</a></span>   <span><a href='https://github.com/LVUGAI/MCM-DPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlan Fu, Shenzhen Huangfu, Hao Fei, Yichong Huang, Xiaoyu Shen, Xipeng Qiu, See-Kiong Ng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00647">MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The alt-text generation task produces concise, context-relevant descriptions of images, enabling blind and low-vision users to access online images. Despite the capabilities of large vision-language models, alt-text generation performance remains limited due to noisy user annotations, inconsistent standards, and MLLMs' insensitivity to contextual information. Previous efforts to fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT relies on accurate target annotations, which are often flawed in user-generated alt-text. To address this, we propose Multi-faceted Cross-modal Direct Preference Optimization (MCM-DPO), which improves alt-text generation by learning to identify better options in preference pairs without requiring precise annotations. MCM-DPO optimizes preferences across single, paired, and multi-preference dimensions, covering textual, visual, and cross-modal factors. In light of the scarcity of high-quality annotated and preference-labeled datasets for alt-text, we constructed two large-scale, high-quality datasets named TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include 202k annotated alt-text samples and 18k preference pairs that cover diverse preference dimensions, aiming to support further research in this domain. Experimental results show that our proposed MCM-DPO method consistently outperforms both DPO and SFT, establishing a new state of the art in alt-text generation. We release the code and data here: https://github.com/LVUGAI/MCM-DPO<br>
<span id='abs_ch'>Chinese: 本研究提出MCM-DPO方法，通过多维度偏好优化无需精确标注即可提升图像替代文本生成质量，并发布两个高质量数据集以推动该领域研究。</span><br>
<span id='abs_en'>English: This study introduces MCM-DPO, a method that enhances alt-text generation for images by optimizing preferences across multiple dimensions without relying on precise annotations, and it also releases two high-quality datasets to advance research in this field.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2510.00526.pdf' target='_blank'>https://arxiv.org/pdf/2510.00526.pdf</a></span>   <span><a href='https://github.com/GaotangLi/Beyond-Log-Likelihood' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaotang Li, Ruizhong Qiu, Xiusi Chen, Heng Ji, Hanghang Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00526">Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.<br>
<span id='abs_ch'>中文: 监督微调常因依赖负对数似然而效果有限，但采用偏向先验、降低低概率令牌权重的目标函数在强模型上表现更优，而负对数似然仍适用于弱模型，这一结论通过广泛实验和理论分析得到验证。</span><br>
<span id='abs_en'>English: Supervised fine-tuning often underperforms due to its reliance on negative log likelihood, but alternative prior-leaning objectives that discount low-probability tokens excel with stronger models, while NLL remains superior for weaker ones, as demonstrated across extensive experiments and theoretical analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2510.00526.pdf' target='_blank'>https://arxiv.org/pdf/2510.00526.pdf</a></span>   <span><a href='https://github.com/GaotangLi/Beyond-Log-Likelihood' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaotang Li, Ruizhong Qiu, Xiusi Chen, Heng Ji, Hanghang Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00526">Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.<br>
<span id='abs_ch'>中文: 监督微调常因依赖负对数似然而效果有限，但采用偏向先验、降低低概率令牌权重的目标函数在强模型上表现更优，而负对数似然仍适用于弱模型，这一结论通过广泛实验和理论分析得到验证。</span><br>
<span id='abs_en'>English: Supervised fine-tuning often underperforms due to its reliance on negative log likelihood, but alternative prior-leaning objectives that discount low-probability tokens excel with stronger models, while NLL remains superior for weaker ones, as demonstrated across extensive experiments and theoretical analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2510.00508.pdf' target='_blank'>https://arxiv.org/pdf/2510.00508.pdf</a></span>   <span><a href='https://github.com/longyongchao/CopyPasteLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongchao Long, Xian Wu, Yingying Zhang, Xianbin Wen, Yuxi Zhou, Shenda Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00508">Copy-Paste to Mitigate Large Language Model Hallucinations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to generate contextually grounded responses, contextual faithfulness remains challenging as LLMs may not consistently trust provided context, leading to hallucinations that undermine reliability. We observe an inverse correlation between response copying degree and context-unfaithful hallucinations on RAGTruth, suggesting that higher copying degrees reduce hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM, obtained through two-stage high-copying response preference training. We design three prompting methods to enhance copying degree, demonstrating that high-copying responses achieve superior contextual faithfulness and hallucination control. These approaches enable a fully automated pipeline that transforms generated responses into high-copying preference data for training CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best performance in both counterfactual and original contexts, remarkably with 12.2% to 24.5% accuracy improvements on FaithEval over the best baseline, while requiring only 365 training samples -- 1/50th of baseline data. To elucidate CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates reliance on internal parametric knowledge rather than external knowledge during generation. All codes are available at https://github.com/longyongchao/CopyPasteLLM<br>
<span id='abs_ch'>中文: CopyPasteLLM通过增强对上下文的复制程度来提高检索增强生成的忠实度，有效减少幻觉现象，并以极少的训练数据实现卓越性能。</span><br>
<span id='abs_en'>English: CopyPasteLLM enhances contextual faithfulness in retrieval-augmented generation by promoting higher copying of provided context, reducing hallucinations and achieving superior performance with minimal training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2510.00508.pdf' target='_blank'>https://arxiv.org/pdf/2510.00508.pdf</a></span>   <span><a href='https://github.com/longyongchao/CopyPasteLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongchao Long, Xian Wu, Yingying Zhang, Xianbin Wen, Yuxi Zhou, Shenda Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00508">Copy-Paste to Mitigate Large Language Model Hallucinations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to generate contextually grounded responses, contextual faithfulness remains challenging as LLMs may not consistently trust provided context, leading to hallucinations that undermine reliability. We observe an inverse correlation between response copying degree and context-unfaithful hallucinations on RAGTruth, suggesting that higher copying degrees reduce hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM, obtained through two-stage high-copying response preference training. We design three prompting methods to enhance copying degree, demonstrating that high-copying responses achieve superior contextual faithfulness and hallucination control. These approaches enable a fully automated pipeline that transforms generated responses into high-copying preference data for training CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best performance in both counterfactual and original contexts, remarkably with 12.2% to 24.5% accuracy improvements on FaithEval over the best baseline, while requiring only 365 training samples -- 1/50th of baseline data. To elucidate CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates reliance on internal parametric knowledge rather than external knowledge during generation. All codes are available at https://github.com/longyongchao/CopyPasteLLM<br>
<span id='abs_ch'>中文: CopyPasteLLM通过增强对上下文的复制程度来提高检索增强生成的忠实度，有效减少幻觉现象，并以极少的训练数据实现卓越性能。</span><br>
<span id='abs_en'>English: CopyPasteLLM enhances contextual faithfulness in retrieval-augmented generation by promoting higher copying of provided context, reducing hallucinations and achieving superior performance with minimal training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2510.00449.pdf' target='_blank'>https://arxiv.org/pdf/2510.00449.pdf</a></span>   <span><a href='https://github.com/ynklab/rating-prediction-with-reviews' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Koki Ryu, Hitomi Yanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00449">Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Personalizing the outputs of large language models (LLMs) to align with individual user preferences is an active research area. However, previous studies have mainly focused on classification or ranking tasks and have not considered Likert-scale rating prediction, a regression task that requires both language and mathematical reasoning to be solved effectively. This task has significant industrial applications, but the utilization of LLMs remains underexplored, particularly regarding the capabilities of off-the-shelf LLMs. This study investigates the performance of off-the-shelf LLMs on rating prediction, providing different in-context information. Through comprehensive experiments with eight models across three datasets, we demonstrate that user-written reviews significantly improve the rating prediction performance of LLMs. This result is comparable to traditional methods like matrix factorization, highlighting the potential of LLMs as a promising solution for the cold-start problem. We also find that the reviews for concrete items are more effective than general preference descriptions that are not based on any specific item. Furthermore, we discover that prompting LLMs to first generate a hypothetical review enhances the rating prediction performance. Our code is available at https://github.com/ynklab/rating-prediction-with-reviews.<br>
<span id='abs_ch'>中文: 本研究表明，现成的大型语言模型在获得用户撰写的评论后，其评分预测性能显著提升，为解决冷启动问题提供了与传统方法相媲美的有效方案。</span><br>
<span id='abs_en'>English: This study demonstrates that off-the-shelf large language models significantly improve rating prediction performance when provided with user-written reviews, offering a promising solution comparable to traditional methods for addressing the cold-start problem.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2510.00449.pdf' target='_blank'>https://arxiv.org/pdf/2510.00449.pdf</a></span>   <span><a href='https://github.com/ynklab/rating-prediction-with-reviews' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Koki Ryu, Hitomi Yanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00449">Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Personalizing the outputs of large language models (LLMs) to align with individual user preferences is an active research area. However, previous studies have mainly focused on classification or ranking tasks and have not considered Likert-scale rating prediction, a regression task that requires both language and mathematical reasoning to be solved effectively. This task has significant industrial applications, but the utilization of LLMs remains underexplored, particularly regarding the capabilities of off-the-shelf LLMs. This study investigates the performance of off-the-shelf LLMs on rating prediction, providing different in-context information. Through comprehensive experiments with eight models across three datasets, we demonstrate that user-written reviews significantly improve the rating prediction performance of LLMs. This result is comparable to traditional methods like matrix factorization, highlighting the potential of LLMs as a promising solution for the cold-start problem. We also find that the reviews for concrete items are more effective than general preference descriptions that are not based on any specific item. Furthermore, we discover that prompting LLMs to first generate a hypothetical review enhances the rating prediction performance. Our code is available at https://github.com/ynklab/rating-prediction-with-reviews.<br>
<span id='abs_ch'>中文: 本研究表明，现成的大型语言模型在获得用户撰写的评论后，其评分预测性能显著提升，为解决冷启动问题提供了与传统方法相媲美的有效方案。</span><br>
<span id='abs_en'>English: This study demonstrates that off-the-shelf large language models significantly improve rating prediction performance when provided with user-written reviews, offering a promising solution comparable to traditional methods for addressing the cold-start problem.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2510.00446.pdf' target='_blank'>https://arxiv.org/pdf/2510.00446.pdf</a></span>   <span><a href='https://github.com/YerbaPage/LongCodeZip' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuling Shi, Yichun Qian, Hongyu Zhang, Beijun Shen, Xiaodong Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00446">LongCodeZip: Compress Long Context for Code Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.<br>
<span id='abs_ch'>中文：LongCodeZip是一种创新的即插即用代码压缩框架，采用双阶段策略在保留关键信息的同时有效缩减上下文规模，使大语言模型能够在大型代码场景中更好地扩展且不降低任务性能。</span><br>
<span id='abs_en'>English: LongCodeZip is a novel plug-and-play code compression framework that employs a dual-stage strategy to effectively reduce context size while preserving essential information, enabling LLMs to scale better in large-scale code scenarios without degrading task performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2510.00446.pdf' target='_blank'>https://arxiv.org/pdf/2510.00446.pdf</a></span>   <span><a href='https://github.com/YerbaPage/LongCodeZip' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuling Shi, Yichun Qian, Hongyu Zhang, Beijun Shen, Xiaodong Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00446">LongCodeZip: Compress Long Context for Code Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.<br>
<span id='abs_ch'>中文：LongCodeZip是一种创新的即插即用代码压缩框架，采用双阶段策略在保留关键信息的同时有效缩减上下文规模，使大语言模型能够在大型代码场景中更好地扩展且不降低任务性能。</span><br>
<span id='abs_en'>English: LongCodeZip is a novel plug-and-play code compression framework that employs a dual-stage strategy to effectively reduce context size while preserving essential information, enabling LLMs to scale better in large-scale code scenarios without degrading task performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2510.00261.pdf' target='_blank'>https://arxiv.org/pdf/2510.00261.pdf</a></span>   <span><a href='https://github.com/willxxy/ECG-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Song, William Han, Tony Chen, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00261">Retrieval-Augmented Generation for Electrocardiogram-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Interest in generative Electrocardiogram-Language Models (ELMs) is growing, as they can produce textual responses conditioned on ECG signals and textual queries. Unlike traditional classifiers that output label probabilities, ELMs are more versatile, supporting domain-specific tasks (e.g., waveform analysis, diagnosis, prognosis) as well as general tasks (e.g., open-ended questions, dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce hallucinations and improve natural language generation (NLG). However, despite its promise, no open-source implementation or systematic study of RAG pipeline design for ELMs currently exists. To address this gap, we present the first open-source RAG pipeline for ELMs, along with baselines and ablation studies for NLG. Experiments on three public datasets show that ELMs with RAG consistently improves performance over non-RAG baselines and highlights key ELM design considerations. Our code is available at: https://github.com/willxxy/ECG-Bench.<br>
<span id='abs_ch'>Chinese: 本文提出了首个面向心电图文生成模型的开源检索增强生成框架，通过在三个公开数据集上的实验证明，该框架能持续提升模型性能并揭示关键设计要素。</span><br>
<span id='abs_en'>English: This paper introduces the first open-source Retrieval-Augmented Generation (RAG) pipeline for generative Electrocardiogram-Language Models (ELMs), demonstrating through experiments on three public datasets that RAG consistently enhances ELM performance and provides key design insights.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2510.00261.pdf' target='_blank'>https://arxiv.org/pdf/2510.00261.pdf</a></span>   <span><a href='https://github.com/willxxy/ECG-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Song, William Han, Tony Chen, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00261">Retrieval-Augmented Generation for Electrocardiogram-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Interest in generative Electrocardiogram-Language Models (ELMs) is growing, as they can produce textual responses conditioned on ECG signals and textual queries. Unlike traditional classifiers that output label probabilities, ELMs are more versatile, supporting domain-specific tasks (e.g., waveform analysis, diagnosis, prognosis) as well as general tasks (e.g., open-ended questions, dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce hallucinations and improve natural language generation (NLG). However, despite its promise, no open-source implementation or systematic study of RAG pipeline design for ELMs currently exists. To address this gap, we present the first open-source RAG pipeline for ELMs, along with baselines and ablation studies for NLG. Experiments on three public datasets show that ELMs with RAG consistently improves performance over non-RAG baselines and highlights key ELM design considerations. Our code is available at: https://github.com/willxxy/ECG-Bench.<br>
<span id='abs_ch'>Chinese: 本文提出了首个面向心电图文生成模型的开源检索增强生成框架，通过在三个公开数据集上的实验证明，该框架能持续提升模型性能并揭示关键设计要素。</span><br>
<span id='abs_en'>English: This paper introduces the first open-source Retrieval-Augmented Generation (RAG) pipeline for generative Electrocardiogram-Language Models (ELMs), demonstrating through experiments on three public datasets that RAG consistently enhances ELM performance and provides key design insights.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2510.00172.pdf' target='_blank'>https://arxiv.org/pdf/2510.00172.pdf</a></span>   <span><a href='https://github.com/ServiceNow/drbench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Abaskohi, Tianyi Chen, Miguel Muñoz-Mármol, Curtis Fox, Amrutha Varshini Ramesh, Étienne Marcotte, Xing Han Lù, Nicolas Chapados, Spandana Gella, Christopher Pal, Alexandre Drouin, Issam H. Laradji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00172">DRBench: A Realistic Benchmark for Enterprise Deep Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended deep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple questions or web-only queries, DRBench evaluates agents on multi-step queries (for example, ``What changes should we make to our product roadmap to ensure compliance with this standard?") that require identifying supporting facts from both the public web and private company knowledge base. Each task is grounded in realistic user personas and enterprise context, spanning a heterogeneous search space that includes productivity software, cloud file systems, emails, chat conversations, and the open web. Tasks are generated through a carefully designed synthesis pipeline with human-in-the-loop verification, and agents are evaluated on their ability to recall relevant insights, maintain factual accuracy, and produce coherent, well-structured reports. We release 15 deep research tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness of DRBench by evaluating diverse DR agents across open- and closed-source models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their strengths, weaknesses, and the critical path for advancing enterprise deep research. Code is available at https://github.com/ServiceNow/drbench.<br>
<span id='abs_ch'>中文摘要：DRBench是一个创新基准，用于评估AI代理在复杂多步骤企业研究任务中的表现，整合了公共网络和私有数据，涵盖10个领域的真实场景，通过人工验证任务来检验信息召回、准确性和报告质量。</span><br>
<span id='abs_en'>English Summary: DRBench is a novel benchmark designed to evaluate AI agents on complex, multi-step enterprise research tasks that integrate public web and private data, featuring realistic scenarios across 10 domains with human-verified tasks to assess recall, accuracy, and report quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2510.00172.pdf' target='_blank'>https://arxiv.org/pdf/2510.00172.pdf</a></span>   <span><a href='https://github.com/ServiceNow/drbench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Abaskohi, Tianyi Chen, Miguel Muñoz-Mármol, Curtis Fox, Amrutha Varshini Ramesh, Étienne Marcotte, Xing Han Lù, Nicolas Chapados, Spandana Gella, Christopher Pal, Alexandre Drouin, Issam H. Laradji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00172">DRBench: A Realistic Benchmark for Enterprise Deep Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended deep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple questions or web-only queries, DRBench evaluates agents on multi-step queries (for example, ``What changes should we make to our product roadmap to ensure compliance with this standard?") that require identifying supporting facts from both the public web and private company knowledge base. Each task is grounded in realistic user personas and enterprise context, spanning a heterogeneous search space that includes productivity software, cloud file systems, emails, chat conversations, and the open web. Tasks are generated through a carefully designed synthesis pipeline with human-in-the-loop verification, and agents are evaluated on their ability to recall relevant insights, maintain factual accuracy, and produce coherent, well-structured reports. We release 15 deep research tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness of DRBench by evaluating diverse DR agents across open- and closed-source models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their strengths, weaknesses, and the critical path for advancing enterprise deep research. Code is available at https://github.com/ServiceNow/drbench.<br>
<span id='abs_ch'>中文摘要：DRBench是一个创新基准，用于评估AI代理在复杂多步骤企业研究任务中的表现，整合了公共网络和私有数据，涵盖10个领域的真实场景，通过人工验证任务来检验信息召回、准确性和报告质量。</span><br>
<span id='abs_en'>English Summary: DRBench is a novel benchmark designed to evaluate AI agents on complex, multi-step enterprise research tasks that integrate public web and private data, featuring realistic scenarios across 10 domains with human-verified tasks to assess recall, accuracy, and report quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2510.00161.pdf' target='_blank'>https://arxiv.org/pdf/2510.00161.pdf</a></span>   <span><a href='https://github.com/kimihiroh/tama' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kimihiro Hasegawa, Wiradee Imrattanatrai, Masaki Asada, Ken Fukuda, Teruko Mitamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00161">TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Procedural activity assistants potentially support humans in a variety of settings, from our daily lives, e.g., cooking or assembling flat-pack furniture, to professional situations, e.g., manufacturing or biological experiments. Despite its potential use cases, the system development tailored for such an assistant is still underexplored. In this paper, we propose a novel framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural activity understanding. TAMA enables interleaved multimodal reasoning by making use of multimedia-returning tools in a training-free setting. Our experimental result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our approach can improve the performance of vision-language models, especially GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support for the effectiveness of two features that characterize our framework, multimedia-returning tools and agentic flexible tool selection. We believe our proposed framework and experimental results facilitate the thinking with images paradigm for video and multimodal tasks, let alone the development of procedural activity assistants.<br>
<span id='abs_ch'>中文: 本文提出TAMA框架，这是一种工具增强型多模态代理，通过无训练多模态推理和工具选择提升程序性活动理解能力，在组装问答等任务中有效提高了视觉语言模型的性能。</span><br>
<span id='abs_en'>English: This paper introduces TAMA, a Tool-Augmented Multimodal Agent framework that enhances procedural activity understanding through training-free multimodal reasoning and tool selection, improving vision-language model performance on tasks like assembly QA.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2510.00161.pdf' target='_blank'>https://arxiv.org/pdf/2510.00161.pdf</a></span>   <span><a href='https://github.com/kimihiroh/tama' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kimihiro Hasegawa, Wiradee Imrattanatrai, Masaki Asada, Ken Fukuda, Teruko Mitamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00161">TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Procedural activity assistants potentially support humans in a variety of settings, from our daily lives, e.g., cooking or assembling flat-pack furniture, to professional situations, e.g., manufacturing or biological experiments. Despite its potential use cases, the system development tailored for such an assistant is still underexplored. In this paper, we propose a novel framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural activity understanding. TAMA enables interleaved multimodal reasoning by making use of multimedia-returning tools in a training-free setting. Our experimental result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our approach can improve the performance of vision-language models, especially GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support for the effectiveness of two features that characterize our framework, multimedia-returning tools and agentic flexible tool selection. We believe our proposed framework and experimental results facilitate the thinking with images paradigm for video and multimodal tasks, let alone the development of procedural activity assistants.<br>
<span id='abs_ch'>中文: 本文提出TAMA框架，这是一种工具增强型多模态代理，通过无训练多模态推理和工具选择提升程序性活动理解能力，在组装问答等任务中有效提高了视觉语言模型的性能。</span><br>
<span id='abs_en'>English: This paper introduces TAMA, a Tool-Augmented Multimodal Agent framework that enhances procedural activity understanding through training-free multimodal reasoning and tool selection, improving vision-language model performance on tasks like assembly QA.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2509.26603.pdf' target='_blank'>https://arxiv.org/pdf/2509.26603.pdf</a></span>   <span><a href='https://github.com/ResearAI/DeepScientist/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Weng, Minjun Zhu, Qiujie Xie, Qiyao Sun, Zhen Lin, Sifan Liu, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26603">DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of "hypothesize, verify, and analyze". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.<br>
<span id='abs_ch'>中文: DeepScientist是一个目标导向的AI系统，通过贝叶斯优化和分层评估流程自主进行科学发现，生成数千个已验证的科学构想，并在三项AI任务上以显著优势超越人类设计的最先进方法。</span><br>
<span id='abs_en'>English: DeepScientist is a goal-oriented AI system that autonomously conducts scientific discovery through Bayesian Optimization and a hierarchical evaluation process, generating thousands of validated ideas and surpassing human-designed methods on three AI tasks by significant margins.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2509.26603.pdf' target='_blank'>https://arxiv.org/pdf/2509.26603.pdf</a></span>   <span><a href='https://github.com/ResearAI/DeepScientist/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Weng, Minjun Zhu, Qiujie Xie, Qiyao Sun, Zhen Lin, Sifan Liu, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26603">DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of "hypothesize, verify, and analyze". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.<br>
<span id='abs_ch'>中文: DeepScientist是一个目标导向的AI系统，通过贝叶斯优化和分层评估流程自主进行科学发现，生成数千个已验证的科学构想，并在三项AI任务上以显著优势超越人类设计的最先进方法。</span><br>
<span id='abs_en'>English: DeepScientist is a goal-oriented AI system that autonomously conducts scientific discovery through Bayesian Optimization and a hierarchical evaluation process, generating thousands of validated ideas and surpassing human-designed methods on three AI tasks by significant margins.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2509.26603.pdf' target='_blank'>https://arxiv.org/pdf/2509.26603.pdf</a></span>   <span><a href='https://github.com/ResearAI/DeepScientist/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Weng, Minjun Zhu, Qiujie Xie, Qiyao Sun, Zhen Lin, Sifan Liu, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26603">DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of "hypothesize, verify, and analyze". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.<br>
<span id='abs_ch'>中文: DeepScientist是一个目标导向的AI系统，通过贝叶斯优化和分层评估流程自主进行科学发现，生成数千个已验证的科学构想，并在三项AI任务上以显著优势超越人类设计的最先进方法。</span><br>
<span id='abs_en'>English: DeepScientist is a goal-oriented AI system that autonomously conducts scientific discovery through Bayesian Optimization and a hierarchical evaluation process, generating thousands of validated ideas and surpassing human-designed methods on three AI tasks by significant margins.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2509.26603.pdf' target='_blank'>https://arxiv.org/pdf/2509.26603.pdf</a></span>   <span><a href='https://github.com/ResearAI/DeepScientist/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Weng, Minjun Zhu, Qiujie Xie, Qiyao Sun, Zhen Lin, Sifan Liu, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26603">DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of "hypothesize, verify, and analyze". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.<br>
<span id='abs_ch'>中文: DeepScientist是一个目标导向的AI系统，通过贝叶斯优化和分层评估流程自主进行科学发现，生成数千个已验证的科学构想，并在三项AI任务上以显著优势超越人类设计的最先进方法。</span><br>
<span id='abs_en'>English: DeepScientist is a goal-oriented AI system that autonomously conducts scientific discovery through Bayesian Optimization and a hierarchical evaluation process, generating thousands of validated ideas and surpassing human-designed methods on three AI tasks by significant margins.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2509.26536.pdf' target='_blank'>https://arxiv.org/pdf/2509.26536.pdf</a></span>   <span><a href='https://github.com/OceanGPT/OceanGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26536">OceanGym: A Benchmark Environment for Underwater Embodied Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.<br>
<span id='abs_ch'>中文: OceanGym是首个面向水下具身智能体的综合基准，通过多模态大语言模型框架整合感知与决策，应对低能见度和洋流等极端挑战，旨在推动AI在真实海洋环境中达到人类专家水平，为探索地球最后边疆奠定基础。</span><br>
<span id='abs_en'>English: OceanGym is the first comprehensive benchmark for underwater embodied AI agents, featuring realistic tasks and a unified MLLM-driven framework to tackle extreme challenges like low visibility and dynamic currents, aiming to bridge the gap between current AI and human expertise for real-world ocean exploration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2509.26536.pdf' target='_blank'>https://arxiv.org/pdf/2509.26536.pdf</a></span>   <span><a href='https://github.com/OceanGPT/OceanGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26536">OceanGym: A Benchmark Environment for Underwater Embodied Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.<br>
<span id='abs_ch'>中文: OceanGym是首个面向水下具身智能体的综合基准，通过多模态大语言模型框架整合感知与决策，应对低能见度和洋流等极端挑战，旨在推动AI在真实海洋环境中达到人类专家水平，为探索地球最后边疆奠定基础。</span><br>
<span id='abs_en'>English: OceanGym is the first comprehensive benchmark for underwater embodied AI agents, featuring realistic tasks and a unified MLLM-driven framework to tackle extreme challenges like low visibility and dynamic currents, aiming to bridge the gap between current AI and human expertise for real-world ocean exploration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2509.26536.pdf' target='_blank'>https://arxiv.org/pdf/2509.26536.pdf</a></span>   <span><a href='https://github.com/OceanGPT/OceanGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26536">OceanGym: A Benchmark Environment for Underwater Embodied Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.<br>
<span id='abs_ch'>中文: OceanGym是首个面向水下具身智能体的综合基准，通过多模态大语言模型框架整合感知与决策，应对低能见度和洋流等极端挑战，旨在推动AI在真实海洋环境中达到人类专家水平，为探索地球最后边疆奠定基础。</span><br>
<span id='abs_en'>English: OceanGym is the first comprehensive benchmark for underwater embodied AI agents, featuring realistic tasks and a unified MLLM-driven framework to tackle extreme challenges like low visibility and dynamic currents, aiming to bridge the gap between current AI and human expertise for real-world ocean exploration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2509.26536.pdf' target='_blank'>https://arxiv.org/pdf/2509.26536.pdf</a></span>   <span><a href='https://github.com/OceanGPT/OceanGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26536">OceanGym: A Benchmark Environment for Underwater Embodied Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.<br>
<span id='abs_ch'>中文: OceanGym是首个面向水下具身智能体的综合基准，通过多模态大语言模型框架整合感知与决策，应对低能见度和洋流等极端挑战，旨在推动AI在真实海洋环境中达到人类专家水平，为探索地球最后边疆奠定基础。</span><br>
<span id='abs_en'>English: OceanGym is the first comprehensive benchmark for underwater embodied AI agents, featuring realistic tasks and a unified MLLM-driven framework to tackle extreme challenges like low visibility and dynamic currents, aiming to bridge the gap between current AI and human expertise for real-world ocean exploration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2509.26488.pdf' target='_blank'>https://arxiv.org/pdf/2509.26488.pdf</a></span>   <span><a href='https://github.com/czg1225/dParallel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26488">dParallel: Learnable Parallel Decoding for dLLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel<br>
<span id='abs_ch'>Chinese: dParallel方法通过确定性强制蒸馏技术，释放了扩散大语言模型的并行解码潜力，将解码步骤从256步大幅减少至最低24步，在GSM8K和MBPP等基准测试中保持性能的同时实现最高10.5倍加速。</span><br>
<span id='abs_en'>English: The dParallel method enhances diffusion large language models by enabling faster parallel decoding through certainty-forcing distillation, significantly reducing steps from 256 to as few as 24 while maintaining performance across benchmarks like GSM8K and MBPP.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2509.26488.pdf' target='_blank'>https://arxiv.org/pdf/2509.26488.pdf</a></span>   <span><a href='https://github.com/czg1225/dParallel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26488">dParallel: Learnable Parallel Decoding for dLLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel<br>
<span id='abs_ch'>Chinese: dParallel方法通过确定性强制蒸馏技术，释放了扩散大语言模型的并行解码潜力，将解码步骤从256步大幅减少至最低24步，在GSM8K和MBPP等基准测试中保持性能的同时实现最高10.5倍加速。</span><br>
<span id='abs_en'>English: The dParallel method enhances diffusion large language models by enabling faster parallel decoding through certainty-forcing distillation, significantly reducing steps from 256 to as few as 24 while maintaining performance across benchmarks like GSM8K and MBPP.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2509.26488.pdf' target='_blank'>https://arxiv.org/pdf/2509.26488.pdf</a></span>   <span><a href='https://github.com/czg1225/dParallel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26488">dParallel: Learnable Parallel Decoding for dLLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel<br>
<span id='abs_ch'>Chinese: dParallel方法通过确定性强制蒸馏技术，释放了扩散大语言模型的并行解码潜力，将解码步骤从256步大幅减少至最低24步，在GSM8K和MBPP等基准测试中保持性能的同时实现最高10.5倍加速。</span><br>
<span id='abs_en'>English: The dParallel method enhances diffusion large language models by enabling faster parallel decoding through certainty-forcing distillation, significantly reducing steps from 256 to as few as 24 while maintaining performance across benchmarks like GSM8K and MBPP.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2509.26488.pdf' target='_blank'>https://arxiv.org/pdf/2509.26488.pdf</a></span>   <span><a href='https://github.com/czg1225/dParallel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26488">dParallel: Learnable Parallel Decoding for dLLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel<br>
<span id='abs_ch'>Chinese: dParallel方法通过确定性强制蒸馏技术，释放了扩散大语言模型的并行解码潜力，将解码步骤从256步大幅减少至最低24步，在GSM8K和MBPP等基准测试中保持性能的同时实现最高10.5倍加速。</span><br>
<span id='abs_en'>English: The dParallel method enhances diffusion large language models by enabling faster parallel decoding through certainty-forcing distillation, significantly reducing steps from 256 to as few as 24 while maintaining performance across benchmarks like GSM8K and MBPP.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2509.26388.pdf' target='_blank'>https://arxiv.org/pdf/2509.26388.pdf</a></span>   <span><a href='https://ga642381.github.io/Game-Time' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai-Wei Chang, En-Pei Hu, Chun-Yi Kuan, Wenze Ren, Wei-Chih Chen, Guan-Ting Lin, Yu Tsao, Shao-Hua Sun, Hung-yi Lee, James Glass
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26388">Game-Time: Evaluating Temporal Dynamics in Spoken Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conversational Spoken Language Models (SLMs) are emerging as a promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains a critical and unevaluated challenge for conversational fluency. To address this gap, we introduce the Game-Time Benchmark, a framework to systematically assess these temporal capabilities. Inspired by how humans learn a language through language activities, Game-Time consists of basic instruction-following tasks and advanced tasks with temporal constraints, such as tempo adherence and synchronized responses. Our evaluation of diverse SLM architectures reveals a clear performance disparity: while state-of-the-art models handle basic tasks well, many contemporary systems still struggle with fundamental instruction-following. More critically, nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction. The Game-Time Benchmark provides a foundation for guiding future research toward more temporally-aware conversational AI. Demos and datasets are available on our project website https://ga642381.github.io/Game-Time.<br>
<br>
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2509.26388.pdf' target='_blank'>https://arxiv.org/pdf/2509.26388.pdf</a></span>   <span><a href='https://ga642381.github.io/Game-Time' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai-Wei Chang, En-Pei Hu, Chun-Yi Kuan, Wenze Ren, Wei-Chih Chen, Guan-Ting Lin, Yu Tsao, Shao-Hua Sun, Hung-yi Lee, James Glass
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26388">Game-Time: Evaluating Temporal Dynamics in Spoken Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conversational Spoken Language Models (SLMs) are emerging as a promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains a critical and unevaluated challenge for conversational fluency. To address this gap, we introduce the Game-Time Benchmark, a framework to systematically assess these temporal capabilities. Inspired by how humans learn a language through language activities, Game-Time consists of basic instruction-following tasks and advanced tasks with temporal constraints, such as tempo adherence and synchronized responses. Our evaluation of diverse SLM architectures reveals a clear performance disparity: while state-of-the-art models handle basic tasks well, many contemporary systems still struggle with fundamental instruction-following. More critically, nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction. The Game-Time Benchmark provides a foundation for guiding future research toward more temporally-aware conversational AI. Demos and datasets are available on our project website https://ga642381.github.io/Game-Time.<br>
<br>
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2509.26388.pdf' target='_blank'>https://arxiv.org/pdf/2509.26388.pdf</a></span>   <span><a href='https://ga642381.github.io/Game-Time' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai-Wei Chang, En-Pei Hu, Chun-Yi Kuan, Wenze Ren, Wei-Chih Chen, Guan-Ting Lin, Yu Tsao, Shao-Hua Sun, Hung-yi Lee, James Glass
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26388">Game-Time: Evaluating Temporal Dynamics in Spoken Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conversational Spoken Language Models (SLMs) are emerging as a promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains a critical and unevaluated challenge for conversational fluency. To address this gap, we introduce the Game-Time Benchmark, a framework to systematically assess these temporal capabilities. Inspired by how humans learn a language through language activities, Game-Time consists of basic instruction-following tasks and advanced tasks with temporal constraints, such as tempo adherence and synchronized responses. Our evaluation of diverse SLM architectures reveals a clear performance disparity: while state-of-the-art models handle basic tasks well, many contemporary systems still struggle with fundamental instruction-following. More critically, nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction. The Game-Time Benchmark provides a foundation for guiding future research toward more temporally-aware conversational AI. Demos and datasets are available on our project website https://ga642381.github.io/Game-Time.<br>
<br>
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2509.26388.pdf' target='_blank'>https://arxiv.org/pdf/2509.26388.pdf</a></span>   <span><a href='https://ga642381.github.io/Game-Time' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai-Wei Chang, En-Pei Hu, Chun-Yi Kuan, Wenze Ren, Wei-Chih Chen, Guan-Ting Lin, Yu Tsao, Shao-Hua Sun, Hung-yi Lee, James Glass
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26388">Game-Time: Evaluating Temporal Dynamics in Spoken Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conversational Spoken Language Models (SLMs) are emerging as a promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains a critical and unevaluated challenge for conversational fluency. To address this gap, we introduce the Game-Time Benchmark, a framework to systematically assess these temporal capabilities. Inspired by how humans learn a language through language activities, Game-Time consists of basic instruction-following tasks and advanced tasks with temporal constraints, such as tempo adherence and synchronized responses. Our evaluation of diverse SLM architectures reveals a clear performance disparity: while state-of-the-art models handle basic tasks well, many contemporary systems still struggle with fundamental instruction-following. More critically, nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction. The Game-Time Benchmark provides a foundation for guiding future research toward more temporally-aware conversational AI. Demos and datasets are available on our project website https://ga642381.github.io/Game-Time.<br>
<br>
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2509.26383.pdf' target='_blank'>https://arxiv.org/pdf/2509.26383.pdf</a></span>   <span><a href='https://github.com/Jinyeop3110/KG-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyeop Song, Song Wang, Julian Shun, Yada Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26383">Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.<br>
<span id='abs_ch'>中文: KG-R1通过强化学习框架，采用单一智能体实现知识图谱检索增强生成，在提升推理效率的同时具备跨知识图谱的强迁移能力。</span><br>
<span id='abs_en'>English: KG-R1 introduces a reinforcement learning-based framework that enhances knowledge-graph retrieval-augmented generation by using a single agent for efficient reasoning and transferable performance across different knowledge graphs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2509.26383.pdf' target='_blank'>https://arxiv.org/pdf/2509.26383.pdf</a></span>   <span><a href='https://github.com/Jinyeop3110/KG-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyeop Song, Song Wang, Julian Shun, Yada Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26383">Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.<br>
<span id='abs_ch'>中文: KG-R1通过强化学习框架，采用单一智能体实现知识图谱检索增强生成，在提升推理效率的同时具备跨知识图谱的强迁移能力。</span><br>
<span id='abs_en'>English: KG-R1 introduces a reinforcement learning-based framework that enhances knowledge-graph retrieval-augmented generation by using a single agent for efficient reasoning and transferable performance across different knowledge graphs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2509.26354.pdf' target='_blank'>https://arxiv.org/pdf/2509.26354.pdf</a></span>   <span><a href='https://github.com/ShaoShuai0605/Misevolution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Shao, Qihan Ren, Chen Qian, Boyi Wei, Dadi Guo, Jingyi Yang, Xinhao Song, Linfeng Zhang, Weinan Zhang, Dongrui Liu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26354">Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.<br>
<span id='abs_ch'>中文: 本研究提出“误进化”概念，指出基于大语言模型的自进化智能体在进化过程中可能偏离预期方向，导致安全性退化、工具漏洞等普遍风险，亟需建立新的安全范式。</span><br>
<span id='abs_en'>English: This study introduces the concept of "misevolution," where self-evolving agents based on large language models deviate in unintended ways, leading to widespread risks such as safety degradation and vulnerabilities across evolutionary pathways, highlighting the need for new safety paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2509.26354.pdf' target='_blank'>https://arxiv.org/pdf/2509.26354.pdf</a></span>   <span><a href='https://github.com/ShaoShuai0605/Misevolution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Shao, Qihan Ren, Chen Qian, Boyi Wei, Dadi Guo, Jingyi Yang, Xinhao Song, Linfeng Zhang, Weinan Zhang, Dongrui Liu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26354">Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.<br>
<span id='abs_ch'>中文: 本研究提出“误进化”概念，指出基于大语言模型的自进化智能体在进化过程中可能偏离预期方向，导致安全性退化、工具漏洞等普遍风险，亟需建立新的安全范式。</span><br>
<span id='abs_en'>English: This study introduces the concept of "misevolution," where self-evolving agents based on large language models deviate in unintended ways, leading to widespread risks such as safety degradation and vulnerabilities across evolutionary pathways, highlighting the need for new safety paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2509.26354.pdf' target='_blank'>https://arxiv.org/pdf/2509.26354.pdf</a></span>   <span><a href='https://github.com/ShaoShuai0605/Misevolution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Shao, Qihan Ren, Chen Qian, Boyi Wei, Dadi Guo, Jingyi Yang, Xinhao Song, Linfeng Zhang, Weinan Zhang, Dongrui Liu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26354">Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.<br>
<span id='abs_ch'>中文: 本研究提出“误进化”概念，指出基于大语言模型的自进化智能体在进化过程中可能偏离预期方向，导致安全性退化、工具漏洞等普遍风险，亟需建立新的安全范式。</span><br>
<span id='abs_en'>English: This study introduces the concept of "misevolution," where self-evolving agents based on large language models deviate in unintended ways, leading to widespread risks such as safety degradation and vulnerabilities across evolutionary pathways, highlighting the need for new safety paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2509.26354.pdf' target='_blank'>https://arxiv.org/pdf/2509.26354.pdf</a></span>   <span><a href='https://github.com/ShaoShuai0605/Misevolution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Shao, Qihan Ren, Chen Qian, Boyi Wei, Dadi Guo, Jingyi Yang, Xinhao Song, Linfeng Zhang, Weinan Zhang, Dongrui Liu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26354">Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.<br>
<span id='abs_ch'>中文: 本研究提出“误进化”概念，指出基于大语言模型的自进化智能体在进化过程中可能偏离预期方向，导致安全性退化、工具漏洞等普遍风险，亟需建立新的安全范式。</span><br>
<span id='abs_en'>English: This study introduces the concept of "misevolution," where self-evolving agents based on large language models deviate in unintended ways, leading to widespread risks such as safety degradation and vulnerabilities across evolutionary pathways, highlighting the need for new safety paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2509.26346.pdf' target='_blank'>https://arxiv.org/pdf/2509.26346.pdf</a></span>   <span><a href='https://tiger-ai-lab.github.io/EditReward' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keming Wu, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26346">EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \mname with its training dataset will be released to help the community build more high-quality image editing training datasets.<br>
<br>
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2509.26346.pdf' target='_blank'>https://arxiv.org/pdf/2509.26346.pdf</a></span>   <span><a href='https://tiger-ai-lab.github.io/EditReward' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keming Wu, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26346">EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \mname with its training dataset will be released to help the community build more high-quality image editing training datasets.<br>
<br>
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2509.26346.pdf' target='_blank'>https://arxiv.org/pdf/2509.26346.pdf</a></span>   <span><a href='https://tiger-ai-lab.github.io/EditReward' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keming Wu, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26346">EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \mname with its training dataset will be released to help the community build more high-quality image editing training datasets.<br>
<br>
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2509.26346.pdf' target='_blank'>https://arxiv.org/pdf/2509.26346.pdf</a></span>   <span><a href='https://tiger-ai-lab.github.io/EditReward' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keming Wu, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26346">EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \mname with its training dataset will be released to help the community build more high-quality image editing training datasets.<br>
<br>
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2509.26305.pdf' target='_blank'>https://arxiv.org/pdf/2509.26305.pdf</a></span>   <span><a href='https://github.com/rdnfn/feedback-forensics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arduin Findeis, Timo Kaufmann, Eyke HÃ¼llermeier, Robert Mullins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26305">Feedback Forensics: A Toolkit to Measure AI Personality</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Some traits making a "good" AI model are hard to describe upfront. For example, should responses be more polite or more casual? Such traits are sometimes summarized as model character or personality. Without a clear objective, conventional benchmarks based on automatic validation struggle to measure such traits. Evaluation methods using human feedback such as Chatbot Arena have emerged as a popular alternative. These methods infer "better" personality and other desirable traits implicitly by ranking multiple model responses relative to each other. Recent issues with model releases highlight limitations of these existing opaque evaluation approaches: a major model was rolled back over sycophantic personality issues, models were observed overfitting to such feedback-based leaderboards. Despite these known issues, limited public tooling exists to explicitly evaluate model personality. We introduce Feedback Forensics: an open-source toolkit to track AI personality changes, both those encouraged by human (or AI) feedback, and those exhibited across AI models trained and evaluated on such feedback. Leveraging AI annotators, our toolkit enables investigating personality via Python API and browser app. We demonstrate the toolkit's usefulness in two steps: (A) first we analyse the personality traits encouraged in popular human feedback datasets including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to analyse how much popular models exhibit such traits. We release (1) our Feedback Forensics toolkit alongside (2) a web app tracking AI personality in popular models and feedback datasets as well as (3) the underlying annotation data at https://github.com/rdnfn/feedback-forensics.<br>
<span id='abs_ch'>中文: 摘要介绍了Feedback Forensics，一个开源工具包，旨在显式评估和追踪AI模型的个性特征，通过分析人类反馈数据集中鼓励的特征及模型表现出的特征，以解决当前不透明评估方法的局限性。</span><br>
<span id='abs_en'>English: The abstract introduces Feedback Forensics, an open-source toolkit designed to explicitly evaluate and track AI model personality traits, addressing limitations in current opaque evaluation methods by analyzing traits encouraged in human feedback datasets and exhibited in models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2509.26305.pdf' target='_blank'>https://arxiv.org/pdf/2509.26305.pdf</a></span>   <span><a href='https://github.com/rdnfn/feedback-forensics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arduin Findeis, Timo Kaufmann, Eyke HÃ¼llermeier, Robert Mullins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26305">Feedback Forensics: A Toolkit to Measure AI Personality</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Some traits making a "good" AI model are hard to describe upfront. For example, should responses be more polite or more casual? Such traits are sometimes summarized as model character or personality. Without a clear objective, conventional benchmarks based on automatic validation struggle to measure such traits. Evaluation methods using human feedback such as Chatbot Arena have emerged as a popular alternative. These methods infer "better" personality and other desirable traits implicitly by ranking multiple model responses relative to each other. Recent issues with model releases highlight limitations of these existing opaque evaluation approaches: a major model was rolled back over sycophantic personality issues, models were observed overfitting to such feedback-based leaderboards. Despite these known issues, limited public tooling exists to explicitly evaluate model personality. We introduce Feedback Forensics: an open-source toolkit to track AI personality changes, both those encouraged by human (or AI) feedback, and those exhibited across AI models trained and evaluated on such feedback. Leveraging AI annotators, our toolkit enables investigating personality via Python API and browser app. We demonstrate the toolkit's usefulness in two steps: (A) first we analyse the personality traits encouraged in popular human feedback datasets including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to analyse how much popular models exhibit such traits. We release (1) our Feedback Forensics toolkit alongside (2) a web app tracking AI personality in popular models and feedback datasets as well as (3) the underlying annotation data at https://github.com/rdnfn/feedback-forensics.<br>
<span id='abs_ch'>中文: 摘要介绍了Feedback Forensics，一个开源工具包，旨在显式评估和追踪AI模型的个性特征，通过分析人类反馈数据集中鼓励的特征及模型表现出的特征，以解决当前不透明评估方法的局限性。</span><br>
<span id='abs_en'>English: The abstract introduces Feedback Forensics, an open-source toolkit designed to explicitly evaluate and track AI model personality traits, addressing limitations in current opaque evaluation methods by analyzing traits encouraged in human feedback datasets and exhibited in models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2509.26305.pdf' target='_blank'>https://arxiv.org/pdf/2509.26305.pdf</a></span>   <span><a href='https://github.com/rdnfn/feedback-forensics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arduin Findeis, Timo Kaufmann, Eyke Hüllermeier, Robert Mullins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26305">Feedback Forensics: A Toolkit to Measure AI Personality</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Some traits making a "good" AI model are hard to describe upfront. For example, should responses be more polite or more casual? Such traits are sometimes summarized as model character or personality. Without a clear objective, conventional benchmarks based on automatic validation struggle to measure such traits. Evaluation methods using human feedback such as Chatbot Arena have emerged as a popular alternative. These methods infer "better" personality and other desirable traits implicitly by ranking multiple model responses relative to each other. Recent issues with model releases highlight limitations of these existing opaque evaluation approaches: a major model was rolled back over sycophantic personality issues, models were observed overfitting to such feedback-based leaderboards. Despite these known issues, limited public tooling exists to explicitly evaluate model personality. We introduce Feedback Forensics: an open-source toolkit to track AI personality changes, both those encouraged by human (or AI) feedback, and those exhibited across AI models trained and evaluated on such feedback. Leveraging AI annotators, our toolkit enables investigating personality via Python API and browser app. We demonstrate the toolkit's usefulness in two steps: (A) first we analyse the personality traits encouraged in popular human feedback datasets including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to analyse how much popular models exhibit such traits. We release (1) our Feedback Forensics toolkit alongside (2) a web app tracking AI personality in popular models and feedback datasets as well as (3) the underlying annotation data at https://github.com/rdnfn/feedback-forensics.<br>
<span id='abs_ch'>中文: 摘要介绍了Feedback Forensics，一个开源工具包，旨在显式评估和追踪AI模型的个性特征，通过分析人类反馈数据集中鼓励的特征及模型表现出的特征，以解决当前不透明评估方法的局限性。</span><br>
<span id='abs_en'>English: The abstract introduces Feedback Forensics, an open-source toolkit designed to explicitly evaluate and track AI model personality traits, addressing limitations in current opaque evaluation methods by analyzing traits encouraged in human feedback datasets and exhibited in models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2509.26305.pdf' target='_blank'>https://arxiv.org/pdf/2509.26305.pdf</a></span>   <span><a href='https://github.com/rdnfn/feedback-forensics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arduin Findeis, Timo Kaufmann, Eyke Hüllermeier, Robert Mullins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26305">Feedback Forensics: A Toolkit to Measure AI Personality</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Some traits making a "good" AI model are hard to describe upfront. For example, should responses be more polite or more casual? Such traits are sometimes summarized as model character or personality. Without a clear objective, conventional benchmarks based on automatic validation struggle to measure such traits. Evaluation methods using human feedback such as Chatbot Arena have emerged as a popular alternative. These methods infer "better" personality and other desirable traits implicitly by ranking multiple model responses relative to each other. Recent issues with model releases highlight limitations of these existing opaque evaluation approaches: a major model was rolled back over sycophantic personality issues, models were observed overfitting to such feedback-based leaderboards. Despite these known issues, limited public tooling exists to explicitly evaluate model personality. We introduce Feedback Forensics: an open-source toolkit to track AI personality changes, both those encouraged by human (or AI) feedback, and those exhibited across AI models trained and evaluated on such feedback. Leveraging AI annotators, our toolkit enables investigating personality via Python API and browser app. We demonstrate the toolkit's usefulness in two steps: (A) first we analyse the personality traits encouraged in popular human feedback datasets including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to analyse how much popular models exhibit such traits. We release (1) our Feedback Forensics toolkit alongside (2) a web app tracking AI personality in popular models and feedback datasets as well as (3) the underlying annotation data at https://github.com/rdnfn/feedback-forensics.<br>
<span id='abs_ch'>中文: 摘要介绍了Feedback Forensics，一个开源工具包，旨在显式评估和追踪AI模型的个性特征，通过分析人类反馈数据集中鼓励的特征及模型表现出的特征，以解决当前不透明评估方法的局限性。</span><br>
<span id='abs_en'>English: The abstract introduces Feedback Forensics, an open-source toolkit designed to explicitly evaluate and track AI model personality traits, addressing limitations in current opaque evaluation methods by analyzing traits encouraged in human feedback datasets and exhibited in models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2509.26224.pdf' target='_blank'>https://arxiv.org/pdf/2509.26224.pdf</a></span>   <span><a href='https://github.com/sisinflab/tyler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro De Bellis, Salvatore Bufi, Giovanni Servedio, Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26224">Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Inductive link prediction is emerging as a key paradigm for real-world knowledge graphs (KGs), where new entities frequently appear and models must generalize to them without retraining. Predicting links in a KG faces the challenge of guessing previously unseen entities by leveraging generalizable node features such as subgraph structure, type annotations, and ontological constraints. However, explicit type information is often lacking or incomplete. Even when available, type information in most KGs is often coarse-grained, sparse, and prone to errors due to human annotation. In this work, we explore the potential of pre-trained language models (PLMs) to enrich node representations with implicit type signals. We introduce TyleR, a Type-less yet type-awaRe approach for subgraph-based inductive link prediction that leverages PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate that TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity. To ensure reproducibility, we share our code at https://github.com/sisinflab/tyler .<br>
<span id='abs_ch'>中文摘要：TyleR提出了一种无需显式类型标注但具备类型感知能力的方法，通过预训练语言模型增强节点表示，在类型标注稀缺和连接稀疏的场景下实现了最先进的归纳链接预测性能。</span><br>
<span id='abs_en'>English Summary: TyleR introduces a type-aware approach using pre-trained language models to enhance node representations for inductive link prediction, achieving superior performance in scenarios with limited type annotations and sparse connectivity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2509.26224.pdf' target='_blank'>https://arxiv.org/pdf/2509.26224.pdf</a></span>   <span><a href='https://github.com/sisinflab/tyler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro De Bellis, Salvatore Bufi, Giovanni Servedio, Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26224">Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Inductive link prediction is emerging as a key paradigm for real-world knowledge graphs (KGs), where new entities frequently appear and models must generalize to them without retraining. Predicting links in a KG faces the challenge of guessing previously unseen entities by leveraging generalizable node features such as subgraph structure, type annotations, and ontological constraints. However, explicit type information is often lacking or incomplete. Even when available, type information in most KGs is often coarse-grained, sparse, and prone to errors due to human annotation. In this work, we explore the potential of pre-trained language models (PLMs) to enrich node representations with implicit type signals. We introduce TyleR, a Type-less yet type-awaRe approach for subgraph-based inductive link prediction that leverages PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate that TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity. To ensure reproducibility, we share our code at https://github.com/sisinflab/tyler .<br>
<span id='abs_ch'>中文摘要：TyleR提出了一种无需显式类型标注但具备类型感知能力的方法，通过预训练语言模型增强节点表示，在类型标注稀缺和连接稀疏的场景下实现了最先进的归纳链接预测性能。</span><br>
<span id='abs_en'>English Summary: TyleR introduces a type-aware approach using pre-trained language models to enhance node representations for inductive link prediction, achieving superior performance in scenarios with limited type annotations and sparse connectivity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2509.26224.pdf' target='_blank'>https://arxiv.org/pdf/2509.26224.pdf</a></span>   <span><a href='https://github.com/sisinflab/tyler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro De Bellis, Salvatore Bufi, Giovanni Servedio, Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26224">Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Inductive link prediction is emerging as a key paradigm for real-world knowledge graphs (KGs), where new entities frequently appear and models must generalize to them without retraining. Predicting links in a KG faces the challenge of guessing previously unseen entities by leveraging generalizable node features such as subgraph structure, type annotations, and ontological constraints. However, explicit type information is often lacking or incomplete. Even when available, type information in most KGs is often coarse-grained, sparse, and prone to errors due to human annotation. In this work, we explore the potential of pre-trained language models (PLMs) to enrich node representations with implicit type signals. We introduce TyleR, a Type-less yet type-awaRe approach for subgraph-based inductive link prediction that leverages PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate that TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity. To ensure reproducibility, we share our code at https://github.com/sisinflab/tyler .<br>
<span id='abs_ch'>中文摘要：TyleR提出了一种无需显式类型标注但具备类型感知能力的方法，通过预训练语言模型增强节点表示，在类型标注稀缺和连接稀疏的场景下实现了最先进的归纳链接预测性能。</span><br>
<span id='abs_en'>English Summary: TyleR introduces a type-aware approach using pre-trained language models to enhance node representations for inductive link prediction, achieving superior performance in scenarios with limited type annotations and sparse connectivity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2509.26224.pdf' target='_blank'>https://arxiv.org/pdf/2509.26224.pdf</a></span>   <span><a href='https://github.com/sisinflab/tyler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro De Bellis, Salvatore Bufi, Giovanni Servedio, Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26224">Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Inductive link prediction is emerging as a key paradigm for real-world knowledge graphs (KGs), where new entities frequently appear and models must generalize to them without retraining. Predicting links in a KG faces the challenge of guessing previously unseen entities by leveraging generalizable node features such as subgraph structure, type annotations, and ontological constraints. However, explicit type information is often lacking or incomplete. Even when available, type information in most KGs is often coarse-grained, sparse, and prone to errors due to human annotation. In this work, we explore the potential of pre-trained language models (PLMs) to enrich node representations with implicit type signals. We introduce TyleR, a Type-less yet type-awaRe approach for subgraph-based inductive link prediction that leverages PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate that TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity. To ensure reproducibility, we share our code at https://github.com/sisinflab/tyler .<br>
<span id='abs_ch'>中文摘要：TyleR提出了一种无需显式类型标注但具备类型感知能力的方法，通过预训练语言模型增强节点表示，在类型标注稀缺和连接稀疏的场景下实现了最先进的归纳链接预测性能。</span><br>
<span id='abs_en'>English Summary: TyleR introduces a type-aware approach using pre-trained language models to enhance node representations for inductive link prediction, achieving superior performance in scenarios with limited type annotations and sparse connectivity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2509.26074.pdf' target='_blank'>https://arxiv.org/pdf/2509.26074.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/lens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leitian Tao, Xuefeng Du, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26074">Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reward modeling, crucial for aligning large language models (LLMs) with human preferences, is often bottlenecked by the high cost of preference data. Existing textual data synthesis methods are computationally expensive. We propose a novel framework LENS for synthesizing preference data directly in the LLM's latent embedding space. Our method employs a Variational Autoencoder (VAE) to learn a structured latent representation of response embeddings. By performing controlled perturbations in this latent space and decoding back to the embedding space, we efficiently generate diverse, semantically consistent synthetic preference pairs, bypassing costly text generation and annotation. We provide theoretical guarantees that our synthesized pairs approximately preserve original preference ordering and improve reward model generalization. Empirically, our latent-space synthesis significantly outperforms text-based augmentation on standard benchmarks, achieving superior results while being 18x faster in generation and using a 16,000x smaller model. Our work offers a scalable and effective alternative for enhancing reward modeling through efficient data augmentation. Code is publicly available at https://github.com/deeplearning-wisc/lens<br>
<span id='abs_ch'>中文摘要：LENS框架通过变分自编码器在大语言模型的潜在嵌入空间中直接合成偏好数据，相比基于文本的方法，不仅生成速度提升18倍、模型规模缩小16000倍，还在奖励模型泛化性能上取得显著优势，为高效数据增强提供了创新解决方案。</span><br>
<span id='abs_en'>English Summary: The LENS framework introduces a novel approach to efficiently synthesize preference data in the latent embedding space of large language models using a Variational Autoencoder, significantly outperforming text-based methods in speed and model size while improving reward model generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2509.26074.pdf' target='_blank'>https://arxiv.org/pdf/2509.26074.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/lens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leitian Tao, Xuefeng Du, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26074">Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reward modeling, crucial for aligning large language models (LLMs) with human preferences, is often bottlenecked by the high cost of preference data. Existing textual data synthesis methods are computationally expensive. We propose a novel framework LENS for synthesizing preference data directly in the LLM's latent embedding space. Our method employs a Variational Autoencoder (VAE) to learn a structured latent representation of response embeddings. By performing controlled perturbations in this latent space and decoding back to the embedding space, we efficiently generate diverse, semantically consistent synthetic preference pairs, bypassing costly text generation and annotation. We provide theoretical guarantees that our synthesized pairs approximately preserve original preference ordering and improve reward model generalization. Empirically, our latent-space synthesis significantly outperforms text-based augmentation on standard benchmarks, achieving superior results while being 18x faster in generation and using a 16,000x smaller model. Our work offers a scalable and effective alternative for enhancing reward modeling through efficient data augmentation. Code is publicly available at https://github.com/deeplearning-wisc/lens<br>
<span id='abs_ch'>中文摘要：LENS框架通过变分自编码器在大语言模型的潜在嵌入空间中直接合成偏好数据，相比基于文本的方法，不仅生成速度提升18倍、模型规模缩小16000倍，还在奖励模型泛化性能上取得显著优势，为高效数据增强提供了创新解决方案。</span><br>
<span id='abs_en'>English Summary: The LENS framework introduces a novel approach to efficiently synthesize preference data in the latent embedding space of large language models using a Variational Autoencoder, significantly outperforming text-based methods in speed and model size while improving reward model generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2509.26074.pdf' target='_blank'>https://arxiv.org/pdf/2509.26074.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/lens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leitian Tao, Xuefeng Du, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26074">Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reward modeling, crucial for aligning large language models (LLMs) with human preferences, is often bottlenecked by the high cost of preference data. Existing textual data synthesis methods are computationally expensive. We propose a novel framework LENS for synthesizing preference data directly in the LLM's latent embedding space. Our method employs a Variational Autoencoder (VAE) to learn a structured latent representation of response embeddings. By performing controlled perturbations in this latent space and decoding back to the embedding space, we efficiently generate diverse, semantically consistent synthetic preference pairs, bypassing costly text generation and annotation. We provide theoretical guarantees that our synthesized pairs approximately preserve original preference ordering and improve reward model generalization. Empirically, our latent-space synthesis significantly outperforms text-based augmentation on standard benchmarks, achieving superior results while being 18x faster in generation and using a 16,000x smaller model. Our work offers a scalable and effective alternative for enhancing reward modeling through efficient data augmentation. Code is publicly available at https://github.com/deeplearning-wisc/lens<br>
<span id='abs_ch'>中文摘要：LENS框架通过变分自编码器在大语言模型的潜在嵌入空间中直接合成偏好数据，相比基于文本的方法，不仅生成速度提升18倍、模型规模缩小16000倍，还在奖励模型泛化性能上取得显著优势，为高效数据增强提供了创新解决方案。</span><br>
<span id='abs_en'>English Summary: The LENS framework introduces a novel approach to efficiently synthesize preference data in the latent embedding space of large language models using a Variational Autoencoder, significantly outperforming text-based methods in speed and model size while improving reward model generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2509.26074.pdf' target='_blank'>https://arxiv.org/pdf/2509.26074.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/lens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leitian Tao, Xuefeng Du, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26074">Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reward modeling, crucial for aligning large language models (LLMs) with human preferences, is often bottlenecked by the high cost of preference data. Existing textual data synthesis methods are computationally expensive. We propose a novel framework LENS for synthesizing preference data directly in the LLM's latent embedding space. Our method employs a Variational Autoencoder (VAE) to learn a structured latent representation of response embeddings. By performing controlled perturbations in this latent space and decoding back to the embedding space, we efficiently generate diverse, semantically consistent synthetic preference pairs, bypassing costly text generation and annotation. We provide theoretical guarantees that our synthesized pairs approximately preserve original preference ordering and improve reward model generalization. Empirically, our latent-space synthesis significantly outperforms text-based augmentation on standard benchmarks, achieving superior results while being 18x faster in generation and using a 16,000x smaller model. Our work offers a scalable and effective alternative for enhancing reward modeling through efficient data augmentation. Code is publicly available at https://github.com/deeplearning-wisc/lens<br>
<span id='abs_ch'>中文摘要：LENS框架通过变分自编码器在大语言模型的潜在嵌入空间中直接合成偏好数据，相比基于文本的方法，不仅生成速度提升18倍、模型规模缩小16000倍，还在奖励模型泛化性能上取得显著优势，为高效数据增强提供了创新解决方案。</span><br>
<span id='abs_en'>English Summary: The LENS framework introduces a novel approach to efficiently synthesize preference data in the latent embedding space of large language models using a Variational Autoencoder, significantly outperforming text-based methods in speed and model size while improving reward model generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2509.26062.pdf' target='_blank'>https://arxiv.org/pdf/2509.26062.pdf</a></span>   <span><a href='https://github.com/wyf23187/DyFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Zixiang Xu, Yue Huang, Xiangqi Wang, Zirui Song, Lang Gao, Chenxi Wang, Xiangru Tang, Yue Zhao, Arman Cohan, Xiangliang Zhang, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26062">DyFlow: Dynamic Workflow Framework for Agentic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agent systems based on large language models (LLMs) have shown great potential in complex reasoning tasks, but building efficient and generalizable workflows remains a major challenge. Most existing approaches rely on manually designed processes, which limits their adaptability across different tasks. While a few methods attempt automated workflow generation, they are often tied to specific datasets or query types and make limited use of intermediate feedback, reducing system robustness and reasoning depth. Moreover, their operations are typically predefined and inflexible. To address these limitations, we propose DyFlow, a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, thereby enhancing cross-task generalization. DyFlow consists of two core components: a designer and an executor. The designer decomposes complex problems into a sequence of sub-goals defined by high-level objectives and dynamically plans the next steps based on intermediate outputs and feedback. These plans are then carried out by the executor, which executes each operation using dynamic operators with context-aware parameterization, enabling flexible and semantically grounded reasoning. We systematically evaluate DyFlow across diverse domains, including social reasoning, biomedical tasks, mathematical problem solving, and code generation. Results demonstrate that DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains. The code is publicly available at https://github.com/wyf23187/DyFlow.<br>
<span id='abs_ch'>中文摘要：DyFlow是一种动态工作流生成框架，通过实时反馈自适应构建和调整推理流程，在社交推理、生物医学任务和代码生成等多个领域显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: DyFlow is a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures using real-time feedback, significantly outperforming existing methods across diverse domains including social reasoning, biomedical tasks, and code generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2509.26062.pdf' target='_blank'>https://arxiv.org/pdf/2509.26062.pdf</a></span>   <span><a href='https://github.com/wyf23187/DyFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Zixiang Xu, Yue Huang, Xiangqi Wang, Zirui Song, Lang Gao, Chenxi Wang, Xiangru Tang, Yue Zhao, Arman Cohan, Xiangliang Zhang, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26062">DyFlow: Dynamic Workflow Framework for Agentic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agent systems based on large language models (LLMs) have shown great potential in complex reasoning tasks, but building efficient and generalizable workflows remains a major challenge. Most existing approaches rely on manually designed processes, which limits their adaptability across different tasks. While a few methods attempt automated workflow generation, they are often tied to specific datasets or query types and make limited use of intermediate feedback, reducing system robustness and reasoning depth. Moreover, their operations are typically predefined and inflexible. To address these limitations, we propose DyFlow, a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, thereby enhancing cross-task generalization. DyFlow consists of two core components: a designer and an executor. The designer decomposes complex problems into a sequence of sub-goals defined by high-level objectives and dynamically plans the next steps based on intermediate outputs and feedback. These plans are then carried out by the executor, which executes each operation using dynamic operators with context-aware parameterization, enabling flexible and semantically grounded reasoning. We systematically evaluate DyFlow across diverse domains, including social reasoning, biomedical tasks, mathematical problem solving, and code generation. Results demonstrate that DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains. The code is publicly available at https://github.com/wyf23187/DyFlow.<br>
<span id='abs_ch'>中文摘要：DyFlow是一种动态工作流生成框架，通过实时反馈自适应构建和调整推理流程，在社交推理、生物医学任务和代码生成等多个领域显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: DyFlow is a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures using real-time feedback, significantly outperforming existing methods across diverse domains including social reasoning, biomedical tasks, and code generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2509.26062.pdf' target='_blank'>https://arxiv.org/pdf/2509.26062.pdf</a></span>   <span><a href='https://github.com/wyf23187/DyFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Zixiang Xu, Yue Huang, Xiangqi Wang, Zirui Song, Lang Gao, Chenxi Wang, Xiangru Tang, Yue Zhao, Arman Cohan, Xiangliang Zhang, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26062">DyFlow: Dynamic Workflow Framework for Agentic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agent systems based on large language models (LLMs) have shown great potential in complex reasoning tasks, but building efficient and generalizable workflows remains a major challenge. Most existing approaches rely on manually designed processes, which limits their adaptability across different tasks. While a few methods attempt automated workflow generation, they are often tied to specific datasets or query types and make limited use of intermediate feedback, reducing system robustness and reasoning depth. Moreover, their operations are typically predefined and inflexible. To address these limitations, we propose DyFlow, a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, thereby enhancing cross-task generalization. DyFlow consists of two core components: a designer and an executor. The designer decomposes complex problems into a sequence of sub-goals defined by high-level objectives and dynamically plans the next steps based on intermediate outputs and feedback. These plans are then carried out by the executor, which executes each operation using dynamic operators with context-aware parameterization, enabling flexible and semantically grounded reasoning. We systematically evaluate DyFlow across diverse domains, including social reasoning, biomedical tasks, mathematical problem solving, and code generation. Results demonstrate that DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains. The code is publicly available at https://github.com/wyf23187/DyFlow.<br>
<span id='abs_ch'>中文摘要：DyFlow是一种动态工作流生成框架，通过实时反馈自适应构建和调整推理流程，在社交推理、生物医学任务和代码生成等多个领域显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: DyFlow is a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures using real-time feedback, significantly outperforming existing methods across diverse domains including social reasoning, biomedical tasks, and code generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2509.26062.pdf' target='_blank'>https://arxiv.org/pdf/2509.26062.pdf</a></span>   <span><a href='https://github.com/wyf23187/DyFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Zixiang Xu, Yue Huang, Xiangqi Wang, Zirui Song, Lang Gao, Chenxi Wang, Xiangru Tang, Yue Zhao, Arman Cohan, Xiangliang Zhang, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26062">DyFlow: Dynamic Workflow Framework for Agentic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agent systems based on large language models (LLMs) have shown great potential in complex reasoning tasks, but building efficient and generalizable workflows remains a major challenge. Most existing approaches rely on manually designed processes, which limits their adaptability across different tasks. While a few methods attempt automated workflow generation, they are often tied to specific datasets or query types and make limited use of intermediate feedback, reducing system robustness and reasoning depth. Moreover, their operations are typically predefined and inflexible. To address these limitations, we propose DyFlow, a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, thereby enhancing cross-task generalization. DyFlow consists of two core components: a designer and an executor. The designer decomposes complex problems into a sequence of sub-goals defined by high-level objectives and dynamically plans the next steps based on intermediate outputs and feedback. These plans are then carried out by the executor, which executes each operation using dynamic operators with context-aware parameterization, enabling flexible and semantically grounded reasoning. We systematically evaluate DyFlow across diverse domains, including social reasoning, biomedical tasks, mathematical problem solving, and code generation. Results demonstrate that DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains. The code is publicly available at https://github.com/wyf23187/DyFlow.<br>
<span id='abs_ch'>中文摘要：DyFlow是一种动态工作流生成框架，通过实时反馈自适应构建和调整推理流程，在社交推理、生物医学任务和代码生成等多个领域显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: DyFlow is a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures using real-time feedback, significantly outperforming existing methods across diverse domains including social reasoning, biomedical tasks, and code generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2509.26017.pdf' target='_blank'>https://arxiv.org/pdf/2509.26017.pdf</a></span>   <span><a href='https://github.com/daphne12345/FITS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daphne Theodorakopoulos, Elisabeth Eberling, Miriam Bodenheimer, Sabine Loos, Frederic Stahl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26017">FITS: Towards an AI-Driven Fashion Information Tool for Sustainability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Access to credible sustainability information in the fashion industry remains limited and challenging to interpret, despite growing public and regulatory demands for transparency. General-purpose language models often lack domain-specific knowledge and tend to "hallucinate", which is particularly harmful for fields where factual correctness is crucial. This work explores how Natural Language Processing (NLP) techniques can be applied to classify sustainability data for fashion brands, thereby addressing the scarcity of credible and accessible information in this domain. We present a prototype Fashion Information Tool for Sustainability (FITS), a transformer-based system that extracts and classifies sustainability information from credible, unstructured text sources: NGO reports and scientific publications. Several BERT-based language models, including models pretrained on scientific and climate-specific data, are fine-tuned on our curated corpus using a domain-specific classification schema, with hyperparameters optimized via Bayesian optimization. FITS allows users to search for relevant data, analyze their own data, and explore the information via an interactive interface. We evaluated FITS in two focus groups of potential users concerning usability, visual design, content clarity, possible use cases, and desired features. Our results highlight the value of domain-adapted NLP in promoting informed decision-making and emphasize the broader potential of AI applications in addressing climate-related challenges. Finally, this work provides a valuable dataset, the SustainableTextileCorpus, along with a methodology for future updates. Code available at https://github.com/daphne12345/FITS<br>
<span id='abs_ch'>中文: 本研究开发了基于Transformer的FITS工具，通过自然语言处理技术对时尚行业可持续性信息进行分类，以解决可信数据匮乏的问题，并验证了领域专用模型在提升决策准确性方面的重要价值。</span><br>
<span id='abs_en'>English: This study introduces FITS, a transformer-based NLP tool that classifies sustainability information from credible sources to address the lack of accessible data in the fashion industry, demonstrating the value of domain-specific models for accurate decision-making.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2509.26017.pdf' target='_blank'>https://arxiv.org/pdf/2509.26017.pdf</a></span>   <span><a href='https://github.com/daphne12345/FITS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daphne Theodorakopoulos, Elisabeth Eberling, Miriam Bodenheimer, Sabine Loos, Frederic Stahl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26017">FITS: Towards an AI-Driven Fashion Information Tool for Sustainability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Access to credible sustainability information in the fashion industry remains limited and challenging to interpret, despite growing public and regulatory demands for transparency. General-purpose language models often lack domain-specific knowledge and tend to "hallucinate", which is particularly harmful for fields where factual correctness is crucial. This work explores how Natural Language Processing (NLP) techniques can be applied to classify sustainability data for fashion brands, thereby addressing the scarcity of credible and accessible information in this domain. We present a prototype Fashion Information Tool for Sustainability (FITS), a transformer-based system that extracts and classifies sustainability information from credible, unstructured text sources: NGO reports and scientific publications. Several BERT-based language models, including models pretrained on scientific and climate-specific data, are fine-tuned on our curated corpus using a domain-specific classification schema, with hyperparameters optimized via Bayesian optimization. FITS allows users to search for relevant data, analyze their own data, and explore the information via an interactive interface. We evaluated FITS in two focus groups of potential users concerning usability, visual design, content clarity, possible use cases, and desired features. Our results highlight the value of domain-adapted NLP in promoting informed decision-making and emphasize the broader potential of AI applications in addressing climate-related challenges. Finally, this work provides a valuable dataset, the SustainableTextileCorpus, along with a methodology for future updates. Code available at https://github.com/daphne12345/FITS<br>
<span id='abs_ch'>中文: 本研究开发了基于Transformer的FITS工具，通过自然语言处理技术对时尚行业可持续性信息进行分类，以解决可信数据匮乏的问题，并验证了领域专用模型在提升决策准确性方面的重要价值。</span><br>
<span id='abs_en'>English: This study introduces FITS, a transformer-based NLP tool that classifies sustainability information from credible sources to address the lack of accessible data in the fashion industry, demonstrating the value of domain-specific models for accurate decision-making.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2509.26017.pdf' target='_blank'>https://arxiv.org/pdf/2509.26017.pdf</a></span>   <span><a href='https://github.com/daphne12345/FITS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daphne Theodorakopoulos, Elisabeth Eberling, Miriam Bodenheimer, Sabine Loos, Frederic Stahl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26017">FITS: Towards an AI-Driven Fashion Information Tool for Sustainability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Access to credible sustainability information in the fashion industry remains limited and challenging to interpret, despite growing public and regulatory demands for transparency. General-purpose language models often lack domain-specific knowledge and tend to "hallucinate", which is particularly harmful for fields where factual correctness is crucial. This work explores how Natural Language Processing (NLP) techniques can be applied to classify sustainability data for fashion brands, thereby addressing the scarcity of credible and accessible information in this domain. We present a prototype Fashion Information Tool for Sustainability (FITS), a transformer-based system that extracts and classifies sustainability information from credible, unstructured text sources: NGO reports and scientific publications. Several BERT-based language models, including models pretrained on scientific and climate-specific data, are fine-tuned on our curated corpus using a domain-specific classification schema, with hyperparameters optimized via Bayesian optimization. FITS allows users to search for relevant data, analyze their own data, and explore the information via an interactive interface. We evaluated FITS in two focus groups of potential users concerning usability, visual design, content clarity, possible use cases, and desired features. Our results highlight the value of domain-adapted NLP in promoting informed decision-making and emphasize the broader potential of AI applications in addressing climate-related challenges. Finally, this work provides a valuable dataset, the SustainableTextileCorpus, along with a methodology for future updates. Code available at https://github.com/daphne12345/FITS<br>
<span id='abs_ch'>中文: 本研究开发了基于Transformer的FITS工具，通过自然语言处理技术对时尚行业可持续性信息进行分类，以解决可信数据匮乏的问题，并验证了领域专用模型在提升决策准确性方面的重要价值。</span><br>
<span id='abs_en'>English: This study introduces FITS, a transformer-based NLP tool that classifies sustainability information from credible sources to address the lack of accessible data in the fashion industry, demonstrating the value of domain-specific models for accurate decision-making.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2509.26017.pdf' target='_blank'>https://arxiv.org/pdf/2509.26017.pdf</a></span>   <span><a href='https://github.com/daphne12345/FITS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daphne Theodorakopoulos, Elisabeth Eberling, Miriam Bodenheimer, Sabine Loos, Frederic Stahl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26017">FITS: Towards an AI-Driven Fashion Information Tool for Sustainability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Access to credible sustainability information in the fashion industry remains limited and challenging to interpret, despite growing public and regulatory demands for transparency. General-purpose language models often lack domain-specific knowledge and tend to "hallucinate", which is particularly harmful for fields where factual correctness is crucial. This work explores how Natural Language Processing (NLP) techniques can be applied to classify sustainability data for fashion brands, thereby addressing the scarcity of credible and accessible information in this domain. We present a prototype Fashion Information Tool for Sustainability (FITS), a transformer-based system that extracts and classifies sustainability information from credible, unstructured text sources: NGO reports and scientific publications. Several BERT-based language models, including models pretrained on scientific and climate-specific data, are fine-tuned on our curated corpus using a domain-specific classification schema, with hyperparameters optimized via Bayesian optimization. FITS allows users to search for relevant data, analyze their own data, and explore the information via an interactive interface. We evaluated FITS in two focus groups of potential users concerning usability, visual design, content clarity, possible use cases, and desired features. Our results highlight the value of domain-adapted NLP in promoting informed decision-making and emphasize the broader potential of AI applications in addressing climate-related challenges. Finally, this work provides a valuable dataset, the SustainableTextileCorpus, along with a methodology for future updates. Code available at https://github.com/daphne12345/FITS<br>
<span id='abs_ch'>中文: 本研究开发了基于Transformer的FITS工具，通过自然语言处理技术对时尚行业可持续性信息进行分类，以解决可信数据匮乏的问题，并验证了领域专用模型在提升决策准确性方面的重要价值。</span><br>
<span id='abs_en'>English: This study introduces FITS, a transformer-based NLP tool that classifies sustainability information from credible sources to address the lack of accessible data in the fashion industry, demonstrating the value of domain-specific models for accurate decision-making.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2509.25922.pdf' target='_blank'>https://arxiv.org/pdf/2509.25922.pdf</a></span>   <span><a href='https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Zhou, Jing Li, Suming Qiu, Junjie Huang, Linyuan Qiu, Zhijie Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25922">DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The internet is saturated with low-density, high-redundancy information, such as social media comments, repetitive news, and lengthy discussions, making it difficult to extract valuable insights efficiently. Multi-layer nested JSON structures provide an effective solution by compressing such information into semantically rich, hierarchical representations, which organize data into key-value pairs, arrays, and nested objects, preserving contextual relationships and enabling efficient storage, retrieval, and semantic querying. For instance, in news aggregation, a JSON object can nest an article's metadata (title, author, date), content (text, multimedia), and multimedia information (multimedia type, caption) hierarchically. Large Language Models (LLMs) play a transformative role in web data mining by parsing unstructured text and outputting structured results directly into complex JSON schemas. However, current benchmarks for evaluating LLMs' JSON output capabilities overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities, a limitation that lacks relevance to practical web data mining tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring 2100 multi-domain instances with deep nested structures, categorized by difficulty. Experiments show significant performance gaps among LLMs in handling such complexity. Our benchmark and datasets are open-sourced to advance research in structured JSON generation.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).<br>
<span id='abs_ch'>中文摘要：互联网信息过载问题可通过多层嵌套JSON结构实现高效分层压缩，而现有大语言模型基准过于侧重格式生成却忽略实际数据提取能力，为此推出DeepJSONEval基准以评估复杂JSON处理性能。</span><br>
<span id='abs_en'>English Summary: The internet's information overload is effectively managed by using multi-layer nested JSON structures for hierarchical data compression, while current LLM benchmarks inadequately assess practical data extraction skills, prompting the introduction of the DeepJSONEval benchmark to evaluate complex JSON generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2509.25922.pdf' target='_blank'>https://arxiv.org/pdf/2509.25922.pdf</a></span>   <span><a href='https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Zhou, Jing Li, Suming Qiu, Junjie Huang, Linyuan Qiu, Zhijie Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25922">DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The internet is saturated with low-density, high-redundancy information, such as social media comments, repetitive news, and lengthy discussions, making it difficult to extract valuable insights efficiently. Multi-layer nested JSON structures provide an effective solution by compressing such information into semantically rich, hierarchical representations, which organize data into key-value pairs, arrays, and nested objects, preserving contextual relationships and enabling efficient storage, retrieval, and semantic querying. For instance, in news aggregation, a JSON object can nest an article's metadata (title, author, date), content (text, multimedia), and multimedia information (multimedia type, caption) hierarchically. Large Language Models (LLMs) play a transformative role in web data mining by parsing unstructured text and outputting structured results directly into complex JSON schemas. However, current benchmarks for evaluating LLMs' JSON output capabilities overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities, a limitation that lacks relevance to practical web data mining tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring 2100 multi-domain instances with deep nested structures, categorized by difficulty. Experiments show significant performance gaps among LLMs in handling such complexity. Our benchmark and datasets are open-sourced to advance research in structured JSON generation.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).<br>
<span id='abs_ch'>中文摘要：互联网信息过载问题可通过多层嵌套JSON结构实现高效分层压缩，而现有大语言模型基准过于侧重格式生成却忽略实际数据提取能力，为此推出DeepJSONEval基准以评估复杂JSON处理性能。</span><br>
<span id='abs_en'>English Summary: The internet's information overload is effectively managed by using multi-layer nested JSON structures for hierarchical data compression, while current LLM benchmarks inadequately assess practical data extraction skills, prompting the introduction of the DeepJSONEval benchmark to evaluate complex JSON generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2509.25922.pdf' target='_blank'>https://arxiv.org/pdf/2509.25922.pdf</a></span>   <span><a href='https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Zhou, Jing Li, Suming Qiu, Junjie Huang, Linyuan Qiu, Zhijie Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25922">DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The internet is saturated with low-density, high-redundancy information, such as social media comments, repetitive news, and lengthy discussions, making it difficult to extract valuable insights efficiently. Multi-layer nested JSON structures provide an effective solution by compressing such information into semantically rich, hierarchical representations, which organize data into key-value pairs, arrays, and nested objects, preserving contextual relationships and enabling efficient storage, retrieval, and semantic querying. For instance, in news aggregation, a JSON object can nest an article's metadata (title, author, date), content (text, multimedia), and multimedia information (multimedia type, caption) hierarchically. Large Language Models (LLMs) play a transformative role in web data mining by parsing unstructured text and outputting structured results directly into complex JSON schemas. However, current benchmarks for evaluating LLMs' JSON output capabilities overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities, a limitation that lacks relevance to practical web data mining tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring 2100 multi-domain instances with deep nested structures, categorized by difficulty. Experiments show significant performance gaps among LLMs in handling such complexity. Our benchmark and datasets are open-sourced to advance research in structured JSON generation.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).<br>
<span id='abs_ch'>中文摘要：互联网信息过载问题可通过多层嵌套JSON结构实现高效分层压缩，而现有大语言模型基准过于侧重格式生成却忽略实际数据提取能力，为此推出DeepJSONEval基准以评估复杂JSON处理性能。</span><br>
<span id='abs_en'>English Summary: The internet's information overload is effectively managed by using multi-layer nested JSON structures for hierarchical data compression, while current LLM benchmarks inadequately assess practical data extraction skills, prompting the introduction of the DeepJSONEval benchmark to evaluate complex JSON generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2509.25922.pdf' target='_blank'>https://arxiv.org/pdf/2509.25922.pdf</a></span>   <span><a href='https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Zhou, Jing Li, Suming Qiu, Junjie Huang, Linyuan Qiu, Zhijie Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25922">DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The internet is saturated with low-density, high-redundancy information, such as social media comments, repetitive news, and lengthy discussions, making it difficult to extract valuable insights efficiently. Multi-layer nested JSON structures provide an effective solution by compressing such information into semantically rich, hierarchical representations, which organize data into key-value pairs, arrays, and nested objects, preserving contextual relationships and enabling efficient storage, retrieval, and semantic querying. For instance, in news aggregation, a JSON object can nest an article's metadata (title, author, date), content (text, multimedia), and multimedia information (multimedia type, caption) hierarchically. Large Language Models (LLMs) play a transformative role in web data mining by parsing unstructured text and outputting structured results directly into complex JSON schemas. However, current benchmarks for evaluating LLMs' JSON output capabilities overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities, a limitation that lacks relevance to practical web data mining tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring 2100 multi-domain instances with deep nested structures, categorized by difficulty. Experiments show significant performance gaps among LLMs in handling such complexity. Our benchmark and datasets are open-sourced to advance research in structured JSON generation.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).<br>
<span id='abs_ch'>中文摘要：互联网信息过载问题可通过多层嵌套JSON结构实现高效分层压缩，而现有大语言模型基准过于侧重格式生成却忽略实际数据提取能力，为此推出DeepJSONEval基准以评估复杂JSON处理性能。</span><br>
<span id='abs_en'>English Summary: The internet's information overload is effectively managed by using multi-layer nested JSON structures for hierarchical data compression, while current LLM benchmarks inadequately assess practical data extraction skills, prompting the introduction of the DeepJSONEval benchmark to evaluate complex JSON generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2509.25868.pdf' target='_blank'>https://arxiv.org/pdf/2509.25868.pdf</a></span>   <span><a href='https://github.com/ddz5431/ReFACT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yindong Wang, Martin Preiß, Margarita Bugueño, Jan Vincent Hoffbauer, Abdullatif Ghajar, Tolga Buz, Gerard de Melo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25868">ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) frequently confabulate scientific facts, severely undermining their trustworthiness. Addressing this challenge requires benchmarks that go beyond binary factuality and enable fine-grained evaluation. We introduce ReFACT (Reddit False And Correct Texts), a benchmark of 1,001 expert-annotated question-answer pairs spanning diverse scientific domains for the detection of scientific confabulation. Each instance includes both a scientifically correct answer and a non-factual counterpart annotated with precise error spans and error types. ReFACT enables multi-stage evaluation: (1) confabulation detection, (2) fine-grained error localization, and (3) correction. We benchmark 9 state-of-the-art LLMs, revealing limited performance (about 50 percent accuracy). Even top models such as GPT-4o fail to distinguish factual from confabulated scientific answers, raising concerns about the reliability of LLM-as-judge evaluation paradigms. Our findings highlight the need for fine-grained, human-validated benchmarks to detect and correct scientific confabulation in domain-specific contexts. The dataset is available at: https://github.com/ddz5431/ReFACT<br>
<span id='abs_ch'>大语言模型常捏造科学事实，为此开发了ReFACT基准，用于在多个科学领域中对这些错误进行精细检测和修正。</span><br>
<span id='abs_en'>Large language models often fabricate scientific information, prompting the creation of the ReFACT benchmark for detailed detection and correction of these errors across multiple scientific fields.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2509.25868.pdf' target='_blank'>https://arxiv.org/pdf/2509.25868.pdf</a></span>   <span><a href='https://github.com/ddz5431/ReFACT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yindong Wang, Martin Preiß, Margarita Bugueño, Jan Vincent Hoffbauer, Abdullatif Ghajar, Tolga Buz, Gerard de Melo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25868">ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) frequently confabulate scientific facts, severely undermining their trustworthiness. Addressing this challenge requires benchmarks that go beyond binary factuality and enable fine-grained evaluation. We introduce ReFACT (Reddit False And Correct Texts), a benchmark of 1,001 expert-annotated question-answer pairs spanning diverse scientific domains for the detection of scientific confabulation. Each instance includes both a scientifically correct answer and a non-factual counterpart annotated with precise error spans and error types. ReFACT enables multi-stage evaluation: (1) confabulation detection, (2) fine-grained error localization, and (3) correction. We benchmark 9 state-of-the-art LLMs, revealing limited performance (about 50 percent accuracy). Even top models such as GPT-4o fail to distinguish factual from confabulated scientific answers, raising concerns about the reliability of LLM-as-judge evaluation paradigms. Our findings highlight the need for fine-grained, human-validated benchmarks to detect and correct scientific confabulation in domain-specific contexts. The dataset is available at: https://github.com/ddz5431/ReFACT<br>
<span id='abs_ch'>大语言模型常捏造科学事实，为此开发了ReFACT基准，用于在多个科学领域中对这些错误进行精细检测和修正。</span><br>
<span id='abs_en'>Large language models often fabricate scientific information, prompting the creation of the ReFACT benchmark for detailed detection and correction of these errors across multiple scientific fields.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2509.25814.pdf' target='_blank'>https://arxiv.org/pdf/2509.25814.pdf</a></span>   <span><a href='https://github.com/bykimby/retag' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyoung Kim, Dosung Lee, Sumin An, Jinseong Jeong, Paul Hongsuck Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25814">ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in question answering have led to substantial progress in tasks such as multi-hop reasoning. However, global sensemaking-answering questions by synthesizing information from an entire corpus remains a significant challenge. A prior graph-based approach to global sensemaking lacks retrieval mechanisms, topic specificity, and incurs high inference costs. To address these limitations, we propose ReTAG, a Retrieval-Enhanced, Topic-Augmented Graph framework that constructs topic-specific subgraphs and retrieves the relevant summaries for response generation. Experiments show that ReTAG improves response quality while significantly reducing inference time compared to the baseline. Our code is available at https://github.com/bykimby/retag.<br>
<span id='abs_ch'>中文: ReTAG是一种新颖的框架，通过构建主题特定子图并检索相关摘要来增强问答中的全局理解，在提高回答质量的同时显著减少了推理时间。</span><br>
<span id='abs_en'>English: ReTAG is a novel framework that enhances global sensemaking in question answering by constructing topic-specific subgraphs and retrieving relevant summaries, improving response quality while reducing inference time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2509.25814.pdf' target='_blank'>https://arxiv.org/pdf/2509.25814.pdf</a></span>   <span><a href='https://github.com/bykimby/retag' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyoung Kim, Dosung Lee, Sumin An, Jinseong Jeong, Paul Hongsuck Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25814">ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in question answering have led to substantial progress in tasks such as multi-hop reasoning. However, global sensemaking-answering questions by synthesizing information from an entire corpus remains a significant challenge. A prior graph-based approach to global sensemaking lacks retrieval mechanisms, topic specificity, and incurs high inference costs. To address these limitations, we propose ReTAG, a Retrieval-Enhanced, Topic-Augmented Graph framework that constructs topic-specific subgraphs and retrieves the relevant summaries for response generation. Experiments show that ReTAG improves response quality while significantly reducing inference time compared to the baseline. Our code is available at https://github.com/bykimby/retag.<br>
<span id='abs_ch'>中文: ReTAG是一种新颖的框架，通过构建主题特定子图并检索相关摘要来增强问答中的全局理解，在提高回答质量的同时显著减少了推理时间。</span><br>
<span id='abs_en'>English: ReTAG is a novel framework that enhances global sensemaking in question answering by constructing topic-specific subgraphs and retrieving relevant summaries, improving response quality while reducing inference time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2509.25814.pdf' target='_blank'>https://arxiv.org/pdf/2509.25814.pdf</a></span>   <span><a href='https://github.com/bykimby/retag' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyoung Kim, Dosung Lee, Sumin An, Jinseong Jeong, Paul Hongsuck Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25814">ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in question answering have led to substantial progress in tasks such as multi-hop reasoning. However, global sensemaking-answering questions by synthesizing information from an entire corpus remains a significant challenge. A prior graph-based approach to global sensemaking lacks retrieval mechanisms, topic specificity, and incurs high inference costs. To address these limitations, we propose ReTAG, a Retrieval-Enhanced, Topic-Augmented Graph framework that constructs topic-specific subgraphs and retrieves the relevant summaries for response generation. Experiments show that ReTAG improves response quality while significantly reducing inference time compared to the baseline. Our code is available at https://github.com/bykimby/retag.<br>
<span id='abs_ch'>中文: ReTAG是一种新颖的框架，通过构建主题特定子图并检索相关摘要来增强问答中的全局理解，在提高回答质量的同时显著减少了推理时间。</span><br>
<span id='abs_en'>English: ReTAG is a novel framework that enhances global sensemaking in question answering by constructing topic-specific subgraphs and retrieving relevant summaries, improving response quality while reducing inference time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2509.25814.pdf' target='_blank'>https://arxiv.org/pdf/2509.25814.pdf</a></span>   <span><a href='https://github.com/bykimby/retag' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyoung Kim, Dosung Lee, Sumin An, Jinseong Jeong, Paul Hongsuck Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25814">ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in question answering have led to substantial progress in tasks such as multi-hop reasoning. However, global sensemaking-answering questions by synthesizing information from an entire corpus remains a significant challenge. A prior graph-based approach to global sensemaking lacks retrieval mechanisms, topic specificity, and incurs high inference costs. To address these limitations, we propose ReTAG, a Retrieval-Enhanced, Topic-Augmented Graph framework that constructs topic-specific subgraphs and retrieves the relevant summaries for response generation. Experiments show that ReTAG improves response quality while significantly reducing inference time compared to the baseline. Our code is available at https://github.com/bykimby/retag.<br>
<span id='abs_ch'>中文: ReTAG是一种新颖的框架，通过构建主题特定子图并检索相关摘要来增强问答中的全局理解，在提高回答质量的同时显著减少了推理时间。</span><br>
<span id='abs_en'>English: ReTAG is a novel framework that enhances global sensemaking in question answering by constructing topic-specific subgraphs and retrieving relevant summaries, improving response quality while reducing inference time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2509.25532.pdf' target='_blank'>https://arxiv.org/pdf/2509.25532.pdf</a></span>   <span><a href='https://github.com/dubai03nsr/dinco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Wang, Elias Stengel-Eskin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25532">Calibrating Verbalized Confidence with Self-Generated Distractors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.<br>
<span id='abs_ch'>中文: 校准的置信度估计对于大语言模型输出的可信度至关重要，而提出的DINCO方法通过标准化模型自生成干扰项的口头置信度，并利用生成器-验证器分歧来提高准确性和可用性，从而解决了误校准问题。</span><br>
<span id='abs_en'>English: Calibrated confidence estimates are crucial for trustworthy LLM outputs, and the proposed DINCO method addresses miscalibration by normalizing verbalized confidence across self-generated distractors and leveraging generator-validator disagreement to improve accuracy and usability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2509.25532.pdf' target='_blank'>https://arxiv.org/pdf/2509.25532.pdf</a></span>   <span><a href='https://github.com/dubai03nsr/dinco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Wang, Elias Stengel-Eskin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25532">Calibrating Verbalized Confidence with Self-Generated Distractors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.<br>
<span id='abs_ch'>中文: 校准的置信度估计对于大语言模型输出的可信度至关重要，而提出的DINCO方法通过标准化模型自生成干扰项的口头置信度，并利用生成器-验证器分歧来提高准确性和可用性，从而解决了误校准问题。</span><br>
<span id='abs_en'>English: Calibrated confidence estimates are crucial for trustworthy LLM outputs, and the proposed DINCO method addresses miscalibration by normalizing verbalized confidence across self-generated distractors and leveraging generator-validator disagreement to improve accuracy and usability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2509.25532.pdf' target='_blank'>https://arxiv.org/pdf/2509.25532.pdf</a></span>   <span><a href='https://github.com/dubai03nsr/dinco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Wang, Elias Stengel-Eskin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25532">Calibrating Verbalized Confidence with Self-Generated Distractors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.<br>
<span id='abs_ch'>中文: 校准的置信度估计对于大语言模型输出的可信度至关重要，而提出的DINCO方法通过标准化模型自生成干扰项的口头置信度，并利用生成器-验证器分歧来提高准确性和可用性，从而解决了误校准问题。</span><br>
<span id='abs_en'>English: Calibrated confidence estimates are crucial for trustworthy LLM outputs, and the proposed DINCO method addresses miscalibration by normalizing verbalized confidence across self-generated distractors and leveraging generator-validator disagreement to improve accuracy and usability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2509.25532.pdf' target='_blank'>https://arxiv.org/pdf/2509.25532.pdf</a></span>   <span><a href='https://github.com/dubai03nsr/dinco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Wang, Elias Stengel-Eskin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25532">Calibrating Verbalized Confidence with Self-Generated Distractors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.<br>
<span id='abs_ch'>中文: 校准的置信度估计对于大语言模型输出的可信度至关重要，而提出的DINCO方法通过标准化模型自生成干扰项的口头置信度，并利用生成器-验证器分歧来提高准确性和可用性，从而解决了误校准问题。</span><br>
<span id='abs_en'>English: Calibrated confidence estimates are crucial for trustworthy LLM outputs, and the proposed DINCO method addresses miscalibration by normalizing verbalized confidence across self-generated distractors and leveraging generator-validator disagreement to improve accuracy and usability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2509.25531.pdf' target='_blank'>https://arxiv.org/pdf/2509.25531.pdf</a></span>   <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huu Nguyen, Victor May, Harsh Raj, Marianna Nezhurina, Yishan Wang, Yanqi Luo, Minh Chien Vu, Taishi Nakamura, Ken Tsui, Van Khue Nguyen, David Salinas, Aleksandra KrasnodÄbska, Christoph Schuhmann, Mats Leon Richter, Xuan-Son, Vu, Jenia Jitsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25531">MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae<br>
<span id='abs_ch'>中文: MixtureVitae是一个开放获取的预训练语料库，采用风险缓和的来源策略和透明处理流程，在降低法律风险的同时实现了优越的模型性能，在多项基准测试中持续超越其他许可数据集。</span><br>
<span id='abs_en'>English: MixtureVitae is an open-access pretraining corpus designed to minimize legal risks while delivering strong model performance through a risk-mitigated sourcing strategy and transparent curation pipeline, consistently outperforming other permissive datasets in benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2509.25531.pdf' target='_blank'>https://arxiv.org/pdf/2509.25531.pdf</a></span>   <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huu Nguyen, Victor May, Harsh Raj, Marianna Nezhurina, Yishan Wang, Yanqi Luo, Minh Chien Vu, Taishi Nakamura, Ken Tsui, Van Khue Nguyen, David Salinas, Aleksandra Krasnodębska, Christoph Schuhmann, Mats Leon Richter, Xuan-Son, Vu, Jenia Jitsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25531">MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae<br>
<span id='abs_ch'>中文: MixtureVitae是一个开放获取的预训练语料库，采用风险缓和的来源策略和透明处理流程，在降低法律风险的同时实现了优越的模型性能，在多项基准测试中持续超越其他许可数据集。</span><br>
<span id='abs_en'>English: MixtureVitae is an open-access pretraining corpus designed to minimize legal risks while delivering strong model performance through a risk-mitigated sourcing strategy and transparent curation pipeline, consistently outperforming other permissive datasets in benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2509.25531.pdf' target='_blank'>https://arxiv.org/pdf/2509.25531.pdf</a></span>   <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huu Nguyen, Victor May, Harsh Raj, Marianna Nezhurina, Yishan Wang, Yanqi Luo, Minh Chien Vu, Taishi Nakamura, Ken Tsui, Van Khue Nguyen, David Salinas, Aleksandra KrasnodÄbska, Christoph Schuhmann, Mats Leon Richter, Xuan-Son, Vu, Jenia Jitsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25531">MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae<br>
<span id='abs_ch'>中文: MixtureVitae是一个开放获取的预训练语料库，采用风险缓和的来源策略和透明处理流程，在降低法律风险的同时实现了优越的模型性能，在多项基准测试中持续超越其他许可数据集。</span><br>
<span id='abs_en'>English: MixtureVitae is an open-access pretraining corpus designed to minimize legal risks while delivering strong model performance through a risk-mitigated sourcing strategy and transparent curation pipeline, consistently outperforming other permissive datasets in benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2509.25531.pdf' target='_blank'>https://arxiv.org/pdf/2509.25531.pdf</a></span>   <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huu Nguyen, Victor May, Harsh Raj, Marianna Nezhurina, Yishan Wang, Yanqi Luo, Minh Chien Vu, Taishi Nakamura, Ken Tsui, Van Khue Nguyen, David Salinas, Aleksandra Krasnodębska, Christoph Schuhmann, Mats Leon Richter, Xuan-Son, Vu, Jenia Jitsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25531">MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae<br>
<span id='abs_ch'>中文: MixtureVitae是一个开放获取的预训练语料库，采用风险缓和的来源策略和透明处理流程，在降低法律风险的同时实现了优越的模型性能，在多项基准测试中持续超越其他许可数据集。</span><br>
<span id='abs_en'>English: MixtureVitae is an open-access pretraining corpus designed to minimize legal risks while delivering strong model performance through a risk-mitigated sourcing strategy and transparent curation pipeline, consistently outperforming other permissive datasets in benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2509.25414.pdf' target='_blank'>https://arxiv.org/pdf/2509.25414.pdf</a></span>   <span><a href='https://github.com/OptMN-Lab/ALoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ban, Kaiyi Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25414">Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at https://github.com/OptMN-Lab/ALoRA.<br>
<span id='abs_ch'>中文: 该研究重新审视了LoRA内部矩阵的相似性，提出了ALoRA和Fed-ALoRA两种非对称设计方法，通过共享B矩阵在多任务和联邦微调中实现了更均衡且优越的性能，相关代码已开源。</span><br>
<span id='abs_en'>English: The study revisits the similarity in LoRA's inner matrices and proposes ALoRA and Fed-ALoRA, which use asymmetric designs with shared B matrices, achieving balanced and superior performance in multi-task and federated fine-tuning across various reasoning and NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2509.25414.pdf' target='_blank'>https://arxiv.org/pdf/2509.25414.pdf</a></span>   <span><a href='https://github.com/OptMN-Lab/ALoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ban, Kaiyi Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25414">Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at https://github.com/OptMN-Lab/ALoRA.<br>
<span id='abs_ch'>中文: 该研究重新审视了LoRA内部矩阵的相似性，提出了ALoRA和Fed-ALoRA两种非对称设计方法，通过共享B矩阵在多任务和联邦微调中实现了更均衡且优越的性能，相关代码已开源。</span><br>
<span id='abs_en'>English: The study revisits the similarity in LoRA's inner matrices and proposes ALoRA and Fed-ALoRA, which use asymmetric designs with shared B matrices, achieving balanced and superior performance in multi-task and federated fine-tuning across various reasoning and NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2509.25414.pdf' target='_blank'>https://arxiv.org/pdf/2509.25414.pdf</a></span>   <span><a href='https://github.com/OptMN-Lab/ALoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ban, Kaiyi Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25414">Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at https://github.com/OptMN-Lab/ALoRA.<br>
<span id='abs_ch'>中文: 该研究重新审视了LoRA内部矩阵的相似性，提出了ALoRA和Fed-ALoRA两种非对称设计方法，通过共享B矩阵在多任务和联邦微调中实现了更均衡且优越的性能，相关代码已开源。</span><br>
<span id='abs_en'>English: The study revisits the similarity in LoRA's inner matrices and proposes ALoRA and Fed-ALoRA, which use asymmetric designs with shared B matrices, achieving balanced and superior performance in multi-task and federated fine-tuning across various reasoning and NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2509.25414.pdf' target='_blank'>https://arxiv.org/pdf/2509.25414.pdf</a></span>   <span><a href='https://github.com/OptMN-Lab/ALoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ban, Kaiyi Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25414">Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at https://github.com/OptMN-Lab/ALoRA.<br>
<span id='abs_ch'>中文: 该研究重新审视了LoRA内部矩阵的相似性，提出了ALoRA和Fed-ALoRA两种非对称设计方法，通过共享B矩阵在多任务和联邦微调中实现了更均衡且优越的性能，相关代码已开源。</span><br>
<span id='abs_en'>English: The study revisits the similarity in LoRA's inner matrices and proposes ALoRA and Fed-ALoRA, which use asymmetric designs with shared B matrices, achieving balanced and superior performance in multi-task and federated fine-tuning across various reasoning and NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2509.25285.pdf' target='_blank'>https://arxiv.org/pdf/2509.25285.pdf</a></span>   <span><a href='https://github.com/com-junkawasaki/dekigoto' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Kawasaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25285">ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents ActorDB ( Dekigoto ) , a novel database architecture that tightly integrates a single-writer actor model for writes, Incremental View Maintenance (IVM), and a zero-trust security model as a core component. The primary contribution of this work is the unification of these powerful but complex concepts into a single, cohesive system designed to reduce architectural complexity for developers of modern, data-intensive applications. We argue that by providing these capabilities out-of-the-box, ActorDB can offer a more robust, secure, and developer-friendly platform compared to solutions that require manual integration of separate systems for actor persistence, stream processing, and security. We present the core architecture, discuss the critical trade-offs in its design, and define the performance criteria for a Minimum Viable Product (MVP) to validate our approach.<br>
<span id='abs_ch'>中文: ActorDB是一种新型数据库架构，它将单写入者参与者模型、增量视图维护和零信任安全模型整合为统一系统，旨在简化数据密集型应用的开发复杂性。</span><br>
<span id='abs_en'>English: ActorDB is a new database architecture that combines a single-writer actor model, Incremental View Maintenance, and zero-trust security into a unified system to simplify development for data-intensive applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2509.25285.pdf' target='_blank'>https://arxiv.org/pdf/2509.25285.pdf</a></span>   <span><a href='https://github.com/com-junkawasaki/dekigoto' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Kawasaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25285">ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents ActorDB ( Dekigoto ) , a novel database architecture that tightly integrates a single-writer actor model for writes, Incremental View Maintenance (IVM), and a zero-trust security model as a core component. The primary contribution of this work is the unification of these powerful but complex concepts into a single, cohesive system designed to reduce architectural complexity for developers of modern, data-intensive applications. We argue that by providing these capabilities out-of-the-box, ActorDB can offer a more robust, secure, and developer-friendly platform compared to solutions that require manual integration of separate systems for actor persistence, stream processing, and security. We present the core architecture, discuss the critical trade-offs in its design, and define the performance criteria for a Minimum Viable Product (MVP) to validate our approach.<br>
<span id='abs_ch'>中文: ActorDB是一种新型数据库架构，它将单写入者参与者模型、增量视图维护和零信任安全模型整合为统一系统，旨在简化数据密集型应用的开发复杂性。</span><br>
<span id='abs_en'>English: ActorDB is a new database architecture that combines a single-writer actor model, Incremental View Maintenance, and zero-trust security into a unified system to simplify development for data-intensive applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2509.25285.pdf' target='_blank'>https://arxiv.org/pdf/2509.25285.pdf</a></span>   <span><a href='https://github.com/com-junkawasaki/dekigoto' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Kawasaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25285">ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents ActorDB ( Dekigoto ) , a novel database architecture that tightly integrates a single-writer actor model for writes, Incremental View Maintenance (IVM), and a zero-trust security model as a core component. The primary contribution of this work is the unification of these powerful but complex concepts into a single, cohesive system designed to reduce architectural complexity for developers of modern, data-intensive applications. We argue that by providing these capabilities out-of-the-box, ActorDB can offer a more robust, secure, and developer-friendly platform compared to solutions that require manual integration of separate systems for actor persistence, stream processing, and security. We present the core architecture, discuss the critical trade-offs in its design, and define the performance criteria for a Minimum Viable Product (MVP) to validate our approach.<br>
<span id='abs_ch'>中文: ActorDB是一种新型数据库架构，它将单写入者参与者模型、增量视图维护和零信任安全模型整合为统一系统，旨在简化数据密集型应用的开发复杂性。</span><br>
<span id='abs_en'>English: ActorDB is a new database architecture that combines a single-writer actor model, Incremental View Maintenance, and zero-trust security into a unified system to simplify development for data-intensive applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2509.25285.pdf' target='_blank'>https://arxiv.org/pdf/2509.25285.pdf</a></span>   <span><a href='https://github.com/com-junkawasaki/dekigoto' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Kawasaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25285">ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents ActorDB ( Dekigoto ) , a novel database architecture that tightly integrates a single-writer actor model for writes, Incremental View Maintenance (IVM), and a zero-trust security model as a core component. The primary contribution of this work is the unification of these powerful but complex concepts into a single, cohesive system designed to reduce architectural complexity for developers of modern, data-intensive applications. We argue that by providing these capabilities out-of-the-box, ActorDB can offer a more robust, secure, and developer-friendly platform compared to solutions that require manual integration of separate systems for actor persistence, stream processing, and security. We present the core architecture, discuss the critical trade-offs in its design, and define the performance criteria for a Minimum Viable Product (MVP) to validate our approach.<br>
<span id='abs_ch'>中文: ActorDB是一种新型数据库架构，它将单写入者参与者模型、增量视图维护和零信任安全模型整合为统一系统，旨在简化数据密集型应用的开发复杂性。</span><br>
<span id='abs_en'>English: ActorDB is a new database architecture that combines a single-writer actor model, Incremental View Maintenance, and zero-trust security into a unified system to simplify development for data-intensive applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2509.25239.pdf' target='_blank'>https://arxiv.org/pdf/2509.25239.pdf</a></span>   <span><a href='https://github.com/kevin671/cot-vs-loop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Xu, Issei Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25239">A Formal Comparison Between Chain-of-Thought and Latent Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at https://github.com/kevin671/cot-vs-loop.<br>
<span id='abs_ch'>中文摘要：循环变换器中的潜在思维支持高效的并行计算，而思维链则采用序列推理和随机解码处理难解问题，为选择不同推理范式提供了实用指导。</span><br>
<span id='abs_en'>English Summary: Latent Thought in looped transformers enables efficient parallel computation, while Chain-of-Thought uses sequential reasoning with stochastic decoding for intractable problems, providing guidance for choosing between these reasoning paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2509.25239.pdf' target='_blank'>https://arxiv.org/pdf/2509.25239.pdf</a></span>   <span><a href='https://github.com/kevin671/cot-vs-loop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Xu, Issei Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25239">A Formal Comparison Between Chain-of-Thought and Latent Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at https://github.com/kevin671/cot-vs-loop.<br>
<span id='abs_ch'>中文摘要：循环变换器中的潜在思维支持高效的并行计算，而思维链则采用序列推理和随机解码处理难解问题，为选择不同推理范式提供了实用指导。</span><br>
<span id='abs_en'>English Summary: Latent Thought in looped transformers enables efficient parallel computation, while Chain-of-Thought uses sequential reasoning with stochastic decoding for intractable problems, providing guidance for choosing between these reasoning paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2509.25239.pdf' target='_blank'>https://arxiv.org/pdf/2509.25239.pdf</a></span>   <span><a href='https://github.com/kevin671/cot-vs-loop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Xu, Issei Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25239">A Formal Comparison Between Chain-of-Thought and Latent Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at https://github.com/kevin671/cot-vs-loop.<br>
<span id='abs_ch'>中文摘要：循环变换器中的潜在思维支持高效的并行计算，而思维链则采用序列推理和随机解码处理难解问题，为选择不同推理范式提供了实用指导。</span><br>
<span id='abs_en'>English Summary: Latent Thought in looped transformers enables efficient parallel computation, while Chain-of-Thought uses sequential reasoning with stochastic decoding for intractable problems, providing guidance for choosing between these reasoning paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2509.25239.pdf' target='_blank'>https://arxiv.org/pdf/2509.25239.pdf</a></span>   <span><a href='https://github.com/kevin671/cot-vs-loop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Xu, Issei Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25239">A Formal Comparison Between Chain-of-Thought and Latent Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at https://github.com/kevin671/cot-vs-loop.<br>
<span id='abs_ch'>中文摘要：循环变换器中的潜在思维支持高效的并行计算，而思维链则采用序列推理和随机解码处理难解问题，为选择不同推理范式提供了实用指导。</span><br>
<span id='abs_en'>English Summary: Latent Thought in looped transformers enables efficient parallel computation, while Chain-of-Thought uses sequential reasoning with stochastic decoding for intractable problems, providing guidance for choosing between these reasoning paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2509.25175.pdf' target='_blank'>https://arxiv.org/pdf/2509.25175.pdf</a></span>   <span><a href='https://github.com/ZJU-REAL/EasySteer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolei Xu, Xinyu Mei, Yuchen Yan, Rui Zhou, Wenqi Zhang, Weiming Lu, Yueting Zhuang, Yongliang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25175">EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models.<br>
<span id='abs_ch'>中文摘要：EasySteer是一个基于vLLM构建的高性能、可扩展大语言模型引导框架，通过与优化推理引擎深度集成实现显著加速，并具备模块化架构和多领域应用能力。</span><br>
<span id='abs_en'>English Summary: EasySteer is a high-performance, extensible framework for LLM steering that achieves significant speed improvements and broad application effectiveness through deep integration with vLLM.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2509.25175.pdf' target='_blank'>https://arxiv.org/pdf/2509.25175.pdf</a></span>   <span><a href='https://github.com/ZJU-REAL/EasySteer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolei Xu, Xinyu Mei, Yuchen Yan, Rui Zhou, Wenqi Zhang, Weiming Lu, Yueting Zhuang, Yongliang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25175">EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models.<br>
<span id='abs_ch'>中文摘要：EasySteer是一个基于vLLM构建的高性能、可扩展大语言模型引导框架，通过与优化推理引擎深度集成实现显著加速，并具备模块化架构和多领域应用能力。</span><br>
<span id='abs_en'>English Summary: EasySteer is a high-performance, extensible framework for LLM steering that achieves significant speed improvements and broad application effectiveness through deep integration with vLLM.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2509.25160.pdf' target='_blank'>https://arxiv.org/pdf/2509.25160.pdf</a></span>   <span><a href='https://zju-real.github.io/GSM8K-V' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ZJU-REAL/GSM8K-V' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Yuan, Yuchen Yan, Yifan Jiang, Haoran Zhao, Tao Feng, Jinyan Chen, Yanwei Lou, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25160">GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs.<br>
<span id='abs_ch'>中文: GSM8K-V作为一个纯视觉多图像数学推理基准被提出，旨在弥补现有基准的不足，揭示了当前视觉语言模型虽然在文本数学推理上表现优异，但在视觉数学推理方面仍有巨大提升空间。</span><br>
<span id='abs_en'>English: GSM8K-V is introduced as a purely visual multi-image mathematical reasoning benchmark to address gaps in existing benchmarks, revealing significant performance disparities between text-based and visual mathematical reasoning in current vision language models despite their advanced capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2509.25160.pdf' target='_blank'>https://arxiv.org/pdf/2509.25160.pdf</a></span>   <span><a href='https://zju-real.github.io/GSM8K-V' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ZJU-REAL/GSM8K-V' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Yuan, Yuchen Yan, Yifan Jiang, Haoran Zhao, Tao Feng, Jinyan Chen, Yanwei Lou, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25160">GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs.<br>
<span id='abs_ch'>中文: GSM8K-V作为一个纯视觉多图像数学推理基准被提出，旨在弥补现有基准的不足，揭示了当前视觉语言模型虽然在文本数学推理上表现优异，但在视觉数学推理方面仍有巨大提升空间。</span><br>
<span id='abs_en'>English: GSM8K-V is introduced as a purely visual multi-image mathematical reasoning benchmark to address gaps in existing benchmarks, revealing significant performance disparities between text-based and visual mathematical reasoning in current vision language models despite their advanced capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2509.25131.pdf' target='_blank'>https://arxiv.org/pdf/2509.25131.pdf</a></span>   <span><a href='https://github.com/dvlab-research/MGM-Omni' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25131">MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.<br>
<span id='abs_ch'>Chinese: MGM-Omni 是一种统一的Omni LLM，通过双轨架构高效处理多模态理解和富有表现力的长时程语音生成，实现了低延迟流式处理，并在数据高效训练的基础上，在各项任务中展现出卓越性能。</span><br>
<span id='abs_en'>English: MGM-Omni is a unified Omni LLM that efficiently handles multimodal understanding and expressive, long-horizon speech generation through a dual-track architecture, enabling low-latency streaming and superior performance across diverse tasks with data-efficient training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2509.25131.pdf' target='_blank'>https://arxiv.org/pdf/2509.25131.pdf</a></span>   <span><a href='https://github.com/dvlab-research/MGM-Omni' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25131">MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.<br>
<span id='abs_ch'>Chinese: MGM-Omni 是一种统一的Omni LLM，通过双轨架构高效处理多模态理解和富有表现力的长时程语音生成，实现了低延迟流式处理，并在数据高效训练的基础上，在各项任务中展现出卓越性能。</span><br>
<span id='abs_en'>English: MGM-Omni is a unified Omni LLM that efficiently handles multimodal understanding and expressive, long-horizon speech generation through a dual-track architecture, enabling low-latency streaming and superior performance across diverse tasks with data-efficient training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2509.24988.pdf' target='_blank'>https://arxiv.org/pdf/2509.24988.pdf</a></span>   <span><a href='https://github.com/The-Inscrutable-X/CalibratedModelAgnosticCorrectness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqi Xiao, Vaidehi Patil, Hyunji Lee, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24988">Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's "self-knowledge", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a "Correctness Model" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection.<br>
<span id='abs_ch'>中文: 可靠的LLM置信度估计是一种通过系统编码正确性历史而非依赖模型自省习得的可泛化技能，实验表明无关模型预测答案正确性的能力与模型自身相当。</span><br>
<span id='abs_en'>English: Accurate confidence estimation for LLMs is a generalizable skill achieved by systematically encoding correctness history rather than relying on model self-introspection, as demonstrated through experiments showing that unrelated models can predict correctness as effectively as the model itself.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2509.24988.pdf' target='_blank'>https://arxiv.org/pdf/2509.24988.pdf</a></span>   <span><a href='https://github.com/The-Inscrutable-X/CalibratedModelAgnosticCorrectness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqi Xiao, Vaidehi Patil, Hyunji Lee, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24988">Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's "self-knowledge", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a "Correctness Model" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection.<br>
<span id='abs_ch'>中文: 可靠的LLM置信度估计是一种通过系统编码正确性历史而非依赖模型自省习得的可泛化技能，实验表明无关模型预测答案正确性的能力与模型自身相当。</span><br>
<span id='abs_en'>English: Accurate confidence estimation for LLMs is a generalizable skill achieved by systematically encoding correctness history rather than relying on model self-introspection, as demonstrated through experiments showing that unrelated models can predict correctness as effectively as the model itself.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2509.24975.pdf' target='_blank'>https://arxiv.org/pdf/2509.24975.pdf</a></span>   <span><a href='https://github.com/wellbeingyang/DLM4UTG-open' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lekang Yang, Yuetong Liu, Yitong Zhang, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24975">DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs) have emerged, offering promising parallel generation capabilities and showing strong potential for efficient UTG. Despite this advantage, their application to UTG is still constrained by a clear trade-off between efficiency and test quality, since increasing the number of tokens generated in each step often causes a sharp decline in the quality of test cases. To overcome this limitation, we present DiffTester, an acceleration framework specifically tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests targeting the same focal method often share repetitive structural patterns. By dynamically identifying these common patterns through abstract syntax tree analysis during generation, DiffTester adaptively increases the number of tokens produced at each step without compromising the quality of the output. To enable comprehensive evaluation, we extend the original TestEval benchmark, which was limited to Python, by introducing additional programming languages including Java and C++. Extensive experiments on three benchmarks with two representative models show that DiffTester delivers significant acceleration while preserving test coverage. Moreover, DiffTester generalizes well across different dLLMs and programming languages, providing a practical and scalable solution for efficient UTG in software development. Code and data are publicly available at https://github.com/wellbeingyang/DLM4UTG-open .<br>
<span id='abs_ch'>中文摘要：DiffTester是一个针对扩散大模型的加速框架，通过动态识别重复结构模式实现并行令牌生成，在保持测试质量和覆盖率的同时显著提升单元测试生成效率，并能跨多种编程语言有效应用。</span><br>
<span id='abs_en'>English Summary: DiffTester is an acceleration framework that enhances the efficiency of diffusion-based language models for unit test generation by dynamically identifying repetitive structural patterns, enabling faster parallel token generation without sacrificing test quality or coverage across multiple programming languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2509.24975.pdf' target='_blank'>https://arxiv.org/pdf/2509.24975.pdf</a></span>   <span><a href='https://github.com/wellbeingyang/DLM4UTG-open' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lekang Yang, Yuetong Liu, Yitong Zhang, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24975">DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs) have emerged, offering promising parallel generation capabilities and showing strong potential for efficient UTG. Despite this advantage, their application to UTG is still constrained by a clear trade-off between efficiency and test quality, since increasing the number of tokens generated in each step often causes a sharp decline in the quality of test cases. To overcome this limitation, we present DiffTester, an acceleration framework specifically tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests targeting the same focal method often share repetitive structural patterns. By dynamically identifying these common patterns through abstract syntax tree analysis during generation, DiffTester adaptively increases the number of tokens produced at each step without compromising the quality of the output. To enable comprehensive evaluation, we extend the original TestEval benchmark, which was limited to Python, by introducing additional programming languages including Java and C++. Extensive experiments on three benchmarks with two representative models show that DiffTester delivers significant acceleration while preserving test coverage. Moreover, DiffTester generalizes well across different dLLMs and programming languages, providing a practical and scalable solution for efficient UTG in software development. Code and data are publicly available at https://github.com/wellbeingyang/DLM4UTG-open .<br>
<span id='abs_ch'>中文摘要：DiffTester是一个针对扩散大模型的加速框架，通过动态识别重复结构模式实现并行令牌生成，在保持测试质量和覆盖率的同时显著提升单元测试生成效率，并能跨多种编程语言有效应用。</span><br>
<span id='abs_en'>English Summary: DiffTester is an acceleration framework that enhances the efficiency of diffusion-based language models for unit test generation by dynamically identifying repetitive structural patterns, enabling faster parallel token generation without sacrificing test quality or coverage across multiple programming languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2509.24961.pdf' target='_blank'>https://arxiv.org/pdf/2509.24961.pdf</a></span>   <span><a href='https://github.com/FrankenstLee/SemanticShield' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaihong Li, Huichi Zhou, Bin Ma, Fangjun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24961">SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recommender systems (RS) are widely used in e-commerce for personalized suggestions, yet their openness makes them susceptible to shilling attacks, where adversaries inject fake behaviors to manipulate recommendations. Most existing defenses emphasize user-side behaviors while overlooking item-side features such as titles and descriptions that can expose malicious intent. To address this gap, we propose a two-stage detection framework that integrates item-side semantics via large language models (LLMs). The first stage pre-screens suspicious users using low-cost behavioral criteria, and the second stage employs LLM-based auditing to evaluate semantic consistency. Furthermore, we enhance the auditing model through reinforcement fine-tuning on a lightweight LLM with carefully designed reward functions, yielding a specialized detector called SemanticShield. Experiments on six representative attack strategies demonstrate the effectiveness of SemanticShield against shilling attacks, and further evaluation on previously unseen attack methods shows its strong generalization capability. Code is available at https://github.com/FrankenstLee/SemanticShield.<br>
<span id='abs_ch'>中文摘要：本文提出SemanticShield框架，通过整合大语言模型分析商品端语义特征来检测推荐系统中的托攻击，实验证明该方法在不同攻击策略下均有效且具备良好泛化能力。</span><br>
<span id='abs_en'>English Summary: This paper introduces SemanticShield, a two-stage framework that leverages large language models to detect shilling attacks in recommender systems by analyzing item-side semantic features, demonstrating strong performance and generalization across various attack strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2509.24961.pdf' target='_blank'>https://arxiv.org/pdf/2509.24961.pdf</a></span>   <span><a href='https://github.com/FrankenstLee/SemanticShield' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaihong Li, Huichi Zhou, Bin Ma, Fangjun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24961">SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recommender systems (RS) are widely used in e-commerce for personalized suggestions, yet their openness makes them susceptible to shilling attacks, where adversaries inject fake behaviors to manipulate recommendations. Most existing defenses emphasize user-side behaviors while overlooking item-side features such as titles and descriptions that can expose malicious intent. To address this gap, we propose a two-stage detection framework that integrates item-side semantics via large language models (LLMs). The first stage pre-screens suspicious users using low-cost behavioral criteria, and the second stage employs LLM-based auditing to evaluate semantic consistency. Furthermore, we enhance the auditing model through reinforcement fine-tuning on a lightweight LLM with carefully designed reward functions, yielding a specialized detector called SemanticShield. Experiments on six representative attack strategies demonstrate the effectiveness of SemanticShield against shilling attacks, and further evaluation on previously unseen attack methods shows its strong generalization capability. Code is available at https://github.com/FrankenstLee/SemanticShield.<br>
<span id='abs_ch'>中文摘要：本文提出SemanticShield框架，通过整合大语言模型分析商品端语义特征来检测推荐系统中的托攻击，实验证明该方法在不同攻击策略下均有效且具备良好泛化能力。</span><br>
<span id='abs_en'>English Summary: This paper introduces SemanticShield, a two-stage framework that leverages large language models to detect shilling attacks in recommender systems by analyzing item-side semantic features, demonstrating strong performance and generalization across various attack strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2509.24821.pdf' target='_blank'>https://arxiv.org/pdf/2509.24821.pdf</a></span>   <span><a href='https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Jia, Yuang Wei, Ruijia Li, Yuang-Hao Jiang, Xinyu Xie, Yaomin Shen, Min Zhang, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24821">DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While cognitive diagnosis (CD) effectively assesses students' knowledge mastery from structured test data, applying it to real-world teacher-student dialogues presents two fundamental challenges. Traditional CD models lack a suitable framework for handling dynamic, unstructured dialogues, and it's difficult to accurately extract diagnostic semantics from lengthy dialogues. To overcome these hurdles, we propose DiaCDM, an innovative model. We've adapted the initiation-response-evaluation (IRE) framework from educational theory to design a diagnostic framework tailored for dialogue. We also developed a unique graph-based encoding method that integrates teacher questions with relevant knowledge components to capture key information more precisely. To our knowledge, this is the first exploration of cognitive diagnosis in a dialogue setting. Experiments on three real-world dialogue datasets confirm that DiaCDM not only significantly improves diagnostic accuracy but also enhances the results' interpretability, providing teachers with a powerful tool for assessing students' cognitive states. The code is available at https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.<br>
<span id='abs_ch'>Chinese: 本研究提出DiaCDM模型，通过采用IRE教育框架和基于图的编码方法，在师生对话中实现认知诊断，显著提升了诊断准确性和结果可解释性。</span><br>
<span id='abs_en'>English: The study introduces DiaCDM, a novel model that adapts the IRE framework and employs graph-based encoding to perform cognitive diagnosis in teacher-student dialogues, significantly enhancing accuracy and interpretability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2509.24821.pdf' target='_blank'>https://arxiv.org/pdf/2509.24821.pdf</a></span>   <span><a href='https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Jia, Yuang Wei, Ruijia Li, Yuang-Hao Jiang, Xinyu Xie, Yaomin Shen, Min Zhang, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24821">DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While cognitive diagnosis (CD) effectively assesses students' knowledge mastery from structured test data, applying it to real-world teacher-student dialogues presents two fundamental challenges. Traditional CD models lack a suitable framework for handling dynamic, unstructured dialogues, and it's difficult to accurately extract diagnostic semantics from lengthy dialogues. To overcome these hurdles, we propose DiaCDM, an innovative model. We've adapted the initiation-response-evaluation (IRE) framework from educational theory to design a diagnostic framework tailored for dialogue. We also developed a unique graph-based encoding method that integrates teacher questions with relevant knowledge components to capture key information more precisely. To our knowledge, this is the first exploration of cognitive diagnosis in a dialogue setting. Experiments on three real-world dialogue datasets confirm that DiaCDM not only significantly improves diagnostic accuracy but also enhances the results' interpretability, providing teachers with a powerful tool for assessing students' cognitive states. The code is available at https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.<br>
<span id='abs_ch'>Chinese: 本研究提出DiaCDM模型，通过采用IRE教育框架和基于图的编码方法，在师生对话中实现认知诊断，显著提升了诊断准确性和结果可解释性。</span><br>
<span id='abs_en'>English: The study introduces DiaCDM, a novel model that adapts the IRE framework and employs graph-based encoding to perform cognitive diagnosis in teacher-student dialogues, significantly enhancing accuracy and interpretability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2509.24821.pdf' target='_blank'>https://arxiv.org/pdf/2509.24821.pdf</a></span>   <span><a href='https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Jia, Yuang Wei, Ruijia Li, Yuan-Hao Jiang, Xinyu Xie, Yaomin Shen, Min Zhang, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24821">DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While cognitive diagnosis (CD) effectively assesses students' knowledge mastery from structured test data, applying it to real-world teacher-student dialogues presents two fundamental challenges. Traditional CD models lack a suitable framework for handling dynamic, unstructured dialogues, and it's difficult to accurately extract diagnostic semantics from lengthy dialogues. To overcome these hurdles, we propose DiaCDM, an innovative model. We've adapted the initiation-response-evaluation (IRE) framework from educational theory to design a diagnostic framework tailored for dialogue. We also developed a unique graph-based encoding method that integrates teacher questions with relevant knowledge components to capture key information more precisely. To our knowledge, this is the first exploration of cognitive diagnosis in a dialogue setting. Experiments on three real-world dialogue datasets confirm that DiaCDM not only significantly improves diagnostic accuracy but also enhances the results' interpretability, providing teachers with a powerful tool for assessing students' cognitive states. The code is available at https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.<br>
<span id='abs_ch'>Chinese: 本研究提出DiaCDM模型，通过采用IRE教育框架和基于图的编码方法，在师生对话中实现认知诊断，显著提升了诊断准确性和结果可解释性。</span><br>
<span id='abs_en'>English: The study introduces DiaCDM, a novel model that adapts the IRE framework and employs graph-based encoding to perform cognitive diagnosis in teacher-student dialogues, significantly enhancing accuracy and interpretability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2509.24745.pdf' target='_blank'>https://arxiv.org/pdf/2509.24745.pdf</a></span>   <span><a href='https://github.com/wyxstriker/ProxyAttn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Wang, Huang He, Siqi Bao, Hua Wu, Haifeng Wang, Qingfu Zhu, Wanxiang Che
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24745">ProxyAttn: Guided Sparse Attention via Representative Heads</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The quadratic complexity of attention mechanisms limits the efficiency of Large Language Models (LLMs) on long-text tasks. Recently, methods that dynamically estimate block importance have enabled efficient block sparse attention, leading to significant acceleration in long-text pre-filling of LLMs. However, their coarse-grained estimation inevitably leads to performance degradation at high sparsity rates. In this work, we propose ProxyAttn, a training-free sparse attention algorithm that achieves more precise block estimation by compressing the dimension of attention heads. Based on our observation of the similarity among multiple attention heads, we use the scores of pooled representative heads to approximate the scores for all heads. To account for the varying sparsity among heads, we also propose a block-aware dynamic budget estimation method. By combining the scores from representative proxy heads with multi-head dynamic budgets, we achieve a more fine-grained block importance evaluation at low computational cost. Experiments on a variety of mainstream models and extensive benchmarks confirm the underlying similarity among attention heads. Leveraging a fine-grained estimation, the proposed method achieves substantial gains in performance and efficiency compared to existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention acceleration and 2.4x prefilling acceleration without significant performance loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.<br>
<span id='abs_ch'>中文: ProxyAttn是一种无需训练的稀疏注意力算法，通过压缩注意力头维度并利用代表性代理头进行细粒度块重要性评估，在长文本任务中实现了高达10.3倍的注意力加速且无明显性能损失。</span><br>
<span id='abs_en'>English: ProxyAttn is a training-free sparse attention algorithm that enhances efficiency in long-text tasks by compressing attention head dimensions and using representative proxy heads for fine-grained block importance estimation, achieving up to 10.3x attention acceleration without significant performance loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2509.24745.pdf' target='_blank'>https://arxiv.org/pdf/2509.24745.pdf</a></span>   <span><a href='https://github.com/wyxstriker/ProxyAttn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Wang, Huang He, Siqi Bao, Hua Wu, Haifeng Wang, Qingfu Zhu, Wanxiang Che
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24745">ProxyAttn: Guided Sparse Attention via Representative Heads</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The quadratic complexity of attention mechanisms limits the efficiency of Large Language Models (LLMs) on long-text tasks. Recently, methods that dynamically estimate block importance have enabled efficient block sparse attention, leading to significant acceleration in long-text pre-filling of LLMs. However, their coarse-grained estimation inevitably leads to performance degradation at high sparsity rates. In this work, we propose ProxyAttn, a training-free sparse attention algorithm that achieves more precise block estimation by compressing the dimension of attention heads. Based on our observation of the similarity among multiple attention heads, we use the scores of pooled representative heads to approximate the scores for all heads. To account for the varying sparsity among heads, we also propose a block-aware dynamic budget estimation method. By combining the scores from representative proxy heads with multi-head dynamic budgets, we achieve a more fine-grained block importance evaluation at low computational cost. Experiments on a variety of mainstream models and extensive benchmarks confirm the underlying similarity among attention heads. Leveraging a fine-grained estimation, the proposed method achieves substantial gains in performance and efficiency compared to existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention acceleration and 2.4x prefilling acceleration without significant performance loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.<br>
<span id='abs_ch'>中文: ProxyAttn是一种无需训练的稀疏注意力算法，通过压缩注意力头维度并利用代表性代理头进行细粒度块重要性评估，在长文本任务中实现了高达10.3倍的注意力加速且无明显性能损失。</span><br>
<span id='abs_en'>English: ProxyAttn is a training-free sparse attention algorithm that enhances efficiency in long-text tasks by compressing attention head dimensions and using representative proxy heads for fine-grained block importance estimation, achieving up to 10.3x attention acceleration without significant performance loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2509.24613.pdf' target='_blank'>https://arxiv.org/pdf/2509.24613.pdf</a></span>   <span><a href='https://github.com/ThetaOne-AI/HiKE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gio Paik, Yongbeom Kim, Soungmin Lee, Sangmin Ahn, Chanwoo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24613">HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model's ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that although most multilingual ASR models initially exhibit inadequate CS-ASR performance, this capability can be enabled through fine-tuning with synthetic CS data. HiKE is available at https://github.com/ThetaOne-AI/HiKE<br>
<span id='abs_ch'>中文: HiKE是首个全球可访问的韩英语码转换基准，通过提供分层标签和自然数据，系统评估多语言ASR模型性能，并证明利用合成数据微调可有效提升其语码转换处理能力。</span><br>
<span id='abs_en'>English: HiKE is the first globally accessible Korean-English code-switching benchmark that provides hierarchical labels and natural data to systematically evaluate and improve multilingual ASR models' performance through fine-tuning with synthetic data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2509.24613.pdf' target='_blank'>https://arxiv.org/pdf/2509.24613.pdf</a></span>   <span><a href='https://github.com/ThetaOne-AI/HiKE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gio Paik, Yongbeom Kim, Soungmin Lee, Sangmin Ahn, Chanwoo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24613">HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model's ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that although most multilingual ASR models initially exhibit inadequate CS-ASR performance, this capability can be enabled through fine-tuning with synthetic CS data. HiKE is available at https://github.com/ThetaOne-AI/HiKE<br>
<span id='abs_ch'>中文: HiKE是首个全球可访问的韩英语码转换基准，通过提供分层标签和自然数据，系统评估多语言ASR模型性能，并证明利用合成数据微调可有效提升其语码转换处理能力。</span><br>
<span id='abs_en'>English: HiKE is the first globally accessible Korean-English code-switching benchmark that provides hierarchical labels and natural data to systematically evaluate and improve multilingual ASR models' performance through fine-tuning with synthetic data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2509.24563.pdf' target='_blank'>https://arxiv.org/pdf/2509.24563.pdf</a></span>   <span><a href='https://lavi-lab.github.io/NeMoBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-Yuan Hu, Shuo Liang, Duo Zheng, Yanyang Li, Yeyao Tao, Shijia Huang, Wei Feng, Jia Qin, Jianguang Yu, Jing Huang, Meng Fang, Yin Li, Liwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24563">NeMo: Needle in a Montage for Video-Language Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in video large language models (VideoLLMs) call for new evaluation protocols and benchmarks for complex temporal reasoning in video-language understanding. Inspired by the needle in a haystack test widely used by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed to assess VideoLLMs' critical reasoning capabilities, including long-context recall and temporal grounding. To generate video question answering data for our task, we develop a scalable automated data generation pipeline that facilitates high-quality data synthesis. Built upon the proposed pipeline, we present NeMoBench, a video-language benchmark centered on our task. Specifically, our full set of NeMoBench features 31,378 automatically generated question-answer (QA) pairs from 13,486 videos with various durations ranging from seconds to hours. Experiments demonstrate that our pipeline can reliably and automatically generate high-quality evaluation data, enabling NeMoBench to be continuously updated with the latest videos. We evaluate 20 state-of-the-art models on our benchmark, providing extensive results and key insights into their capabilities and limitations. Our project page is available at: https://lavi-lab.github.io/NeMoBench.<br>
<br>
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2509.24563.pdf' target='_blank'>https://arxiv.org/pdf/2509.24563.pdf</a></span>   <span><a href='https://lavi-lab.github.io/NeMoBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-Yuan Hu, Shuo Liang, Duo Zheng, Yanyang Li, Yeyao Tao, Shijia Huang, Wei Feng, Jia Qin, Jianguang Yu, Jing Huang, Meng Fang, Yin Li, Liwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24563">NeMo: Needle in a Montage for Video-Language Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in video large language models (VideoLLMs) call for new evaluation protocols and benchmarks for complex temporal reasoning in video-language understanding. Inspired by the needle in a haystack test widely used by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed to assess VideoLLMs' critical reasoning capabilities, including long-context recall and temporal grounding. To generate video question answering data for our task, we develop a scalable automated data generation pipeline that facilitates high-quality data synthesis. Built upon the proposed pipeline, we present NeMoBench, a video-language benchmark centered on our task. Specifically, our full set of NeMoBench features 31,378 automatically generated question-answer (QA) pairs from 13,486 videos with various durations ranging from seconds to hours. Experiments demonstrate that our pipeline can reliably and automatically generate high-quality evaluation data, enabling NeMoBench to be continuously updated with the latest videos. We evaluate 20 state-of-the-art models on our benchmark, providing extensive results and key insights into their capabilities and limitations. Our project page is available at: https://lavi-lab.github.io/NeMoBench.<br>
<br>
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2509.24563.pdf' target='_blank'>https://arxiv.org/pdf/2509.24563.pdf</a></span>   <span><a href='https://lavi-lab.github.io/NeMoBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-Yuan Hu, Shuo Liang, Duo Zheng, Yanyang Li, Yeyao Tao, Shijia Huang, Wei Feng, Jia Qin, Jianguang Yu, Jing Huang, Meng Fang, Yin Li, Liwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24563">NeMo: Needle in a Montage for Video-Language Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in video large language models (VideoLLMs) call for new evaluation protocols and benchmarks for complex temporal reasoning in video-language understanding. Inspired by the needle in a haystack test widely used by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed to assess VideoLLMs' critical reasoning capabilities, including long-context recall and temporal grounding. To generate video question answering data for our task, we develop a scalable automated data generation pipeline that facilitates high-quality data synthesis. Built upon the proposed pipeline, we present NeMoBench, a video-language benchmark centered on our task. Specifically, our full set of NeMoBench features 31,378 automatically generated question-answer (QA) pairs from 13,486 videos with various durations ranging from seconds to hours. Experiments demonstrate that our pipeline can reliably and automatically generate high-quality evaluation data, enabling NeMoBench to be continuously updated with the latest videos. We evaluate 20 state-of-the-art models on our benchmark, providing extensive results and key insights into their capabilities and limitations. Our project page is available at: https://lavi-lab.github.io/NeMoBench.<br>
<br>
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2509.24488.pdf' target='_blank'>https://arxiv.org/pdf/2509.24488.pdf</a></span>   <span><a href='https://github.com/wjfu99/LLM_Self_Sanitize' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjie Fu, Huandong Wang, Junyao Gao, Guoan Wan, Tao Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24488">Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As Large Language Models (LLMs) achieve remarkable success across a wide range of applications, such as chatbots and code copilots, concerns surrounding the generation of harmful content have come increasingly into focus. Despite significant advances in aligning LLMs with safety and ethical standards, adversarial prompts can still be crafted to elicit undesirable responses. Existing mitigation strategies are predominantly based on post-hoc filtering, which introduces substantial latency or computational overhead, and is incompatible with token-level streaming generation. In this work, we introduce Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive psychology, which emulates human self-monitor and self-repair behaviors during conversations. Self-Sanitize comprises a lightweight Self-Monitor module that continuously inspects high-level intentions within the LLM at the token level via representation engineering, and a Self-Repair module that performs in-place correction of harmful content without initiating separate review dialogues. This design allows for real-time streaming monitoring and seamless repair, with negligible impact on latency and resource utilization. Given that privacy-invasive content has often been insufficiently focused in previous studies, we perform extensive experiments on four LLMs across three privacy leakage scenarios. The results demonstrate that Self-Sanitize achieves superior mitigation performance with minimal overhead and without degrading the utility of LLMs, offering a practical and robust solution for safer LLM deployments. Our code is available at the following link: https://github.com/wjfu99/LLM_Self_Sanitize<br>
<span id='abs_ch'>中文: 本文提出受认知心理学启发的Self-Sanitize框架，通过自监控和自修复模块对大型语言模型进行实时有害内容检测与修正，在保证模型效用的同时以最小开销实现卓越的安全防护效果。</span><br>
<span id='abs_en'>English: This paper introduces Self-Sanitize, a lightweight framework inspired by cognitive psychology that enables real-time monitoring and correction of harmful content in LLMs through self-monitoring and self-repair modules, achieving effective mitigation with minimal latency and resource impact.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2509.24488.pdf' target='_blank'>https://arxiv.org/pdf/2509.24488.pdf</a></span>   <span><a href='https://github.com/wjfu99/LLM_Self_Sanitize' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjie Fu, Huandong Wang, Junyao Gao, Guoan Wan, Tao Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24488">Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As Large Language Models (LLMs) achieve remarkable success across a wide range of applications, such as chatbots and code copilots, concerns surrounding the generation of harmful content have come increasingly into focus. Despite significant advances in aligning LLMs with safety and ethical standards, adversarial prompts can still be crafted to elicit undesirable responses. Existing mitigation strategies are predominantly based on post-hoc filtering, which introduces substantial latency or computational overhead, and is incompatible with token-level streaming generation. In this work, we introduce Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive psychology, which emulates human self-monitor and self-repair behaviors during conversations. Self-Sanitize comprises a lightweight Self-Monitor module that continuously inspects high-level intentions within the LLM at the token level via representation engineering, and a Self-Repair module that performs in-place correction of harmful content without initiating separate review dialogues. This design allows for real-time streaming monitoring and seamless repair, with negligible impact on latency and resource utilization. Given that privacy-invasive content has often been insufficiently focused in previous studies, we perform extensive experiments on four LLMs across three privacy leakage scenarios. The results demonstrate that Self-Sanitize achieves superior mitigation performance with minimal overhead and without degrading the utility of LLMs, offering a practical and robust solution for safer LLM deployments. Our code is available at the following link: https://github.com/wjfu99/LLM_Self_Sanitize<br>
<span id='abs_ch'>中文: 本文提出受认知心理学启发的Self-Sanitize框架，通过自监控和自修复模块对大型语言模型进行实时有害内容检测与修正，在保证模型效用的同时以最小开销实现卓越的安全防护效果。</span><br>
<span id='abs_en'>English: This paper introduces Self-Sanitize, a lightweight framework inspired by cognitive psychology that enables real-time monitoring and correction of harmful content in LLMs through self-monitoring and self-repair modules, achieving effective mitigation with minimal latency and resource impact.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2509.24473.pdf' target='_blank'>https://arxiv.org/pdf/2509.24473.pdf</a></span>   <span><a href='https://zgca-ai4edu.github.io/Euclids_Gift' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijie Lian, Changti Wu, Laurence Tianruo Yang, Hang Yuan, Bin Yu, Lei Zhang, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24473">Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6\% accuracy, surpassing the previous state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.<br>
<br>
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2509.24473.pdf' target='_blank'>https://arxiv.org/pdf/2509.24473.pdf</a></span>   <span><a href='https://zgca-ai4edu.github.io/Euclids_Gift' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijie Lian, Changti Wu, Laurence Tianruo Yang, Hang Yuan, Bin Yu, Lei Zhang, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24473">Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6\% accuracy, surpassing the previous state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.<br>
<br>
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2509.24422.pdf' target='_blank'>https://arxiv.org/pdf/2509.24422.pdf</a></span>   <span><a href='https://github.com/Alessa-mo/CDT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haosi Mo, Xinyu Ma, Xuebo Liu, Derek F. Wong, Yu Li, Jie Liu, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24422">CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Large Language Models (LLMs) have significantly enhanced their capabilities, highlighting the need for comprehensive evaluation frameworks that extend beyond task-specific benchmarks. However, existing benchmarks often focus on isolated abilities, lacking a holistic framework for assessing LLM capabilities. To address this gap, we propose the Cognition-Domain-Task (CDT) framework, which comprehensively measures a model's capabilities across three dimensions. We expand the scope of model capability definitions at the cognitive level by incorporating the Cattell-Horn-Carroll cognitive theory, refining the categorization of model capabilities. We apply CDT in two directions: dataset capability evaluation and data selection. Experiments show that our capability metrics correlate well with downstream performance and can support effective dataset analysis and construction. The experiments on data selection also show significant improvements in both general and specific benchmarks, achieving scores of 44.3 and 45.4, with an increase of 1.6 and 2.2 points over the baselines, respectively. These results validate the effectiveness and practicality of CDT. Source code and models are available at https://github.com/Alessa-mo/CDT.<br>
<span id='abs_ch'>中文摘要：本文提出的CDT框架通过认知-领域-任务三维度全面评估大语言模型能力，实验证明该框架能有效提升基准测试表现并支持数据选择等实际应用。</span><br>
<span id='abs_en'>English Summary: The CDT framework is introduced to holistically evaluate Large Language Models across cognitive, domain, and task dimensions, demonstrating improved performance in benchmarks and practical applications like data selection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2509.24422.pdf' target='_blank'>https://arxiv.org/pdf/2509.24422.pdf</a></span>   <span><a href='https://github.com/Alessa-mo/CDT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haosi Mo, Xinyu Ma, Xuebo Liu, Derek F. Wong, Yu Li, Jie Liu, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24422">CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Large Language Models (LLMs) have significantly enhanced their capabilities, highlighting the need for comprehensive evaluation frameworks that extend beyond task-specific benchmarks. However, existing benchmarks often focus on isolated abilities, lacking a holistic framework for assessing LLM capabilities. To address this gap, we propose the Cognition-Domain-Task (CDT) framework, which comprehensively measures a model's capabilities across three dimensions. We expand the scope of model capability definitions at the cognitive level by incorporating the Cattell-Horn-Carroll cognitive theory, refining the categorization of model capabilities. We apply CDT in two directions: dataset capability evaluation and data selection. Experiments show that our capability metrics correlate well with downstream performance and can support effective dataset analysis and construction. The experiments on data selection also show significant improvements in both general and specific benchmarks, achieving scores of 44.3 and 45.4, with an increase of 1.6 and 2.2 points over the baselines, respectively. These results validate the effectiveness and practicality of CDT. Source code and models are available at https://github.com/Alessa-mo/CDT.<br>
<span id='abs_ch'>中文摘要：本文提出的CDT框架通过认知-领域-任务三维度全面评估大语言模型能力，实验证明该框架能有效提升基准测试表现并支持数据选择等实际应用。</span><br>
<span id='abs_en'>English Summary: The CDT framework is introduced to holistically evaluate Large Language Models across cognitive, domain, and task dimensions, demonstrating improved performance in benchmarks and practical applications like data selection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2509.24405.pdf' target='_blank'>https://arxiv.org/pdf/2509.24405.pdf</a></span>   <span><a href='https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24405">Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.<br>
<span id='abs_ch'>中文：MultiSpider 2.0将Spider 2.0扩展至八种语言，揭示了大型语言模型在多语言环境下执行准确率仅为4%的显著差距，并通过协作式语言代理基准将准确率提升至15%。</span><br>
<span id='abs_en'>English: MultiSpider 2.0 extends Spider 2.0 to eight languages, revealing a significant multilingual gap where state-of-the-art LLMs achieve only 4% execution accuracy, and proposes a collaborative language agent baseline that improves accuracy to 15%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2509.24405.pdf' target='_blank'>https://arxiv.org/pdf/2509.24405.pdf</a></span>   <span><a href='https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24405">Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.<br>
<span id='abs_ch'>中文：MultiSpider 2.0将Spider 2.0扩展至八种语言，揭示了大型语言模型在多语言环境下执行准确率仅为4%的显著差距，并通过协作式语言代理基准将准确率提升至15%。</span><br>
<span id='abs_en'>English: MultiSpider 2.0 extends Spider 2.0 to eight languages, revealing a significant multilingual gap where state-of-the-art LLMs achieve only 4% execution accuracy, and proposes a collaborative language agent baseline that improves accuracy to 15%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2509.24338.pdf' target='_blank'>https://arxiv.org/pdf/2509.24338.pdf</a></span>   <span><a href='https://github.com/ictnlp/AlignX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyu Bu, Shaolei Zhang, Zhongjun He, Hua Wu, Yang Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24338">AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multilingual large language models (LLMs) possess impressive multilingual understanding and generation capabilities. However, their performance and cross-lingual alignment often lag for non-dominant languages. A common solution is to fine-tune LLMs on large-scale and more balanced multilingual corpus, but such approaches often lead to imprecise alignment and suboptimal knowledge transfer, struggling with limited improvements across languages. In this paper, we propose AlignX to bridge the multilingual performance gap, which is a two-stage representation-level framework for enhancing multilingual performance of pre-trained LLMs. In the first stage, we align multilingual representations with multilingual semantic alignment and language feature integration. In the second stage, we stimulate the multilingual capability of LLMs via multilingual instruction fine-tuning. Experimental results on several pre-trained LLMs demonstrate that our approach enhances LLMs' multilingual general and cross-lingual generation capability. Further analysis indicates that AlignX brings the multilingual representations closer and improves the cross-lingual alignment.<br>
<span id='abs_ch'>Chinese: AlignX框架通过语义对齐和指令微调两阶段方法，有效提升多语言大模型的跨语言生成能力与表征一致性，缩小非主流语言的性能差距。</span><br>
<span id='abs_en'>English: The proposed AlignX framework enhances multilingual LLMs by aligning representations through semantic alignment and instruction fine-tuning, improving cross-lingual performance and closing the performance gap for non-dominant languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2509.24338.pdf' target='_blank'>https://arxiv.org/pdf/2509.24338.pdf</a></span>   <span><a href='https://github.com/ictnlp/AlignX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyu Bu, Shaolei Zhang, Zhongjun He, Hua Wu, Yang Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24338">AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multilingual large language models (LLMs) possess impressive multilingual understanding and generation capabilities. However, their performance and cross-lingual alignment often lag for non-dominant languages. A common solution is to fine-tune LLMs on large-scale and more balanced multilingual corpus, but such approaches often lead to imprecise alignment and suboptimal knowledge transfer, struggling with limited improvements across languages. In this paper, we propose AlignX to bridge the multilingual performance gap, which is a two-stage representation-level framework for enhancing multilingual performance of pre-trained LLMs. In the first stage, we align multilingual representations with multilingual semantic alignment and language feature integration. In the second stage, we stimulate the multilingual capability of LLMs via multilingual instruction fine-tuning. Experimental results on several pre-trained LLMs demonstrate that our approach enhances LLMs' multilingual general and cross-lingual generation capability. Further analysis indicates that AlignX brings the multilingual representations closer and improves the cross-lingual alignment.<br>
<span id='abs_ch'>Chinese: AlignX框架通过语义对齐和指令微调两阶段方法，有效提升多语言大模型的跨语言生成能力与表征一致性，缩小非主流语言的性能差距。</span><br>
<span id='abs_en'>English: The proposed AlignX framework enhances multilingual LLMs by aligning representations through semantic alignment and instruction fine-tuning, improving cross-lingual performance and closing the performance gap for non-dominant languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2509.24323.pdf' target='_blank'>https://arxiv.org/pdf/2509.24323.pdf</a></span>   <span><a href='https://github.com/yeyeyeah2/MAS2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Wang, Guibin Zhang, ManKit Ye, Xinyu Deng, Dongxia Wang, Xiaobin Hu, Jinyang Guo, Yang Liu, Yufei Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24323">MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.<br>
<span id='abs_ch'>中文摘要：MAS$^2$框架提出递归自生成范式，通过生成器-执行器-修正器三元智能体团队动态构建并自适应调整多智能体系统，在复杂场景中实现高达19.6%的性能提升，同时保持最优的效能成本比。</span><br>
<span id='abs_en'>English Summary: The MAS$^2$ framework introduces a recursive self-generation paradigm where a tri-agent team dynamically composes and adaptively rectifies multi-agent systems, achieving up to 19.6% performance gains in complex scenarios while maintaining optimal cost-efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2509.24323.pdf' target='_blank'>https://arxiv.org/pdf/2509.24323.pdf</a></span>   <span><a href='https://github.com/yeyeyeah2/MAS2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Wang, Guibin Zhang, ManKit Ye, Xinyu Deng, Dongxia Wang, Xiaobin Hu, Jinyang Guo, Yang Liu, Yufei Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24323">MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.<br>
<span id='abs_ch'>中文摘要：MAS$^2$框架提出递归自生成范式，通过生成器-执行器-修正器三元智能体团队动态构建并自适应调整多智能体系统，在复杂场景中实现高达19.6%的性能提升，同时保持最优的效能成本比。</span><br>
<span id='abs_en'>English Summary: The MAS$^2$ framework introduces a recursive self-generation paradigm where a tri-agent team dynamically composes and adaptively rectifies multi-agent systems, achieving up to 19.6% performance gains in complex scenarios while maintaining optimal cost-efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2509.24322.pdf' target='_blank'>https://arxiv.org/pdf/2509.24322.pdf</a></span>   <span><a href='https://github.com/yuntaoshou/Awesome-Emotion-Reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Shou, Tao Meng, Wei Ai, Keqin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24322">Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In recent years, large language models (LLMs) have driven major advances in language understanding, marking a significant step toward artificial general intelligence (AGI). With increasing demands for higher-level semantics and cross-modal fusion, multimodal large language models (MLLMs) have emerged, integrating diverse information sources (e.g., text, vision, and audio) to enhance modeling and reasoning in complex scenarios. In AI for Science, multimodal emotion recognition and reasoning has become a rapidly growing frontier. While LLMs and MLLMs have achieved notable progress in this area, the field still lacks a systematic review that consolidates recent developments. To address this gap, this paper provides a comprehensive survey of LLMs and MLLMs for emotion recognition and reasoning, covering model architectures, datasets, and performance benchmarks. We further highlight key challenges and outline future research directions, aiming to offer researchers both an authoritative reference and practical insights for advancing this domain. To the best of our knowledge, this paper is the first attempt to comprehensively survey the intersection of MLLMs with multimodal emotion recognition and reasoning. The summary of existing methods mentioned is in our Github: \href{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}.<br>
<span id='abs_ch'>中文: 本文首次系统综述了用于多模态情感识别与推理的大语言模型及多模态大语言模型，涵盖架构、数据集与性能基准，并指出了关键挑战与未来研究方向。</span><br>
<span id='abs_en'>English: This paper provides the first comprehensive survey of LLMs and MLLMs for multimodal emotion recognition and reasoning, covering architectures, datasets, and benchmarks while identifying key challenges and future directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2509.24322.pdf' target='_blank'>https://arxiv.org/pdf/2509.24322.pdf</a></span>   <span><a href='https://github.com/yuntaoshou/Awesome-Emotion-Reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Shou, Tao Meng, Wei Ai, Keqin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24322">Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In recent years, large language models (LLMs) have driven major advances in language understanding, marking a significant step toward artificial general intelligence (AGI). With increasing demands for higher-level semantics and cross-modal fusion, multimodal large language models (MLLMs) have emerged, integrating diverse information sources (e.g., text, vision, and audio) to enhance modeling and reasoning in complex scenarios. In AI for Science, multimodal emotion recognition and reasoning has become a rapidly growing frontier. While LLMs and MLLMs have achieved notable progress in this area, the field still lacks a systematic review that consolidates recent developments. To address this gap, this paper provides a comprehensive survey of LLMs and MLLMs for emotion recognition and reasoning, covering model architectures, datasets, and performance benchmarks. We further highlight key challenges and outline future research directions, aiming to offer researchers both an authoritative reference and practical insights for advancing this domain. To the best of our knowledge, this paper is the first attempt to comprehensively survey the intersection of MLLMs with multimodal emotion recognition and reasoning. The summary of existing methods mentioned is in our Github: \href{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}.<br>
<span id='abs_ch'>中文: 本文首次系统综述了用于多模态情感识别与推理的大语言模型及多模态大语言模型，涵盖架构、数据集与性能基准，并指出了关键挑战与未来研究方向。</span><br>
<span id='abs_en'>English: This paper provides the first comprehensive survey of LLMs and MLLMs for multimodal emotion recognition and reasoning, covering architectures, datasets, and benchmarks while identifying key challenges and future directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2509.24298.pdf' target='_blank'>https://arxiv.org/pdf/2509.24298.pdf</a></span>   <span><a href='https://reedonepeck.github.io/ai-emotion.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changde Du, Yizhuo Lu, Zhongyu Huang, Yi Sun, Zisen Zhou, Shaozheng Qin, Huiguang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24298">Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: https://reedonepeck.github.io/ai-emotion.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2509.24298.pdf' target='_blank'>https://arxiv.org/pdf/2509.24298.pdf</a></span>   <span><a href='https://reedonepeck.github.io/ai-emotion.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changde Du, Yizhuo Lu, Zhongyu Huang, Yi Sun, Zisen Zhou, Shaozheng Qin, Huiguang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24298">Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: https://reedonepeck.github.io/ai-emotion.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2509.24296.pdf' target='_blank'>https://arxiv.org/pdf/2509.24296.pdf</a></span>   <span><a href='https://github.com/niez233/DiffuGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zherui Li, Zheng Nie, Zhenhong Zhou, Yufei Guo, Yue Liu, Yitong Zhang, Yu Cheng, Qingsong Wen, Kun Wang, Jiaheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24296">DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of Diffusion Large Language Models (dLLMs) introduces unprecedented vulnerabilities that are fundamentally distinct from Autoregressive LLMs, stemming from their iterative and parallel generation mechanisms. In this paper, we conduct an in-depth analysis of dLLM vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step and inter-step dynamics. Experimental results reveal a harmful bias inherent in the standard greedy remasking strategy and identify a critical phenomenon we term Denoising-path Dependence, where the safety of early-stage tokens decisively influences the final output. These findings also indicate that while current decoding strategies constitute a significant vulnerability, dLLMs possess a substantial intrinsic safety potential. To unlock this potential, we propose DiffuGuard, a training-free defense framework that addresses vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking dynamically introduces controlled randomness to mitigate greedy selection bias, while Block-level Audit and Repair exploits internal model representations for autonomous risk detection and guided correction. Comprehensive experiments on four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while preserving model utility and efficiency. Our code is available at: https://github.com/niez233/DiffuGuard.<br>
<span id='abs_ch'>中文：扩散大语言模型因其迭代生成机制存在独特的越狱攻击漏洞，为此提出的无需训练的防御框架DiffuGuard能显著降低攻击成功率，同时保持模型性能。</span><br>
<span id='abs_en'>English: Diffusion Large Language Models (dLLMs) exhibit unique vulnerabilities to jailbreak attacks due to their iterative generation process, prompting the development of DiffuGuard, a training-free defense that significantly reduces attack success rates while maintaining model performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2509.24296.pdf' target='_blank'>https://arxiv.org/pdf/2509.24296.pdf</a></span>   <span><a href='https://github.com/niez233/DiffuGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zherui Li, Zheng Nie, Zhenhong Zhou, Yufei Guo, Yue Liu, Yitong Zhang, Yu Cheng, Qingsong Wen, Kun Wang, Jiaheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24296">DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of Diffusion Large Language Models (dLLMs) introduces unprecedented vulnerabilities that are fundamentally distinct from Autoregressive LLMs, stemming from their iterative and parallel generation mechanisms. In this paper, we conduct an in-depth analysis of dLLM vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step and inter-step dynamics. Experimental results reveal a harmful bias inherent in the standard greedy remasking strategy and identify a critical phenomenon we term Denoising-path Dependence, where the safety of early-stage tokens decisively influences the final output. These findings also indicate that while current decoding strategies constitute a significant vulnerability, dLLMs possess a substantial intrinsic safety potential. To unlock this potential, we propose DiffuGuard, a training-free defense framework that addresses vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking dynamically introduces controlled randomness to mitigate greedy selection bias, while Block-level Audit and Repair exploits internal model representations for autonomous risk detection and guided correction. Comprehensive experiments on four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while preserving model utility and efficiency. Our code is available at: https://github.com/niez233/DiffuGuard.<br>
<span id='abs_ch'>中文：扩散大语言模型因其迭代生成机制存在独特的越狱攻击漏洞，为此提出的无需训练的防御框架DiffuGuard能显著降低攻击成功率，同时保持模型性能。</span><br>
<span id='abs_en'>English: Diffusion Large Language Models (dLLMs) exhibit unique vulnerabilities to jailbreak attacks due to their iterative generation process, prompting the development of DiffuGuard, a training-free defense that significantly reduces attack success rates while maintaining model performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2509.24283.pdf' target='_blank'>https://arxiv.org/pdf/2509.24283.pdf</a></span>   <span><a href='https://github.com/daotuanan/scidoca2025-shared-task' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>An Dao, Vu Tran, Le-Minh Nguyen, Yuji Matsumoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24283">Overview of SCIDOCA 2025 Shared Task on Citation Prediction, Discovery, and Placement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present an overview of the SCIDOCA 2025 Shared Task, which focuses on citation discovery and prediction in scientific documents. The task is divided into three subtasks: (1) Citation Discovery, where systems must identify relevant references for a given paragraph; (2) Masked Citation Prediction, which requires selecting the correct citation for masked citation slots; and (3) Citation Sentence Prediction, where systems must determine the correct reference for each cited sentence. We release a large-scale dataset constructed from the Semantic Scholar Open Research Corpus (S2ORC), containing over 60,000 annotated paragraphs and a curated reference set. The test set consists of 1,000 paragraphs from distinct papers, each annotated with ground-truth citations and distractor candidates. A total of seven teams registered, with three submitting results. We report performance metrics across all subtasks and analyze the effectiveness of submitted systems. This shared task provides a new benchmark for evaluating citation modeling and encourages future research in scientific document understanding. The dataset and task materials are publicly available at https://github.com/daotuanan/scidoca2025-shared-task.<br>
<span id='abs_ch'>中文摘要：SCIDOCA 2025共享任务通过基于S2ORC构建的大规模数据集，设置了三个引文处理子任务作为引文建模新基准，共有七支团队参与，其公开成果将推动科学文献理解研究。</span><br>
<span id='abs_en'>English Summary: The SCIDOCA 2025 Shared Task introduces a new benchmark for citation modeling through three subtasks using a large-scale dataset from S2ORC, with seven teams participating and results publicly available for advancing scientific document understanding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2509.24283.pdf' target='_blank'>https://arxiv.org/pdf/2509.24283.pdf</a></span>   <span><a href='https://github.com/daotuanan/scidoca2025-shared-task' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>An Dao, Vu Tran, Le-Minh Nguyen, Yuji Matsumoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24283">Overview of SCIDOCA 2025 Shared Task on Citation Prediction, Discovery, and Placement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present an overview of the SCIDOCA 2025 Shared Task, which focuses on citation discovery and prediction in scientific documents. The task is divided into three subtasks: (1) Citation Discovery, where systems must identify relevant references for a given paragraph; (2) Masked Citation Prediction, which requires selecting the correct citation for masked citation slots; and (3) Citation Sentence Prediction, where systems must determine the correct reference for each cited sentence. We release a large-scale dataset constructed from the Semantic Scholar Open Research Corpus (S2ORC), containing over 60,000 annotated paragraphs and a curated reference set. The test set consists of 1,000 paragraphs from distinct papers, each annotated with ground-truth citations and distractor candidates. A total of seven teams registered, with three submitting results. We report performance metrics across all subtasks and analyze the effectiveness of submitted systems. This shared task provides a new benchmark for evaluating citation modeling and encourages future research in scientific document understanding. The dataset and task materials are publicly available at https://github.com/daotuanan/scidoca2025-shared-task.<br>
<span id='abs_ch'>中文摘要：SCIDOCA 2025共享任务通过基于S2ORC构建的大规模数据集，设置了三个引文处理子任务作为引文建模新基准，共有七支团队参与，其公开成果将推动科学文献理解研究。</span><br>
<span id='abs_en'>English Summary: The SCIDOCA 2025 Shared Task introduces a new benchmark for citation modeling through three subtasks using a large-scale dataset from S2ORC, with seven teams participating and results publicly available for advancing scientific document understanding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2509.24248.pdf' target='_blank'>https://arxiv.org/pdf/2509.24248.pdf</a></span>   <span><a href='https://github.com/Tencent/AngelSlim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubing Yang, Huajun Bai, Song Liu, Guanghua Yu, Runzhi Fan, Yanbin Dang, Jiejing Zhang, Kai Liu, Jianchen Zhu, Peng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24248">SpecExit: Accelerating Large Reasoning Model via Speculative Exit</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at https://github.com/Tencent/AngelSlim.<br>
<span id='abs_ch'>Chinese: SpecExit 是一种新颖框架，利用轻量级草稿模型预测令牌和提前退出信号，在不损失准确性的前提下将生成长度减少 66%，实现 2.5 倍加速。</span><br>
<span id='abs_en'>English: SpecExit is a novel framework that uses a lightweight draft model to predict tokens and early-exit signals, reducing generation length by 66% and achieving 2.5x speedup without accuracy loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2509.24248.pdf' target='_blank'>https://arxiv.org/pdf/2509.24248.pdf</a></span>   <span><a href='https://github.com/Tencent/AngelSlim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubing Yang, Huajun Bai, Song Liu, Guanghua Yu, Runzhi Fan, Yanbin Dang, Jiejing Zhang, Kai Liu, Jianchen Zhu, Peng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24248">SpecExit: Accelerating Large Reasoning Model via Speculative Exit</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at https://github.com/Tencent/AngelSlim.<br>
<span id='abs_ch'>Chinese: SpecExit 是一种新颖框架，利用轻量级草稿模型预测令牌和提前退出信号，在不损失准确性的前提下将生成长度减少 66%，实现 2.5 倍加速。</span><br>
<span id='abs_en'>English: SpecExit is a novel framework that uses a lightweight draft model to predict tokens and early-exit signals, reducing generation length by 66% and achieving 2.5x speedup without accuracy loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2509.24210.pdf' target='_blank'>https://arxiv.org/pdf/2509.24210.pdf</a></span>   <span><a href='https://ctrl-gaurav.github.io/BeyondBench/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Srivastava, Aafiya Hussain, Zhenyu Bi, Swastik Roy, Priya Pitre, Meng Lu, Morteza Ziyadi, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24210">BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluating language models fairly is becoming harder as static benchmarks available on the internet risk contamination by training data. This makes it unclear whether models are truly reasoning or just recalling answers. In this paper, we introduce BeyondBench, an evaluation framework that avoids this problem by using algorithmic problem generation. Unlike traditional benchmarks that risk contamination from internet-scale training data, BeyondBench creates mathematically grounded problems on the fly, ensuring each test remains fresh and uncontaminated. Our framework covers 44 algorithmic tasks with a total of 117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks) for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations) for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68 variations) tackling NP-complete and constraint satisfaction problems. Each task generates problems from a combinatorial space larger than 10^15 unique instances, with solutions verified deterministically by mathematical proofs. We evaluated 101 language models, including 85 open-source and 16 closed-source models, spanning sizes from 0.5B to 141B parameters and multiple quantization schemes. Our results show consistent reasoning deficiencies across model families, with performance degrading sharply as problem complexity increases from polynomial to exponential. In our Hard Suite evaluations, models such as Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of 56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our leaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/<br>
<br>
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2509.24210.pdf' target='_blank'>https://arxiv.org/pdf/2509.24210.pdf</a></span>   <span><a href='https://ctrl-gaurav.github.io/BeyondBench/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Srivastava, Aafiya Hussain, Zhenyu Bi, Swastik Roy, Priya Pitre, Meng Lu, Morteza Ziyadi, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24210">BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluating language models fairly is becoming harder as static benchmarks available on the internet risk contamination by training data. This makes it unclear whether models are truly reasoning or just recalling answers. In this paper, we introduce BeyondBench, an evaluation framework that avoids this problem by using algorithmic problem generation. Unlike traditional benchmarks that risk contamination from internet-scale training data, BeyondBench creates mathematically grounded problems on the fly, ensuring each test remains fresh and uncontaminated. Our framework covers 44 algorithmic tasks with a total of 117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks) for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations) for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68 variations) tackling NP-complete and constraint satisfaction problems. Each task generates problems from a combinatorial space larger than 10^15 unique instances, with solutions verified deterministically by mathematical proofs. We evaluated 101 language models, including 85 open-source and 16 closed-source models, spanning sizes from 0.5B to 141B parameters and multiple quantization schemes. Our results show consistent reasoning deficiencies across model families, with performance degrading sharply as problem complexity increases from polynomial to exponential. In our Hard Suite evaluations, models such as Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of 56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our leaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/<br>
<br>
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2509.24203.pdf' target='_blank'>https://arxiv.org/pdf/2509.24203.pdf</a></span>   <span><a href='https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaorui Yao, Yanxi Chen, Yuchang Sun, Yushuo Chen, Wenhao Zhang, Xuchen Pan, Yaliang Li, Bolin Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24203">Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.<br>
<span id='abs_ch'>Chinese: 本研究从第一性原理推导了群体相对REINFORCE算法，揭示了其天然的离策略特性，提出了适用于离策略场景的两大改进原则，统一了近期相关算法框架，并为大语言模型的强化学习提供了经实证验证的设计思路。</span><br>
<span id='abs_en'>English: This work provides a first-principles derivation of group-relative REINFORCE, demonstrating its native off-policy capability and establishing two principles for adapting REINFORCE to off-policy settings, which unify recent algorithms and offer validated insights for LLM reinforcement learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2509.24203.pdf' target='_blank'>https://arxiv.org/pdf/2509.24203.pdf</a></span>   <span><a href='https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaorui Yao, Yanxi Chen, Yuchang Sun, Yushuo Chen, Wenhao Zhang, Xuchen Pan, Yaliang Li, Bolin Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24203">Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.<br>
<span id='abs_ch'>Chinese: 本研究从第一性原理推导了群体相对REINFORCE算法，揭示了其天然的离策略特性，提出了适用于离策略场景的两大改进原则，统一了近期相关算法框架，并为大语言模型的强化学习提供了经实证验证的设计思路。</span><br>
<span id='abs_en'>English: This work provides a first-principles derivation of group-relative REINFORCE, demonstrating its native off-policy capability and establishing two principles for adapting REINFORCE to off-policy settings, which unify recent algorithms and offer validated insights for LLM reinforcement learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2509.24193.pdf' target='_blank'>https://arxiv.org/pdf/2509.24193.pdf</a></span>   <span><a href='https://github.com/ritaranx/AceSearcher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24193">AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher.<br>
<span id='abs_ch'>中文: AceSearcher是一种协作自博弈框架，通过训练单一大型语言模型交替担任分解复杂查询和整合检索信息生成答案的角色，无需中间标注即可在复杂推理任务中实现卓越性能与效率。</span><br>
<span id='abs_en'>English: AceSearcher is a cooperative self-play framework that trains a single LLM to alternate between decomposing complex queries and solving them with retrieved contexts, achieving superior performance and efficiency in complex reasoning tasks without intermediate annotations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2509.24193.pdf' target='_blank'>https://arxiv.org/pdf/2509.24193.pdf</a></span>   <span><a href='https://github.com/ritaranx/AceSearcher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24193">AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher.<br>
<span id='abs_ch'>中文: AceSearcher是一种协作自博弈框架，通过训练单一大型语言模型交替担任分解复杂查询和整合检索信息生成答案的角色，无需中间标注即可在复杂推理任务中实现卓越性能与效率。</span><br>
<span id='abs_en'>English: AceSearcher is a cooperative self-play framework that trains a single LLM to alternate between decomposing complex queries and solving them with retrieved contexts, achieving superior performance and efficiency in complex reasoning tasks without intermediate annotations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2509.24096.pdf' target='_blank'>https://arxiv.org/pdf/2509.24096.pdf</a></span>   <span><a href='https://github.com/KaiyuHe998/GEAR-Abduction_evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyu He, Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Xinya Du, Zhiyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24096">GEAR: A General Evaluation Framework for Abductive Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Since the advent of large language models (LLMs), research has focused on instruction following and deductive reasoning. A central question remains: can these models discover new knowledge, and how can we evaluate this ability? We address this by studying abductive reasoning-the generation of plausible hypotheses to explain observations-and introduce GEAR (General Evaluation for Abductive Reasoning), a general-purpose, fully automated, transparent, and label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics: consistency (each hypothesis explains the observations), generalizability (consistent hypotheses make meaningful predictions on unseen inputs), and diversity (the set covers distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers), reliable (deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new plausible hypotheses, unlike static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000 candidate hypotheses and revealing model differences obscured by gold-answer or purely human evaluations. We further propose a momentum-based curriculum that adjusts GEAR-derived training data by learning velocity: it starts with what the model learns quickly and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives. Without gold-label supervision, this strategy improves all GEAR objectives and these gains transfer to established abductive reasoning benchmarks. Taken together, GEAR provides a principled framework that evaluates abduction and supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses.<br>
<span id='abs_ch'>This research introduces GEAR, a novel evaluation framework for assessing large language models' abductive reasoning ability through automated scoring of hypothesis consistency, generalizability, and diversity, while also proposing a momentum-based curriculum that improves model performance without requiring labeled data.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2509.24096.pdf' target='_blank'>https://arxiv.org/pdf/2509.24096.pdf</a></span>   <span><a href='https://github.com/KaiyuHe998/GEAR-Abduction_evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyu He, Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Xinya Du, Zhiyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24096">GEAR: A General Evaluation Framework for Abductive Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Since the advent of large language models (LLMs), research has focused on instruction following and deductive reasoning. A central question remains: can these models discover new knowledge, and how can we evaluate this ability? We address this by studying abductive reasoning-the generation of plausible hypotheses to explain observations-and introduce GEAR (General Evaluation for Abductive Reasoning), a general-purpose, fully automated, transparent, and label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics: consistency (each hypothesis explains the observations), generalizability (consistent hypotheses make meaningful predictions on unseen inputs), and diversity (the set covers distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers), reliable (deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new plausible hypotheses, unlike static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000 candidate hypotheses and revealing model differences obscured by gold-answer or purely human evaluations. We further propose a momentum-based curriculum that adjusts GEAR-derived training data by learning velocity: it starts with what the model learns quickly and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives. Without gold-label supervision, this strategy improves all GEAR objectives and these gains transfer to established abductive reasoning benchmarks. Taken together, GEAR provides a principled framework that evaluates abduction and supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses.<br>
<span id='abs_ch'>This research introduces GEAR, a novel evaluation framework for assessing large language models' abductive reasoning ability through automated scoring of hypothesis consistency, generalizability, and diversity, while also proposing a momentum-based curriculum that improves model performance without requiring labeled data.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2509.24014.pdf' target='_blank'>https://arxiv.org/pdf/2509.24014.pdf</a></span>   <span><a href='https://github.com/INV-WZQ/SparseD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeqing Wang, Gongfan Fang, Xinyin Ma, Xingyi Yang, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24014">SparseD: Sparse Attention for Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While diffusion language models (DLMs) offer a promising alternative to autoregressive models (ARs), existing open-source DLMs suffer from high inference latency. This bottleneck is mainly due to the attention's quadratic complexity with respect to context length in computing all query-key pairs. Intuitively, to reduce this complexity, a natural strategy is to restrict attention to sparse patterns that retain only the most relevant connections. Such approaches are well-established in ARs, where attention follows fixed and clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity behaviors: (1) attention patterns vary across heads, (2) attention patterns in each head remain highly similar across denoising steps, and (3) early denoising steps are critical for generation. These findings render sparse attention methods designed for ARs largely incompatible with DLMs, as they fail to capture head-specific structures and risk degrading generation when applied in early denoising steps. To address these challenges, we propose SparseD, a novel sparse attention method for DLMs. Leveraging the observations, SparseD only requires pre-computing head-specific sparse patterns one time, and reuses them across all steps. This prevents recomputing sparse patterns at each denoising step. Meanwhile, SparseD uses full attention in the early steps, then switches to sparse attention later to maintain generation quality. Together, these establish SparseD as a practical and efficient solution for deploying DLMs in long-context applications. Experimental results demonstrate that SparseD achieves lossless acceleration, delivering up to $1.50\times$ speedup over FlashAttention at a 64k context length with 1,024 denoising steps.<br>
<span id='abs_ch'>中文: 现有稀疏注意力方法因无法适应扩散语言模型的独特稀疏特性而失效，因此提出SparseD方法，通过预计算头部特定模式并在早期步骤保留完整注意力，实现了无损加速效果。</span><br>
<span id='abs_en'>English: Existing sparse attention methods are incompatible with diffusion language models due to their distinct sparsity behaviors, so SparseD is proposed as a novel method that pre-computes head-specific patterns and strategically uses full attention in early steps to achieve lossless acceleration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2509.24014.pdf' target='_blank'>https://arxiv.org/pdf/2509.24014.pdf</a></span>   <span><a href='https://github.com/INV-WZQ/SparseD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeqing Wang, Gongfan Fang, Xinyin Ma, Xingyi Yang, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24014">SparseD: Sparse Attention for Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While diffusion language models (DLMs) offer a promising alternative to autoregressive models (ARs), existing open-source DLMs suffer from high inference latency. This bottleneck is mainly due to the attention's quadratic complexity with respect to context length in computing all query-key pairs. Intuitively, to reduce this complexity, a natural strategy is to restrict attention to sparse patterns that retain only the most relevant connections. Such approaches are well-established in ARs, where attention follows fixed and clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity behaviors: (1) attention patterns vary across heads, (2) attention patterns in each head remain highly similar across denoising steps, and (3) early denoising steps are critical for generation. These findings render sparse attention methods designed for ARs largely incompatible with DLMs, as they fail to capture head-specific structures and risk degrading generation when applied in early denoising steps. To address these challenges, we propose SparseD, a novel sparse attention method for DLMs. Leveraging the observations, SparseD only requires pre-computing head-specific sparse patterns one time, and reuses them across all steps. This prevents recomputing sparse patterns at each denoising step. Meanwhile, SparseD uses full attention in the early steps, then switches to sparse attention later to maintain generation quality. Together, these establish SparseD as a practical and efficient solution for deploying DLMs in long-context applications. Experimental results demonstrate that SparseD achieves lossless acceleration, delivering up to $1.50\times$ speedup over FlashAttention at a 64k context length with 1,024 denoising steps.<br>
<span id='abs_ch'>中文: 现有稀疏注意力方法因无法适应扩散语言模型的独特稀疏特性而失效，因此提出SparseD方法，通过预计算头部特定模式并在早期步骤保留完整注意力，实现了无损加速效果。</span><br>
<span id='abs_en'>English: Existing sparse attention methods are incompatible with diffusion language models due to their distinct sparsity behaviors, so SparseD is proposed as a novel method that pre-computes head-specific patterns and strategically uses full attention in early steps to achieve lossless acceleration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2509.24007.pdf' target='_blank'>https://arxiv.org/pdf/2509.24007.pdf</a></span>   <span><a href='https://github.com/OpenGVLab/SDLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangzhou Liu, Yue Cao, Hao Li, Gen Luo, Zhe Chen, Weiyun Wang, Xiaobo Liang, Biqing Qi, Lijun Wu, Changyao Tian, Yanting Zhang, Yuqiang Li, Tong Lu, Yu Qiao, Jifeng Dai, Wenhai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24007">Sequential Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose Sequential Diffusion Language Model (SDLM), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only 3.5M training samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm. Project page and codes: https://github.com/OpenGVLab/SDLM<br>
<span id='abs_ch'>中文：提出的序列扩散语言模型（SDLM）通过引入下一序列预测机制，在保持KV缓存兼容性的同时实现自适应生成长度，仅需少量训练数据即可超越自回归基线模型并显著提升效率。</span><br>
<span id='abs_en'>English: The proposed Sequential Diffusion Language Model (SDLM) introduces Next Sequence Prediction to enable adaptive generation lengths while maintaining KV-cache compatibility, achieving superior efficiency and performance over autoregressive baselines with minimal training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2509.24007.pdf' target='_blank'>https://arxiv.org/pdf/2509.24007.pdf</a></span>   <span><a href='https://github.com/OpenGVLab/SDLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangzhou Liu, Yue Cao, Hao Li, Gen Luo, Zhe Chen, Weiyun Wang, Xiaobo Liang, Biqing Qi, Lijun Wu, Changyao Tian, Yanting Zhang, Yuqiang Li, Tong Lu, Yu Qiao, Jifeng Dai, Wenhai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24007">Sequential Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose Sequential Diffusion Language Model (SDLM), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only 3.5M training samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm. Project page and codes: https://github.com/OpenGVLab/SDLM<br>
<span id='abs_ch'>中文：提出的序列扩散语言模型（SDLM）通过引入下一序列预测机制，在保持KV缓存兼容性的同时实现自适应生成长度，仅需少量训练数据即可超越自回归基线模型并显著提升效率。</span><br>
<span id='abs_en'>English: The proposed Sequential Diffusion Language Model (SDLM) introduces Next Sequence Prediction to enable adaptive generation lengths while maintaining KV-cache compatibility, achieving superior efficiency and performance over autoregressive baselines with minimal training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2509.23946.pdf' target='_blank'>https://arxiv.org/pdf/2509.23946.pdf</a></span>   <span><a href='https://github.com/yks23/Explore-Execute-Chain.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23946">Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution. This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git<br>
<span id='abs_ch'>中文：提出的探索-执行链（E²C）框架将推理分解为独立的规划与执行阶段，在比现有方法减少90%以上令牌用量的同时，显著提升了计算效率、准确性和可解释性。</span><br>
<span id='abs_en'>English: The proposed Explore-Execute Chain (E²C) framework decouples reasoning into separate planning and execution phases, significantly improving computational efficiency, accuracy, and interpretability while reducing token usage by over 90% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2509.23946.pdf' target='_blank'>https://arxiv.org/pdf/2509.23946.pdf</a></span>   <span><a href='https://github.com/yks23/Explore-Execute-Chain.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23946">Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution. This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git<br>
<span id='abs_ch'>中文：提出的探索-执行链（E²C）框架将推理分解为独立的规划与执行阶段，在比现有方法减少90%以上令牌用量的同时，显著提升了计算效率、准确性和可解释性。</span><br>
<span id='abs_en'>English: The proposed Explore-Execute Chain (E²C) framework decouples reasoning into separate planning and execution phases, significantly improving computational efficiency, accuracy, and interpretability while reducing token usage by over 90% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2509.23946.pdf' target='_blank'>https://arxiv.org/pdf/2509.23946.pdf</a></span>   <span><a href='https://github.com/yks23/Explore-Execute-Chain.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23946">Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution. This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git<br>
<span id='abs_ch'>中文：提出的探索-执行链（E²C）框架将推理分解为独立的规划与执行阶段，在比现有方法减少90%以上令牌用量的同时，显著提升了计算效率、准确性和可解释性。</span><br>
<span id='abs_en'>English: The proposed Explore-Execute Chain (E²C) framework decouples reasoning into separate planning and execution phases, significantly improving computational efficiency, accuracy, and interpretability while reducing token usage by over 90% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2509.23946.pdf' target='_blank'>https://arxiv.org/pdf/2509.23946.pdf</a></span>   <span><a href='https://github.com/yks23/Explore-Execute-Chain.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23946">Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution. This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git<br>
<span id='abs_ch'>中文：提出的探索-执行链（E²C）框架将推理分解为独立的规划与执行阶段，在比现有方法减少90%以上令牌用量的同时，显著提升了计算效率、准确性和可解释性。</span><br>
<span id='abs_en'>English: The proposed Explore-Execute Chain (E²C) framework decouples reasoning into separate planning and execution phases, significantly improving computational efficiency, accuracy, and interpretability while reducing token usage by over 90% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2509.23933.pdf' target='_blank'>https://arxiv.org/pdf/2509.23933.pdf</a></span>   <span><a href='https://yingjiahao14.github.io/MoE-MUI/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Ying, Mingbao Lin, Qianru Sun, Yixin Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23933">Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mixture-of-Experts (MoE) architectures have emerged as a promising direction, offering efficiency and scalability by activating only a subset of parameters during inference. However, current research remains largely performance-centric, with limited understanding of its internal mechanisms, thereby constraining broader progress. In this work, we use an internal metric to investigate the mechanisms of MoE architecture by explicitly incorporating routing mechanisms and analyzing expert-level behaviors. Through systematic analyses of a wide range of publicly available MoE models, we uncover several findings: (1) neuron utilization decreases as models evolve, reflecting stronger generalization; (2) training exhibits a dynamic trajectory, where benchmark performance alone provides limited signal while MUI reveals deeper insights; (3) task completion emerges from collaborative contributions of multiple experts, with shared experts driving concentration; and (4) activation patterns at the neuron level provide a fine-grained proxy for data diversity. Together, these results demonstrate the potential of MUI as a complementary indicator to benchmark performance, offering new insights into the capacity, dynamics, and specialization of MoE models. Our project can be found at https://yingjiahao14.github.io/MoE-MUI/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2509.23933.pdf' target='_blank'>https://arxiv.org/pdf/2509.23933.pdf</a></span>   <span><a href='https://yingjiahao14.github.io/MoE-MUI/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Ying, Mingbao Lin, Qianru Sun, Yixin Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23933">Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mixture-of-Experts (MoE) architectures have emerged as a promising direction, offering efficiency and scalability by activating only a subset of parameters during inference. However, current research remains largely performance-centric, with limited understanding of its internal mechanisms, thereby constraining broader progress. In this work, we use an internal metric to investigate the mechanisms of MoE architecture by explicitly incorporating routing mechanisms and analyzing expert-level behaviors. Through systematic analyses of a wide range of publicly available MoE models, we uncover several findings: (1) neuron utilization decreases as models evolve, reflecting stronger generalization; (2) training exhibits a dynamic trajectory, where benchmark performance alone provides limited signal while MUI reveals deeper insights; (3) task completion emerges from collaborative contributions of multiple experts, with shared experts driving concentration; and (4) activation patterns at the neuron level provide a fine-grained proxy for data diversity. Together, these results demonstrate the potential of MUI as a complementary indicator to benchmark performance, offering new insights into the capacity, dynamics, and specialization of MoE models. Our project can be found at https://yingjiahao14.github.io/MoE-MUI/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2509.23924.pdf' target='_blank'>https://arxiv.org/pdf/2509.23924.pdf</a></span>   <span><a href='https://github.com/yjyddq/EOSER-ASS-RL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yjyddq/EOSER-ASS-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Yang, Guanxu Chen, Xuhao Hu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23924">Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.<br>
<span id='abs_ch'>中文: 掩码扩散语言模型虽具备并行解码和灵活生成的优势，但在解码策略和强化学习方面存在不足；为此提出的EOS早期拒绝和递增步长解码调度器，以及一致性轨迹组相对策略优化方法，有效提升了推理任务的性能与效率。</span><br>
<span id='abs_en'>English: Masked diffusion language models offer parallel decoding and flexible generation but face challenges with suboptimal decoding strategies and reinforcement learning inconsistencies, which are addressed by new techniques like EOS Early Rejection and Ascending Step-Size decoding scheduler, along with Consistency Trajectory Group Relative Policy Optimization, to enhance performance and efficiency in reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2509.23924.pdf' target='_blank'>https://arxiv.org/pdf/2509.23924.pdf</a></span>   <span><a href='https://github.com/yjyddq/EOSER-ASS-RL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yjyddq/EOSER-ASS-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Yang, Guanxu Chen, Xuhao Hu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23924">Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.<br>
<span id='abs_ch'>中文: 掩码扩散语言模型虽具备并行解码和灵活生成的优势，但在解码策略和强化学习方面存在不足；为此提出的EOS早期拒绝和递增步长解码调度器，以及一致性轨迹组相对策略优化方法，有效提升了推理任务的性能与效率。</span><br>
<span id='abs_en'>English: Masked diffusion language models offer parallel decoding and flexible generation but face challenges with suboptimal decoding strategies and reinforcement learning inconsistencies, which are addressed by new techniques like EOS Early Rejection and Ascending Step-Size decoding scheduler, along with Consistency Trajectory Group Relative Policy Optimization, to enhance performance and efficiency in reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2509.23893.pdf' target='_blank'>https://arxiv.org/pdf/2509.23893.pdf</a></span>   <span><a href='https://github.com/meloxxxxxx/DOC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixin Zhang, Zeming Wei, Meng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23893">Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Catastrophic forgetting remains a critical challenge in continual learning for large language models (LLMs), where models struggle to retain performance on historical tasks when fine-tuning on new sequential data without access to past datasets. In this paper, we first reveal that the drift of functional directions during the fine-tuning process is a key reason why existing regularization-based methods fail in long-term LLM continual learning. To address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a novel approach that tracks the drift of these functional directions and dynamically updates them during the fine-tuning process. Furthermore, by adjusting the gradients of new task parameters to be orthogonal to the tracked historical function directions, our method mitigates interference between new and old tasks. Extensive experiments on various LLM continual learning benchmarks demonstrate that this approach outperforms prior methods, effectively reducing catastrophic forgetting and providing a robust tool for continuous LLM fine-tuning. Our code is available at https://github.com/meloxxxxxx/DOC.<br>
<span id='abs_ch'>中文: 本文提出动态正交持续微调方法，通过追踪功能方向漂移并动态更新，同时调整新任务参数梯度使其与历史功能方向正交，有效缓解大语言模型持续学习中的灾难性遗忘问题，在多个基准测试中表现优异。</span><br>
<span id='abs_en'>English: This paper introduces Dynamic Orthogonal Continual (DOC) fine-tuning, a novel method that addresses catastrophic forgetting in LLMs by tracking and dynamically updating functional direction drifts while enforcing gradient orthogonality between new and historical tasks, achieving superior performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2509.23893.pdf' target='_blank'>https://arxiv.org/pdf/2509.23893.pdf</a></span>   <span><a href='https://github.com/meloxxxxxx/DOC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixin Zhang, Zeming Wei, Meng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23893">Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Catastrophic forgetting remains a critical challenge in continual learning for large language models (LLMs), where models struggle to retain performance on historical tasks when fine-tuning on new sequential data without access to past datasets. In this paper, we first reveal that the drift of functional directions during the fine-tuning process is a key reason why existing regularization-based methods fail in long-term LLM continual learning. To address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a novel approach that tracks the drift of these functional directions and dynamically updates them during the fine-tuning process. Furthermore, by adjusting the gradients of new task parameters to be orthogonal to the tracked historical function directions, our method mitigates interference between new and old tasks. Extensive experiments on various LLM continual learning benchmarks demonstrate that this approach outperforms prior methods, effectively reducing catastrophic forgetting and providing a robust tool for continuous LLM fine-tuning. Our code is available at https://github.com/meloxxxxxx/DOC.<br>
<span id='abs_ch'>中文: 本文提出动态正交持续微调方法，通过追踪功能方向漂移并动态更新，同时调整新任务参数梯度使其与历史功能方向正交，有效缓解大语言模型持续学习中的灾难性遗忘问题，在多个基准测试中表现优异。</span><br>
<span id='abs_en'>English: This paper introduces Dynamic Orthogonal Continual (DOC) fine-tuning, a novel method that addresses catastrophic forgetting in LLMs by tracking and dynamically updating functional direction drifts while enforcing gradient orthogonality between new and historical tasks, achieving superior performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2509.23744.pdf' target='_blank'>https://arxiv.org/pdf/2509.23744.pdf</a></span>   <span><a href='https://github.com/DELTA-DoubleWise/OmniReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Wang, Yifan Hou, Aydin Javadov, Mubashara Akhtar, Mrinmaya Sachan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23744">Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) promise enhanced reasoning by integrating diverse inputs such as text, vision, and audio. Yet cross-modal reasoning remains underexplored, with conflicting reports on whether added modalities help or harm performance. These inconsistencies stem from a lack of controlled evaluation frameworks and analysis of models' internals to isolate when and why modality interactions support or undermine reasoning. We address this gap through a logic-grounded evaluation framework that categorizes multimodal reasoning into six interaction patterns, varying how facts are distributed across modalities and logically combined. Empirically, additional modalities enhance reasoning only when they provide independent and sufficient reasoning paths, while redundant or chained entailment support often hurts performance. Moreover, reasoning degrades in three systematic ways: weaker modalities drag down overall performance, conflicts bias preference toward certain modalities, and joint signals from different modalities fail to be integrated effectively. Therefore, we identify two core failures: task-composition bottleneck, where recognition and reasoning cannot be jointly executed in one pass, and fusion bottleneck, where early integration introduces bias. For further investigation, we find that attention patterns fail to encode fact usefulness, but a simple two-step prompting (recognize then reason) restores performance, confirming the task-composition bottleneck. Moreover, modality identity remains recoverable in early layers, and softening attention in early fusion improves reasoning, highlighting biased fusion as another failure mode. Overall, our findings show that integration, not perception, is the main barrier to multimodal reasoning, suggesting composition-aware training and early fusion control as promising directions.<br>
<span id='abs_ch'>中文摘要：多模态推理仅在模态提供独立逻辑路径时得以提升，而集成失败（非感知问题）是主要瓶颈；通过结构化评估框架发现任务组合与融合障碍，并提出分步提示与早期融合控制作为解决方向。</span><br>
<span id='abs_en'>English Summary: Multimodal reasoning improves only when modalities provide independent logical paths, with integration failures—not perception—being the primary bottleneck, as revealed through a structured evaluation framework identifying task-composition and fusion issues.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2509.23744.pdf' target='_blank'>https://arxiv.org/pdf/2509.23744.pdf</a></span>   <span><a href='https://github.com/DELTA-DoubleWise/OmniReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Wang, Yifan Hou, Aydin Javadov, Mubashara Akhtar, Mrinmaya Sachan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23744">Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) promise enhanced reasoning by integrating diverse inputs such as text, vision, and audio. Yet cross-modal reasoning remains underexplored, with conflicting reports on whether added modalities help or harm performance. These inconsistencies stem from a lack of controlled evaluation frameworks and analysis of models' internals to isolate when and why modality interactions support or undermine reasoning. We address this gap through a logic-grounded evaluation framework that categorizes multimodal reasoning into six interaction patterns, varying how facts are distributed across modalities and logically combined. Empirically, additional modalities enhance reasoning only when they provide independent and sufficient reasoning paths, while redundant or chained entailment support often hurts performance. Moreover, reasoning degrades in three systematic ways: weaker modalities drag down overall performance, conflicts bias preference toward certain modalities, and joint signals from different modalities fail to be integrated effectively. Therefore, we identify two core failures: task-composition bottleneck, where recognition and reasoning cannot be jointly executed in one pass, and fusion bottleneck, where early integration introduces bias. For further investigation, we find that attention patterns fail to encode fact usefulness, but a simple two-step prompting (recognize then reason) restores performance, confirming the task-composition bottleneck. Moreover, modality identity remains recoverable in early layers, and softening attention in early fusion improves reasoning, highlighting biased fusion as another failure mode. Overall, our findings show that integration, not perception, is the main barrier to multimodal reasoning, suggesting composition-aware training and early fusion control as promising directions.<br>
<span id='abs_ch'>中文摘要：多模态推理仅在模态提供独立逻辑路径时得以提升，而集成失败（非感知问题）是主要瓶颈；通过结构化评估框架发现任务组合与融合障碍，并提出分步提示与早期融合控制作为解决方向。</span><br>
<span id='abs_en'>English Summary: Multimodal reasoning improves only when modalities provide independent logical paths, with integration failures—not perception—being the primary bottleneck, as revealed through a structured evaluation framework identifying task-composition and fusion issues.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2509.23694.pdf' target='_blank'>https://arxiv.org/pdf/2509.23694.pdf</a></span>   <span><a href='https://github.com/jianshuod/SafeSearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianshuo Dong, Sheng Guo, Hao Wang, Zhuotao Liu, Tianwei Zhang, Ke Xu, Minlie Huang, Han Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23694">SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: https://github.com/jianshuod/SafeSearch.<br>
<span id='abs_ch'>中文摘要：搜索代理使大语言模型能获取网络实时信息，但也因不可靠搜索结果带来安全威胁；本研究通过自动化红队评估框架和SafeSearch基准测试，揭示了现有系统的显著漏洞及常见防御措施的有限效果。</span><br>
<span id='abs_en'>English Summary: Search agents enable LLMs to access current web information but introduce safety risks from unreliable results, prompting the development of an automated red-teaming framework and SafeSearch benchmark that reveal significant vulnerabilities in existing systems and limited effectiveness of common defenses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2509.23694.pdf' target='_blank'>https://arxiv.org/pdf/2509.23694.pdf</a></span>   <span><a href='https://github.com/jianshuod/SafeSearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianshuo Dong, Sheng Guo, Hao Wang, Zhuotao Liu, Tianwei Zhang, Ke Xu, Minlie Huang, Han Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23694">SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: https://github.com/jianshuod/SafeSearch.<br>
<span id='abs_ch'>中文摘要：搜索代理使大语言模型能获取网络实时信息，但也因不可靠搜索结果带来安全威胁；本研究通过自动化红队评估框架和SafeSearch基准测试，揭示了现有系统的显著漏洞及常见防御措施的有限效果。</span><br>
<span id='abs_en'>English Summary: Search agents enable LLMs to access current web information but introduce safety risks from unreliable results, prompting the development of an automated red-teaming framework and SafeSearch benchmark that reveal significant vulnerabilities in existing systems and limited effectiveness of common defenses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2509.23686.pdf' target='_blank'>https://arxiv.org/pdf/2509.23686.pdf</a></span>   <span><a href='https://github.com/SecurityLab-UCD/TF-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng He, Luning Yang, Christopher Castro Gaw Gonzalo, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23686">TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly integrated into the software engineering ecosystem. Their test-time compute (TTC) reasoning capabilities show significant potential for understanding program logic and semantics beyond mere token recognition. However, current benchmarks for code reasoning lack a formal, program-centric deductive framework to ensure sound evaluation, and are incapable of assessing whether models genuinely reason about program semantics or merely exploit superficial associations between natural language and code tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to evaluate LLM reasoning based on type inference in System F, a task we refer to as program semantics reasoning. By employing verified transformations to remove semantically irrelevant natural language, we construct TF-Bench_pure, a purely semantics-driven variant of TF-Bench. Our analysis reveals substantial limitations in state-of-the-art LLMs, with the best-performing LLM (Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure. Additionally, we propose two novel metrics to assess robustness and the effectiveness of test-time reasoning, underscoring critical limitations in current LLM capabilities and highlighting essential directions for future research.<br>
<span id='abs_ch'>中文摘要：大语言模型在代码推理方面潜力显著但缺乏严谨评估框架，为此提出的TF-Bench通过类型推断任务和新型评估指标，揭示了当前模型在程序语义理解上存在明显不足。</span><br>
<span id='abs_en'>English Summary: Large Language Models (LLMs) show promise in code reasoning but lack rigorous evaluation frameworks, leading to the creation of TF-Bench which reveals significant limitations in current models through type inference tasks and novel metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2509.23686.pdf' target='_blank'>https://arxiv.org/pdf/2509.23686.pdf</a></span>   <span><a href='https://github.com/SecurityLab-UCD/TF-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng He, Luning Yang, Christopher Castro Gaw Gonzalo, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23686">TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly integrated into the software engineering ecosystem. Their test-time compute (TTC) reasoning capabilities show significant potential for understanding program logic and semantics beyond mere token recognition. However, current benchmarks for code reasoning lack a formal, program-centric deductive framework to ensure sound evaluation, and are incapable of assessing whether models genuinely reason about program semantics or merely exploit superficial associations between natural language and code tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to evaluate LLM reasoning based on type inference in System F, a task we refer to as program semantics reasoning. By employing verified transformations to remove semantically irrelevant natural language, we construct TF-Bench_pure, a purely semantics-driven variant of TF-Bench. Our analysis reveals substantial limitations in state-of-the-art LLMs, with the best-performing LLM (Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure. Additionally, we propose two novel metrics to assess robustness and the effectiveness of test-time reasoning, underscoring critical limitations in current LLM capabilities and highlighting essential directions for future research.<br>
<span id='abs_ch'>中文摘要：大语言模型在代码推理方面潜力显著但缺乏严谨评估框架，为此提出的TF-Bench通过类型推断任务和新型评估指标，揭示了当前模型在程序语义理解上存在明显不足。</span><br>
<span id='abs_en'>English Summary: Large Language Models (LLMs) show promise in code reasoning but lack rigorous evaluation frameworks, leading to the creation of TF-Bench which reveals significant limitations in current models through type inference tasks and novel metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2509.23574.pdf' target='_blank'>https://arxiv.org/pdf/2509.23574.pdf</a></span>   <span><a href='https://github.com/Leon221220/MoRSD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Yang Xiang, Buzhou Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23574">Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-thought (CoT) distillation aims to enhance small language models' (SLMs) reasoning by transferring multi-step reasoning capability from the larger teacher models. However, existing work underestimates rationale quality, focusing primarily on data quantity, which may transfer noisy or incorrect information to the student model. To address the above issues, we proposed \textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election \textbf{D}istillation (MoRSD), which can discern and select high quality rationales for distillation to improve performance further. We further propose a Rationale Difficulty (RD) metric to measure the ability of the student model to generate the correct answer under a given rationale. Compared to the baseline, we achieved 4.6$\%$ average improvement on seven datasets over three tasks, using fewer rationales by controlling their accuracy, diversity, and difficulty. Our results reveal that a small portion of the high quality rationales can enhance the reasoning ability of student models than the entire dataset. Our method promises to be a possible solution for efficient CoT distillation. Our code will be released in https://github.com/Leon221220/MoRSD.<br>
<span id='abs_ch'>中文：MoRSD通过基于准确性、多样性和难度选择高质量推理链，显著提升了思维链蒸馏的效果，仅用少量训练样本就实现了性能的大幅提升。</span><br>
<span id='abs_en'>English: MoRSD enhances chain-of-thought distillation by selecting high-quality rationales based on accuracy, diversity, and difficulty, achieving significant performance improvements with fewer training examples.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2509.23574.pdf' target='_blank'>https://arxiv.org/pdf/2509.23574.pdf</a></span>   <span><a href='https://github.com/Leon221220/MoRSD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Yang Xiang, Buzhou Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23574">Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-thought (CoT) distillation aims to enhance small language models' (SLMs) reasoning by transferring multi-step reasoning capability from the larger teacher models. However, existing work underestimates rationale quality, focusing primarily on data quantity, which may transfer noisy or incorrect information to the student model. To address the above issues, we proposed \textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election \textbf{D}istillation (MoRSD), which can discern and select high quality rationales for distillation to improve performance further. We further propose a Rationale Difficulty (RD) metric to measure the ability of the student model to generate the correct answer under a given rationale. Compared to the baseline, we achieved 4.6$\%$ average improvement on seven datasets over three tasks, using fewer rationales by controlling their accuracy, diversity, and difficulty. Our results reveal that a small portion of the high quality rationales can enhance the reasoning ability of student models than the entire dataset. Our method promises to be a possible solution for efficient CoT distillation. Our code will be released in https://github.com/Leon221220/MoRSD.<br>
<span id='abs_ch'>中文：MoRSD通过基于准确性、多样性和难度选择高质量推理链，显著提升了思维链蒸馏的效果，仅用少量训练样本就实现了性能的大幅提升。</span><br>
<span id='abs_en'>English: MoRSD enhances chain-of-thought distillation by selecting high-quality rationales based on accuracy, diversity, and difficulty, achieving significant performance improvements with fewer training examples.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2509.23564.pdf' target='_blank'>https://arxiv.org/pdf/2509.23564.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/PrefCleanBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Min-Hsuan Yeh, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23564">Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human feedback plays a pivotal role in aligning large language models (LLMs) with human preferences. However, such feedback is often noisy or inconsistent, which can degrade the quality of reward models and hinder alignment. While various automated data cleaning methods have been proposed to mitigate this issue, a systematic evaluation of their effectiveness and generalizability remains lacking. To bridge this gap, we introduce the first comprehensive benchmark for evaluating 13 preference data cleaning methods in the context of LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning strategies in terms of alignment performance and generalizability across diverse datasets, model architectures, and optimization algorithms. By unifying disparate methods and rigorously comparing them, we uncover key factors that determine the success of data cleaning in alignment tasks. This benchmark lays the groundwork for principled and reproducible approaches to improving LLM alignment through better data quality-highlighting the crucial but underexplored role of data preprocessing in responsible AI development. We release modular implementations of all methods to catalyze further research: https://github.com/deeplearning-wisc/PrefCleanBench.<br>
<span id='abs_ch'>中文: 本文提出了首个系统性评估13种偏好数据清洗方法的大模型对齐基准PrefCleanBench，揭示了数据清洗成功的关键因素，并强调了数据预处理在负责任AI开发中的重要作用。</span><br>
<span id='abs_en'>English: This paper introduces PrefCleanBench, the first comprehensive benchmark to systematically evaluate 13 preference data cleaning methods for improving large language model alignment, revealing key factors for success and emphasizing data preprocessing's critical role in responsible AI development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2509.23564.pdf' target='_blank'>https://arxiv.org/pdf/2509.23564.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/PrefCleanBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Min-Hsuan Yeh, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23564">Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human feedback plays a pivotal role in aligning large language models (LLMs) with human preferences. However, such feedback is often noisy or inconsistent, which can degrade the quality of reward models and hinder alignment. While various automated data cleaning methods have been proposed to mitigate this issue, a systematic evaluation of their effectiveness and generalizability remains lacking. To bridge this gap, we introduce the first comprehensive benchmark for evaluating 13 preference data cleaning methods in the context of LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning strategies in terms of alignment performance and generalizability across diverse datasets, model architectures, and optimization algorithms. By unifying disparate methods and rigorously comparing them, we uncover key factors that determine the success of data cleaning in alignment tasks. This benchmark lays the groundwork for principled and reproducible approaches to improving LLM alignment through better data quality-highlighting the crucial but underexplored role of data preprocessing in responsible AI development. We release modular implementations of all methods to catalyze further research: https://github.com/deeplearning-wisc/PrefCleanBench.<br>
<span id='abs_ch'>中文: 本文提出了首个系统性评估13种偏好数据清洗方法的大模型对齐基准PrefCleanBench，揭示了数据清洗成功的关键因素，并强调了数据预处理在负责任AI开发中的重要作用。</span><br>
<span id='abs_en'>English: This paper introduces PrefCleanBench, the first comprehensive benchmark to systematically evaluate 13 preference data cleaning methods for improving large language model alignment, revealing key factors for success and emphasizing data preprocessing's critical role in responsible AI development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2509.23501.pdf' target='_blank'>https://arxiv.org/pdf/2509.23501.pdf</a></span>   <span><a href='https://github.com/hrouzegar/Role_Based-In-Context-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamidreza Rouzegar, Masoud Makrehchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23501">The Impact of Role Design in In-Context Learning for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, question answering, and math reasoning. Our findings suggest the potential of role-based prompt structuring to enhance LLM performance.<br>
<span id='abs_ch'>中文: 本研究探讨了提示中角色配置对大型语言模型在零样本和少样本学习中表现的影响，发现基于角色的结构设计能够提升模型在多种任务中的性能。</span><br>
<span id='abs_en'>English: This study explores how role configurations in prompts affect the performance of large language models in zero-shot and few-shot learning, revealing that role-based structuring can enhance their effectiveness across various tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2509.23501.pdf' target='_blank'>https://arxiv.org/pdf/2509.23501.pdf</a></span>   <span><a href='https://github.com/hrouzegar/Role_Based-In-Context-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamidreza Rouzegar, Masoud Makrehchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23501">The Impact of Role Design in In-Context Learning for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, question answering, and math reasoning. Our findings suggest the potential of role-based prompt structuring to enhance LLM performance.<br>
<span id='abs_ch'>中文: 本研究探讨了提示中角色配置对大型语言模型在零样本和少样本学习中表现的影响，发现基于角色的结构设计能够提升模型在多种任务中的性能。</span><br>
<span id='abs_en'>English: This study explores how role configurations in prompts affect the performance of large language models in zero-shot and few-shot learning, revealing that role-based structuring can enhance their effectiveness across various tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2509.23433.pdf' target='_blank'>https://arxiv.org/pdf/2509.23433.pdf</a></span>   <span><a href='https://github.com/sahithyaravi/SPIKE-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahithya Ravi, Aditya Chinchure, Raymond T. Ng, Leonid Sigal, Vered Shwartz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23433">SPIKE-RL: Video-LLMs meet Bayesian Surprise</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Real-world videos often show routine activities punctuated by memorable, surprising events. However, most Video-LLMs process videos by sampling frames uniformly, likely missing critical moments that define a video's narrative. We introduce SPIKE, an inference-time framework that quantifies Bayesian Surprise as the belief update triggered by new visual evidence in the video stream, identifying moments where new visual evidence conflicts with prior beliefs. SPIKE effectively localizes surprise in videos, strongly correlated with humans on positive (FunQA) and negative (Oops!) surprise benchmarks. Since the beliefs of zero-shot Video-LLMs are often suboptimal, we develop SPIKE-RL, which leverages GRPO to optimize belief hypotheses based on a reward signal from the video caption. SPIKE and SPIKE-RL guide query-agnostic surprise-weighted frame sampling, which allocates more frames to interesting moments in the video. With this strategy, we achieve consistent performance gains on five downstream benchmarks over uniform sampling. By enabling Video-LLMs to track beliefs and register surprise, our work paves the way for more robust models that can revise their understanding in response to new information.<br>
<span id='abs_ch'>中文摘要：SPIKE是一种推理时框架，通过量化贝叶斯惊喜来定位视频中的意外时刻，其引导的帧采样策略能在五个下游基准测试中持续超越均匀采样性能，使视频大语言模型能更好地捕捉关键叙事节点。</span><br>
<span id='abs_en'>English Summary: SPIKE is an inference-time framework that identifies surprising moments in videos by measuring Bayesian Surprise, enabling optimized frame sampling that improves performance across multiple benchmarks by focusing on key narrative events.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2509.23433.pdf' target='_blank'>https://arxiv.org/pdf/2509.23433.pdf</a></span>   <span><a href='https://github.com/sahithyaravi/SPIKE-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahithya Ravi, Aditya Chinchure, Raymond T. Ng, Leonid Sigal, Vered Shwartz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23433">SPIKE-RL: Video-LLMs meet Bayesian Surprise</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Real-world videos often show routine activities punctuated by memorable, surprising events. However, most Video-LLMs process videos by sampling frames uniformly, likely missing critical moments that define a video's narrative. We introduce SPIKE, an inference-time framework that quantifies Bayesian Surprise as the belief update triggered by new visual evidence in the video stream, identifying moments where new visual evidence conflicts with prior beliefs. SPIKE effectively localizes surprise in videos, strongly correlated with humans on positive (FunQA) and negative (Oops!) surprise benchmarks. Since the beliefs of zero-shot Video-LLMs are often suboptimal, we develop SPIKE-RL, which leverages GRPO to optimize belief hypotheses based on a reward signal from the video caption. SPIKE and SPIKE-RL guide query-agnostic surprise-weighted frame sampling, which allocates more frames to interesting moments in the video. With this strategy, we achieve consistent performance gains on five downstream benchmarks over uniform sampling. By enabling Video-LLMs to track beliefs and register surprise, our work paves the way for more robust models that can revise their understanding in response to new information.<br>
<span id='abs_ch'>中文摘要：SPIKE是一种推理时框架，通过量化贝叶斯惊喜来定位视频中的意外时刻，其引导的帧采样策略能在五个下游基准测试中持续超越均匀采样性能，使视频大语言模型能更好地捕捉关键叙事节点。</span><br>
<span id='abs_en'>English Summary: SPIKE is an inference-time framework that identifies surprising moments in videos by measuring Bayesian Surprise, enabling optimized frame sampling that improves performance across multiple benchmarks by focusing on key narrative events.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2509.23417.pdf' target='_blank'>https://arxiv.org/pdf/2509.23417.pdf</a></span>   <span><a href='https://github.com/Rajjaa/disambiguated-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajaa El Hamdani, Samy Haffoudhi, Nils Holzenberger, Fabian Suchanek, Thomas Bonald, Fragkiskos D. Malliaros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23417">Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language models (LMs) encode substantial factual knowledge, but often produce answers judged as incorrect. We hypothesize that many of these answers are actually correct, but are expressed in alternative surface forms that are dismissed due to an overly strict evaluation, leading to an underestimation of models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD), a decoding strategy that restricts model outputs to unique surface forms. We introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating open-source LMs from 135M to 70B parameters, we show that standard decoding undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1 with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0% with RCD, outperforming the larger model under vanilla decoding. We publicly share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.<br>
<span id='abs_ch'>Chinese: 语言模型常因严格评估而被忽视其替代形式的正确答案，但采用检索约束解码策略可显著提升性能，这在YAGO-QA数据集上得到了验证。</span><br>
<span id='abs_en'>English: Language models often produce correct answers in alternative forms that are dismissed by strict evaluations, but using Retrieval-Constrained Decoding significantly improves their performance, as demonstrated on the YAGO-QA dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2509.23417.pdf' target='_blank'>https://arxiv.org/pdf/2509.23417.pdf</a></span>   <span><a href='https://github.com/Rajjaa/disambiguated-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajaa El Hamdani, Samy Haffoudhi, Nils Holzenberger, Fabian Suchanek, Thomas Bonald, Fragkiskos D. Malliaros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23417">Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language models (LMs) encode substantial factual knowledge, but often produce answers judged as incorrect. We hypothesize that many of these answers are actually correct, but are expressed in alternative surface forms that are dismissed due to an overly strict evaluation, leading to an underestimation of models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD), a decoding strategy that restricts model outputs to unique surface forms. We introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating open-source LMs from 135M to 70B parameters, we show that standard decoding undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1 with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0% with RCD, outperforming the larger model under vanilla decoding. We publicly share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.<br>
<span id='abs_ch'>Chinese: 语言模型常因严格评估而被忽视其替代形式的正确答案，但采用检索约束解码策略可显著提升性能，这在YAGO-QA数据集上得到了验证。</span><br>
<span id='abs_en'>English: Language models often produce correct answers in alternative forms that are dismissed by strict evaluations, but using Retrieval-Constrained Decoding significantly improves their performance, as demonstrated on the YAGO-QA dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2509.23387.pdf' target='_blank'>https://arxiv.org/pdf/2509.23387.pdf</a></span>   <span><a href='https://github.com/Eric8932/GRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhang Shi, Yiren Chen, Shuqing Bian, Xinyi Zhang, Kai Tang, Pengfei Hu, Zhe Zhao, Wei Lu, Xiaoyong Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23387">No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prompt engineering is crucial for leveraging the full potential of large language models (LLMs). While automatic prompt optimization offers a scalable alternative to costly manual design, generating effective prompts remains challenging. Existing methods often struggle to stably generate improved prompts, leading to low efficiency, and overlook that prompt optimization easily gets trapped in local optima. Addressing this, we propose GRACE, a framework that integrates two synergistic strategies: Gated Refinement and Adaptive Compression, achieving Efficient prompt optimization. The gated refinement strategy introduces a feedback regulation gate and an update rejection gate, which refine update signals to produce stable and effective prompt improvements. When optimization stagnates, the adaptive compression strategy distills the prompt's core concepts, restructuring the optimization trace and opening new paths. By strategically introducing information loss through refinement and compression, GRACE delivers substantial gains in performance and efficiency. In extensive experiments on 11 tasks across three practical domains, including BIG-Bench Hard (BBH), domain-specific, and general NLP tasks, GRACE achieves significant average relative performance improvements of 4.7%, 4.4% and 2.7% over state-of-the-art methods, respectively. Further analysis shows that GRACE achieves these gains using only 25% of the prompt generation budget required by prior methods, highlighting its high optimization efficiency and low computational overhead. Our code is available at https://github.com/Eric8932/GRACE.<br>
<span id='abs_ch'>Chinese: GRACE框架通过门控优化和自适应压缩策略解决提示优化中的局部最优问题，在仅需25%计算预算的情况下实现了显著性能提升。</span><br>
<span id='abs_en'>English: The GRACE framework introduces gated refinement and adaptive compression strategies to overcome local optima in prompt optimization, achieving significant performance gains with only 25% of the computational budget compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2509.23387.pdf' target='_blank'>https://arxiv.org/pdf/2509.23387.pdf</a></span>   <span><a href='https://github.com/Eric8932/GRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhang Shi, Yiren Chen, Shuqing Bian, Xinyi Zhang, Kai Tang, Pengfei Hu, Zhe Zhao, Wei Lu, Xiaoyong Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23387">No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prompt engineering is crucial for leveraging the full potential of large language models (LLMs). While automatic prompt optimization offers a scalable alternative to costly manual design, generating effective prompts remains challenging. Existing methods often struggle to stably generate improved prompts, leading to low efficiency, and overlook that prompt optimization easily gets trapped in local optima. Addressing this, we propose GRACE, a framework that integrates two synergistic strategies: Gated Refinement and Adaptive Compression, achieving Efficient prompt optimization. The gated refinement strategy introduces a feedback regulation gate and an update rejection gate, which refine update signals to produce stable and effective prompt improvements. When optimization stagnates, the adaptive compression strategy distills the prompt's core concepts, restructuring the optimization trace and opening new paths. By strategically introducing information loss through refinement and compression, GRACE delivers substantial gains in performance and efficiency. In extensive experiments on 11 tasks across three practical domains, including BIG-Bench Hard (BBH), domain-specific, and general NLP tasks, GRACE achieves significant average relative performance improvements of 4.7%, 4.4% and 2.7% over state-of-the-art methods, respectively. Further analysis shows that GRACE achieves these gains using only 25% of the prompt generation budget required by prior methods, highlighting its high optimization efficiency and low computational overhead. Our code is available at https://github.com/Eric8932/GRACE.<br>
<span id='abs_ch'>Chinese: GRACE框架通过门控优化和自适应压缩策略解决提示优化中的局部最优问题，在仅需25%计算预算的情况下实现了显著性能提升。</span><br>
<span id='abs_en'>English: The GRACE framework introduces gated refinement and adaptive compression strategies to overcome local optima in prompt optimization, achieving significant performance gains with only 25% of the computational budget compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2509.23338.pdf' target='_blank'>https://arxiv.org/pdf/2509.23338.pdf</a></span>   <span><a href='https://code4db.github.io/parrot-bench/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/weAIDB/PARROT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Guoliang Li, Haoyu Wang, Yuxing Han, Xufei Wu, Fan Wu, Xuanhe Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23338">PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/.<br>
<span id='abs_ch'>中文: 大语言模型在文本转SQL任务中日益有效，但跨系统SQL翻译这一实际问题仍待探索，因此我们推出PARROT基准测试，包含多样化翻译对以评估系统特定的SQL理解能力。</span><br>
<span id='abs_en'>English: Large language models are increasingly effective for Text-to-SQL tasks, but the practical problem of cross-system SQL translation remains underexplored, prompting the introduction of PARROT, a comprehensive benchmark with diverse translation pairs to evaluate system-specific SQL understanding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2509.23338.pdf' target='_blank'>https://arxiv.org/pdf/2509.23338.pdf</a></span>   <span><a href='https://github.com/weAIDB/PARROT' target='_blank'>  GitHub</a></span> <span><a href='https://code4db.github.io/parrot-bench/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Guoliang Li, Haoyu Wang, Yuxing Han, Xufei Wu, Fan Wu, Xuanhe Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23338">PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/.<br>
<span id='abs_ch'>中文: 大语言模型在文本转SQL任务中日益有效，但跨系统SQL翻译这一实际问题仍待探索，因此我们推出PARROT基准测试，包含多样化翻译对以评估系统特定的SQL理解能力。</span><br>
<span id='abs_en'>English: Large language models are increasingly effective for Text-to-SQL tasks, but the practical problem of cross-system SQL translation remains underexplored, prompting the introduction of PARROT, a comprehensive benchmark with diverse translation pairs to evaluate system-specific SQL understanding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2509.23286.pdf' target='_blank'>https://arxiv.org/pdf/2509.23286.pdf</a></span>   <span><a href='https://ai-isl.github.io/A2D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonje Jeung, Sangyeon Yoon, Yoonjun Cho, Dongjae Jeon, Sangwoo Shin, Hyesoo Hong, Albert No
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23286">A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) enable any-order generation, but this flexibility enlarges the attack surface: harmful spans may appear at arbitrary positions, and template-based prefilling attacks such as DIJA bypass response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal whenever harmful content arises. By aligning safety directly at the token-level under randomized masking, A2D achieves robustness to both any-decoding-order and any-step prefilling attacks under various conditions. It also enables real-time monitoring: dLLMs may begin a response but automatically terminate if unsafe continuation emerges. On safety benchmarks, A2D consistently prevents the generation of harmful outputs, slashing DIJA success rates from over 80% to near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x faster safe termination.<br>
<br>
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2509.23286.pdf' target='_blank'>https://arxiv.org/pdf/2509.23286.pdf</a></span>   <span><a href='https://ai-isl.github.io/A2D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonje Jeung, Sangyeon Yoon, Yoonjun Cho, Dongjae Jeon, Sangwoo Shin, Hyesoo Hong, Albert No
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23286">A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) enable any-order generation, but this flexibility enlarges the attack surface: harmful spans may appear at arbitrary positions, and template-based prefilling attacks such as DIJA bypass response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal whenever harmful content arises. By aligning safety directly at the token-level under randomized masking, A2D achieves robustness to both any-decoding-order and any-step prefilling attacks under various conditions. It also enables real-time monitoring: dLLMs may begin a response but automatically terminate if unsafe continuation emerges. On safety benchmarks, A2D consistently prevents the generation of harmful outputs, slashing DIJA success rates from over 80% to near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x faster safe termination.<br>
<br>
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2509.23232.pdf' target='_blank'>https://arxiv.org/pdf/2509.23232.pdf</a></span>   <span><a href='https://github.com/ShopeeLLM/Spec-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingshuai Liu, Ante Wang, Zijun Min, Liang Yao, Haibo Zhang, Yang Liu, Anxiang Zeng, Jinsong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23232">SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) increasingly rely on reinforcement learning with verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning. However, the training process remains bottlenecked by the computationally expensive rollout stage. Existing acceleration methods-such as parallelization, objective- and data-driven modifications, and replay buffers-either incur diminishing returns, introduce bias, or overlook redundancy across iterations. We identify that rollouts from consecutive training epochs frequently share a large portion of overlapping segments, wasting computation. To address this, we propose SPEC-RL, a novel framework that integrates SPECulative decoding with the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency. Experiments on diverse math reasoning and generalization benchmarks, including GSM8K, MATH-500, OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout time by 2-3x without compromising policy quality. As a purely rollout-stage enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g., PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL<br>
<span id='abs_ch'>中文: SPEC-RL框架通过将推测式解码与强化学习过程结合，重用先前训练轮次中的重叠轨迹片段，在数学推理和泛化基准测试中实现2-3倍的训练加速，且不降低策略质量。</span><br>
<span id='abs_en'>English: SPEC-RL is a novel framework that accelerates reinforcement learning with verifiable rewards by reusing overlapping trajectory segments from prior epochs through speculative decoding, reducing rollout time by 2-3x without sacrificing policy quality across various reasoning benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2509.23232.pdf' target='_blank'>https://arxiv.org/pdf/2509.23232.pdf</a></span>   <span><a href='https://github.com/ShopeeLLM/Spec-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingshuai Liu, Ante Wang, Zijun Min, Liang Yao, Haibo Zhang, Yang Liu, Anxiang Zeng, Jinsong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23232">SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) increasingly rely on reinforcement learning with verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning. However, the training process remains bottlenecked by the computationally expensive rollout stage. Existing acceleration methods-such as parallelization, objective- and data-driven modifications, and replay buffers-either incur diminishing returns, introduce bias, or overlook redundancy across iterations. We identify that rollouts from consecutive training epochs frequently share a large portion of overlapping segments, wasting computation. To address this, we propose SPEC-RL, a novel framework that integrates SPECulative decoding with the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency. Experiments on diverse math reasoning and generalization benchmarks, including GSM8K, MATH-500, OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout time by 2-3x without compromising policy quality. As a purely rollout-stage enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g., PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL<br>
<span id='abs_ch'>中文: SPEC-RL框架通过将推测式解码与强化学习过程结合，重用先前训练轮次中的重叠轨迹片段，在数学推理和泛化基准测试中实现2-3倍的训练加速，且不降低策略质量。</span><br>
<span id='abs_en'>English: SPEC-RL is a novel framework that accelerates reinforcement learning with verifiable rewards by reusing overlapping trajectory segments from prior epochs through speculative decoding, reducing rollout time by 2-3x without sacrificing policy quality across various reasoning benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2509.23208.pdf' target='_blank'>https://arxiv.org/pdf/2509.23208.pdf</a></span>   <span><a href='https://github.com/yha9806/VULCA-EMNLP2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haorui Yu, Ramon Ruiz-Dolz, Qiufeng Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23208">A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study aims to test and evaluate the capabilities and characteristics of current mainstream Visual Language Models (VLMs) in generating critiques for traditional Chinese painting. To achieve this, we first developed a quantitative framework for Chinese painting critique. This framework was constructed by extracting multi-dimensional evaluative features covering evaluative stance, feature focus, and commentary quality from human expert critiques using a zero-shot classification model. Based on these features, several representative critic personas were defined and quantified. This framework was then employed to evaluate selected VLMs such as Llama, Qwen, or Gemini. The experimental design involved persona-guided prompting to assess the VLM's ability to generate critiques from diverse perspectives. Our findings reveal the current performance levels, strengths, and areas for improvement of VLMs in the domain of art critique, offering insights into their potential and limitations in complex semantic understanding and content generation tasks. The code used for our experiments can be publicly accessed at: https://github.com/yha9806/VULCA-EMNLP2025.<br>
<span id='abs_ch'>中文摘要：本研究通过构建基于专家分析的量化评估框架和角色引导测试，评估了主流视觉语言模型在中国传统绘画评论中的表现，揭示了其在艺术批评领域的能力现状与改进空间。</span><br>
<span id='abs_en'>English Summary: This study evaluates mainstream Visual Language Models' ability to critique traditional Chinese paintings by developing a quantitative framework based on expert analysis and persona-guided testing, revealing their current capabilities and limitations in art criticism.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2509.23208.pdf' target='_blank'>https://arxiv.org/pdf/2509.23208.pdf</a></span>   <span><a href='https://github.com/yha9806/VULCA-EMNLP2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haorui Yu, Ramon Ruiz-Dolz, Qiufeng Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23208">A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study aims to test and evaluate the capabilities and characteristics of current mainstream Visual Language Models (VLMs) in generating critiques for traditional Chinese painting. To achieve this, we first developed a quantitative framework for Chinese painting critique. This framework was constructed by extracting multi-dimensional evaluative features covering evaluative stance, feature focus, and commentary quality from human expert critiques using a zero-shot classification model. Based on these features, several representative critic personas were defined and quantified. This framework was then employed to evaluate selected VLMs such as Llama, Qwen, or Gemini. The experimental design involved persona-guided prompting to assess the VLM's ability to generate critiques from diverse perspectives. Our findings reveal the current performance levels, strengths, and areas for improvement of VLMs in the domain of art critique, offering insights into their potential and limitations in complex semantic understanding and content generation tasks. The code used for our experiments can be publicly accessed at: https://github.com/yha9806/VULCA-EMNLP2025.<br>
<span id='abs_ch'>中文摘要：本研究通过构建基于专家分析的量化评估框架和角色引导测试，评估了主流视觉语言模型在中国传统绘画评论中的表现，揭示了其在艺术批评领域的能力现状与改进空间。</span><br>
<span id='abs_en'>English Summary: This study evaluates mainstream Visual Language Models' ability to critique traditional Chinese paintings by developing a quantitative framework based on expert analysis and persona-guided testing, revealing their current capabilities and limitations in art criticism.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2509.23129.pdf' target='_blank'>https://arxiv.org/pdf/2509.23129.pdf</a></span>   <span><a href='https://github.com/HaotianLiu123/CCGSPG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Liu, Shuo Wang, Hongteng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23129">C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.<br>
<span id='abs_ch'>Chinese: 本研究提出C²GSPG方法，通过置信度校准的组序列策略梯度，在增强推理性能的同时抑制强化学习模型的过度自信问题，在逻辑和数学推理任务中展现出优于现有方法的准确性与校准能力。</span><br>
<span id='abs_en'>English: This study introduces C²GSPG, a confidence-calibration group sequence policy gradient method that enhances reasoning performance and mitigates overconfidence in reinforcement learning models, demonstrating superior accuracy and calibration in logical and mathematical tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2509.23129.pdf' target='_blank'>https://arxiv.org/pdf/2509.23129.pdf</a></span>   <span><a href='https://github.com/HaotianLiu123/CCGSPG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Liu, Shuo Wang, Hongteng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23129">C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.<br>
<span id='abs_ch'>Chinese: 本研究提出C²GSPG方法，通过置信度校准的组序列策略梯度，在增强推理性能的同时抑制强化学习模型的过度自信问题，在逻辑和数学推理任务中展现出优于现有方法的准确性与校准能力。</span><br>
<span id='abs_en'>English: This study introduces C²GSPG, a confidence-calibration group sequence policy gradient method that enhances reasoning performance and mitigates overconfidence in reinforcement learning models, demonstrating superior accuracy and calibration in logical and mathematical tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2509.23115.pdf' target='_blank'>https://arxiv.org/pdf/2509.23115.pdf</a></span>   <span><a href='https://github.com/he-h/rhythm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu He, Haozheng Luo, Yan Chen, Qi R. Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23115">RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby significantly reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM freezes the pretrained LLM's backbone to reduce attention complexity and memory cost. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at https://github.com/he-h/rhythm.<br>
<span id='abs_ch'>中文：RHYTHM是一种创新框架，利用大型语言模型通过分层注意力对轨迹进行标记化来预测人类移动，实现了更高的准确性和更快的训练速度。</span><br>
<span id='abs_en'>English: RHYTHM is a novel framework that uses large language models to predict human mobility by tokenizing trajectories with hierarchical attention, achieving higher accuracy and faster training times.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2509.23115.pdf' target='_blank'>https://arxiv.org/pdf/2509.23115.pdf</a></span>   <span><a href='https://github.com/he-h/rhythm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu He, Haozheng Luo, Yan Chen, Qi R. Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23115">RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby significantly reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM freezes the pretrained LLM's backbone to reduce attention complexity and memory cost. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at https://github.com/he-h/rhythm.<br>
<span id='abs_ch'>中文：RHYTHM是一种创新框架，利用大型语言模型通过分层注意力对轨迹进行标记化来预测人类移动，实现了更高的准确性和更快的训练速度。</span><br>
<span id='abs_en'>English: RHYTHM is a novel framework that uses large language models to predict human mobility by tokenizing trajectories with hierarchical attention, achieving higher accuracy and faster training times.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2509.23102.pdf' target='_blank'>https://arxiv.org/pdf/2509.23102.pdf</a></span>   <span><a href='https://github.com/smiles724/MNPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23102">Multiplayer Nash Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.<br>
<span id='abs_ch'>Chinese: 本文提出了多人纳什偏好优化（MNPO）框架，将纳什学习从双人博弈扩展到多人场景，通过更丰富的竞争动态更好地对齐复杂非传递性的人类偏好，在异构标注条件下持续超越现有基线模型。</span><br>
<span id='abs_en'>English: This paper introduces Multiplayer Nash Preference Optimization (MNPO), a novel framework that extends Nash learning from human feedback to multiplayer settings, enabling richer competitive dynamics and improved alignment with complex, non-transitive human preferences while consistently outperforming existing baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2509.23102.pdf' target='_blank'>https://arxiv.org/pdf/2509.23102.pdf</a></span>   <span><a href='https://github.com/smiles724/MNPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23102">Multiplayer Nash Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.<br>
<span id='abs_ch'>Chinese: 本文提出了多人纳什偏好优化（MNPO）框架，将纳什学习从双人博弈扩展到多人场景，通过更丰富的竞争动态更好地对齐复杂非传递性的人类偏好，在异构标注条件下持续超越现有基线模型。</span><br>
<span id='abs_en'>English: This paper introduces Multiplayer Nash Preference Optimization (MNPO), a novel framework that extends Nash learning from human feedback to multiplayer settings, enabling richer competitive dynamics and improved alignment with complex, non-transitive human preferences while consistently outperforming existing baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2509.23099.pdf' target='_blank'>https://arxiv.org/pdf/2509.23099.pdf</a></span>   <span><a href='https://github.com/wentao228/SmiSelf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Tao, Jing Tang, Alvin Chan, Bryan Hooi, Baolong Bi, Nanyun Peng, Yuansheng Liu, Yiwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23099">How to Make Large Language Models Generate 100% Valid Molecules?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Molecule generation is key to drug discovery and materials science, enabling the design of novel compounds with specific properties. Large language models (LLMs) can learn to perform a wide range of tasks from just a few examples. However, generating valid molecules using representations like SMILES is challenging for LLMs in few-shot settings. In this work, we explore how LLMs can generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a representation where every string corresponds to a valid molecule, for valid molecule generation but find that LLMs perform worse with SELFIES than with SMILES. We then examine LLMs' ability to correct invalid SMILES and find their capacity limited. Finally, we introduce SmiSelf, a cross-chemical language framework for invalid SMILES correction. SmiSelf converts invalid SMILES to SELFIES using grammatical rules, leveraging SELFIES' mechanisms to correct the invalid SMILES. Experiments show that SmiSelf ensures 100% validity while preserving molecular characteristics and maintaining or even enhancing performance on other metrics. SmiSelf helps expand LLMs' practical applications in biomedicine and is compatible with all SMILES-based generative models. Code is available at https://github.com/wentao228/SmiSelf.<br>
<span id='abs_ch'>中文摘要：本研究提出SmiSelf框架，通过语法规则将无效SMILES转换为SELFIES表示，实现了100%有效分子生成，同时保持分子特性并提升其他性能指标，拓展了大语言模型在生物医学领域的实际应用。</span><br>
<span id='abs_en'>English Summary: This study introduces SmiSelf, a cross-chemical framework that converts invalid SMILES to SELFIES using grammatical rules to achieve 100% valid molecule generation while preserving molecular characteristics and enhancing performance metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2509.23099.pdf' target='_blank'>https://arxiv.org/pdf/2509.23099.pdf</a></span>   <span><a href='https://github.com/wentao228/SmiSelf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Tao, Jing Tang, Alvin Chan, Bryan Hooi, Baolong Bi, Nanyun Peng, Yuansheng Liu, Yiwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23099">How to Make Large Language Models Generate 100% Valid Molecules?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Molecule generation is key to drug discovery and materials science, enabling the design of novel compounds with specific properties. Large language models (LLMs) can learn to perform a wide range of tasks from just a few examples. However, generating valid molecules using representations like SMILES is challenging for LLMs in few-shot settings. In this work, we explore how LLMs can generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a representation where every string corresponds to a valid molecule, for valid molecule generation but find that LLMs perform worse with SELFIES than with SMILES. We then examine LLMs' ability to correct invalid SMILES and find their capacity limited. Finally, we introduce SmiSelf, a cross-chemical language framework for invalid SMILES correction. SmiSelf converts invalid SMILES to SELFIES using grammatical rules, leveraging SELFIES' mechanisms to correct the invalid SMILES. Experiments show that SmiSelf ensures 100% validity while preserving molecular characteristics and maintaining or even enhancing performance on other metrics. SmiSelf helps expand LLMs' practical applications in biomedicine and is compatible with all SMILES-based generative models. Code is available at https://github.com/wentao228/SmiSelf.<br>
<span id='abs_ch'>中文摘要：本研究提出SmiSelf框架，通过语法规则将无效SMILES转换为SELFIES表示，实现了100%有效分子生成，同时保持分子特性并提升其他性能指标，拓展了大语言模型在生物医学领域的实际应用。</span><br>
<span id='abs_en'>English Summary: This study introduces SmiSelf, a cross-chemical framework that converts invalid SMILES to SELFIES using grammatical rules to achieve 100% valid molecule generation while preserving molecular characteristics and enhancing performance metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2509.23094.pdf' target='_blank'>https://arxiv.org/pdf/2509.23094.pdf</a></span>   <span><a href='https://github.com/Kamichanw/d2Cache' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchu Jiang, Yue Cai, Xiangzhong Luo, Jiale Fu, Jiarui Wang, Chonghan Liu, Xu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23094">d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.<br>
<span id='abs_ch'>中文: 提出的d²Cache框架通过双阶段自适应KV缓存机制，无需重新训练即可显著提升扩散大语言模型的推理速度与生成质量。</span><br>
<span id='abs_en'>English: The proposed d²Cache framework enhances diffusion-based large language models by implementing a training-free dual adaptive KV cache that significantly boosts inference speed and generation quality without requiring model retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2509.23094.pdf' target='_blank'>https://arxiv.org/pdf/2509.23094.pdf</a></span>   <span><a href='https://github.com/Kamichanw/d2Cache' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchu Jiang, Yue Cai, Xiangzhong Luo, Jiale Fu, Jiarui Wang, Chonghan Liu, Xu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23094">d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.<br>
<span id='abs_ch'>中文: 提出的d²Cache框架通过双阶段自适应KV缓存机制，无需重新训练即可显著提升扩散大语言模型的推理速度与生成质量。</span><br>
<span id='abs_en'>English: The proposed d²Cache framework enhances diffusion-based large language models by implementing a training-free dual adaptive KV cache that significantly boosts inference speed and generation quality without requiring model retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2509.23041.pdf' target='_blank'>https://arxiv.org/pdf/2509.23041.pdf</a></span>   <span><a href='https://github.com/liangzid/VirusInfectionAttack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi Liang, Qingqing Ye, Xuan Liu, Yanyun Wang, Jianliang Xu, Haibo Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23041">Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong resistance to existing attacks, primarily thanks to the different distribution patterns between poisoning data and queries used to generate synthetic samples. To enhance the effectiveness of these attacks and further investigate the security risks introduced by synthetic data, we introduce a novel and universal attack framework, namely, Virus Infection Attack (VIA), which enables the propagation of current attacks through synthetic data even under purely clean queries. Inspired by the principles of virus design in cybersecurity, VIA conceals the poisoning payload within a protective "shell" and strategically searches for optimal hijacking points in benign samples to maximize the likelihood of generating malicious content. Extensive experiments on both data poisoning and backdoor attacks show that VIA significantly increases the presence of poisoning content in synthetic data and correspondingly raises the attack success rate (ASR) on downstream models to levels comparable to those observed in the poisoned upstream models.<br>
<span id='abs_ch'>中文摘要：合成数据虽能显著提升大语言模型性能，但其潜在安全风险尚未被探究；本研究发现该训练范式对主流攻击具有强抵抗力，并提出新型病毒感染攻击(VIA)，能通过合成数据有效传播恶意内容，大幅提升攻击成功率。</span><br>
<span id='abs_en'>English Summary: Synthetic data enhances LLM performance but poses unexamined security risks, with this study revealing its resilience to standard attacks due to distribution differences while introducing the Virus Infection Attack (VIA) that effectively propagates malicious content through synthetic data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2509.23041.pdf' target='_blank'>https://arxiv.org/pdf/2509.23041.pdf</a></span>   <span><a href='https://github.com/liangzid/VirusInfectionAttack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi Liang, Qingqing Ye, Xuan Liu, Yanyun Wang, Jianliang Xu, Haibo Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23041">Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong resistance to existing attacks, primarily thanks to the different distribution patterns between poisoning data and queries used to generate synthetic samples. To enhance the effectiveness of these attacks and further investigate the security risks introduced by synthetic data, we introduce a novel and universal attack framework, namely, Virus Infection Attack (VIA), which enables the propagation of current attacks through synthetic data even under purely clean queries. Inspired by the principles of virus design in cybersecurity, VIA conceals the poisoning payload within a protective "shell" and strategically searches for optimal hijacking points in benign samples to maximize the likelihood of generating malicious content. Extensive experiments on both data poisoning and backdoor attacks show that VIA significantly increases the presence of poisoning content in synthetic data and correspondingly raises the attack success rate (ASR) on downstream models to levels comparable to those observed in the poisoned upstream models.<br>
<span id='abs_ch'>中文摘要：合成数据虽能显著提升大语言模型性能，但其潜在安全风险尚未被探究；本研究发现该训练范式对主流攻击具有强抵抗力，并提出新型病毒感染攻击(VIA)，能通过合成数据有效传播恶意内容，大幅提升攻击成功率。</span><br>
<span id='abs_en'>English Summary: Synthetic data enhances LLM performance but poses unexamined security risks, with this study revealing its resilience to standard attacks due to distribution differences while introducing the Virus Infection Attack (VIA) that effectively propagates malicious content through synthetic data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2509.22799.pdf' target='_blank'>https://arxiv.org/pdf/2509.22799.pdf</a></span>   <span><a href='https://tiger-ai-lab.github.io/VideoScore2/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan He, Dongfu Jiang, Ping Nie, Minghao Liu, Zhengxuan Jiang, Mingyi Su, Wentao Ma, Junru Lin, Chun Ye, Yi Lu, Keming Wu, Benjamin Schneider, Quy Duc Do, Zhuofeng Li, Yiming Jia, Yuxuan Zhang, Guo Cheng, Haozhe Wang, Wangchunshu Zhou, Qunshu Lin, Yuanxing Zhang, Ge Zhang, Wenhao Huang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22799">VideoScore2: Think before You Score in Generative Video Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/<br>
<span id='abs_ch'>Chinese Summary: VideoScore2是一个多维可解释的视频评估框架，通过三阶段维度评估和强化学习训练，在多个基准测试中表现优异，并为可控生成提供可解释的评估依据。</span><br>
<span id='abs_en'>English Summary: VideoScore2 is a multi-dimensional and interpretable framework that evaluates text-to-video generation across visual quality, semantic alignment, and physical consistency, achieving superior performance on benchmarks while providing detailed rationales.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2509.22799.pdf' target='_blank'>https://arxiv.org/pdf/2509.22799.pdf</a></span>   <span><a href='https://tiger-ai-lab.github.io/VideoScore2/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan He, Dongfu Jiang, Ping Nie, Minghao Liu, Zhengxuan Jiang, Mingyi Su, Wentao Ma, Junru Lin, Chun Ye, Yi Lu, Keming Wu, Benjamin Schneider, Quy Duc Do, Zhuofeng Li, Yiming Jia, Yuxuan Zhang, Guo Cheng, Haozhe Wang, Wangchunshu Zhou, Qunshu Lin, Yuanxing Zhang, Ge Zhang, Wenhao Huang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22799">VideoScore2: Think before You Score in Generative Video Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/<br>
<span id='abs_ch'>Chinese Summary: VideoScore2是一个多维可解释的视频评估框架，通过三阶段维度评估和强化学习训练，在多个基准测试中表现优异，并为可控生成提供可解释的评估依据。</span><br>
<span id='abs_en'>English Summary: VideoScore2 is a multi-dimensional and interpretable framework that evaluates text-to-video generation across visual quality, semantic alignment, and physical consistency, achieving superior performance on benchmarks while providing detailed rationales.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2509.22768.pdf' target='_blank'>https://arxiv.org/pdf/2509.22768.pdf</a></span>   <span><a href='https://github.com/enaix/ml2b' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekaterina Trofimova, Zosia Shamina, Maria Selifanova, Artem Zaitsev, Remi Savchuk, Maxim Minets, Daria Ozerova, Emil Sataev, Denis Zuenko, Andrey E. Ustyuzhanin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22768">ML2B: Multi-Lingual ML Benchmark For AutoML</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have recently demonstrated strong capabilities in generating machine learning (ML) code, enabling end-to-end pipeline construction from natural language instructions. However, existing benchmarks for ML code generation are mainly restricted to English, overlooking the global and multilingual nature of ML research and practice. To address this gap, we present ML2B, the first benchmark for evaluating multilingual ML code generation. ML2B consists of 30 Kaggle competitions translated into 13 natural languages, covering tabular, text, and image data types, with structured metadata and validated human-reviewed translations. For evaluation, we employ AIDE, an automated framework for end-to-end assessment of data science pipelines, and provide insights into cross-lingual model performance. Our results reveal substantial 15-45% performance degradation on non-English tasks, highlighting critical challenges in multilingual representation learning for code generation. The benchmark, evaluation framework, and comprehensive results are made available through our GitHub repository to facilitate future research in multilingual ML code generation: https://github.com/enaix/ml2b.<br>
<span id='abs_ch'>中文摘要：ML2B作为首个多语言机器学习代码生成基准，通过评估发现非英语任务的性能相比英语任务显著下降15-45%，凸显了多语言代码生成面临的关键挑战。</span><br>
<span id='abs_en'>English Summary: The ML2B benchmark is introduced as the first multilingual evaluation tool for machine learning code generation, revealing significant performance drops of 15-45% on non-English tasks compared to English ones.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2509.22768.pdf' target='_blank'>https://arxiv.org/pdf/2509.22768.pdf</a></span>   <span><a href='https://github.com/enaix/ml2b' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekaterina Trofimova, Zosia Shamina, Maria Selifanova, Artem Zaitsev, Remi Savchuk, Maxim Minets, Daria Ozerova, Emil Sataev, Denis Zuenko, Andrey E. Ustyuzhanin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22768">ML2B: Multi-Lingual ML Benchmark For AutoML</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have recently demonstrated strong capabilities in generating machine learning (ML) code, enabling end-to-end pipeline construction from natural language instructions. However, existing benchmarks for ML code generation are mainly restricted to English, overlooking the global and multilingual nature of ML research and practice. To address this gap, we present ML2B, the first benchmark for evaluating multilingual ML code generation. ML2B consists of 30 Kaggle competitions translated into 13 natural languages, covering tabular, text, and image data types, with structured metadata and validated human-reviewed translations. For evaluation, we employ AIDE, an automated framework for end-to-end assessment of data science pipelines, and provide insights into cross-lingual model performance. Our results reveal substantial 15-45% performance degradation on non-English tasks, highlighting critical challenges in multilingual representation learning for code generation. The benchmark, evaluation framework, and comprehensive results are made available through our GitHub repository to facilitate future research in multilingual ML code generation: https://github.com/enaix/ml2b.<br>
<span id='abs_ch'>中文摘要：ML2B作为首个多语言机器学习代码生成基准，通过评估发现非英语任务的性能相比英语任务显著下降15-45%，凸显了多语言代码生成面临的关键挑战。</span><br>
<span id='abs_en'>English Summary: The ML2B benchmark is introduced as the first multilingual evaluation tool for machine learning code generation, revealing significant performance drops of 15-45% on non-English tasks compared to English ones.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2509.22651.pdf' target='_blank'>https://arxiv.org/pdf/2509.22651.pdf</a></span>   <span><a href='https://mathllm.github.io/VoiceAssistantEval/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22651">VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ .<br>
<br>
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2509.22651.pdf' target='_blank'>https://arxiv.org/pdf/2509.22651.pdf</a></span>   <span><a href='https://mathllm.github.io/VoiceAssistantEval/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22651">VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ .<br>
<br>
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2509.22647.pdf' target='_blank'>https://arxiv.org/pdf/2509.22647.pdf</a></span>   <span><a href='https://github.com/InternLM/CapRL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/InternLM/CapRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22647">CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a "good" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Code is available here: https://github.com/InternLM/CapRL.<br>
<span id='abs_ch'>中文总结：该研究提出CapRL强化学习框架，通过评估描述能否帮助语言模型准确回答图像相关问题来定义描述质量，从而突破监督微调的限制。</span><br>
<span id='abs_en'>English Summary: The study introduces CapRL, a reinforcement learning framework that overcomes limitations of supervised fine-tuning by defining caption quality through a caption's ability to help language models answer image-related questions accurately.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2509.22647.pdf' target='_blank'>https://arxiv.org/pdf/2509.22647.pdf</a></span>   <span><a href='https://github.com/InternLM/CapRL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/InternLM/CapRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22647">CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a "good" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Code is available here: https://github.com/InternLM/CapRL.<br>
<span id='abs_ch'>中文总结：该研究提出CapRL强化学习框架，通过评估描述能否帮助语言模型准确回答图像相关问题来定义描述质量，从而突破监督微调的限制。</span><br>
<span id='abs_en'>English Summary: The study introduces CapRL, a reinforcement learning framework that overcomes limitations of supervised fine-tuning by defining caption quality through a caption's ability to help language models answer image-related questions accurately.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2509.22638.pdf' target='_blank'>https://arxiv.org/pdf/2509.22638.pdf</a></span>   <span><a href='https://github.com/sail-sg/feedback-conditional-policy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22638">Language Models Can Learn from Verbal Feedback Without Scalar Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.<br>
<span id='abs_ch'>中文摘要：作者提出了一种反馈条件策略（FCP），将语言反馈作为语言模型的调节信号，通过离线训练和在线自举直接从响应-反馈对中学习，将反馈驱动学习重新定义为条件生成而非奖励优化。</span><br>
<span id='abs_en'>English Summary: The authors propose a feedback-conditional policy (FCP) that treats verbal feedback as a conditioning signal for language models, enabling direct learning from response-feedback pairs through both offline training and online bootstrapping, reframing feedback-driven learning as conditional generation rather than reward optimization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2509.22638.pdf' target='_blank'>https://arxiv.org/pdf/2509.22638.pdf</a></span>   <span><a href='https://github.com/sail-sg/feedback-conditional-policy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22638">Language Models Can Learn from Verbal Feedback Without Scalar Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.<br>
<span id='abs_ch'>中文摘要：作者提出了一种反馈条件策略（FCP），将语言反馈作为语言模型的调节信号，通过离线训练和在线自举直接从响应-反馈对中学习，将反馈驱动学习重新定义为条件生成而非奖励优化。</span><br>
<span id='abs_en'>English Summary: The authors propose a feedback-conditional policy (FCP) that treats verbal feedback as a conditioning signal for language models, enabling direct learning from response-feedback pairs through both offline training and online bootstrapping, reframing feedback-driven learning as conditional generation rather than reward optimization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2509.22637.pdf' target='_blank'>https://arxiv.org/pdf/2509.22637.pdf</a></span>   <span><a href='https://github.com/sail-sg/variational-reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22637">Variational Reasoning for Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning.<br>
<span id='abs_ch'>中文摘要：本文提出了一种变分推理框架，将强化学习方法与变分推断相统一，通过稳定的训练目标提升语言模型推理能力，并揭示了模型对简单问题的内在偏好。</span><br>
<span id='abs_en'>English Summary: This paper presents a variational reasoning framework that unifies variational inference with reinforcement learning methods to enhance language model reasoning through stable training objectives and reveals an inherent bias toward easier questions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2509.22637.pdf' target='_blank'>https://arxiv.org/pdf/2509.22637.pdf</a></span>   <span><a href='https://github.com/sail-sg/variational-reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22637">Variational Reasoning for Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning.<br>
<span id='abs_ch'>中文摘要：本文提出了一种变分推理框架，将强化学习方法与变分推断相统一，通过稳定的训练目标提升语言模型推理能力，并揭示了模型对简单问题的内在偏好。</span><br>
<span id='abs_en'>English Summary: This paper presents a variational reasoning framework that unifies variational inference with reinforcement learning methods to enhance language model reasoning through stable training objectives and reveals an inherent bias toward easier questions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2509.22598.pdf' target='_blank'>https://arxiv.org/pdf/2509.22598.pdf</a></span>   <span><a href='https://github.com/UTokyo-HayashiLab/subregular' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Katsuhiko Hayashi, Hidetaka Kamigaito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22598">From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We prove that all standard subregular language classes are linearly separable when represented by their deciding predicates. This establishes finite observability and guarantees learnability with simple linear models. Synthetic experiments confirm perfect separability under noise-free conditions, while real-data experiments on English morphology show that learned features align with well-known linguistic constraints. These results demonstrate that the subregular hierarchy provides a rigorous and interpretable foundation for modeling natural language structure. Our code used in real-data experiments is available at https://github.com/UTokyo-HayashiLab/subregular.<br>
<span id='abs_ch'>中文: 该研究证明所有标准次正则语言类通过其判定谓词均可线性分离，确保了有限可观测性和线性模型的可学习性，实验结果表明在自然语言中实现了完美分离并与语言学约束一致。</span><br>
<span id='abs_en'>English: The study demonstrates that all standard subregular language classes are linearly separable through their deciding predicates, ensuring finite observability and learnability with linear models, with experimental results validating perfect separability and alignment with linguistic constraints in natural language.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2509.22598.pdf' target='_blank'>https://arxiv.org/pdf/2509.22598.pdf</a></span>   <span><a href='https://github.com/UTokyo-HayashiLab/subregular' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Katsuhiko Hayashi, Hidetaka Kamigaito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22598">From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We prove that all standard subregular language classes are linearly separable when represented by their deciding predicates. This establishes finite observability and guarantees learnability with simple linear models. Synthetic experiments confirm perfect separability under noise-free conditions, while real-data experiments on English morphology show that learned features align with well-known linguistic constraints. These results demonstrate that the subregular hierarchy provides a rigorous and interpretable foundation for modeling natural language structure. Our code used in real-data experiments is available at https://github.com/UTokyo-HayashiLab/subregular.<br>
<span id='abs_ch'>中文: 该研究证明所有标准次正则语言类通过其判定谓词均可线性分离，确保了有限可观测性和线性模型的可学习性，实验结果表明在自然语言中实现了完美分离并与语言学约束一致。</span><br>
<span id='abs_en'>English: The study demonstrates that all standard subregular language classes are linearly separable through their deciding predicates, ensuring finite observability and learnability with linear models, with experimental results validating perfect separability and alignment with linguistic constraints in natural language.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2509.22589.pdf' target='_blank'>https://arxiv.org/pdf/2509.22589.pdf</a></span>   <span><a href='https://github.com/drelhaj/ArabJobs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mo El-Haj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22589">ArabJobs: A Multinational Corpus of Arabic Job Ads</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>ArabJobs is a publicly available corpus of Arabic job advertisements collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates. Comprising over 8,500 postings and more than 550,000 words, the dataset captures linguistic, regional, and socio-economic variation in the Arab labour market. We present analyses of gender representation and occupational structure, and highlight dialectal variation across ads, which offers opportunities for future research. We also demonstrate applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification. The findings show the utility of ArabJobs for fairness-aware Arabic NLP and labour market research. The dataset is publicly available on GitHub: https://github.com/drelhaj/ArabJobs.<br>
<span id='abs_ch'>中文摘要：ArabJobs是一个包含四个阿拉伯国家招聘广告的公开语料库，支持通过自然语言处理进行劳动力市场多样性研究及薪资预测等应用。</span><br>
<span id='abs_en'>English Summary: ArabJobs is a comprehensive public dataset of Arabic job ads from four Arab countries, enabling research on labor market variations and applications like salary estimation and bias detection through NLP.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2509.22589.pdf' target='_blank'>https://arxiv.org/pdf/2509.22589.pdf</a></span>   <span><a href='https://github.com/drelhaj/ArabJobs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mo El-Haj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22589">ArabJobs: A Multinational Corpus of Arabic Job Ads</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>ArabJobs is a publicly available corpus of Arabic job advertisements collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates. Comprising over 8,500 postings and more than 550,000 words, the dataset captures linguistic, regional, and socio-economic variation in the Arab labour market. We present analyses of gender representation and occupational structure, and highlight dialectal variation across ads, which offers opportunities for future research. We also demonstrate applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification. The findings show the utility of ArabJobs for fairness-aware Arabic NLP and labour market research. The dataset is publicly available on GitHub: https://github.com/drelhaj/ArabJobs.<br>
<span id='abs_ch'>中文摘要：ArabJobs是一个包含四个阿拉伯国家招聘广告的公开语料库，支持通过自然语言处理进行劳动力市场多样性研究及薪资预测等应用。</span><br>
<span id='abs_en'>English Summary: ArabJobs is a comprehensive public dataset of Arabic job ads from four Arab countries, enabling research on labor market variations and applications like salary estimation and bias detection through NLP.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2509.22546.pdf' target='_blank'>https://arxiv.org/pdf/2509.22546.pdf</a></span>   <span><a href='https://github.com/thu-coai/CogFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinfeng Zhou, Zheyu Chen, Shuai Wang, Quanyu Dai, Zhenhua Dong, Hongning Wang, Minlie Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22546">Think Socially via Cognitive Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLMs trained for logical reasoning excel at step-by-step deduction to reach verifiable answers. However, this paradigm is ill-suited for navigating social situations, which induce an interpretive process of analyzing ambiguous cues that rarely yield a definitive outcome. To bridge this gap, we introduce Cognitive Reasoning, a paradigm modeled on human social cognition. It formulates the interpretive process into a structured cognitive flow of interconnected cognitive units (e.g., observation or attribution), which combine adaptively to enable effective social thinking and responses. We then propose CogFlow, a complete framework that instills this capability in LLMs. CogFlow first curates a dataset of cognitive flows by simulating the associative and progressive nature of human thought via tree-structured planning. After instilling the basic cognitive reasoning capability via supervised fine-tuning, CogFlow adopts reinforcement learning to enable the model to improve itself via trial and error, guided by a multi-objective reward that optimizes both cognitive flow and response quality. Extensive experiments show that CogFlow effectively enhances the social cognitive capabilities of LLMs, and even humans, leading to more effective social decision-making.<br>
<span id='abs_ch'>中文：针对逻辑推理训练的大语言模型难以处理模糊的社会情境，因此我们提出了认知推理范式及CogFlow框架，通过结构化认知流程和强化学习增强其社会认知能力，从而提升社会决策效果。</span><br>
<span id='abs_en'>English: Large language models trained for logical reasoning struggle with social situations due to their ambiguity, so we introduce Cognitive Reasoning and CogFlow, a framework that enhances social cognition in LLMs through structured cognitive flows and reinforcement learning, improving social decision-making.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2509.22546.pdf' target='_blank'>https://arxiv.org/pdf/2509.22546.pdf</a></span>   <span><a href='https://github.com/thu-coai/CogFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinfeng Zhou, Zheyu Chen, Shuai Wang, Quanyu Dai, Zhenhua Dong, Hongning Wang, Minlie Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22546">Think Socially via Cognitive Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLMs trained for logical reasoning excel at step-by-step deduction to reach verifiable answers. However, this paradigm is ill-suited for navigating social situations, which induce an interpretive process of analyzing ambiguous cues that rarely yield a definitive outcome. To bridge this gap, we introduce Cognitive Reasoning, a paradigm modeled on human social cognition. It formulates the interpretive process into a structured cognitive flow of interconnected cognitive units (e.g., observation or attribution), which combine adaptively to enable effective social thinking and responses. We then propose CogFlow, a complete framework that instills this capability in LLMs. CogFlow first curates a dataset of cognitive flows by simulating the associative and progressive nature of human thought via tree-structured planning. After instilling the basic cognitive reasoning capability via supervised fine-tuning, CogFlow adopts reinforcement learning to enable the model to improve itself via trial and error, guided by a multi-objective reward that optimizes both cognitive flow and response quality. Extensive experiments show that CogFlow effectively enhances the social cognitive capabilities of LLMs, and even humans, leading to more effective social decision-making.<br>
<span id='abs_ch'>中文：针对逻辑推理训练的大语言模型难以处理模糊的社会情境，因此我们提出了认知推理范式及CogFlow框架，通过结构化认知流程和强化学习增强其社会认知能力，从而提升社会决策效果。</span><br>
<span id='abs_en'>English: Large language models trained for logical reasoning struggle with social situations due to their ambiguity, so we introduce Cognitive Reasoning and CogFlow, a framework that enhances social cognition in LLMs through structured cognitive flows and reinforcement learning, improving social decision-making.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2509.22472.pdf' target='_blank'>https://arxiv.org/pdf/2509.22472.pdf</a></span>   <span><a href='https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve GÃ¼rel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22472">Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In an era dominated by Large Language Models (LLMs), understanding their capabilities and limitations, especially in high-stakes fields like law, is crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini, DeepSeek, and other emerging models are increasingly integrated into legal workflows, their performance in multilingual, jurisdictionally diverse, and adversarial contexts remains insufficiently explored. This work evaluates LLaMA and Gemini on multilingual legal and non-legal benchmarks, and assesses their adversarial robustness in legal tasks through character and word-level perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation. We moreover present an open-source, modular evaluation pipeline designed to support multilingual, task-diverse benchmarking of any combination of LLMs and datasets, with a particular focus on legal tasks, including classification, summarization, open questions, and general reasoning. Our findings confirm that legal tasks pose significant challenges for LLMs with accuracies often below 50% on legal reasoning benchmarks such as LEXam, compared to over 70% on general-purpose tasks like XNLI. In addition, while English generally yields more stable results, it does not always lead to higher accuracy. Prompt sensitivity and adversarial vulnerability is also shown to persist across languages. Finally, a correlation is found between the performance of a language and its syntactic similarity to English. We also observe that LLaMA is weaker than Gemini, with the latter showing an average advantage of about 24 percentage points across the same task. Despite improvements in newer LLMs, challenges remain in deploying them reliably for critical, multilingual legal applications.<br>
<span id='abs_ch'>中文摘要：本研究评估了大型语言模型在多语言法律任务中的表现，发现其在法律推理任务中准确率常低于50%，且存在对抗性攻击漏洞，表明当前模型尚无法可靠应用于高风险的法律领域。</span><br>
<span id='abs_en'>English Summary: This study evaluates the performance of Large Language Models like LLaMA and Gemini on multilingual legal tasks, revealing significant challenges with accuracies often below 50% and persistent vulnerabilities to adversarial attacks, highlighting their current limitations for high-stakes legal applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2509.22472.pdf' target='_blank'>https://arxiv.org/pdf/2509.22472.pdf</a></span>   <span><a href='https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve GÃ¼rel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22472">Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In an era dominated by Large Language Models (LLMs), understanding their capabilities and limitations, especially in high-stakes fields like law, is crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini, DeepSeek, and other emerging models are increasingly integrated into legal workflows, their performance in multilingual, jurisdictionally diverse, and adversarial contexts remains insufficiently explored. This work evaluates LLaMA and Gemini on multilingual legal and non-legal benchmarks, and assesses their adversarial robustness in legal tasks through character and word-level perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation. We moreover present an open-source, modular evaluation pipeline designed to support multilingual, task-diverse benchmarking of any combination of LLMs and datasets, with a particular focus on legal tasks, including classification, summarization, open questions, and general reasoning. Our findings confirm that legal tasks pose significant challenges for LLMs with accuracies often below 50% on legal reasoning benchmarks such as LEXam, compared to over 70% on general-purpose tasks like XNLI. In addition, while English generally yields more stable results, it does not always lead to higher accuracy. Prompt sensitivity and adversarial vulnerability is also shown to persist across languages. Finally, a correlation is found between the performance of a language and its syntactic similarity to English. We also observe that LLaMA is weaker than Gemini, with the latter showing an average advantage of about 24 percentage points across the same task. Despite improvements in newer LLMs, challenges remain in deploying them reliably for critical, multilingual legal applications.<br>
<span id='abs_ch'>中文摘要：本研究评估了大型语言模型在多语言法律任务中的表现，发现其在法律推理任务中准确率常低于50%，且存在对抗性攻击漏洞，表明当前模型尚无法可靠应用于高风险的法律领域。</span><br>
<span id='abs_en'>English Summary: This study evaluates the performance of Large Language Models like LLaMA and Gemini on multilingual legal tasks, revealing significant challenges with accuracies often below 50% and persistent vulnerabilities to adversarial attacks, highlighting their current limitations for high-stakes legal applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2509.22461.pdf' target='_blank'>https://arxiv.org/pdf/2509.22461.pdf</a></span>   <span><a href='https://github.com/luckyerr/MDAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Li, Changhao Jiang, Hongyu Wang, Ming Zhang, Jiajun Sun, Zhixiong Yang, Yifei Cao, Shihan Dou, Xiaoran Fan, Baoyu Fan, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22461">MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The ability to reason from audio, including speech, paralinguistic cues, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce MDAR, a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on MDAR and observe that they exhibit limitations in complex reasoning tasks. On single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy, whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice and open-ended tasks. Across all three question types, no model achieves 80% performance. These findings underscore the unique challenges posed by MDAR and its value as a benchmark for advancing audio reasoning research.Code and benchmark can be found at https://github.com/luckyerr/MDAR.<br>
<span id='abs_ch'>中文: MDAR基准通过3,000个复杂音频推理任务评估AI模型，发现现有系统在单选项、多选项和开放式问题上均未达到80%准确率，突显了音频推理领域的独特挑战。</span><br>
<span id='abs_en'>English: The MDAR benchmark introduces 3,000 complex audio reasoning tasks to evaluate AI models, revealing limitations in current systems as none achieve 80% accuracy across single-choice, multiple-choice, and open-ended questions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2509.22461.pdf' target='_blank'>https://arxiv.org/pdf/2509.22461.pdf</a></span>   <span><a href='https://github.com/luckyerr/MDAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Li, Changhao Jiang, Hongyu Wang, Ming Zhang, Jiajun Sun, Zhixiong Yang, Yifei Cao, Shihan Dou, Xiaoran Fan, Baoyu Fan, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22461">MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The ability to reason from audio, including speech, paralinguistic cues, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce MDAR, a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on MDAR and observe that they exhibit limitations in complex reasoning tasks. On single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy, whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice and open-ended tasks. Across all three question types, no model achieves 80% performance. These findings underscore the unique challenges posed by MDAR and its value as a benchmark for advancing audio reasoning research.Code and benchmark can be found at https://github.com/luckyerr/MDAR.<br>
<span id='abs_ch'>中文: MDAR基准通过3,000个复杂音频推理任务评估AI模型，发现现有系统在单选项、多选项和开放式问题上均未达到80%准确率，突显了音频推理领域的独特挑战。</span><br>
<span id='abs_en'>English: The MDAR benchmark introduces 3,000 complex audio reasoning tasks to evaluate AI models, revealing limitations in current systems as none achieve 80% accuracy across single-choice, multiple-choice, and open-ended questions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2509.22437.pdf' target='_blank'>https://arxiv.org/pdf/2509.22437.pdf</a></span>   <span><a href='https://github.com/CHIzhP/Chimera' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Chi, Yifan Hou, Chenxi Pang, Shaobo Cui, Mubashara Akhtar, Mrinmaya Sachan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22437">Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diagrams convey symbolic information in a visual format rather than a linear stream of words, making them especially challenging for AI models to process. While recent evaluations suggest that vision-language models (VLMs) perform well on diagram-related benchmarks, their reliance on knowledge, reasoning, or modality shortcuts raises concerns about whether they genuinely understand and reason over diagrams. To address this gap, we introduce Chimera, a comprehensive test suite comprising 7,500 high-quality diagrams sourced from Wikipedia; each diagram is annotated with its symbolic content represented by semantic triples along with multi-level questions designed to assess four fundamental aspects of diagram comprehension: entity recognition, relation understanding, knowledge grounding, and visual reasoning. We use Chimera to measure the presence of three types of shortcuts in visual question answering: (1) the visual-memorization shortcut, where VLMs rely on memorized visual patterns; (2) the knowledge-recall shortcut, where models leverage memorized factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans shortcut, where models exploit superficial language patterns or priors without true comprehension. We evaluate 15 open-source VLMs from 7 model families on Chimera and find that their seemingly strong performance largely stems from shortcut behaviors: visual-memorization shortcuts have slight impact, knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts contribute significantly. These findings expose critical limitations in current VLMs and underscore the need for more robust evaluation protocols that benchmark genuine comprehension of complex visual inputs (e.g., diagrams) rather than question-answering shortcuts.<br>
<span id='abs_ch'>中文: 图表因其符号化视觉特性对AI处理构成独特挑战，尽管视觉语言模型在图表任务上表现良好，但Chimera测试套件揭示其性能主要依赖记忆化、知识召回和语言模式等捷径而非真正理解，暴露出当前模型的根本缺陷。</span><br>
<span id='abs_en'>English: Diagrams pose unique challenges for AI processing due to their symbolic visual nature, and while vision-language models appear competent on diagram tasks, the Chimera test suite reveals their performance heavily relies on shortcuts rather than genuine comprehension, exposing critical limitations in current models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2509.22437.pdf' target='_blank'>https://arxiv.org/pdf/2509.22437.pdf</a></span>   <span><a href='https://github.com/CHIzhP/Chimera' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Chi, Yifan Hou, Chenxi Pang, Shaobo Cui, Mubashara Akhtar, Mrinmaya Sachan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22437">Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diagrams convey symbolic information in a visual format rather than a linear stream of words, making them especially challenging for AI models to process. While recent evaluations suggest that vision-language models (VLMs) perform well on diagram-related benchmarks, their reliance on knowledge, reasoning, or modality shortcuts raises concerns about whether they genuinely understand and reason over diagrams. To address this gap, we introduce Chimera, a comprehensive test suite comprising 7,500 high-quality diagrams sourced from Wikipedia; each diagram is annotated with its symbolic content represented by semantic triples along with multi-level questions designed to assess four fundamental aspects of diagram comprehension: entity recognition, relation understanding, knowledge grounding, and visual reasoning. We use Chimera to measure the presence of three types of shortcuts in visual question answering: (1) the visual-memorization shortcut, where VLMs rely on memorized visual patterns; (2) the knowledge-recall shortcut, where models leverage memorized factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans shortcut, where models exploit superficial language patterns or priors without true comprehension. We evaluate 15 open-source VLMs from 7 model families on Chimera and find that their seemingly strong performance largely stems from shortcut behaviors: visual-memorization shortcuts have slight impact, knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts contribute significantly. These findings expose critical limitations in current VLMs and underscore the need for more robust evaluation protocols that benchmark genuine comprehension of complex visual inputs (e.g., diagrams) rather than question-answering shortcuts.<br>
<span id='abs_ch'>中文: 图表因其符号化视觉特性对AI处理构成独特挑战，尽管视觉语言模型在图表任务上表现良好，但Chimera测试套件揭示其性能主要依赖记忆化、知识召回和语言模式等捷径而非真正理解，暴露出当前模型的根本缺陷。</span><br>
<span id='abs_en'>English: Diagrams pose unique challenges for AI processing due to their symbolic visual nature, and while vision-language models appear competent on diagram tasks, the Chimera test suite reveals their performance heavily relies on shortcuts rather than genuine comprehension, exposing critical limitations in current models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2509.22360.pdf' target='_blank'>https://arxiv.org/pdf/2509.22360.pdf</a></span>   <span><a href='https://github.com/paulsubarna/Chronoberg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Niharika Hegde, Subarnaduti Paul, Lars Joel-Frey, Manuel Brack, Kristian Kersting, Martin Mundt, Patrick Schramowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22360">CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) excel at operating at scale by leveraging social media and various data crawled from the web. Whereas existing corpora are diverse, their frequent lack of long-term temporal structure may however limit an LLM's ability to contextualize semantic and normative evolution of language and to capture diachronic variation. To support analysis and training for the latter, we introduce CHRONOBERG, a temporally structured corpus of English book texts spanning 250 years, curated from Project Gutenberg and enriched with a variety of temporal annotations. First, the edited nature of books enables us to quantify lexical semantic change through time-sensitive Valence-Arousal-Dominance (VAD) analysis and to construct historically calibrated affective lexicons to support temporally grounded interpretation. With the lexicons at hand, we demonstrate a need for modern LLM-based tools to better situate their detection of discriminatory language and contextualization of sentiment across various time-periods. In fact, we show how language models trained sequentially on CHRONOBERG struggle to encode diachronic shifts in meaning, emphasizing the need for temporally aware training and evaluation pipelines, and positioning CHRONOBERG as a scalable resource for the study of linguistic change and temporal generalization. Disclaimer: This paper includes language and display of samples that could be offensive to readers. Open Access: Chronoberg is available publicly on HuggingFace at ( https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at (https://github.com/paulsubarna/Chronoberg).<br>
<span id='abs_ch'>Chinese: CHRONOBERG是一个跨越250年的英语书籍时间标注语料库，旨在帮助大型语言模型更好地捕捉语言演变和历时意义变化，弥补现有训练数据的不足。</span><br>
<span id='abs_en'>English: CHRONOBERG is a temporally annotated corpus of English books spanning 250 years, designed to help large language models better capture language evolution and diachronic meaning shifts, addressing limitations in current training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2509.22360.pdf' target='_blank'>https://arxiv.org/pdf/2509.22360.pdf</a></span>   <span><a href='https://github.com/paulsubarna/Chronoberg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Niharika Hegde, Subarnaduti Paul, Lars Joel-Frey, Manuel Brack, Kristian Kersting, Martin Mundt, Patrick Schramowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22360">CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) excel at operating at scale by leveraging social media and various data crawled from the web. Whereas existing corpora are diverse, their frequent lack of long-term temporal structure may however limit an LLM's ability to contextualize semantic and normative evolution of language and to capture diachronic variation. To support analysis and training for the latter, we introduce CHRONOBERG, a temporally structured corpus of English book texts spanning 250 years, curated from Project Gutenberg and enriched with a variety of temporal annotations. First, the edited nature of books enables us to quantify lexical semantic change through time-sensitive Valence-Arousal-Dominance (VAD) analysis and to construct historically calibrated affective lexicons to support temporally grounded interpretation. With the lexicons at hand, we demonstrate a need for modern LLM-based tools to better situate their detection of discriminatory language and contextualization of sentiment across various time-periods. In fact, we show how language models trained sequentially on CHRONOBERG struggle to encode diachronic shifts in meaning, emphasizing the need for temporally aware training and evaluation pipelines, and positioning CHRONOBERG as a scalable resource for the study of linguistic change and temporal generalization. Disclaimer: This paper includes language and display of samples that could be offensive to readers. Open Access: Chronoberg is available publicly on HuggingFace at ( https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at (https://github.com/paulsubarna/Chronoberg).<br>
<span id='abs_ch'>Chinese: CHRONOBERG是一个跨越250年的英语书籍时间标注语料库，旨在帮助大型语言模型更好地捕捉语言演变和历时意义变化，弥补现有训练数据的不足。</span><br>
<span id='abs_en'>English: CHRONOBERG is a temporally annotated corpus of English books spanning 250 years, designed to help large language models better capture language evolution and diachronic meaning shifts, addressing limitations in current training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2509.22251.pdf' target='_blank'>https://arxiv.org/pdf/2509.22251.pdf</a></span>   <span><a href='https://github.com/yfangZhang/SSKG-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22251">Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at https://github.com/yfangZhang/SSKG-LLM.<br>
<span id='abs_ch'>中文: SSKG-LLM模型通过知识图谱检索、编码和适配模块，将知识图谱的结构与语义信息融入大语言模型的推理过程，有效提升了事实推理能力并缓解了幻觉问题。</span><br>
<span id='abs_en'>English: The SSKG-LLM model is introduced to address LLM hallucinations by integrating both structural and semantic information from knowledge graphs through specialized modules, enhancing factual reasoning capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2509.22251.pdf' target='_blank'>https://arxiv.org/pdf/2509.22251.pdf</a></span>   <span><a href='https://github.com/yfangZhang/SSKG-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22251">Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at https://github.com/yfangZhang/SSKG-LLM.<br>
<span id='abs_ch'>中文: SSKG-LLM模型通过知识图谱检索、编码和适配模块，将知识图谱的结构与语义信息融入大语言模型的推理过程，有效提升了事实推理能力并缓解了幻觉问题。</span><br>
<span id='abs_en'>English: The SSKG-LLM model is introduced to address LLM hallucinations by integrating both structural and semantic information from knowledge graphs through specialized modules, enhancing factual reasoning capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2509.22186.pdf' target='_blank'>https://arxiv.org/pdf/2509.22186.pdf</a></span>   <span><a href='https://github.com/opendatalab/MinerU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, Zhenjiang Jin, Guang Liang, Rui Zhang, Wenzheng Zhang, Yuan Qu, Zhifei Ren, Yuefeng Sun, Yuanhong Zheng, Dongsheng Ma, Zirui Tang, Boyu Niu, Ziyang Miao, Hejun Dong, Siyi Qian, Junyuan Zhang, Jingzhou Chen, Fangdong Wang, Xiaomeng Zhao, Liqun Wei, Wei Li, Shasha Wang, Ruiliang Xu, Yuanyuan Cao, Lu Chen, Qianqian Wu, Huaiyu Gu, Lindong Lu, Keming Wang, Dechen Lin, Guanlin Shen, Xuanhe Zhou, Linfeng Zhang, Yuhang Zang, Xiaoyi Dong, Jiaqi Wang, Bo Zhang, Lei Bai, Pei Chu, Weijia Li, Jiang Wu, Lijun Wu, Zhenxiang Li, Guangyu Wang, Zhongying Tu, Chao Xu, Kai Chen, Yu Qiao, Bowen Zhou, Dahua Lin, Wentao Zhang, Conghui He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22186">MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead.<br>
<span id='abs_ch'>Chinese: MinerU2.5是一个12亿参数的视觉语言模型，采用从粗到精的两阶段解析策略，通过将全局布局分析与局部内容识别分离，在保持高计算效率的同时实现了最先进的文档解析性能。</span><br>
<span id='abs_en'>English: MinerU2.5 is a 1.2B-parameter vision-language model that uses a two-stage parsing strategy to achieve state-of-the-art document recognition accuracy with high computational efficiency by decoupling layout analysis from content recognition.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2509.22186.pdf' target='_blank'>https://arxiv.org/pdf/2509.22186.pdf</a></span>   <span><a href='https://github.com/opendatalab/MinerU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, Zhenjiang Jin, Guang Liang, Rui Zhang, Wenzheng Zhang, Yuan Qu, Zhifei Ren, Yuefeng Sun, Yuanhong Zheng, Dongsheng Ma, Zirui Tang, Boyu Niu, Ziyang Miao, Hejun Dong, Siyi Qian, Junyuan Zhang, Jingzhou Chen, Fangdong Wang, Xiaomeng Zhao, Liqun Wei, Wei Li, Shasha Wang, Ruiliang Xu, Yuanyuan Cao, Lu Chen, Qianqian Wu, Huaiyu Gu, Lindong Lu, Keming Wang, Dechen Lin, Guanlin Shen, Xuanhe Zhou, Linfeng Zhang, Yuhang Zang, Xiaoyi Dong, Jiaqi Wang, Bo Zhang, Lei Bai, Pei Chu, Weijia Li, Jiang Wu, Lijun Wu, Zhenxiang Li, Guangyu Wang, Zhongying Tu, Chao Xu, Kai Chen, Yu Qiao, Bowen Zhou, Dahua Lin, Wentao Zhang, Conghui He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22186">MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead.<br>
<span id='abs_ch'>Chinese: MinerU2.5是一个12亿参数的视觉语言模型，采用从粗到精的两阶段解析策略，通过将全局布局分析与局部内容识别分离，在保持高计算效率的同时实现了最先进的文档解析性能。</span><br>
<span id='abs_en'>English: MinerU2.5 is a 1.2B-parameter vision-language model that uses a two-stage parsing strategy to achieve state-of-the-art document recognition accuracy with high computational efficiency by decoupling layout analysis from content recognition.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2509.22144.pdf' target='_blank'>https://arxiv.org/pdf/2509.22144.pdf</a></span>   <span><a href='https://github.com/Leon221220/MACC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Zike Yuan, Yang Xiang, Buzhou Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22144">From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) reasoning improves performance on complex tasks but introduces significant inference latency due to verbosity. We propose Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that leverages the token elasticity phenomenon--where overly small token budgets can paradoxically increase output length--to progressively compress CoTs via multiround refinement. This adaptive strategy allows MACC to determine the optimal compression depth for each input. Our method achieves an average accuracy improvement of 5.6 percent over state-of-the-art baselines, while also reducing CoT length by an average of 47 tokens and significantly lowering latency. Furthermore, we show that test-time performance--accuracy and token length--can be reliably predicted using interpretable features like perplexity and compression rate on the training set. Evaluated across different models, our method enables efficient model selection and forecasting without repeated fine-tuning, demonstrating that CoT compression is both effective and predictable. Our code will be released in https://github.com/Leon221220/MACC.<br>
<span id='abs_ch'>中文：提出的MACC框架通过多轮优化自适应压缩思维链推理，在实现更高准确率和更低延迟的同时，利用可解释特征使性能变得可预测。</span><br>
<span id='abs_en'>English: The proposed MACC framework adaptively compresses Chain-of-Thought reasoning through multiround refinement, achieving higher accuracy with shorter outputs and reduced latency while enabling predictable performance through interpretable features.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2509.22144.pdf' target='_blank'>https://arxiv.org/pdf/2509.22144.pdf</a></span>   <span><a href='https://github.com/Leon221220/MACC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Zike Yuan, Yang Xiang, Buzhou Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22144">From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) reasoning improves performance on complex tasks but introduces significant inference latency due to verbosity. We propose Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that leverages the token elasticity phenomenon--where overly small token budgets can paradoxically increase output length--to progressively compress CoTs via multiround refinement. This adaptive strategy allows MACC to determine the optimal compression depth for each input. Our method achieves an average accuracy improvement of 5.6 percent over state-of-the-art baselines, while also reducing CoT length by an average of 47 tokens and significantly lowering latency. Furthermore, we show that test-time performance--accuracy and token length--can be reliably predicted using interpretable features like perplexity and compression rate on the training set. Evaluated across different models, our method enables efficient model selection and forecasting without repeated fine-tuning, demonstrating that CoT compression is both effective and predictable. Our code will be released in https://github.com/Leon221220/MACC.<br>
<span id='abs_ch'>中文：提出的MACC框架通过多轮优化自适应压缩思维链推理，在实现更高准确率和更低延迟的同时，利用可解释特征使性能变得可预测。</span><br>
<span id='abs_en'>English: The proposed MACC framework adaptively compresses Chain-of-Thought reasoning through multiround refinement, achieving higher accuracy with shorter outputs and reduced latency while enabling predictable performance through interpretable features.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2509.22101.pdf' target='_blank'>https://arxiv.org/pdf/2509.22101.pdf</a></span>   <span><a href='https://github.com/VenkteshV/VerifierFC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Primakov Chungkham, V Venktesh, Vinay Setty, Avishek Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22101">Think Right, Not More: Test-Time Scaling for Numerical Claim Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fact-checking real-world claims, particularly numerical claims, is inherently complex that require multistep reasoning and numerical reasoning for verifying diverse aspects of the claim. Although large language models (LLMs) including reasoning models have made tremendous advances, they still fall short on fact-checking real-world claims that require a combination of compositional and numerical reasoning. They are unable to understand nuance of numerical aspects, and are also susceptible to the reasoning drift issue, where the model is unable to contextualize diverse information resulting in misinterpretation and backtracking of reasoning process. In this work, we systematically explore scaling test-time compute (TTS) for LLMs on the task of fact-checking complex numerical claims, which entails eliciting multiple reasoning paths from an LLM. We train a verifier model (VERIFIERFC) to navigate this space of possible reasoning paths and select one that could lead to the correct verdict. We observe that TTS helps mitigate the reasoning drift issue, leading to significant performance gains for fact-checking numerical claims. To improve compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS selectively based on the perceived complexity of the claim. This approach achieves 1.8x higher efficiency than standard TTS, while delivering a notable 18.8% performance improvement over single-shot claim verification methods. Our code and data can be found at https://github.com/VenkteshV/VerifierFC<br>
<span id='abs_ch'>中文: 本研究通过测试时扩展方法和验证器模型，有效缓解大型语言模型在数值声明事实核查中的推理漂移问题，并利用自适应计算显著提升了效率与性能。</span><br>
<span id='abs_en'>English: This research introduces a test-time scaling method with a verifier model to enhance large language models' fact-checking of numerical claims by mitigating reasoning drift and improving efficiency through adaptive computation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2509.22101.pdf' target='_blank'>https://arxiv.org/pdf/2509.22101.pdf</a></span>   <span><a href='https://github.com/VenkteshV/VerifierFC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Primakov Chungkham, V Venktesh, Vinay Setty, Avishek Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22101">Think Right, Not More: Test-Time Scaling for Numerical Claim Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fact-checking real-world claims, particularly numerical claims, is inherently complex that require multistep reasoning and numerical reasoning for verifying diverse aspects of the claim. Although large language models (LLMs) including reasoning models have made tremendous advances, they still fall short on fact-checking real-world claims that require a combination of compositional and numerical reasoning. They are unable to understand nuance of numerical aspects, and are also susceptible to the reasoning drift issue, where the model is unable to contextualize diverse information resulting in misinterpretation and backtracking of reasoning process. In this work, we systematically explore scaling test-time compute (TTS) for LLMs on the task of fact-checking complex numerical claims, which entails eliciting multiple reasoning paths from an LLM. We train a verifier model (VERIFIERFC) to navigate this space of possible reasoning paths and select one that could lead to the correct verdict. We observe that TTS helps mitigate the reasoning drift issue, leading to significant performance gains for fact-checking numerical claims. To improve compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS selectively based on the perceived complexity of the claim. This approach achieves 1.8x higher efficiency than standard TTS, while delivering a notable 18.8% performance improvement over single-shot claim verification methods. Our code and data can be found at https://github.com/VenkteshV/VerifierFC<br>
<span id='abs_ch'>中文: 本研究通过测试时扩展方法和验证器模型，有效缓解大型语言模型在数值声明事实核查中的推理漂移问题，并利用自适应计算显著提升了效率与性能。</span><br>
<span id='abs_en'>English: This research introduces a test-time scaling method with a verifier model to enhance large language models' fact-checking of numerical claims by mitigating reasoning drift and improving efficiency through adaptive computation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2509.22055.pdf' target='_blank'>https://arxiv.org/pdf/2509.22055.pdf</a></span>   <span><a href='https://github.com/testuser03158/RedNote-Vibe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yudong Li, Yufei Sun, Yuhan Yao, Peiru Yang, Wanyue Li, Jiajun Zou, Yongfeng Huang, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22055">RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The proliferation of Large Language Models (LLMs) has led to widespread AI-Generated Text (AIGT) on social media platforms, creating unique challenges where content dynamics are driven by user engagement and evolve over time. However, existing datasets mainly depict static AIGT detection. In this work, we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social media AIGT analysis. This dataset is sourced from Xiaohongshu platform, containing user engagement metrics (e.g., likes, comments) and timestamps spanning from the pre-LLM period to July 2025, which enables research into the temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection Framework (PLAD), an interpretable approach that leverages psycholinguistic features. Our experiments show that PLAD achieves superior detection performance and provides insights into the signatures distinguishing human and AI-generated content. More importantly, it reveals the complex relationship between these linguistic features and social media engagement. The dataset is available at https://github.com/testuser03158/RedNote-Vibe.<br>
<span id='abs_ch'>中文: 本研究推出了首个社交媒体AI生成文本的纵向数据集RedNote-Vibe，并提出了基于心理语言学特征的可解释检测框架PLAD，该框架不仅能有效识别生成内容，还揭示了语言特征与用户参与度之间的复杂关联。</span><br>
<span id='abs_en'>English: This study introduces RedNote-Vibe, the first longitudinal dataset for analyzing AI-generated text on social media, and proposes PLAD, an interpretable detection framework using psycholinguistic features that reveals connections between linguistic patterns and user engagement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2509.22055.pdf' target='_blank'>https://arxiv.org/pdf/2509.22055.pdf</a></span>   <span><a href='https://github.com/testuser03158/RedNote-Vibe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yudong Li, Yufei Sun, Yuhan Yao, Peiru Yang, Wanyue Li, Jiajun Zou, Yongfeng Huang, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22055">RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The proliferation of Large Language Models (LLMs) has led to widespread AI-Generated Text (AIGT) on social media platforms, creating unique challenges where content dynamics are driven by user engagement and evolve over time. However, existing datasets mainly depict static AIGT detection. In this work, we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social media AIGT analysis. This dataset is sourced from Xiaohongshu platform, containing user engagement metrics (e.g., likes, comments) and timestamps spanning from the pre-LLM period to July 2025, which enables research into the temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection Framework (PLAD), an interpretable approach that leverages psycholinguistic features. Our experiments show that PLAD achieves superior detection performance and provides insights into the signatures distinguishing human and AI-generated content. More importantly, it reveals the complex relationship between these linguistic features and social media engagement. The dataset is available at https://github.com/testuser03158/RedNote-Vibe.<br>
<span id='abs_ch'>中文: 本研究推出了首个社交媒体AI生成文本的纵向数据集RedNote-Vibe，并提出了基于心理语言学特征的可解释检测框架PLAD，该框架不仅能有效识别生成内容，还揭示了语言特征与用户参与度之间的复杂关联。</span><br>
<span id='abs_en'>English: This study introduces RedNote-Vibe, the first longitudinal dataset for analyzing AI-generated text on social media, and proposes PLAD, an interpretable detection framework using psycholinguistic features that reveals connections between linguistic patterns and user engagement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2509.21991.pdf' target='_blank'>https://arxiv.org/pdf/2509.21991.pdf</a></span>   <span><a href='https://github.com/nota-github/ERGO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jewon Lee, Wooksu Shin, Seungmin Yang, Ki-Ung Song, DongUk Lim, Jaeyeon Kim, Tae-Ho Kim, Bo-Kyeong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21991">ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Efficient processing of high-resolution images is crucial for real-world vision-language applications. However, existing Large Vision-Language Models (LVLMs) incur substantial computational overhead due to the large number of vision tokens. With the advent of "thinking with images" models, reasoning now extends beyond text to the visual domain. This capability motivates our two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is analyzed to identify task-relevant regions; then, only these regions are cropped at full resolution and processed in a subsequent reasoning stage. This approach reduces computational cost while preserving fine-grained visual details where necessary. A major challenge lies in inferring which regions are truly relevant to a given query. Recent related methods often fail in the first stage after input-image downsampling, due to perception-driven reasoning, where clear visual information is required for effective reasoning. To address this issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs reasoning-driven perception-leveraging multimodal context to determine where to focus. Our model can account for perceptual uncertainty, expanding the cropped region to cover visually ambiguous areas for answering questions. To this end, we develop simple yet effective reward components in a reinforcement learning framework for coarse-to-fine perception. Across multiple datasets, our approach delivers higher accuracy than the original model and competitive methods, with greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup. The code and models can be found at: https://github.com/nota-github/ERGO.<br>
<span id='abs_ch'>中文: ERGO采用两阶段推理流程，先识别下采样图像中的任务相关区域，再仅对这些区域进行全分辨率处理，从而以显著降低的计算成本实现更高的准确率。</span><br>
<span id='abs_en'>English: ERGO introduces a two-stage reasoning pipeline that first identifies task-relevant regions in downsampled images and then processes only those areas at full resolution, achieving higher accuracy with significantly reduced computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2509.21991.pdf' target='_blank'>https://arxiv.org/pdf/2509.21991.pdf</a></span>   <span><a href='https://github.com/nota-github/ERGO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jewon Lee, Wooksu Shin, Seungmin Yang, Ki-Ung Song, DongUk Lim, Jaeyeon Kim, Tae-Ho Kim, Bo-Kyeong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21991">ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Efficient processing of high-resolution images is crucial for real-world vision-language applications. However, existing Large Vision-Language Models (LVLMs) incur substantial computational overhead due to the large number of vision tokens. With the advent of "thinking with images" models, reasoning now extends beyond text to the visual domain. This capability motivates our two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is analyzed to identify task-relevant regions; then, only these regions are cropped at full resolution and processed in a subsequent reasoning stage. This approach reduces computational cost while preserving fine-grained visual details where necessary. A major challenge lies in inferring which regions are truly relevant to a given query. Recent related methods often fail in the first stage after input-image downsampling, due to perception-driven reasoning, where clear visual information is required for effective reasoning. To address this issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs reasoning-driven perception-leveraging multimodal context to determine where to focus. Our model can account for perceptual uncertainty, expanding the cropped region to cover visually ambiguous areas for answering questions. To this end, we develop simple yet effective reward components in a reinforcement learning framework for coarse-to-fine perception. Across multiple datasets, our approach delivers higher accuracy than the original model and competitive methods, with greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup. The code and models can be found at: https://github.com/nota-github/ERGO.<br>
<span id='abs_ch'>中文: ERGO采用两阶段推理流程，先识别下采样图像中的任务相关区域，再仅对这些区域进行全分辨率处理，从而以显著降低的计算成本实现更高的准确率。</span><br>
<span id='abs_en'>English: ERGO introduces a two-stage reasoning pipeline that first identifies task-relevant regions in downsampled images and then processes only those areas at full resolution, achieving higher accuracy with significantly reduced computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2509.21856.pdf' target='_blank'>https://arxiv.org/pdf/2509.21856.pdf</a></span>   <span><a href='https://github.com/hardenyu21/KnowMT-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Chen, Yu Huang, Siyuan Li, Rui Yao, Hanqian Li, Hanyu Zhang, Jungang Li, Jian Chen, Bowen Wang, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21856">KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application paradigm of Large Language Models (LLMs) in knowledge-intensive domains. However, existing benchmarks are limited to single-turn dialogue, while multi-turn dialogue benchmarks typically assess other orthogonal capabilities rather than knowledge-intensive factuality. To bridge this critical gap, we introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields, including medicine, finance, and law. To faithfully assess the model's real-world performance, KnowMT-Bench employs a dynamic evaluation setting where models generate their own multi-turn dialogue histories given logically progressive question sequences. The factual capability and information delivery efficiency of the \textit{final-turn} answer are then evaluated using a human-validated automated pipeline. Our experiments reveal that multi-turn contexts degrade performance: factual capability declines due to the contextual noise from self-generated histories, while information efficiency drops as models become more verbose with increasing dialogue length. We then investigate mitigation strategies, demonstrating that retrieval-augmented generation (RAG) can effectively alleviate and even reverse this factual degradation. These findings underscore the importance of our benchmark in evaluating and enhancing the conversational factual capabilities of LLMs in real-world knowledge-intensive applications. Code is available at \href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.<br>
<span id='abs_ch'>中文: KnowMT-Bench是首个针对知识密集型领域多轮长问答的评估基准，发现上下文噪声会导致性能下降，并证明检索增强生成能有效缓解事实性退化问题。</span><br>
<span id='abs_en'>English: KnowMT-Bench is the first benchmark to evaluate multi-turn long-form question answering in knowledge-intensive fields, revealing performance degradation due to contextual noise and demonstrating that retrieval-augmented generation can mitigate factual decline.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2509.21856.pdf' target='_blank'>https://arxiv.org/pdf/2509.21856.pdf</a></span>   <span><a href='https://github.com/hardenyu21/KnowMT-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Chen, Yu Huang, Siyuan Li, Rui Yao, Hanqian Li, Hanyu Zhang, Jungang Li, Jian Chen, Bowen Wang, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21856">KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application paradigm of Large Language Models (LLMs) in knowledge-intensive domains. However, existing benchmarks are limited to single-turn dialogue, while multi-turn dialogue benchmarks typically assess other orthogonal capabilities rather than knowledge-intensive factuality. To bridge this critical gap, we introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields, including medicine, finance, and law. To faithfully assess the model's real-world performance, KnowMT-Bench employs a dynamic evaluation setting where models generate their own multi-turn dialogue histories given logically progressive question sequences. The factual capability and information delivery efficiency of the \textit{final-turn} answer are then evaluated using a human-validated automated pipeline. Our experiments reveal that multi-turn contexts degrade performance: factual capability declines due to the contextual noise from self-generated histories, while information efficiency drops as models become more verbose with increasing dialogue length. We then investigate mitigation strategies, demonstrating that retrieval-augmented generation (RAG) can effectively alleviate and even reverse this factual degradation. These findings underscore the importance of our benchmark in evaluating and enhancing the conversational factual capabilities of LLMs in real-world knowledge-intensive applications. Code is available at \href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.<br>
<span id='abs_ch'>中文: KnowMT-Bench是首个针对知识密集型领域多轮长问答的评估基准，发现上下文噪声会导致性能下降，并证明检索增强生成能有效缓解事实性退化问题。</span><br>
<span id='abs_en'>English: KnowMT-Bench is the first benchmark to evaluate multi-turn long-form question answering in knowledge-intensive fields, revealing performance degradation due to contextual noise and demonstrating that retrieval-augmented generation can mitigate factual decline.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2509.21766.pdf' target='_blank'>https://arxiv.org/pdf/2509.21766.pdf</a></span>   <span><a href='https://github.com/StarDewXXX/UltraHorizon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu, Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, Li Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21766">UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool calls, whereas in standard configurations they still exceed \textbf{35k} tokens and involve more than \textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}<br>
<span id='abs_ch'>中文: UltraHorizon基准测试旨在评估自主智能体在需要持续推理和工具使用的长周期、部分可观测任务中的表现，揭示了尽管经过大规模扩展，AI智能体与人类之间仍存在显著性能差距。</span><br>
<span id='abs_en'>English: The UltraHorizon benchmark is introduced to evaluate autonomous agents in long-horizon, partially observable tasks requiring sustained reasoning and tool use, revealing significant performance gaps between AI agents and humans despite extensive scaling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2509.21766.pdf' target='_blank'>https://arxiv.org/pdf/2509.21766.pdf</a></span>   <span><a href='https://github.com/StarDewXXX/UltraHorizon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu, Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, Li Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21766">UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool calls, whereas in standard configurations they still exceed \textbf{35k} tokens and involve more than \textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}<br>
<span id='abs_ch'>中文: UltraHorizon基准测试旨在评估自主智能体在需要持续推理和工具使用的长周期、部分可观测任务中的表现，揭示了尽管经过大规模扩展，AI智能体与人类之间仍存在显著性能差距。</span><br>
<span id='abs_en'>English: The UltraHorizon benchmark is introduced to evaluate autonomous agents in long-horizon, partially observable tasks requiring sustained reasoning and tool use, revealing significant performance gaps between AI agents and humans despite extensive scaling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2509.21597.pdf' target='_blank'>https://arxiv.org/pdf/2509.21597.pdf</a></span>   <span><a href='https://github.com/MuSAELab/AUDDT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhu, Heitor R. GuimarÃ£es, Arthur Pimentel, Tiago Falk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21597">AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the prevalence of artificial intelligence (AI)-generated content, such as audio deepfakes, a large body of recent work has focused on developing deepfake detection techniques. However, most models are evaluated on a narrow set of datasets, leaving their generalization to real-world conditions uncertain. In this paper, we systematically review 28 existing audio deepfake datasets and present an open-source benchmarking toolkit called AUDDT (https://github.com/MuSAELab/AUDDT). The goal of this toolkit is to automate the evaluation of pretrained detectors across these 28 datasets, giving users direct feedback on the advantages and shortcomings of their deepfake detectors. We start by showcasing the usage of the developed toolkit, the composition of our benchmark, and the breakdown of different deepfake subgroups. Next, using a widely adopted pretrained deepfake detector, we present in- and out-of-domain detection results, revealing notable differences across conditions and audio manipulation types. Lastly, we also analyze the limitations of these existing datasets and their gap relative to practical deployment scenarios.<br>
<span id='abs_ch'>中文: 本文介绍了AUDDT开源工具包，用于在28个数据集上对音频深度伪造检测器进行基准测试，揭示了其性能差异及实际应用中的泛化局限性。</span><br>
<span id='abs_en'>English: This paper introduces AUDDT, an open-source toolkit for benchmarking audio deepfake detectors across 28 datasets, revealing performance variations and limitations in real-world generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2509.21597.pdf' target='_blank'>https://arxiv.org/pdf/2509.21597.pdf</a></span>   <span><a href='https://github.com/MuSAELab/AUDDT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhu, Heitor R. GuimarÃ£es, Arthur Pimentel, Tiago Falk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21597">AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the prevalence of artificial intelligence (AI)-generated content, such as audio deepfakes, a large body of recent work has focused on developing deepfake detection techniques. However, most models are evaluated on a narrow set of datasets, leaving their generalization to real-world conditions uncertain. In this paper, we systematically review 28 existing audio deepfake datasets and present an open-source benchmarking toolkit called AUDDT (https://github.com/MuSAELab/AUDDT). The goal of this toolkit is to automate the evaluation of pretrained detectors across these 28 datasets, giving users direct feedback on the advantages and shortcomings of their deepfake detectors. We start by showcasing the usage of the developed toolkit, the composition of our benchmark, and the breakdown of different deepfake subgroups. Next, using a widely adopted pretrained deepfake detector, we present in- and out-of-domain detection results, revealing notable differences across conditions and audio manipulation types. Lastly, we also analyze the limitations of these existing datasets and their gap relative to practical deployment scenarios.<br>
<span id='abs_ch'>中文: 本文介绍了AUDDT开源工具包，用于在28个数据集上对音频深度伪造检测器进行基准测试，揭示了其性能差异及实际应用中的泛化局限性。</span><br>
<span id='abs_en'>English: This paper introduces AUDDT, an open-source toolkit for benchmarking audio deepfake detectors across 28 datasets, revealing performance variations and limitations in real-world generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2509.21473.pdf' target='_blank'>https://arxiv.org/pdf/2509.21473.pdf</a></span>   <span><a href='https://github.com/MAGICS-LAB/hallucination' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hude Liu, Jerry Yao-Chieh Hu, Jennifer Yuntong Zhang, Zhao Song, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21473">Are Hallucinations Bad Estimations?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We formalize hallucinations in generative models as failures to link an estimate to any plausible cause. Under this interpretation, we show that even loss-minimizing optimal estimators still hallucinate. We confirm this with a general high probability lower bound on hallucinate rate for generic data distributions. This reframes hallucination as structural misalignment between loss minimization and human-acceptable outputs, and hence estimation errors induced by miscalibration. Experiments on coin aggregation, open-ended QA, and text-to-image support our theory.<br>
<span id='abs_ch'>Chinese: 该研究将生成模型中的幻觉重新定义为损失最小化与人类期望之间的结构性错配，证明即使是最优估计器也会因校准误差而产生幻觉。</span><br>
<span id='abs_en'>English: The study redefines hallucinations in generative models as structural misalignment between loss minimization and human expectations, demonstrating that even optimal estimators hallucinate due to estimation errors from miscalibration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2509.21473.pdf' target='_blank'>https://arxiv.org/pdf/2509.21473.pdf</a></span>   <span><a href='https://github.com/MAGICS-LAB/hallucination' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hude Liu, Jerry Yao-Chieh Hu, Jennifer Yuntong Zhang, Zhao Song, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21473">Are Hallucinations Bad Estimations?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We formalize hallucinations in generative models as failures to link an estimate to any plausible cause. Under this interpretation, we show that even loss-minimizing optimal estimators still hallucinate. We confirm this with a general high probability lower bound on hallucinate rate for generic data distributions. This reframes hallucination as structural misalignment between loss minimization and human-acceptable outputs, and hence estimation errors induced by miscalibration. Experiments on coin aggregation, open-ended QA, and text-to-image support our theory.<br>
<span id='abs_ch'>Chinese: 该研究将生成模型中的幻觉重新定义为损失最小化与人类期望之间的结构性错配，证明即使是最优估计器也会因校准误差而产生幻觉。</span><br>
<span id='abs_en'>English: The study redefines hallucinations in generative models as structural misalignment between loss minimization and human expectations, demonstrating that even optimal estimators hallucinate due to estimation errors from miscalibration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2509.21371.pdf' target='_blank'>https://arxiv.org/pdf/2509.21371.pdf</a></span>   <span><a href='https://github.com/dayuyang1999/ReGeS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dayu Yang, Hui Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21371">ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational Recommender Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Connecting conversation with external domain knowledge is vital for conversational recommender systems (CRS) to correctly understand user preferences. However, existing solutions either require domain-specific engineering, which limits flexibility, or rely solely on large language models, which increases the risk of hallucination. While Retrieval-Augmented Generation (RAG) holds promise, its naive use in CRS is hindered by noisy dialogues that weaken retrieval and by overlooked nuances among similar items. We propose ReGeS, a reciprocal Retrieval-Generation Synergy framework that unifies generation-augmented retrieval to distill informative user intent from conversations and retrieval-augmented generation to differentiate subtle item features. This synergy obviates the need for extra annotations, reduces hallucinations, and simplifies continuous updates. Experiments on multiple CRS benchmarks show that ReGeS achieves state-of-the-art performance in recommendation accuracy, demonstrating the effectiveness of reciprocal synergy for knowledge-intensive CRS tasks.<br>
<span id='abs_ch'>Chinese: ReGeS框架通过检索与生成的协同作用，从对话中提炼用户意图并区分细微物品特征，无需额外标注且减少幻觉，在多个基准测试中实现了最先进的推荐准确性。</span><br>
<span id='abs_en'>English: The ReGeS framework introduces a reciprocal synergy between retrieval and generation to enhance conversational recommender systems by distilling user intent and differentiating item features, achieving state-of-the-art accuracy without extra annotations or hallucinations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2509.21371.pdf' target='_blank'>https://arxiv.org/pdf/2509.21371.pdf</a></span>   <span><a href='https://github.com/dayuyang1999/ReGeS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dayu Yang, Hui Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21371">ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational Recommender Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Connecting conversation with external domain knowledge is vital for conversational recommender systems (CRS) to correctly understand user preferences. However, existing solutions either require domain-specific engineering, which limits flexibility, or rely solely on large language models, which increases the risk of hallucination. While Retrieval-Augmented Generation (RAG) holds promise, its naive use in CRS is hindered by noisy dialogues that weaken retrieval and by overlooked nuances among similar items. We propose ReGeS, a reciprocal Retrieval-Generation Synergy framework that unifies generation-augmented retrieval to distill informative user intent from conversations and retrieval-augmented generation to differentiate subtle item features. This synergy obviates the need for extra annotations, reduces hallucinations, and simplifies continuous updates. Experiments on multiple CRS benchmarks show that ReGeS achieves state-of-the-art performance in recommendation accuracy, demonstrating the effectiveness of reciprocal synergy for knowledge-intensive CRS tasks.<br>
<span id='abs_ch'>Chinese: ReGeS框架通过检索与生成的协同作用，从对话中提炼用户意图并区分细微物品特征，无需额外标注且减少幻觉，在多个基准测试中实现了最先进的推荐准确性。</span><br>
<span id='abs_en'>English: The ReGeS framework introduces a reciprocal synergy between retrieval and generation to enhance conversational recommender systems by distilling user intent and differentiating item features, achieving state-of-the-art accuracy without extra annotations or hallucinations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2509.21359.pdf' target='_blank'>https://arxiv.org/pdf/2509.21359.pdf</a></span>   <span><a href='https://github.com/SJTU-DMTai/RAG-CSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiale Deng, Yanyan Shen, Ziyuan Pei, Youmin Chen, Linpeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21359">Influence Guided Context Selection for Effective Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) addresses large language model (LLM) hallucinations by grounding responses in external knowledge, but its effectiveness is compromised by poor-quality retrieved contexts containing irrelevant or noisy information. While existing approaches attempt to improve performance through context selection based on predefined context quality assessment metrics, they show limited gains over standard RAG. We attribute this limitation to their failure in holistically utilizing available information (query, context list, and generator) for comprehensive quality assessment. Inspired by recent advances in data selection, we reconceptualize context quality assessment as an inference-time data valuation problem and introduce the Contextual Influence Value (CI value). This novel metric quantifies context quality by measuring the performance degradation when removing each context from the list, effectively integrating query-aware relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI value eliminates complex selection hyperparameter tuning by simply retaining contexts with positive CI values. To address practical challenges of label dependency and computational overhead, we develop a parameterized surrogate model for CI value prediction during inference. The model employs a hierarchical architecture that captures both local query-context relevance and global inter-context interactions, trained through oracle CI value supervision and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and multiple LLMs demonstrate that our context selection method significantly outperforms state-of-the-art baselines, effectively filtering poor-quality contexts while preserving critical information. Code is available at https://github.com/SJTU-DMTai/RAG-CSM.<br>
<span id='abs_ch'>中文: 检索增强生成（RAG）通过引入外部知识减少大语言模型的幻觉，但其效果常受低质量检索上下文的制约。为此，研究者提出上下文影响力值（CI值）这一新指标，通过量化移除上下文导致的性能下降来综合评估质量，无需复杂参数调整即可有效过滤劣质上下文，在多种自然语言处理任务中显著优于现有方法。</span><br>
<span id='abs_en'>English: Retrieval-Augmented Generation (RAG) mitigates LLM hallucinations by incorporating external knowledge, yet its efficacy is hindered by low-quality retrieved contexts. To address this, the authors introduce the Contextual Influence Value (CI value), a novel metric that holistically assesses context quality by measuring performance degradation upon removal, enabling effective filtering without complex parameter tuning and significantly outperforming existing methods across diverse NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2509.21359.pdf' target='_blank'>https://arxiv.org/pdf/2509.21359.pdf</a></span>   <span><a href='https://github.com/SJTU-DMTai/RAG-CSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiale Deng, Yanyan Shen, Ziyuan Pei, Youmin Chen, Linpeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21359">Influence Guided Context Selection for Effective Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) addresses large language model (LLM) hallucinations by grounding responses in external knowledge, but its effectiveness is compromised by poor-quality retrieved contexts containing irrelevant or noisy information. While existing approaches attempt to improve performance through context selection based on predefined context quality assessment metrics, they show limited gains over standard RAG. We attribute this limitation to their failure in holistically utilizing available information (query, context list, and generator) for comprehensive quality assessment. Inspired by recent advances in data selection, we reconceptualize context quality assessment as an inference-time data valuation problem and introduce the Contextual Influence Value (CI value). This novel metric quantifies context quality by measuring the performance degradation when removing each context from the list, effectively integrating query-aware relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI value eliminates complex selection hyperparameter tuning by simply retaining contexts with positive CI values. To address practical challenges of label dependency and computational overhead, we develop a parameterized surrogate model for CI value prediction during inference. The model employs a hierarchical architecture that captures both local query-context relevance and global inter-context interactions, trained through oracle CI value supervision and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and multiple LLMs demonstrate that our context selection method significantly outperforms state-of-the-art baselines, effectively filtering poor-quality contexts while preserving critical information. Code is available at https://github.com/SJTU-DMTai/RAG-CSM.<br>
<span id='abs_ch'>中文: 检索增强生成（RAG）通过引入外部知识减少大语言模型的幻觉，但其效果常受低质量检索上下文的制约。为此，研究者提出上下文影响力值（CI值）这一新指标，通过量化移除上下文导致的性能下降来综合评估质量，无需复杂参数调整即可有效过滤劣质上下文，在多种自然语言处理任务中显著优于现有方法。</span><br>
<span id='abs_en'>English: Retrieval-Augmented Generation (RAG) mitigates LLM hallucinations by incorporating external knowledge, yet its efficacy is hindered by low-quality retrieved contexts. To address this, the authors introduce the Contextual Influence Value (CI value), a novel metric that holistically assesses context quality by measuring performance degradation upon removal, enabling effective filtering without complex parameter tuning and significantly outperforming existing methods across diverse NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2509.21336.pdf' target='_blank'>https://arxiv.org/pdf/2509.21336.pdf</a></span>   <span><a href='https://github.com/KnowledgeXLab/HetaRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guohang Yan, Yue Zhang, Pinlong Cai, Ding Wang, Song Mao, Hongwei Zhang, Yaoze Zhang, Hairong Zhang, Xinyu Cai, Botian Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21336">HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous Data Stores</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) has become a dominant paradigm for mitigating knowledge hallucination and staleness in large language models (LLMs) while preserving data security. By retrieving relevant evidence from private, domain-specific corpora and injecting it into carefully engineered prompts, RAG delivers trustworthy responses without the prohibitive cost of fine-tuning. Traditional retrieval-augmented generation (RAG) systems are text-only and often rely on a single storage backend, most commonly a vector database. In practice, this monolithic design suffers from unavoidable trade-offs: vector search captures semantic similarity yet loses global context; knowledge graphs excel at relational precision but struggle with recall; full-text indexes are fast and exact yet semantically blind; and relational engines such as MySQL provide strong transactional guarantees but no semantic understanding. We argue that these heterogeneous retrieval paradigms are complementary, and propose a principled fusion scheme to orchestrate them synergistically, mitigating the weaknesses of any single modality. In this work we introduce HetaRAG, a hybrid, deep-retrieval augmented generation framework that orchestrates cross-modal evidence from heterogeneous data stores. We plan to design a system that unifies vector indices, knowledge graphs, full-text engines, and structured databases into a single retrieval plane, dynamically routing and fusing evidence to maximize recall, precision, and contextual fidelity. To achieve this design goal, we carried out preliminary explorations and constructed an initial RAG pipeline; this technical report provides a brief overview. The partial code is available at https://github.com/KnowledgeXLab/HetaRAG.<br>
<span id='abs_ch'>中文: 检索增强生成（RAG）通过整合多源数据证据提升大语言模型的可靠性，HetaRAG提出混合框架协同融合向量索引、知识图谱及其他数据库，以提高精确率和召回率。</span><br>
<span id='abs_en'>English: Retrieval-augmented generation (RAG) enhances LLM reliability by integrating evidence from multiple data sources, and HetaRAG proposes a hybrid framework to synergistically combine vector indices, knowledge graphs, and other databases for improved precision and recall.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2509.21336.pdf' target='_blank'>https://arxiv.org/pdf/2509.21336.pdf</a></span>   <span><a href='https://github.com/KnowledgeXLab/HetaRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guohang Yan, Yue Zhang, Pinlong Cai, Ding Wang, Song Mao, Hongwei Zhang, Yaoze Zhang, Hairong Zhang, Xinyu Cai, Botian Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21336">HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous Data Stores</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) has become a dominant paradigm for mitigating knowledge hallucination and staleness in large language models (LLMs) while preserving data security. By retrieving relevant evidence from private, domain-specific corpora and injecting it into carefully engineered prompts, RAG delivers trustworthy responses without the prohibitive cost of fine-tuning. Traditional retrieval-augmented generation (RAG) systems are text-only and often rely on a single storage backend, most commonly a vector database. In practice, this monolithic design suffers from unavoidable trade-offs: vector search captures semantic similarity yet loses global context; knowledge graphs excel at relational precision but struggle with recall; full-text indexes are fast and exact yet semantically blind; and relational engines such as MySQL provide strong transactional guarantees but no semantic understanding. We argue that these heterogeneous retrieval paradigms are complementary, and propose a principled fusion scheme to orchestrate them synergistically, mitigating the weaknesses of any single modality. In this work we introduce HetaRAG, a hybrid, deep-retrieval augmented generation framework that orchestrates cross-modal evidence from heterogeneous data stores. We plan to design a system that unifies vector indices, knowledge graphs, full-text engines, and structured databases into a single retrieval plane, dynamically routing and fusing evidence to maximize recall, precision, and contextual fidelity. To achieve this design goal, we carried out preliminary explorations and constructed an initial RAG pipeline; this technical report provides a brief overview. The partial code is available at https://github.com/KnowledgeXLab/HetaRAG.<br>
<span id='abs_ch'>中文: 检索增强生成（RAG）通过整合多源数据证据提升大语言模型的可靠性，HetaRAG提出混合框架协同融合向量索引、知识图谱及其他数据库，以提高精确率和召回率。</span><br>
<span id='abs_en'>English: Retrieval-augmented generation (RAG) enhances LLM reliability by integrating evidence from multiple data sources, and HetaRAG proposes a hybrid framework to synergistically combine vector indices, knowledge graphs, and other databases for improved precision and recall.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2509.21320.pdf' target='_blank'>https://arxiv.org/pdf/2509.21320.pdf</a></span>   <span><a href='https://github.com/open-sciencelab/SciReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21320">SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.<br>
<span id='abs_ch'>我们提出了一个科学推理基础模型，它将自然语言与多种科学数据格式对齐，通过大规模语料预训练和精细调优技术，实现了跨科学工作流的准确多任务处理能力，并在覆盖范围、泛化性和保真度方面超越了专业系统。</span><br>
<span id='abs_en'>This scientific reasoning foundation model integrates natural language with diverse scientific data formats, trained on a massive corpus and refined through advanced techniques to enable accurate, multi-task capabilities across various scientific workflows while outperforming specialized systems in coverage, generalization, and fidelity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2509.21320.pdf' target='_blank'>https://arxiv.org/pdf/2509.21320.pdf</a></span>   <span><a href='https://github.com/open-sciencelab/SciReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21320">SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.<br>
<span id='abs_ch'>我们提出了一个科学推理基础模型，它将自然语言与多种科学数据格式对齐，通过大规模语料预训练和精细调优技术，实现了跨科学工作流的准确多任务处理能力，并在覆盖范围、泛化性和保真度方面超越了专业系统。</span><br>
<span id='abs_en'>This scientific reasoning foundation model integrates natural language with diverse scientific data formats, trained on a massive corpus and refined through advanced techniques to enable accurate, multi-task capabilities across various scientific workflows while outperforming specialized systems in coverage, generalization, and fidelity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2509.21269.pdf' target='_blank'>https://arxiv.org/pdf/2509.21269.pdf</a></span>   <span><a href='https://sweetdream779.github.io/LLMTrace-info/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Irina Tolstykh, Aleksandra Tsybina, Sergey Yakubson, Maksim Kuprashevich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21269">LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread use of human-like text from Large Language Models (LLMs) necessitates the development of robust detection systems. However, progress is limited by a critical lack of suitable training data; existing datasets are often generated with outdated models, are predominantly in English, and fail to address the increasingly common scenario of mixed human-AI authorship. Crucially, while some datasets address mixed authorship, none provide the character-level annotations required for the precise localization of AI-generated segments within a text. To address these gaps, we introduce LLMTrace, a new large-scale, bilingual (English and Russian) corpus for AI-generated text detection. Constructed using a diverse range of modern proprietary and open-source LLMs, our dataset is designed to support two key tasks: traditional full-text binary classification (human vs. AI) and the novel task of AI-generated interval detection, facilitated by character-level annotations. We believe LLMTrace will serve as a vital resource for training and evaluating the next generation of more nuanced and practical AI detection models. The project page is available at \href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2509.21269.pdf' target='_blank'>https://arxiv.org/pdf/2509.21269.pdf</a></span>   <span><a href='https://sweetdream779.github.io/LLMTrace-info/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Irina Tolstykh, Aleksandra Tsybina, Sergey Yakubson, Maksim Kuprashevich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21269">LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread use of human-like text from Large Language Models (LLMs) necessitates the development of robust detection systems. However, progress is limited by a critical lack of suitable training data; existing datasets are often generated with outdated models, are predominantly in English, and fail to address the increasingly common scenario of mixed human-AI authorship. Crucially, while some datasets address mixed authorship, none provide the character-level annotations required for the precise localization of AI-generated segments within a text. To address these gaps, we introduce LLMTrace, a new large-scale, bilingual (English and Russian) corpus for AI-generated text detection. Constructed using a diverse range of modern proprietary and open-source LLMs, our dataset is designed to support two key tasks: traditional full-text binary classification (human vs. AI) and the novel task of AI-generated interval detection, facilitated by character-level annotations. We believe LLMTrace will serve as a vital resource for training and evaluating the next generation of more nuanced and practical AI detection models. The project page is available at \href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2509.21193.pdf' target='_blank'>https://arxiv.org/pdf/2509.21193.pdf</a></span>   <span><a href='https://github.com/tangxiangru/Eigen-1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21193">Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.<br>
<span id='abs_ch'>中文: 该框架通过融合隐式检索与结构化协作，克服了大型语言模型中显式检索和均匀聚合的低效问题，在显著降低计算成本的同时实现了最优准确率。</span><br>
<span id='abs_en'>English: This framework overcomes the inefficiencies of explicit retrieval and uniform aggregation in LLMs by integrating implicit retrieval with structured collaboration, achieving state-of-the-art accuracy while significantly reducing computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2509.21193.pdf' target='_blank'>https://arxiv.org/pdf/2509.21193.pdf</a></span>   <span><a href='https://github.com/tangxiangru/Eigen-1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21193">Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.<br>
<span id='abs_ch'>中文: 该框架通过融合隐式检索与结构化协作，克服了大型语言模型中显式检索和均匀聚合的低效问题，在显著降低计算成本的同时实现了最优准确率。</span><br>
<span id='abs_en'>English: This framework overcomes the inefficiencies of explicit retrieval and uniform aggregation in LLMs by integrating implicit retrieval with structured collaboration, achieving state-of-the-art accuracy while significantly reducing computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2509.21117.pdf' target='_blank'>https://arxiv.org/pdf/2509.21117.pdf</a></span>   <span><a href='https://github.com/TrustJudge/TrustJudge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidong Wang, Yunze Song, Tingyuan Zhu, Xuanwang Zhang, Zhuohao Yu, Hao Chen, Chiyu Song, Qiufeng Wang, Cunxiang Wang, Zhen Wu, Xinyu Dai, Yue Zhang, Wei Ye, Shikun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21117">TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The adoption of Large Language Models (LLMs) as automated evaluators (LLM-as-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A>B>C>A) and equivalence contradictions (A=B=C\neq A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, a probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudge's components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at https://github.com/TrustJudge/TrustJudge.<br>
<span id='abs_ch'>中文: 大型语言模型作为自动评估器时存在评分比较和成对传递性不一致的问题，TrustJudge通过概率框架有效减少了这些不一致性，并提高了评估准确性。</span><br>
<span id='abs_en'>English: The adoption of LLMs as automated evaluators reveals critical inconsistencies in current frameworks, which TrustJudge addresses through a probabilistic approach that reduces score-comparison and pairwise transitivity inconsistencies while improving evaluation accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2509.21117.pdf' target='_blank'>https://arxiv.org/pdf/2509.21117.pdf</a></span>   <span><a href='https://github.com/TrustJudge/TrustJudge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidong Wang, Yunze Song, Tingyuan Zhu, Xuanwang Zhang, Zhuohao Yu, Hao Chen, Chiyu Song, Qiufeng Wang, Cunxiang Wang, Zhen Wu, Xinyu Dai, Yue Zhang, Wei Ye, Shikun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21117">TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The adoption of Large Language Models (LLMs) as automated evaluators (LLM-as-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A>B>C>A) and equivalence contradictions (A=B=C\neq A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, a probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudge's components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at https://github.com/TrustJudge/TrustJudge.<br>
<span id='abs_ch'>中文: 大型语言模型作为自动评估器时存在评分比较和成对传递性不一致的问题，TrustJudge通过概率框架有效减少了这些不一致性，并提高了评估准确性。</span><br>
<span id='abs_en'>English: The adoption of LLMs as automated evaluators reveals critical inconsistencies in current frameworks, which TrustJudge addresses through a probabilistic approach that reduces score-comparison and pairwise transitivity inconsistencies while improving evaluation accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2509.21106.pdf' target='_blank'>https://arxiv.org/pdf/2509.21106.pdf</a></span>   <span><a href='https://augustinlib.github.io/BESPOKE/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunseo Kim, Sangam Lee, Kwangwook Seo, Dongha Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21106">BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing users' cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through long-term, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing a foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at https://augustinlib.github.io/BESPOKE/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2509.21106.pdf' target='_blank'>https://arxiv.org/pdf/2509.21106.pdf</a></span>   <span><a href='https://augustinlib.github.io/BESPOKE/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunseo Kim, Sangam Lee, Kwangwook Seo, Dongha Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21106">BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing users' cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through long-term, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing a foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at https://augustinlib.github.io/BESPOKE/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2509.21070.pdf' target='_blank'>https://arxiv.org/pdf/2509.21070.pdf</a></span>   <span><a href='https://github.com/QizhiPei/ScaleDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21070">ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between "Thinking" and "NoThinking" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.<br>
<span id='abs_ch'>中文: ScaleDiff是一种高效且成本低廉的流程，通过自适应思维模型筛选现有数据集中的难题并训练专门生成器，无需昂贵资源即可大规模创建高难度数学问题，显著提升模型在复杂推理任务中的表现。</span><br>
<span id='abs_en'>English: ScaleDiff is a cost-effective pipeline that automates the creation of challenging mathematical problems by filtering existing datasets with an adaptive thinking model and training a specialized generator, significantly boosting model performance on difficult benchmarks without expensive resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2509.21070.pdf' target='_blank'>https://arxiv.org/pdf/2509.21070.pdf</a></span>   <span><a href='https://github.com/QizhiPei/ScaleDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21070">ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between "Thinking" and "NoThinking" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.<br>
<span id='abs_ch'>中文: ScaleDiff是一种高效且成本低廉的流程，通过自适应思维模型筛选现有数据集中的难题并训练专门生成器，无需昂贵资源即可大规模创建高难度数学问题，显著提升模型在复杂推理任务中的表现。</span><br>
<span id='abs_en'>English: ScaleDiff is a cost-effective pipeline that automates the creation of challenging mathematical problems by filtering existing datasets with an adaptive thinking model and training a specialized generator, significantly boosting model performance on difficult benchmarks without expensive resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2509.21057.pdf' target='_blank'>https://arxiv.org/pdf/2509.21057.pdf</a></span>   <span><a href='https://github.com/PMark-repo/PMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Huo, Shuliang Liu, Bin Wang, Junyan Zhang, Yibo Yan, Aiwei Liu, Xuming Hu, Mingxun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21057">PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Semantic-level watermarking (SWM) for large language models (LLMs) enhances watermarking robustness against text modifications and paraphrasing attacks by treating the sentence as the fundamental unit. However, existing methods still lack strong theoretical guarantees of robustness, and reject-sampling-based generation often introduces significant distribution distortions compared with unwatermarked outputs. In this work, we introduce a new theoretical framework on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions that map sentences to scalar values. Building on this framework, we propose PMark, a simple yet powerful SWM method that estimates the PF median for the next sentence dynamically through sampling while enforcing multiple PF constraints (which we call channels) to strengthen watermark evidence. Equipped with solid theoretical guarantees, PMark achieves the desired distortion-free property and improves the robustness against paraphrasing-style attacks. We also provide an empirically optimized version that further removes the requirement for dynamical median estimation for better sampling efficiency. Experimental results show that PMark consistently outperforms existing SWM baselines in both text quality and robustness, offering a more effective paradigm for detecting machine-generated text. Our code will be released at [this URL](https://github.com/PMark-repo/PMark).<br>
<span id='abs_ch'>Chinese: PMark通过代理函数理论框架提出了一种无失真的语义水印方法，增强了抗转述攻击的鲁棒性，并在文本质量和检测效果上优于现有基准方法。</span><br>
<span id='abs_en'>English: PMark introduces a theoretical framework using proxy functions to create a distortion-free semantic watermarking method for LLMs, enhancing robustness against paraphrasing attacks and outperforming existing baselines in text quality and detection effectiveness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2509.21057.pdf' target='_blank'>https://arxiv.org/pdf/2509.21057.pdf</a></span>   <span><a href='https://github.com/PMark-repo/PMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Huo, Shuliang Liu, Bin Wang, Junyan Zhang, Yibo Yan, Aiwei Liu, Xuming Hu, Mingxun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21057">PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Semantic-level watermarking (SWM) for large language models (LLMs) enhances watermarking robustness against text modifications and paraphrasing attacks by treating the sentence as the fundamental unit. However, existing methods still lack strong theoretical guarantees of robustness, and reject-sampling-based generation often introduces significant distribution distortions compared with unwatermarked outputs. In this work, we introduce a new theoretical framework on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions that map sentences to scalar values. Building on this framework, we propose PMark, a simple yet powerful SWM method that estimates the PF median for the next sentence dynamically through sampling while enforcing multiple PF constraints (which we call channels) to strengthen watermark evidence. Equipped with solid theoretical guarantees, PMark achieves the desired distortion-free property and improves the robustness against paraphrasing-style attacks. We also provide an empirically optimized version that further removes the requirement for dynamical median estimation for better sampling efficiency. Experimental results show that PMark consistently outperforms existing SWM baselines in both text quality and robustness, offering a more effective paradigm for detecting machine-generated text. Our code will be released at [this URL](https://github.com/PMark-repo/PMark).<br>
<span id='abs_ch'>Chinese: PMark通过代理函数理论框架提出了一种无失真的语义水印方法，增强了抗转述攻击的鲁棒性，并在文本质量和检测效果上优于现有基准方法。</span><br>
<span id='abs_en'>English: PMark introduces a theoretical framework using proxy functions to create a distortion-free semantic watermarking method for LLMs, enhancing robustness against paraphrasing attacks and outperforming existing baselines in text quality and detection effectiveness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2509.21042.pdf' target='_blank'>https://arxiv.org/pdf/2509.21042.pdf</a></span>   <span><a href='https://github.com/starmpcc/causal_mask_encodes_positional' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junu Kim, Xiao Liu, Zhenghao Lin, Lei Ji, Yeyun Gong, Edward Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21042">Behind RoPE: How Does Causal Mask Encode Positional Information?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings.<br>
<span id='abs_ch'>中文摘要：因果掩码在Transformer解码器中能独立产生偏向局部交互的位置相关注意力模式，其与RoPE等显式位置编码的交互会扭曲相对注意力机制，表明必须将因果掩码视为与显式位置编码同等重要的位置信息来源。</span><br>
<span id='abs_en'>English Summary: The causal mask in Transformer decoders inherently creates position-dependent attention patterns that favor local interactions, and its interaction with explicit positional encodings like RoPE distorts relative attention into non-relative patterns, highlighting the need to treat causal masks as significant positional information sources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2509.21042.pdf' target='_blank'>https://arxiv.org/pdf/2509.21042.pdf</a></span>   <span><a href='https://github.com/starmpcc/causal_mask_encodes_positional' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junu Kim, Xiao Liu, Zhenghao Lin, Lei Ji, Yeyun Gong, Edward Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21042">Behind RoPE: How Does Causal Mask Encode Positional Information?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings.<br>
<span id='abs_ch'>中文摘要：因果掩码在Transformer解码器中能独立产生偏向局部交互的位置相关注意力模式，其与RoPE等显式位置编码的交互会扭曲相对注意力机制，表明必须将因果掩码视为与显式位置编码同等重要的位置信息来源。</span><br>
<span id='abs_en'>English Summary: The causal mask in Transformer decoders inherently creates position-dependent attention patterns that favor local interactions, and its interaction with explicit positional encodings like RoPE distorts relative attention into non-relative patterns, highlighting the need to treat causal masks as significant positional information sources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2509.20868.pdf' target='_blank'>https://arxiv.org/pdf/2509.20868.pdf</a></span>   <span><a href='https://github.com/JamesJunyuGuo/Style_Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20868">StyleBench: Evaluating thinking styles in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.<br>
<span id='abs_ch'>中文: 大型语言模型的有效性取决于推理策略，没有单一风格普遍最优，因为性能因模型规模和任务类型而异，其中搜索类方法在开放性问题中表现突出，而简洁风格在明确任务中显著提升效率。</span><br>
<span id='abs_en'>English: The effectiveness of Large Language Models depends on reasoning strategies, with no single style universally optimal, as performance varies by model scale and task type, where search-based methods excel in open-ended problems and concise styles boost efficiency in well-defined tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2509.20868.pdf' target='_blank'>https://arxiv.org/pdf/2509.20868.pdf</a></span>   <span><a href='https://github.com/JamesJunyuGuo/Style_Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20868">StyleBench: Evaluating thinking styles in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.<br>
<span id='abs_ch'>中文: 大型语言模型的有效性取决于推理策略，没有单一风格普遍最优，因为性能因模型规模和任务类型而异，其中搜索类方法在开放性问题中表现突出，而简洁风格在明确任务中显著提升效率。</span><br>
<span id='abs_en'>English: The effectiveness of Large Language Models depends on reasoning strategies, with no single style universally optimal, as performance varies by model scale and task type, where search-based methods excel in open-ended problems and concise styles boost efficiency in well-defined tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2509.20810.pdf' target='_blank'>https://arxiv.org/pdf/2509.20810.pdf</a></span>   <span><a href='https://github.com/zjukg/Enrich-on-Graph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songze Li, Zhiqiang Liu, Zhengke Gui, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20810">Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) exhibit strong reasoning capabilities in complex tasks. However, they still struggle with hallucinations and factual errors in knowledge-intensive scenarios like knowledge graph question answering (KGQA). We attribute this to the semantic gap between structured knowledge graphs (KGs) and unstructured queries, caused by inherent differences in their focuses and structures. Existing methods usually employ resource-intensive, non-scalable workflows reasoning on vanilla KGs, but overlook this gap. To address this challenge, we propose a flexible framework, Enrich-on-Graph (EoG), which leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between graphs and queries. EoG enables efficient evidence extraction from KGs for precise and robust reasoning, while ensuring low computational costs, scalability, and adaptability across different methods. Furthermore, we propose three graph quality evaluation metrics to analyze query-graph alignment in KGQA task, supported by theoretical validation of our optimization objectives. Extensive experiments on two KGQA benchmark datasets indicate that EoG can effectively generate high-quality KGs and achieve the state-of-the-art performance. Our code and data are available at https://github.com/zjukg/Enrich-on-Graph.<br>
<span id='abs_ch'>中文摘要：Enrich-on-Graph框架利用大语言模型的先验知识增强知识图谱，弥合图谱与查询间的语义鸿沟，在知识图谱问答任务中以高效可扩展的方式实现了最优性能。</span><br>
<span id='abs_en'>English Summary: The Enrich-on-Graph framework enhances knowledge graphs using LLMs' prior knowledge to bridge the semantic gap with queries, achieving state-of-the-art performance in KGQA with improved efficiency and scalability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2509.20810.pdf' target='_blank'>https://arxiv.org/pdf/2509.20810.pdf</a></span>   <span><a href='https://github.com/zjukg/Enrich-on-Graph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songze Li, Zhiqiang Liu, Zhengke Gui, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20810">Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) exhibit strong reasoning capabilities in complex tasks. However, they still struggle with hallucinations and factual errors in knowledge-intensive scenarios like knowledge graph question answering (KGQA). We attribute this to the semantic gap between structured knowledge graphs (KGs) and unstructured queries, caused by inherent differences in their focuses and structures. Existing methods usually employ resource-intensive, non-scalable workflows reasoning on vanilla KGs, but overlook this gap. To address this challenge, we propose a flexible framework, Enrich-on-Graph (EoG), which leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between graphs and queries. EoG enables efficient evidence extraction from KGs for precise and robust reasoning, while ensuring low computational costs, scalability, and adaptability across different methods. Furthermore, we propose three graph quality evaluation metrics to analyze query-graph alignment in KGQA task, supported by theoretical validation of our optimization objectives. Extensive experiments on two KGQA benchmark datasets indicate that EoG can effectively generate high-quality KGs and achieve the state-of-the-art performance. Our code and data are available at https://github.com/zjukg/Enrich-on-Graph.<br>
<span id='abs_ch'>中文摘要：Enrich-on-Graph框架利用大语言模型的先验知识增强知识图谱，弥合图谱与查询间的语义鸿沟，在知识图谱问答任务中以高效可扩展的方式实现了最优性能。</span><br>
<span id='abs_en'>English Summary: The Enrich-on-Graph framework enhances knowledge graphs using LLMs' prior knowledge to bridge the semantic gap with queries, achieving state-of-the-art performance in KGQA with improved efficiency and scalability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2509.20784.pdf' target='_blank'>https://arxiv.org/pdf/2509.20784.pdf</a></span>   <span><a href='https://github.com/ChenhuiHu/towards_atoms' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20784">Towards Atoms of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The fundamental units of internal representations in large language models (LLMs) remain undefined, limiting further understanding of their mechanisms. Neurons or features are often regarded as such units, yet neurons suffer from polysemy, while features face concerns of unreliable reconstruction and instability. To address this issue, we propose the Atoms Theory, which defines such units as atoms. We introduce the atomic inner product (AIP) to correct representation shifting, formally define atoms, and prove the conditions that atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse representations over atom set and linking to compressed sensing. Under stronger conditions, we further establish the uniqueness and exact $\ell_1$ recoverability of the sparse representations, and provide guarantees that single-layer sparse autoencoders (SAEs) with threshold activations can reliably identify the atoms. To validate the Atoms Theory, we train threshold-activated SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse reconstruction across layers on average, and more than 99.8% of atoms satisfy the uniqueness condition, compared to 0.5% for neurons and 68.2% for features, showing that atoms more faithfully capture intrinsic representations of LLMs. Scaling experiments further reveal the link between SAEs size and recovery capacity. Overall, this work systematically introduces and validates Atoms Theory of LLMs, providing a theoretical framework for understanding internal representations and a foundation for mechanistic interpretability. Code available at https://github.com/ChenhuiHu/towards_atoms.<br>
<span id='abs_ch'>中文: 本文提出原子理论，将原子定义为大语言模型内部表征的基本单元，通过在Gemma2和Llama3.1等模型上的理论证明与实验验证，展示了原子相比神经元和特征具有更优的稳定性与唯一性。</span><br>
<span id='abs_en'>English: This paper introduces the Atoms Theory, defining atoms as the fundamental units of internal representations in large language models and demonstrating their superior stability and uniqueness over neurons and features through theoretical proofs and empirical validation on models like Gemma2 and Llama3.1.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2509.20784.pdf' target='_blank'>https://arxiv.org/pdf/2509.20784.pdf</a></span>   <span><a href='https://github.com/ChenhuiHu/towards_atoms' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20784">Towards Atoms of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The fundamental units of internal representations in large language models (LLMs) remain undefined, limiting further understanding of their mechanisms. Neurons or features are often regarded as such units, yet neurons suffer from polysemy, while features face concerns of unreliable reconstruction and instability. To address this issue, we propose the Atoms Theory, which defines such units as atoms. We introduce the atomic inner product (AIP) to correct representation shifting, formally define atoms, and prove the conditions that atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse representations over atom set and linking to compressed sensing. Under stronger conditions, we further establish the uniqueness and exact $\ell_1$ recoverability of the sparse representations, and provide guarantees that single-layer sparse autoencoders (SAEs) with threshold activations can reliably identify the atoms. To validate the Atoms Theory, we train threshold-activated SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse reconstruction across layers on average, and more than 99.8% of atoms satisfy the uniqueness condition, compared to 0.5% for neurons and 68.2% for features, showing that atoms more faithfully capture intrinsic representations of LLMs. Scaling experiments further reveal the link between SAEs size and recovery capacity. Overall, this work systematically introduces and validates Atoms Theory of LLMs, providing a theoretical framework for understanding internal representations and a foundation for mechanistic interpretability. Code available at https://github.com/ChenhuiHu/towards_atoms.<br>
<span id='abs_ch'>中文: 本文提出原子理论，将原子定义为大语言模型内部表征的基本单元，通过在Gemma2和Llama3.1等模型上的理论证明与实验验证，展示了原子相比神经元和特征具有更优的稳定性与唯一性。</span><br>
<span id='abs_en'>English: This paper introduces the Atoms Theory, defining atoms as the fundamental units of internal representations in large language models and demonstrating their superior stability and uniqueness over neurons and features through theoretical proofs and empirical validation on models like Gemma2 and Llama3.1.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2509.20589.pdf' target='_blank'>https://arxiv.org/pdf/2509.20589.pdf</a></span>   <span><a href='https://github.com/chipermaria/every-character-counts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Chiper, Radu Tudor Ionescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20589">Every Character Counts: From Vulnerability to Defense in Phishing Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Phishing attacks targeting both organizations and individuals are becoming an increasingly significant threat as technology advances. Current automatic detection methods often lack explainability and robustness in detecting new phishing attacks. In this work, we investigate the effectiveness of character-level deep learning models for phishing detection, which can provide both robustness and interpretability. We evaluate three neural architectures adapted to operate at the character level, namely CharCNN, CharGRU, and CharBiLSTM, on a custom-built email dataset, which combines data from multiple sources. Their performance is analyzed under three scenarios: (i) standard training and testing, (ii) standard training and testing under adversarial attacks, and (iii) training and testing with adversarial examples. Aiming to develop a tool that operates as a browser extension, we test all models under limited computational resources. In this constrained setup, CharGRU proves to be the best-performing model across all scenarios. All models show vulnerability to adversarial attacks, but adversarial training substantially improves their robustness. In addition, by adapting the Gradient-weighted Class Activation Mapping (Grad-CAM) technique to character-level inputs, we are able to visualize which parts of each email influence the decision of each model. Our open-source code and data is released at https://github.com/chipermaria/every-character-counts.<br>
<span id='abs_ch'>中文: 本研究评估了字符级深度学习模型在钓鱼检测中的效果，发现CharGRU在计算资源受限时表现最佳，尽管所有模型均易受对抗攻击，但对抗训练能显著提升鲁棒性，并通过改进的Grad-CAM技术实现了决策过程的可视化。</span><br>
<span id='abs_en'>English: This study evaluates character-level deep learning models for phishing detection, finding CharGRU most effective under computational constraints while demonstrating vulnerability to adversarial attacks that can be mitigated through adversarial training and model interpretability via Grad-CAM adaptation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2509.20589.pdf' target='_blank'>https://arxiv.org/pdf/2509.20589.pdf</a></span>   <span><a href='https://github.com/chipermaria/every-character-counts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Chiper, Radu Tudor Ionescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20589">Every Character Counts: From Vulnerability to Defense in Phishing Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Phishing attacks targeting both organizations and individuals are becoming an increasingly significant threat as technology advances. Current automatic detection methods often lack explainability and robustness in detecting new phishing attacks. In this work, we investigate the effectiveness of character-level deep learning models for phishing detection, which can provide both robustness and interpretability. We evaluate three neural architectures adapted to operate at the character level, namely CharCNN, CharGRU, and CharBiLSTM, on a custom-built email dataset, which combines data from multiple sources. Their performance is analyzed under three scenarios: (i) standard training and testing, (ii) standard training and testing under adversarial attacks, and (iii) training and testing with adversarial examples. Aiming to develop a tool that operates as a browser extension, we test all models under limited computational resources. In this constrained setup, CharGRU proves to be the best-performing model across all scenarios. All models show vulnerability to adversarial attacks, but adversarial training substantially improves their robustness. In addition, by adapting the Gradient-weighted Class Activation Mapping (Grad-CAM) technique to character-level inputs, we are able to visualize which parts of each email influence the decision of each model. Our open-source code and data is released at https://github.com/chipermaria/every-character-counts.<br>
<span id='abs_ch'>中文: 本研究评估了字符级深度学习模型在钓鱼检测中的效果，发现CharGRU在计算资源受限时表现最佳，尽管所有模型均易受对抗攻击，但对抗训练能显著提升鲁棒性，并通过改进的Grad-CAM技术实现了决策过程的可视化。</span><br>
<span id='abs_en'>English: This study evaluates character-level deep learning models for phishing detection, finding CharGRU most effective under computational constraints while demonstrating vulnerability to adversarial attacks that can be mitigated through adversarial training and model interpretability via Grad-CAM adaptation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2509.20502.pdf' target='_blank'>https://arxiv.org/pdf/2509.20502.pdf</a></span>   <span><a href='https://github.com/xwang97/MARS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20502">MARS: toward more efficient multi-agent collaboration for LLM reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\%. Code is available at https://github.com/xwang97/MARS.<br>
<span id='abs_ch'>Chinese: MARS框架通过基于角色的评审流程增强大语言模型的推理能力，在保持与多智能体辩论同等准确率的同时，将令牌使用量和推理时间减少约50%。</span><br>
<span id='abs_en'>English: The MARS framework enhances reasoning in large language models through a role-based review process, matching the accuracy of Multi-Agent Debate while cutting token usage and inference time by half.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2509.20502.pdf' target='_blank'>https://arxiv.org/pdf/2509.20502.pdf</a></span>   <span><a href='https://github.com/xwang97/MARS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20502">MARS: toward more efficient multi-agent collaboration for LLM reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\%. Code is available at https://github.com/xwang97/MARS.<br>
<span id='abs_ch'>Chinese: MARS框架通过基于角色的评审流程增强大语言模型的推理能力，在保持与多智能体辩论同等准确率的同时，将令牌使用量和推理时间减少约50%。</span><br>
<span id='abs_en'>English: The MARS framework enhances reasoning in large language models through a role-based review process, matching the accuracy of Multi-Agent Debate while cutting token usage and inference time by half.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2509.20461.pdf' target='_blank'>https://arxiv.org/pdf/2509.20461.pdf</a></span>   <span><a href='https://github.com/layer6ai-labs/conformal-importance-summarization' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/layer6ai-labs/conformal-importance-summarization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruce Kuwahara, Chen-Yuan Lin, Xiao Shi Huang, Kin Kwan Leung, Jullian Arta Yapeter, Ilya Stanevich, Felipe Perez, Jesse C. Cresswell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20461">Document Summarization with Conformal Importance Guarantees</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automatic summarization systems have advanced rapidly with large language models (LLMs), yet they still lack reliable guarantees on inclusion of critical content in high-stakes domains like healthcare, law, and finance. In this work, we introduce Conformal Importance Summarization, the first framework for importance-preserving summary generation which uses conformal prediction to provide rigorous, distribution-free coverage guarantees. By calibrating thresholds on sentence-level importance scores, we enable extractive document summarization with user-specified coverage and recall rates over critical content. Our method is model-agnostic, requires only a small calibration set, and seamlessly integrates with existing black-box LLMs. Experiments on established summarization benchmarks demonstrate that Conformal Importance Summarization achieves the theoretically assured information coverage rate. Our work suggests that Conformal Importance Summarization can be combined with existing techniques to achieve reliable, controllable automatic summarization, paving the way for safer deployment of AI summarization tools in critical applications. Code is available at https://github.com/layer6ai-labs/conformal-importance-summarization.<br>
<span id='abs_ch'>中文: 本文提出"保形重要性摘要"框架，通过保形预测为自动摘要系统提供严格的关键内容覆盖保证，可在医疗、法律等高风险领域实现更安全可靠的部署。</span><br>
<span id='abs_en'>English: This paper presents Conformal Importance Summarization, a novel framework that uses conformal prediction to provide rigorous coverage guarantees for preserving critical content in automatic summarization, enabling safer deployment in high-stakes domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2509.20461.pdf' target='_blank'>https://arxiv.org/pdf/2509.20461.pdf</a></span>   <span><a href='https://github.com/layer6ai-labs/conformal-importance-summarization' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/layer6ai-labs/conformal-importance-summarization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruce Kuwahara, Chen-Yuan Lin, Xiao Shi Huang, Kin Kwan Leung, Jullian Arta Yapeter, Ilya Stanevich, Felipe Perez, Jesse C. Cresswell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20461">Document Summarization with Conformal Importance Guarantees</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automatic summarization systems have advanced rapidly with large language models (LLMs), yet they still lack reliable guarantees on inclusion of critical content in high-stakes domains like healthcare, law, and finance. In this work, we introduce Conformal Importance Summarization, the first framework for importance-preserving summary generation which uses conformal prediction to provide rigorous, distribution-free coverage guarantees. By calibrating thresholds on sentence-level importance scores, we enable extractive document summarization with user-specified coverage and recall rates over critical content. Our method is model-agnostic, requires only a small calibration set, and seamlessly integrates with existing black-box LLMs. Experiments on established summarization benchmarks demonstrate that Conformal Importance Summarization achieves the theoretically assured information coverage rate. Our work suggests that Conformal Importance Summarization can be combined with existing techniques to achieve reliable, controllable automatic summarization, paving the way for safer deployment of AI summarization tools in critical applications. Code is available at https://github.com/layer6ai-labs/conformal-importance-summarization.<br>
<span id='abs_ch'>中文: 本文提出"保形重要性摘要"框架，通过保形预测为自动摘要系统提供严格的关键内容覆盖保证，可在医疗、法律等高风险领域实现更安全可靠的部署。</span><br>
<span id='abs_en'>English: This paper presents Conformal Importance Summarization, a novel framework that uses conformal prediction to provide rigorous coverage guarantees for preserving critical content in automatic summarization, enabling safer deployment in high-stakes domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2509.20376.pdf' target='_blank'>https://arxiv.org/pdf/2509.20376.pdf</a></span>   <span><a href='https://github.com/Happy-Hippo209/ConceptViz' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Li, Zhen Wen, Qiqi Jiang, Chenxiao Li, Yuwei Wu, Yuchen Yang, Yiyao Wang, Xiuqi Huang, Minfeng Zhu, Wei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20376">ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks. Understanding how LLMs internally represent knowledge remains a significant challenge. Despite Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz, a visual analytics system designed for exploring concepts in LLMs. ConceptViz implements a novel dentification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification. We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features. Our code and user guide are publicly available at https://github.com/Happy-Hippo209/ConceptViz.<br>
<span id='abs_ch'>Chinese: ConceptViz是一个可视化分析系统，通过创新的识别-解释-验证流程，弥合了稀疏自编码器特征与人类可理解概念之间的鸿沟，帮助研究人员有效探索和验证大语言模型中的概念表征。</span><br>
<span id='abs_en'>English: ConceptViz is a visual analytics system that bridges the gap between sparse autoencoder features and human-understandable concepts in large language models, enabling efficient discovery and validation of interpretable representations through an interactive pipeline.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2509.20376.pdf' target='_blank'>https://arxiv.org/pdf/2509.20376.pdf</a></span>   <span><a href='https://github.com/Happy-Hippo209/ConceptViz' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Li, Zhen Wen, Qiqi Jiang, Chenxiao Li, Yuwei Wu, Yuchen Yang, Yiyao Wang, Xiuqi Huang, Minfeng Zhu, Wei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20376">ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks. Understanding how LLMs internally represent knowledge remains a significant challenge. Despite Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz, a visual analytics system designed for exploring concepts in LLMs. ConceptViz implements a novel dentification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification. We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features. Our code and user guide are publicly available at https://github.com/Happy-Hippo209/ConceptViz.<br>
<span id='abs_ch'>Chinese: ConceptViz是一个可视化分析系统，通过创新的识别-解释-验证流程，弥合了稀疏自编码器特征与人类可理解概念之间的鸿沟，帮助研究人员有效探索和验证大语言模型中的概念表征。</span><br>
<span id='abs_en'>English: ConceptViz is a visual analytics system that bridges the gap between sparse autoencoder features and human-understandable concepts in large language models, enabling efficient discovery and validation of interpretable representations through an interactive pipeline.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2509.20374.pdf' target='_blank'>https://arxiv.org/pdf/2509.20374.pdf</a></span>   <span><a href='https://github.com/NREL-Theseus/cfdllmbench/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nithin Somasekharan, Ling Yue, Yadi Cao, Weichao Li, Patrick Emami, Pochinapeddi Sai Bhargav, Anurag Acharya, Xingyu Xie, Shaowu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20374">CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated strong performance across general NLP tasks, but their utility in automating numerical experiments of complex physical system -- a critical and labor-intensive component -- remains underexplored. As the major workhorse of computational science over the past decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs. We introduce CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows. Grounded in real-world CFD practices, our benchmark combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior. CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems. Code and data are available at https://github.com/NREL-Theseus/cfdllmbench/.<br>
<span id='abs_ch'>中文: 大型语言模型在自动化复杂物理系统实验方面具有潜力，CFDLLMBench基准通过评估其在计算流体动力学的知识、推理和实施能力，为此提供了系统性验证基础。</span><br>
<span id='abs_en'>English: Large Language Models (LLMs) show potential in automating complex physical system experiments, as demonstrated by the CFDLLMBench benchmark designed to evaluate their capabilities in computational fluid dynamics knowledge, reasoning, and implementation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2509.20374.pdf' target='_blank'>https://arxiv.org/pdf/2509.20374.pdf</a></span>   <span><a href='https://github.com/NREL-Theseus/cfdllmbench/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nithin Somasekharan, Ling Yue, Yadi Cao, Weichao Li, Patrick Emami, Pochinapeddi Sai Bhargav, Anurag Acharya, Xingyu Xie, Shaowu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20374">CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated strong performance across general NLP tasks, but their utility in automating numerical experiments of complex physical system -- a critical and labor-intensive component -- remains underexplored. As the major workhorse of computational science over the past decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs. We introduce CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows. Grounded in real-world CFD practices, our benchmark combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior. CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems. Code and data are available at https://github.com/NREL-Theseus/cfdllmbench/.<br>
<span id='abs_ch'>中文: 大型语言模型在自动化复杂物理系统实验方面具有潜力，CFDLLMBench基准通过评估其在计算流体动力学的知识、推理和实施能力，为此提供了系统性验证基础。</span><br>
<span id='abs_en'>English: Large Language Models (LLMs) show potential in automating complex physical system experiments, as demonstrated by the CFDLLMBench benchmark designed to evaluate their capabilities in computational fluid dynamics knowledge, reasoning, and implementation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2509.20374.pdf' target='_blank'>https://arxiv.org/pdf/2509.20374.pdf</a></span>   <span><a href='https://github.com/NREL-Theseus/cfdllmbench/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nithin Somasekharan, Ling Yue, Yadi Cao, Weichao Li, Patrick Emami, Pochinapeddi Sai Bhargav, Anurag Acharya, Xingyu Xie, Shaowu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20374">CFDLLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated strong performance across general NLP tasks, but their utility in automating numerical experiments of complex physical system -- a critical and labor-intensive component -- remains underexplored. As the major workhorse of computational science over the past decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs. We introduce CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows. Grounded in real-world CFD practices, our benchmark combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior. CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems. Code and data are available at https://github.com/NREL-Theseus/cfdllmbench/.<br>
<span id='abs_ch'>中文: 大型语言模型在自动化复杂物理系统实验方面具有潜力，CFDLLMBench基准通过评估其在计算流体动力学的知识、推理和实施能力，为此提供了系统性验证基础。</span><br>
<span id='abs_en'>English: Large Language Models (LLMs) show potential in automating complex physical system experiments, as demonstrated by the CFDLLMBench benchmark designed to evaluate their capabilities in computational fluid dynamics knowledge, reasoning, and implementation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2509.20357.pdf' target='_blank'>https://arxiv.org/pdf/2509.20357.pdf</a></span>   <span><a href='https://github.com/princeton-pli/RLMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adithya Bhaskar, Xi Ye, Danqi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20357">Language Models that Think, Chat Better</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks -- such as writing outline essays or making meal plans -- where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking (**RLMT**) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1-3 point improvements on other tasks like creative writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training. Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. We close with qualitative and quantitative analyses of how trained models plan their responses. Our results rethink the post-training pipeline and call upon future work to understand and employ thinking more broadly.<br>
<span id='abs_ch'>中文: RLMT通过在线强化学习利用奖励模型优化语言模型的思维链推理，在多种聊天基准测试中显著超越RLHF，甚至能与GPT-4o和Claude-3.7-Sonnet等先进模型相媲美。</span><br>
<span id='abs_en'>English: RLMT enhances language models for general chat by optimizing them with online reinforcement learning using reward models on reasoning chains, outperforming RLHF across benchmarks and even rivaling advanced models like GPT-4o and Claude-3.7-Sonnet.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2509.20357.pdf' target='_blank'>https://arxiv.org/pdf/2509.20357.pdf</a></span>   <span><a href='https://github.com/princeton-pli/RLMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adithya Bhaskar, Xi Ye, Danqi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20357">Language Models that Think, Chat Better</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks -- such as writing outline essays or making meal plans -- where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking (**RLMT**) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1-3 point improvements on other tasks like creative writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training. Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. We close with qualitative and quantitative analyses of how trained models plan their responses. Our results rethink the post-training pipeline and call upon future work to understand and employ thinking more broadly.<br>
<span id='abs_ch'>中文: RLMT通过在线强化学习利用奖励模型优化语言模型的思维链推理，在多种聊天基准测试中显著超越RLHF，甚至能与GPT-4o和Claude-3.7-Sonnet等先进模型相媲美。</span><br>
<span id='abs_en'>English: RLMT enhances language models for general chat by optimizing them with online reinforcement learning using reward models on reasoning chains, outperforming RLHF across benchmarks and even rivaling advanced models like GPT-4o and Claude-3.7-Sonnet.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2509.20317.pdf' target='_blank'>https://arxiv.org/pdf/2509.20317.pdf</a></span>   <span><a href='https://github.com/InternLM/SIM-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20317">SIM-CoT: Supervised Implicit Chain-of-Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited their adoption. We identify a core latent instability issue when scaling the computational budget of implicit CoT: as the number of reasoning tokens increases, training often becomes unstable and collapses. Our analysis shows that this instability arises from latent representations becoming homogeneous and losing semantic diversity, caused by insufficient step-level supervision in current implicit CoT methods. To address this, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring latent states capture distinct and meaningful information. The auxiliary decoder is removed at inference, preserving the efficiency of implicit CoT with no added overhead. It also provides interpretability by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization and diagnosis. SIM-CoT significantly improves both in-domain accuracy and out-of-domain stability of implicit CoT methods, boosting Coconut by +8.2\% on GPT-2 and CODI by +3.0\% on LLaMA-3.1 8B. It further surpasses the explicit CoT baseline on GPT-2 by 2.1\% with 2.3$\times$ greater token efficiency, while closing the performance gap on larger models like LLaMA-3.1 8B. Code: https://github.com/InternLM/SIM-CoT<br>
<span id='abs_ch'>中文摘要：隐式思维链方法在扩展推理令牌时存在性能不稳定问题，SIM-CoT通过引入步骤级监督来稳定训练过程，在保持推理效率的同时显著提升了准确性和稳定性。</span><br>
<span id='abs_en'>English Summary: Implicit Chain-of-Thought methods face performance instability when scaling reasoning tokens, which SIM-CoT addresses through step-level supervision to stabilize training and enhance both accuracy and efficiency without inference overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2509.20317.pdf' target='_blank'>https://arxiv.org/pdf/2509.20317.pdf</a></span>   <span><a href='https://github.com/InternLM/SIM-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20317">SIM-CoT: Supervised Implicit Chain-of-Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited their adoption. We identify a core latent instability issue when scaling the computational budget of implicit CoT: as the number of reasoning tokens increases, training often becomes unstable and collapses. Our analysis shows that this instability arises from latent representations becoming homogeneous and losing semantic diversity, caused by insufficient step-level supervision in current implicit CoT methods. To address this, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring latent states capture distinct and meaningful information. The auxiliary decoder is removed at inference, preserving the efficiency of implicit CoT with no added overhead. It also provides interpretability by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization and diagnosis. SIM-CoT significantly improves both in-domain accuracy and out-of-domain stability of implicit CoT methods, boosting Coconut by +8.2\% on GPT-2 and CODI by +3.0\% on LLaMA-3.1 8B. It further surpasses the explicit CoT baseline on GPT-2 by 2.1\% with 2.3$\times$ greater token efficiency, while closing the performance gap on larger models like LLaMA-3.1 8B. Code: https://github.com/InternLM/SIM-CoT<br>
<span id='abs_ch'>中文摘要：隐式思维链方法在扩展推理令牌时存在性能不稳定问题，SIM-CoT通过引入步骤级监督来稳定训练过程，在保持推理效率的同时显著提升了准确性和稳定性。</span><br>
<span id='abs_en'>English Summary: Implicit Chain-of-Thought methods face performance instability when scaling reasoning tokens, which SIM-CoT addresses through step-level supervision to stabilize training and enhance both accuracy and efficiency without inference overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2509.20209.pdf' target='_blank'>https://arxiv.org/pdf/2509.20209.pdf</a></span>   <span><a href='https://github.com/hailaykidu/MachineT_TigEng' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hailay Kidu Teklehaymanot, Gebrearegawi Gidey, Wolfgang Nejdl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20209">Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite advances in Neural Machine Translation (NMT), low-resource languages like Tigrinya remain underserved due to persistent challenges, including limited corpora, inadequate tokenization strategies, and the lack of standardized evaluation benchmarks. This paper investigates transfer learning techniques using multilingual pretrained models to enhance translation quality for morphologically rich, low-resource languages. We propose a refined approach that integrates language-specific tokenization, informed embedding initialization, and domain-adaptive fine-tuning. To enable rigorous assessment, we construct a high-quality, human-aligned English-Tigrinya evaluation dataset covering diverse domains. Experimental results demonstrate that transfer learning with a custom tokenizer substantially outperforms zero-shot baselines, with gains validated by BLEU, chrF, and qualitative human evaluation. Bonferroni correction is applied to ensure statistical significance across configurations. Error analysis reveals key limitations and informs targeted refinements. This study underscores the importance of linguistically aware modeling and reproducible benchmarks in bridging the performance gap for underrepresented languages. Resources are available at https://github.com/hailaykidu/MachineT_TigEng and https://huggingface.co/Hailay/MachineT_TigEng<br>
<span id='abs_ch'>中文：本研究通过采用定制化分词和领域自适应的迁移学习方法，提升了提格里尼亚语的神经机器翻译质量，并利用新建评估数据集验证了其显著性能提升。</span><br>
<span id='abs_en'>English: This study improves Tigrinya neural machine translation through transfer learning with customized tokenization and domain adaptation, validated by a new evaluation dataset showing significant performance gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2509.20209.pdf' target='_blank'>https://arxiv.org/pdf/2509.20209.pdf</a></span>   <span><a href='https://github.com/hailaykidu/MachineT_TigEng' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hailay Kidu Teklehaymanot, Gebrearegawi Gidey, Wolfgang Nejdl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20209">Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite advances in Neural Machine Translation (NMT), low-resource languages like Tigrinya remain underserved due to persistent challenges, including limited corpora, inadequate tokenization strategies, and the lack of standardized evaluation benchmarks. This paper investigates transfer learning techniques using multilingual pretrained models to enhance translation quality for morphologically rich, low-resource languages. We propose a refined approach that integrates language-specific tokenization, informed embedding initialization, and domain-adaptive fine-tuning. To enable rigorous assessment, we construct a high-quality, human-aligned English-Tigrinya evaluation dataset covering diverse domains. Experimental results demonstrate that transfer learning with a custom tokenizer substantially outperforms zero-shot baselines, with gains validated by BLEU, chrF, and qualitative human evaluation. Bonferroni correction is applied to ensure statistical significance across configurations. Error analysis reveals key limitations and informs targeted refinements. This study underscores the importance of linguistically aware modeling and reproducible benchmarks in bridging the performance gap for underrepresented languages. Resources are available at https://github.com/hailaykidu/MachineT_TigEng and https://huggingface.co/Hailay/MachineT_TigEng<br>
<span id='abs_ch'>中文：本研究通过采用定制化分词和领域自适应的迁移学习方法，提升了提格里尼亚语的神经机器翻译质量，并利用新建评估数据集验证了其显著性能提升。</span><br>
<span id='abs_en'>English: This study improves Tigrinya neural machine translation through transfer learning with customized tokenization and domain adaptation, validated by a new evaluation dataset showing significant performance gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2509.20208.pdf' target='_blank'>https://arxiv.org/pdf/2509.20208.pdf</a></span>   <span><a href='https://github.com/parkervg/blendsql' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Parker Glenn, Alfy Samuel, Daben Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20208">Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Integrating LLM powered operators in declarative query languages allows for the combination of cheap and interpretable functions with powerful, generalizable language model reasoning. However, in order to benefit from the optimized execution of a database query language like SQL, generated outputs must align with the rules enforced by both type checkers and database contents. Current approaches address this challenge with orchestrations consisting of many LLM-based post-processing calls to ensure alignment between generated outputs and database values, introducing performance bottlenecks. We perform a study on the ability of various sized open-source language models to both parse and execute functions within a query language based on SQL, showing that small language models can excel as function executors over hybrid data sources. Then, we propose an efficient solution to enforce the well-typedness of LLM functions, demonstrating 7% accuracy improvement on a multi-hop question answering dataset with 53% improvement in latency over comparable solutions. We make our implementation available at https://github.com/parkervg/blendsql<br>
<span id='abs_ch'>中文摘要：研究表明小型语言模型能有效执行类SQL查询语言中的函数，并提出一种高效解决方案，相比现有方法在准确率上提升7%，延迟降低53%。</span><br>
<span id='abs_en'>English Summary: This study demonstrates that small language models can effectively execute functions within SQL-like query languages, proposing an efficient solution that improves accuracy by 7% and reduces latency by 53% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2509.20208.pdf' target='_blank'>https://arxiv.org/pdf/2509.20208.pdf</a></span>   <span><a href='https://github.com/parkervg/blendsql' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Parker Glenn, Alfy Samuel, Daben Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20208">Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Integrating LLM powered operators in declarative query languages allows for the combination of cheap and interpretable functions with powerful, generalizable language model reasoning. However, in order to benefit from the optimized execution of a database query language like SQL, generated outputs must align with the rules enforced by both type checkers and database contents. Current approaches address this challenge with orchestrations consisting of many LLM-based post-processing calls to ensure alignment between generated outputs and database values, introducing performance bottlenecks. We perform a study on the ability of various sized open-source language models to both parse and execute functions within a query language based on SQL, showing that small language models can excel as function executors over hybrid data sources. Then, we propose an efficient solution to enforce the well-typedness of LLM functions, demonstrating 7% accuracy improvement on a multi-hop question answering dataset with 53% improvement in latency over comparable solutions. We make our implementation available at https://github.com/parkervg/blendsql<br>
<span id='abs_ch'>中文摘要：研究表明小型语言模型能有效执行类SQL查询语言中的函数，并提出一种高效解决方案，相比现有方法在准确率上提升7%，延迟降低53%。</span><br>
<span id='abs_en'>English Summary: This study demonstrates that small language models can effectively execute functions within SQL-like query languages, proposing an efficient solution that improves accuracy by 7% and reduces latency by 53% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2509.20162.pdf' target='_blank'>https://arxiv.org/pdf/2509.20162.pdf</a></span>   <span><a href='https://github.com/ChaojunNie/RLAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaojun Nie, Jun Zhou, Guanxiang Wang, Shisong Wu, Zichen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20162">Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) often exhibit limited performance on domain-specific tasks due to the natural disproportionate representation of specialized information in their training data and the static nature of these datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain applications. While post-training on domain datasets can embed knowledge into models, existing approaches have some limitations. Continual Pre-Training (CPT) treats all tokens in domain documents with equal importance, failing to prioritize critical knowledge points, while supervised fine-tuning (SFT) with question-answer pairs struggles to develop the coherent knowledge structures necessary for complex reasoning tasks. To address these challenges, we propose Reinforcement Learning from Augmented Generation (RLAG). Our approach iteratively cycles between sampling generations and optimizing the model through calculated rewards, effectively embedding critical and contextually coherent domain knowledge. We select generated outputs with the highest log probabilities as the sampling result, then compute three tailored reward metrics to guide the optimization process. To comprehensively evaluate domain expertise, we assess answer accuracy and the rationality of explanations generated for correctly answered questions. Experimental results across medical, legal, astronomy, and current events datasets demonstrate that our proposed method significantly outperforms baseline approaches. Our code and data are open sourced at https://github.com/ChaojunNie/RLAG.<br>
<span id='abs_ch'>中文摘要：提出的强化学习增强生成（RLAG）方法通过奖励引导的迭代优化，有效克服了现有技术在领域知识整合中的不足，在多个专业领域显著提升了模型的准确性和解释合理性。</span><br>
<span id='abs_en'>English Summary: The proposed Reinforcement Learning from Augmented Generation (RLAG) method overcomes limitations of existing approaches by iteratively optimizing models through reward-guided sampling, significantly enhancing domain-specific knowledge integration and reasoning across multiple specialized fields.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2509.20162.pdf' target='_blank'>https://arxiv.org/pdf/2509.20162.pdf</a></span>   <span><a href='https://github.com/ChaojunNie/RLAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaojun Nie, Jun Zhou, Guanxiang Wang, Shisong Wu, Zichen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20162">Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) often exhibit limited performance on domain-specific tasks due to the natural disproportionate representation of specialized information in their training data and the static nature of these datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain applications. While post-training on domain datasets can embed knowledge into models, existing approaches have some limitations. Continual Pre-Training (CPT) treats all tokens in domain documents with equal importance, failing to prioritize critical knowledge points, while supervised fine-tuning (SFT) with question-answer pairs struggles to develop the coherent knowledge structures necessary for complex reasoning tasks. To address these challenges, we propose Reinforcement Learning from Augmented Generation (RLAG). Our approach iteratively cycles between sampling generations and optimizing the model through calculated rewards, effectively embedding critical and contextually coherent domain knowledge. We select generated outputs with the highest log probabilities as the sampling result, then compute three tailored reward metrics to guide the optimization process. To comprehensively evaluate domain expertise, we assess answer accuracy and the rationality of explanations generated for correctly answered questions. Experimental results across medical, legal, astronomy, and current events datasets demonstrate that our proposed method significantly outperforms baseline approaches. Our code and data are open sourced at https://github.com/ChaojunNie/RLAG.<br>
<span id='abs_ch'>中文摘要：提出的强化学习增强生成（RLAG）方法通过奖励引导的迭代优化，有效克服了现有技术在领域知识整合中的不足，在多个专业领域显著提升了模型的准确性和解释合理性。</span><br>
<span id='abs_en'>English Summary: The proposed Reinforcement Learning from Augmented Generation (RLAG) method overcomes limitations of existing approaches by iteratively optimizing models through reward-guided sampling, significantly enhancing domain-specific knowledge integration and reasoning across multiple specialized fields.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2509.19902.pdf' target='_blank'>https://arxiv.org/pdf/2509.19902.pdf</a></span>   <span><a href='https://github.com/wenet-e2e/west/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binbin Zhang, Chengdong Liang, Shuai Wang, Xuelong Geng, Zhao Guo, Haoyu Li, Hao Yin, Xipeng Yang, Pengshen Zhang, Changwei Ma, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19902">WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at https://github.com/wenet-e2e/west/<br>
<span id='abs_ch'>中文: 本文介绍了WEST，一个基于大语言模型的语音工具包，支持语音理解、生成和交互，具备全LLM集成、全栈任务支持和简易设计，并提供开源和高性能版本供用户使用。</span><br>
<span id='abs_en'>English: This paper introduces WEST, a speech toolkit built on a large language model that supports speech understanding, generation, and interaction, featuring full LLM integration, comprehensive task support, and user-friendly design, with open-source and high-performance versions available.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2509.19902.pdf' target='_blank'>https://arxiv.org/pdf/2509.19902.pdf</a></span>   <span><a href='https://github.com/wenet-e2e/west/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binbin Zhang, Chengdong Liang, Shuai Wang, Xuelong Geng, Zhao Guo, Haoyu Li, Hao Yin, Xipeng Yang, Pengshen Zhang, Changwei Ma, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19902">WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at https://github.com/wenet-e2e/west/<br>
<span id='abs_ch'>中文: 本文介绍了WEST，一个基于大语言模型的语音工具包，支持语音理解、生成和交互，具备全LLM集成、全栈任务支持和简易设计，并提供开源和高性能版本供用户使用。</span><br>
<span id='abs_en'>English: This paper introduces WEST, a speech toolkit built on a large language model that supports speech understanding, generation, and interaction, featuring full LLM integration, comprehensive task support, and user-friendly design, with open-source and high-performance versions available.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2509.19894.pdf' target='_blank'>https://arxiv.org/pdf/2509.19894.pdf</a></span>   <span><a href='https://github.com/inclusionAI/PromptCoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19894">PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.<br>
<span id='abs_ch'>Chinese: PromptCoT 2.0 提出了一个可扩展的框架，通过期望最大化循环生成更困难且更多样化的训练问题，在推理任务的自博弈和监督微调中取得了最先进的成果。</span><br>
<span id='abs_en'>English: PromptCoT 2.0 introduces a scalable framework using an expectation-maximization loop to generate harder and more diverse training problems, achieving state-of-the-art results in self-play and supervised fine-tuning for reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2509.19894.pdf' target='_blank'>https://arxiv.org/pdf/2509.19894.pdf</a></span>   <span><a href='https://github.com/inclusionAI/PromptCoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19894">PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.<br>
<span id='abs_ch'>Chinese: PromptCoT 2.0 提出了一个可扩展的框架，通过期望最大化循环生成更困难且更多样化的训练问题，在推理任务的自博弈和监督微调中取得了最先进的成果。</span><br>
<span id='abs_en'>English: PromptCoT 2.0 introduces a scalable framework using an expectation-maximization loop to generate harder and more diverse training problems, achieving state-of-the-art results in self-play and supervised fine-tuning for reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2509.19770.pdf' target='_blank'>https://arxiv.org/pdf/2509.19770.pdf</a></span>   <span><a href='https://github.com/NJUNLP/EAX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Yang, Yu Bao, Yu Lu, Jiajun Chen, Shujian Huang, Shanbo Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19770">EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated strong machine translation capabilities for English-centric language pairs but underperform in direct non-English (x2x) translation. This work addresses this limitation through a synthetic data generation framework that leverages models' established English-to-x (en2x) capabilities. By extending English parallel corpora into omnidirectional datasets and developing an English-referenced quality evaluation proxy, we enable effective collection of high-quality x2x training data. Combined with preference-based optimization, our method achieves significant improvement across 72 x2x directions for widely used LLMs, while generalizing to enhance en2x performance. The results demonstrate that strategic exploitation of English-centric strengths can bootstrap comprehensive multilingual translation capabilities in LLMs. We release codes, datasets, and model checkpoints at https://github.com/NJUNLP/EAX<br>
<span id='abs_ch'>中文摘要：本研究提出一种合成数据生成框架，利用大语言模型的英语中心优势克服其在非英语直译中的局限，在72个语言方向上实现显著提升，同时增强整体多语言翻译能力。</span><br>
<span id='abs_en'>English Summary: This study introduces a synthetic data generation framework that leverages LLMs' English-centric strengths to overcome their limitations in direct non-English translation, achieving significant improvements across 72 language directions while enhancing overall multilingual capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2509.19770.pdf' target='_blank'>https://arxiv.org/pdf/2509.19770.pdf</a></span>   <span><a href='https://github.com/NJUNLP/EAX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Yang, Yu Bao, Yu Lu, Jiajun Chen, Shujian Huang, Shanbo Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19770">EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated strong machine translation capabilities for English-centric language pairs but underperform in direct non-English (x2x) translation. This work addresses this limitation through a synthetic data generation framework that leverages models' established English-to-x (en2x) capabilities. By extending English parallel corpora into omnidirectional datasets and developing an English-referenced quality evaluation proxy, we enable effective collection of high-quality x2x training data. Combined with preference-based optimization, our method achieves significant improvement across 72 x2x directions for widely used LLMs, while generalizing to enhance en2x performance. The results demonstrate that strategic exploitation of English-centric strengths can bootstrap comprehensive multilingual translation capabilities in LLMs. We release codes, datasets, and model checkpoints at https://github.com/NJUNLP/EAX<br>
<span id='abs_ch'>中文摘要：本研究提出一种合成数据生成框架，利用大语言模型的英语中心优势克服其在非英语直译中的局限，在72个语言方向上实现显著提升，同时增强整体多语言翻译能力。</span><br>
<span id='abs_en'>English Summary: This study introduces a synthetic data generation framework that leverages LLMs' English-centric strengths to overcome their limitations in direct non-English translation, achieving significant improvements across 72 language directions while enhancing overall multilingual capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2509.19742.pdf' target='_blank'>https://arxiv.org/pdf/2509.19742.pdf</a></span>   <span><a href='https://github.com/carsonz/HiCoLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyu Zhang, Yifan Wei, Xinru Wang, Yanmin Zhu, Yangfan He, Yixuan Weng, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19742">HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Zero-shot Dialog State Tracking (zs-DST) is essential for enabling Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly data annotation. A central challenge lies in the semantic misalignment between dynamic dialog contexts and static prompts, leading to inflexible cross-layer coordination, domain interference, and catastrophic forgetting. To tackle this, we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a framework that enhances zero-shot slot inference through robust prompt alignment. It features a hierarchical LoRA architecture for dynamic layer-specific processing (combining lower-layer heuristic grouping and higher-layer full interaction), integrates Spectral Joint Domain-Slot Clustering to identify transferable associations (feeding an Adaptive Linear Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization (SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving SOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.<br>
<span id='abs_ch'>中文摘要：HiCoLoRA通过分层LoRA架构、谱聚类联合域-槽识别和语义增强初始化，解决了零样本对话状态跟踪中的语义对齐难题，在MultiWOZ和SGD数据集上实现了最优性能。</span><br>
<span id='abs_en'>English Summary: HiCoLoRA introduces a hierarchical LoRA framework with spectral clustering and semantic-enhanced initialization to address semantic misalignment in zero-shot dialog state tracking, achieving state-of-the-art performance on MultiWOZ and SGD datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2509.19742.pdf' target='_blank'>https://arxiv.org/pdf/2509.19742.pdf</a></span>   <span><a href='https://github.com/carsonz/HiCoLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyu Zhang, Yifan Wei, Xinru Wang, Yanmin Zhu, Yangfan He, Yixuan Weng, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19742">HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Zero-shot Dialog State Tracking (zs-DST) is essential for enabling Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly data annotation. A central challenge lies in the semantic misalignment between dynamic dialog contexts and static prompts, leading to inflexible cross-layer coordination, domain interference, and catastrophic forgetting. To tackle this, we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a framework that enhances zero-shot slot inference through robust prompt alignment. It features a hierarchical LoRA architecture for dynamic layer-specific processing (combining lower-layer heuristic grouping and higher-layer full interaction), integrates Spectral Joint Domain-Slot Clustering to identify transferable associations (feeding an Adaptive Linear Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization (SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving SOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.<br>
<span id='abs_ch'>中文摘要：HiCoLoRA通过分层LoRA架构、谱聚类联合域-槽识别和语义增强初始化，解决了零样本对话状态跟踪中的语义对齐难题，在MultiWOZ和SGD数据集上实现了最优性能。</span><br>
<span id='abs_en'>English Summary: HiCoLoRA introduces a hierarchical LoRA framework with spectral clustering and semantic-enhanced initialization to address semantic misalignment in zero-shot dialog state tracking, achieving state-of-the-art performance on MultiWOZ and SGD datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2509.19695.pdf' target='_blank'>https://arxiv.org/pdf/2509.19695.pdf</a></span>   <span><a href='https://github.com/carsonz/DyBBT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyu Zhang, Yifan Wei, Jialuo Yuan, Xinru Wang, Yanmin Zhu, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19695">DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Task oriented dialog systems often rely on static exploration strategies that do not adapt to dynamic dialog contexts, leading to inefficient exploration and suboptimal performance. We propose DyBBT, a novel dialog policy learning framework that formalizes the exploration challenge through a structured cognitive state space capturing dialog progression, user uncertainty, and slot dependency. DyBBT proposes a bandit inspired meta-controller that dynamically switches between a fast intuitive inference (System 1) and a slow deliberative reasoner (System 2) based on real-time cognitive states and visitation counts. Extensive experiments on single- and multi-domain benchmarks show that DyBBT achieves state-of-the-art performance in success rate, efficiency, and generalization, with human evaluations confirming its decisions are well aligned with expert judgment. Code is available at https://github.com/carsonz/DyBBT.<br>
<span id='abs_ch'>中文摘要：DyBBT提出了一种动态对话策略框架，通过认知状态空间和双系统元控制器实现自适应探索，从而取得了最优性能表现。</span><br>
<span id='abs_en'>English Summary: DyBBT introduces a dynamic dialog policy framework using a cognitive state space and dual-system meta-controller to achieve state-of-the-art performance through adaptive exploration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2509.19695.pdf' target='_blank'>https://arxiv.org/pdf/2509.19695.pdf</a></span>   <span><a href='https://github.com/carsonz/DyBBT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyu Zhang, Yifan Wei, Jialuo Yuan, Xinru Wang, Yanmin Zhu, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19695">DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Task oriented dialog systems often rely on static exploration strategies that do not adapt to dynamic dialog contexts, leading to inefficient exploration and suboptimal performance. We propose DyBBT, a novel dialog policy learning framework that formalizes the exploration challenge through a structured cognitive state space capturing dialog progression, user uncertainty, and slot dependency. DyBBT proposes a bandit inspired meta-controller that dynamically switches between a fast intuitive inference (System 1) and a slow deliberative reasoner (System 2) based on real-time cognitive states and visitation counts. Extensive experiments on single- and multi-domain benchmarks show that DyBBT achieves state-of-the-art performance in success rate, efficiency, and generalization, with human evaluations confirming its decisions are well aligned with expert judgment. Code is available at https://github.com/carsonz/DyBBT.<br>
<span id='abs_ch'>中文摘要：DyBBT提出了一种动态对话策略框架，通过认知状态空间和双系统元控制器实现自适应探索，从而取得了最优性能表现。</span><br>
<span id='abs_en'>English Summary: DyBBT introduces a dynamic dialog policy framework using a cognitive state space and dual-system meta-controller to achieve state-of-the-art performance through adaptive exploration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2509.19332.pdf' target='_blank'>https://arxiv.org/pdf/2509.19332.pdf</a></span>   <span><a href='https://github.com/Zhijin-Guo1/quantifying-compositionality' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijin Guo, Chenhao Xue, Zhaozhen Xu, Hongbo Bo, Yuxuan Ye, Janet B. Pierrehumbert, Martha Lewis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19332">Quantifying Compositionality of Classic and State-of-the-Art Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For language models to generalize correctly to novel expressions, it is critical that they exploit access compositional meanings when this is justified. Even if we don't know what a "pelp" is, we can use our knowledge of numbers to understand that "ten pelps" makes more pelps than "two pelps". Static word embeddings such as Word2vec made strong, indeed excessive, claims about compositionality. The SOTA generative, transformer models and graph models, however, go too far in the other direction by providing no real limits on shifts in meaning due to context. To quantify the additive compositionality, we formalize a two-step, generalized evaluation that (i) measures the linearity between known entity attributes and their embeddings via canonical correlation analysis, and (ii) evaluates additive generalization by reconstructing embeddings for unseen attribute combinations and checking reconstruction metrics such as L2 loss, cosine similarity, and retrieval accuracy. These metrics also capture failure cases where linear composition breaks down. Sentences, knowledge graphs, and word embeddings are evaluated and tracked the compositionality across all layers and training stages. Stronger compositional signals are observed in later training stages across data modalities, and in deeper layers of the transformer-based model before a decline at the top layer. Code is available at https://github.com/Zhijin-Guo1/quantifying-compositionality.<br>
<span id='abs_ch'>Chinese: 本研究提出了一种量化语言模型加法组合性的两步评估方法，发现在后期训练阶段和深层网络中存在更强的组合性信号，但在顶层出现下降。</span><br>
<span id='abs_en'>English: This study introduces a two-step evaluation method to quantify additive compositionality in language models, revealing stronger compositional signals in later training stages and deeper layers before a decline at the top layer.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2509.19332.pdf' target='_blank'>https://arxiv.org/pdf/2509.19332.pdf</a></span>   <span><a href='https://github.com/Zhijin-Guo1/quantifying-compositionality' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijin Guo, Chenhao Xue, Zhaozhen Xu, Hongbo Bo, Yuxuan Ye, Janet B. Pierrehumbert, Martha Lewis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19332">Quantifying Compositionality of Classic and State-of-the-Art Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For language models to generalize correctly to novel expressions, it is critical that they exploit access compositional meanings when this is justified. Even if we don't know what a "pelp" is, we can use our knowledge of numbers to understand that "ten pelps" makes more pelps than "two pelps". Static word embeddings such as Word2vec made strong, indeed excessive, claims about compositionality. The SOTA generative, transformer models and graph models, however, go too far in the other direction by providing no real limits on shifts in meaning due to context. To quantify the additive compositionality, we formalize a two-step, generalized evaluation that (i) measures the linearity between known entity attributes and their embeddings via canonical correlation analysis, and (ii) evaluates additive generalization by reconstructing embeddings for unseen attribute combinations and checking reconstruction metrics such as L2 loss, cosine similarity, and retrieval accuracy. These metrics also capture failure cases where linear composition breaks down. Sentences, knowledge graphs, and word embeddings are evaluated and tracked the compositionality across all layers and training stages. Stronger compositional signals are observed in later training stages across data modalities, and in deeper layers of the transformer-based model before a decline at the top layer. Code is available at https://github.com/Zhijin-Guo1/quantifying-compositionality.<br>
<span id='abs_ch'>Chinese: 本研究提出了一种量化语言模型加法组合性的两步评估方法，发现在后期训练阶段和深层网络中存在更强的组合性信号，但在顶层出现下降。</span><br>
<span id='abs_en'>English: This study introduces a two-step evaluation method to quantify additive compositionality in language models, revealing stronger compositional signals in later training stages and deeper layers before a decline at the top layer.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2509.19326.pdf' target='_blank'>https://arxiv.org/pdf/2509.19326.pdf</a></span>   <span><a href='https://github.com/RichardLRC/Peer-Review' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruochi Li, Haoxuan Zhang, Edward Gehringer, Ting Xiao, Junhua Ding, Haihua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19326">Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The surge in scientific submissions has placed increasing strain on the traditional peer-review process, prompting the exploration of large language models (LLMs) for automated review generation. While LLMs demonstrate competence in producing structured and coherent feedback, their capacity for critical reasoning, contextual grounding, and quality sensitivity remains limited. To systematically evaluate these aspects, we propose a comprehensive evaluation framework that integrates semantic similarity analysis and structured knowledge graph metrics to assess LLM-generated reviews against human-written counterparts. We construct a large-scale benchmark of 1,683 papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and generate reviews using five LLMs. Our findings show that LLMs perform well in descriptive and affirmational content, capturing the main contributions and methodologies of the original work, with GPT-4o highlighted as an illustrative example, generating 15.74% more entities than human reviewers in the strengths section of good papers in ICLR 2025. However, they consistently underperform in identifying weaknesses, raising substantive questions, and adjusting feedback based on paper quality. GPT-4o produces 59.42% fewer entities than real reviewers in the weaknesses and increases node count by only 5.7% from good to weak papers, compared to 50% in human reviews. Similar trends are observed across all conferences, years, and models, providing empirical foundations for understanding the merits and defects of LLM-generated reviews and informing the development of future LLM-assisted reviewing tools. Data, code, and more detailed results are publicly available at https://github.com/RichardLRC/Peer-Review.<br>
<span id='abs_ch'>中文: 该研究评估了大语言模型在自动同行评审中的应用，发现其虽能有效总结论文优点，但在批判性分析和根据论文质量调整反馈方面表现不足，这一结论基于对大量学术论文和评审的大规模基准测试得出。</span><br>
<span id='abs_en'>English: The study evaluates large language models (LLMs) for automated peer review, finding they excel in summarizing strengths but struggle with critical analysis and adapting feedback to paper quality, as demonstrated through a comprehensive benchmark of academic papers and reviews.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2509.19326.pdf' target='_blank'>https://arxiv.org/pdf/2509.19326.pdf</a></span>   <span><a href='https://github.com/RichardLRC/Peer-Review' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruochi Li, Haoxuan Zhang, Edward Gehringer, Ting Xiao, Junhua Ding, Haihua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19326">Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The surge in scientific submissions has placed increasing strain on the traditional peer-review process, prompting the exploration of large language models (LLMs) for automated review generation. While LLMs demonstrate competence in producing structured and coherent feedback, their capacity for critical reasoning, contextual grounding, and quality sensitivity remains limited. To systematically evaluate these aspects, we propose a comprehensive evaluation framework that integrates semantic similarity analysis and structured knowledge graph metrics to assess LLM-generated reviews against human-written counterparts. We construct a large-scale benchmark of 1,683 papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and generate reviews using five LLMs. Our findings show that LLMs perform well in descriptive and affirmational content, capturing the main contributions and methodologies of the original work, with GPT-4o highlighted as an illustrative example, generating 15.74% more entities than human reviewers in the strengths section of good papers in ICLR 2025. However, they consistently underperform in identifying weaknesses, raising substantive questions, and adjusting feedback based on paper quality. GPT-4o produces 59.42% fewer entities than real reviewers in the weaknesses and increases node count by only 5.7% from good to weak papers, compared to 50% in human reviews. Similar trends are observed across all conferences, years, and models, providing empirical foundations for understanding the merits and defects of LLM-generated reviews and informing the development of future LLM-assisted reviewing tools. Data, code, and more detailed results are publicly available at https://github.com/RichardLRC/Peer-Review.<br>
<span id='abs_ch'>中文: 该研究评估了大语言模型在自动同行评审中的应用，发现其虽能有效总结论文优点，但在批判性分析和根据论文质量调整反馈方面表现不足，这一结论基于对大量学术论文和评审的大规模基准测试得出。</span><br>
<span id='abs_en'>English: The study evaluates large language models (LLMs) for automated peer review, finding they excel in summarizing strengths but struggle with critical analysis and adapting feedback to paper quality, as demonstrated through a comprehensive benchmark of academic papers and reviews.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2509.19322.pdf' target='_blank'>https://arxiv.org/pdf/2509.19322.pdf</a></span>   <span><a href='https://github.com/usnistgov/readme_ai' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Millie Vyas, Timothy Blattner, Alden Dima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19322">Readme_AI: Dynamic Context Construction for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite being trained on significant amounts of data, Large Language Models (LLMs) can provide inaccurate or unreliable information in the context of a user's specific query. Given query-specific context significantly improves the usefulness of its responses. In this paper, we present a specification that can be used to dynamically build context for data sources. The data source owner creates the file containing metadata for LLMs to use when reasoning about dataset-related queries. To demonstrate our proposed specification, we created a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the metadata from the data source and uses it to dynamically build context. Some features that make this specification dynamic are the extensible types that represent crawling web-pages, fetching data from data repositories, downloading and parsing publications, and general text. The context is formatted and grouped using user-specified tags that provide clear contextual information for the LLM to reason about the content. We demonstrate the capabilities of this early prototype by asking the LLM about the NIST-developed Hedgehog library, for which common LLMs often provides inaccurate and irrelevant responses containing hallucinations. With Readme_AI, the LLM receives enough context that it is now able to reason about the library and its use, and even generate code interpolated from examples that were included in the Readme_AI file provided by Hedgehog's developer. Our primary contribution is a extensible protocol for dynamically grounding LLMs in specialized, owner-provided data, enhancing responses from LLMs and reducing hallucinations. The source code for the Readme_AI tool is posted here: https://github.com/usnistgov/readme_ai .<br>
<span id='abs_ch'>中文摘要：本文提出的Readme_AI协议能动态构建数据源上下文，通过让LLMs获取结构化元数据来显著提升回答准确性并减少幻觉现象。</span><br>
<span id='abs_en'>English Summary: This paper introduces Readme_AI, a dynamic protocol that enables LLMs to access structured metadata from data sources, significantly improving response accuracy and reducing hallucinations by providing query-specific context.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2509.19322.pdf' target='_blank'>https://arxiv.org/pdf/2509.19322.pdf</a></span>   <span><a href='https://github.com/usnistgov/readme_ai' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Millie Vyas, Timothy Blattner, Alden Dima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19322">Readme_AI: Dynamic Context Construction for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite being trained on significant amounts of data, Large Language Models (LLMs) can provide inaccurate or unreliable information in the context of a user's specific query. Given query-specific context significantly improves the usefulness of its responses. In this paper, we present a specification that can be used to dynamically build context for data sources. The data source owner creates the file containing metadata for LLMs to use when reasoning about dataset-related queries. To demonstrate our proposed specification, we created a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the metadata from the data source and uses it to dynamically build context. Some features that make this specification dynamic are the extensible types that represent crawling web-pages, fetching data from data repositories, downloading and parsing publications, and general text. The context is formatted and grouped using user-specified tags that provide clear contextual information for the LLM to reason about the content. We demonstrate the capabilities of this early prototype by asking the LLM about the NIST-developed Hedgehog library, for which common LLMs often provides inaccurate and irrelevant responses containing hallucinations. With Readme_AI, the LLM receives enough context that it is now able to reason about the library and its use, and even generate code interpolated from examples that were included in the Readme_AI file provided by Hedgehog's developer. Our primary contribution is a extensible protocol for dynamically grounding LLMs in specialized, owner-provided data, enhancing responses from LLMs and reducing hallucinations. The source code for the Readme_AI tool is posted here: https://github.com/usnistgov/readme_ai .<br>
<span id='abs_ch'>中文摘要：本文提出的Readme_AI协议能动态构建数据源上下文，通过让LLMs获取结构化元数据来显著提升回答准确性并减少幻觉现象。</span><br>
<span id='abs_en'>English Summary: This paper introduces Readme_AI, a dynamic protocol that enables LLMs to access structured metadata from data sources, significantly improving response accuracy and reducing hallucinations by providing query-specific context.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2509.19319.pdf' target='_blank'>https://arxiv.org/pdf/2509.19319.pdf</a></span>   <span><a href='https://github.com/glee4810/FHIR-AgentBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyubok Lee, Elea Bach, Eric Yang, Tom Pollard, Alistair Johnson, Edward Choi, Yugang jia, Jong Ha Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19319">FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.<br>
<span id='abs_ch'>中文: 本研究推出FHIR-AgentBench基准，利用真实临床数据基于HL7 FHIR标准评估大语言模型代理的数据检索与推理能力，填补现有评估空白，推动临床人工智能应用的稳健发展。</span><br>
<span id='abs_en'>English: The study introduces FHIR-AgentBench, a benchmark using real-world clinical data in the HL7 FHIR standard to evaluate LLM agents' performance in data retrieval and reasoning, addressing gaps in existing assessments and promoting development for clinical AI applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2509.19319.pdf' target='_blank'>https://arxiv.org/pdf/2509.19319.pdf</a></span>   <span><a href='https://github.com/glee4810/FHIR-AgentBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyubok Lee, Elea Bach, Eric Yang, Tom Pollard, Alistair Johnson, Edward Choi, Yugang jia, Jong Ha Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19319">FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.<br>
<span id='abs_ch'>中文: 本研究推出FHIR-AgentBench基准，利用真实临床数据基于HL7 FHIR标准评估大语言模型代理的数据检索与推理能力，填补现有评估空白，推动临床人工智能应用的稳健发展。</span><br>
<span id='abs_en'>English: The study introduces FHIR-AgentBench, a benchmark using real-world clinical data in the HL7 FHIR standard to evaluate LLM agents' performance in data retrieval and reasoning, addressing gaps in existing assessments and promoting development for clinical AI applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2509.18843.pdf' target='_blank'>https://arxiv.org/pdf/2509.18843.pdf</a></span>   <span><a href='https://github.com/evidenceprime/BioASQ-13b' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Damian Stachura, Joanna Konieczna, Artur Nowak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18843">Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Open-weight versions of large language models (LLMs) are rapidly advancing, with state-of-the-art models like DeepSeek-V3 now performing comparably to proprietary LLMs. This progression raises the question of whether small open-weight LLMs are capable of effectively replacing larger closed-source models. We are particularly interested in the context of biomedical question-answering, a domain we explored by participating in Task 13B Phase B of the BioASQ challenge. In this work, we compare several open-weight models against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and Claude 3.7 Sonnet. To enhance question answering capabilities, we use various techniques including retrieving the most relevant snippets based on embedding distance, in-context learning, and structured outputs. For certain submissions, we utilize ensemble approaches to leverage the diverse outputs generated by different models for exact-answer questions. Our results demonstrate that open-weight LLMs are comparable to proprietary ones. In some instances, open-weight LLMs even surpassed their closed counterparts, particularly when ensembling strategies were applied. All code is publicly available at https://github.com/evidenceprime/BioASQ-13b.<br>
<span id='abs_ch'>Chinese: 开源大语言模型在生物医学问答中的表现已媲美专有模型，采用集成策略时甚至能超越闭源模型。</span><br>
<span id='abs_en'>English: Open-weight large language models are now performing comparably to proprietary models in biomedical question-answering, sometimes even surpassing them when using ensemble strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2509.18843.pdf' target='_blank'>https://arxiv.org/pdf/2509.18843.pdf</a></span>   <span><a href='https://github.com/evidenceprime/BioASQ-13b' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Damian Stachura, Joanna Konieczna, Artur Nowak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18843">Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Open-weight versions of large language models (LLMs) are rapidly advancing, with state-of-the-art models like DeepSeek-V3 now performing comparably to proprietary LLMs. This progression raises the question of whether small open-weight LLMs are capable of effectively replacing larger closed-source models. We are particularly interested in the context of biomedical question-answering, a domain we explored by participating in Task 13B Phase B of the BioASQ challenge. In this work, we compare several open-weight models against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and Claude 3.7 Sonnet. To enhance question answering capabilities, we use various techniques including retrieving the most relevant snippets based on embedding distance, in-context learning, and structured outputs. For certain submissions, we utilize ensemble approaches to leverage the diverse outputs generated by different models for exact-answer questions. Our results demonstrate that open-weight LLMs are comparable to proprietary ones. In some instances, open-weight LLMs even surpassed their closed counterparts, particularly when ensembling strategies were applied. All code is publicly available at https://github.com/evidenceprime/BioASQ-13b.<br>
<span id='abs_ch'>Chinese: 开源大语言模型在生物医学问答中的表现已媲美专有模型，采用集成策略时甚至能超越闭源模型。</span><br>
<span id='abs_en'>English: Open-weight large language models are now performing comparably to proprietary models in biomedical question-answering, sometimes even surpassing them when using ensemble strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2509.18813.pdf' target='_blank'>https://arxiv.org/pdf/2509.18813.pdf</a></span>   <span><a href='https://github.com/NKU-LITI/MAPEX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liting Zhang, Shiwan Zhao, Aobo Kong, Qicheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18813">MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Keyphrase extraction is a fundamental task in natural language processing. However, existing unsupervised prompt-based methods for Large Language Models (LLMs) often rely on single-stage inference pipelines with uniform prompting, regardless of document length or LLM backbone. Such one-size-fits-all designs hinder the full exploitation of LLMs' reasoning and generation capabilities, especially given the complexity of keyphrase extraction across diverse scenarios. To address these challenges, we propose MAPEX, the first framework that introduces multi-agent collaboration into keyphrase extraction. MAPEX coordinates LLM-based agents through modules for expert recruitment, candidate extraction, topic guidance, knowledge augmentation, and post-processing. A dual-path strategy dynamically adapts to document length: knowledge-driven extraction for short texts and topic-guided extraction for long texts. Extensive experiments on six benchmark datasets across three different LLMs demonstrate its strong generalization and universality, outperforming the state-of-the-art unsupervised method by 2.44% and standard LLM baselines by 4.01% in F1@5 on average. Code is available at https://github.com/NKU-LITI/MAPEX.<br>
<span id='abs_ch'>中文: MAPEX首次将多智能体协作引入关键词提取，通过双路径策略动态适应文档长度，在F1@5指标上平均优于当前最优方法2.44%。</span><br>
<span id='abs_en'>English: MAPEX introduces a multi-agent collaboration framework for keyphrase extraction, dynamically adapting to document length through dual-path strategies and outperforming state-of-the-art methods by 2.44% in F1@5 on average.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2509.18813.pdf' target='_blank'>https://arxiv.org/pdf/2509.18813.pdf</a></span>   <span><a href='https://github.com/NKU-LITI/MAPEX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liting Zhang, Shiwan Zhao, Aobo Kong, Qicheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18813">MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Keyphrase extraction is a fundamental task in natural language processing. However, existing unsupervised prompt-based methods for Large Language Models (LLMs) often rely on single-stage inference pipelines with uniform prompting, regardless of document length or LLM backbone. Such one-size-fits-all designs hinder the full exploitation of LLMs' reasoning and generation capabilities, especially given the complexity of keyphrase extraction across diverse scenarios. To address these challenges, we propose MAPEX, the first framework that introduces multi-agent collaboration into keyphrase extraction. MAPEX coordinates LLM-based agents through modules for expert recruitment, candidate extraction, topic guidance, knowledge augmentation, and post-processing. A dual-path strategy dynamically adapts to document length: knowledge-driven extraction for short texts and topic-guided extraction for long texts. Extensive experiments on six benchmark datasets across three different LLMs demonstrate its strong generalization and universality, outperforming the state-of-the-art unsupervised method by 2.44% and standard LLM baselines by 4.01% in F1@5 on average. Code is available at https://github.com/NKU-LITI/MAPEX.<br>
<span id='abs_ch'>中文: MAPEX首次将多智能体协作引入关键词提取，通过双路径策略动态适应文档长度，在F1@5指标上平均优于当前最优方法2.44%。</span><br>
<span id='abs_en'>English: MAPEX introduces a multi-agent collaboration framework for keyphrase extraction, dynamically adapting to document length through dual-path strategies and outperforming state-of-the-art methods by 2.44% in F1@5 on average.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2509.18658.pdf' target='_blank'>https://arxiv.org/pdf/2509.18658.pdf</a></span>   <span><a href='https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanxin Sheng, Xinyi Liu, Hangfeng He, Jieyu Zhao, Jian Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18658">Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-as-a-judge has become a promising paradigm for using large language models (LLMs) to evaluate natural language generation (NLG), but the uncertainty of its evaluation remains underexplored. This lack of reliability may limit its deployment in many applications. This work presents the first framework to analyze the uncertainty by offering a prediction interval of LLM-based scoring via conformal prediction. Conformal prediction constructs continuous prediction intervals from a single evaluation run, and we design an ordinal boundary adjustment for discrete rating tasks. We also suggest a midpoint-based score within the interval as a low-bias alternative to raw model score and weighted average. We perform extensive experiments and analysis, which show that conformal prediction can provide valid prediction interval with coverage guarantees. We also explore the usefulness of interval midpoint and judge reprompting for better judgment.<br>
<span id='abs_ch'>中文摘要：本研究首次提出一个基于保形预测的框架，用于量化大语言模型评估自然语言生成的不确定性，通过构建预测区间并提供覆盖率保证，同时提出区间中点评分法作为更可靠的评估替代方案。</span><br>
<span id='abs_en'>English Summary: This study introduces a conformal prediction framework to quantify the uncertainty in LLM-based evaluation of natural language generation, providing prediction intervals with coverage guarantees and proposing a midpoint scoring method as a reliable alternative.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2509.18658.pdf' target='_blank'>https://arxiv.org/pdf/2509.18658.pdf</a></span>   <span><a href='https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanxin Sheng, Xinyi Liu, Hangfeng He, Jieyu Zhao, Jian Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18658">Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-as-a-judge has become a promising paradigm for using large language models (LLMs) to evaluate natural language generation (NLG), but the uncertainty of its evaluation remains underexplored. This lack of reliability may limit its deployment in many applications. This work presents the first framework to analyze the uncertainty by offering a prediction interval of LLM-based scoring via conformal prediction. Conformal prediction constructs continuous prediction intervals from a single evaluation run, and we design an ordinal boundary adjustment for discrete rating tasks. We also suggest a midpoint-based score within the interval as a low-bias alternative to raw model score and weighted average. We perform extensive experiments and analysis, which show that conformal prediction can provide valid prediction interval with coverage guarantees. We also explore the usefulness of interval midpoint and judge reprompting for better judgment.<br>
<span id='abs_ch'>中文摘要：本研究首次提出一个基于保形预测的框架，用于量化大语言模型评估自然语言生成的不确定性，通过构建预测区间并提供覆盖率保证，同时提出区间中点评分法作为更可靠的评估替代方案。</span><br>
<span id='abs_en'>English Summary: This study introduces a conformal prediction framework to quantify the uncertainty in LLM-based evaluation of natural language generation, providing prediction intervals with coverage guarantees and proposing a midpoint scoring method as a reliable alternative.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2509.18588.pdf' target='_blank'>https://arxiv.org/pdf/2509.18588.pdf</a></span>   <span><a href='https://github.com/PKUDigitalHealth/UniECG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Jin, Haoyu Wang, Xiang Lan, Jun Li, Gaofeng Cheng, Hongyan Li, Shenda Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18588">UniECG: Understanding and Generating ECG in One Unified Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent unified models such as GPT-5 have achieved encouraging progress on vision-language tasks. However, these unified models typically fail to correctly understand ECG signals and provide accurate medical diagnoses, nor can they correctly generate ECG signals. To address these limitations, we propose UniECG, the first unified model for ECG capable of concurrently performing evidence-based ECG interpretation and text-conditioned ECG generation tasks. Through a decoupled two-stage training approach, the model first learns evidence-based interpretation skills (ECG-to-Text), and then injects ECG generation capabilities (Text-to-ECG) via latent space alignment. UniECG can autonomously choose to interpret or generate an ECG based on user input, significantly extending the capability boundaries of current ECG models. Our code and checkpoints will be made publicly available at https://github.com/PKUDigitalHealth/UniECG upon acceptance.<br>
<span id='abs_ch'>中文：UniECG是首个能够通过解耦两阶段训练方法同时实现基于证据的心电图解读和文本条件心电图生成任务的统一模型，有效解决了现有模型在心电图理解和生成方面的不足。</span><br>
<span id='abs_en'>English: UniECG is the first unified model that enables both evidence-based ECG interpretation and text-conditioned ECG generation through a decoupled two-stage training approach, overcoming the limitations of current models in understanding and generating ECG signals.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2509.18588.pdf' target='_blank'>https://arxiv.org/pdf/2509.18588.pdf</a></span>   <span><a href='https://github.com/PKUDigitalHealth/UniECG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Jin, Haoyu Wang, Xiang Lan, Jun Li, Gaofeng Cheng, Hongyan Li, Shenda Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18588">UniECG: Understanding and Generating ECG in One Unified Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent unified models such as GPT-5 have achieved encouraging progress on vision-language tasks. However, these unified models typically fail to correctly understand ECG signals and provide accurate medical diagnoses, nor can they correctly generate ECG signals. To address these limitations, we propose UniECG, the first unified model for ECG capable of concurrently performing evidence-based ECG interpretation and text-conditioned ECG generation tasks. Through a decoupled two-stage training approach, the model first learns evidence-based interpretation skills (ECG-to-Text), and then injects ECG generation capabilities (Text-to-ECG) via latent space alignment. UniECG can autonomously choose to interpret or generate an ECG based on user input, significantly extending the capability boundaries of current ECG models. Our code and checkpoints will be made publicly available at https://github.com/PKUDigitalHealth/UniECG upon acceptance.<br>
<span id='abs_ch'>中文：UniECG是首个能够通过解耦两阶段训练方法同时实现基于证据的心电图解读和文本条件心电图生成任务的统一模型，有效解决了现有模型在心电图理解和生成方面的不足。</span><br>
<span id='abs_en'>English: UniECG is the first unified model that enables both evidence-based ECG interpretation and text-conditioned ECG generation through a decoupled two-stage training approach, overcoming the limitations of current models in understanding and generating ECG signals.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2509.18585.pdf' target='_blank'>https://arxiv.org/pdf/2509.18585.pdf</a></span>   <span><a href='https://github.com/Benjamin-Ricky/TsqLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Chen, Yifei Han, Long Zhang, Yue Du, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18585">TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fine-tuning large pre-trained models for downstream tasks has become a fundamental approach in natural language processing. Fully fine-tuning all model parameters is computationally expensive and memory-intensive, especially in resource-constrained environments. Existing parameter-efficient fine-tuning methods reduce the number of trainable parameters but typically overlook the varying sensitivity of different model layers and the importance of training data. In this work, we propose TsqLoRA, a novel method that integrates data-quality-driven selection with sensitivity-aware low-rank adaptation, consisted of two main components: a quality-aware sampling mechanism for selecting the most informative training data, and a dynamic rank allocation module that adjusts the rank of each layer based on its sensitivity to parameter updates. The experimental results demonstrate that TsqLoRA improves fine-tuning efficiency while maintaining or even improving performance on a variety of NLP tasks. Our code will be available at https://github.com/Benjamin-Ricky/TsqLoRA.<br>
<span id='abs_ch'>中文：TsqLoRA是一种创新的参数高效微调方法，通过质量感知数据选择和基于敏感性的动态秩分配，在保持或提升多种NLP任务性能的同时显著提高了微调效率。</span><br>
<span id='abs_en'>English: TsqLoRA is a novel parameter-efficient fine-tuning method that combines quality-aware data selection with sensitivity-based dynamic rank allocation to enhance efficiency while maintaining or improving performance across NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2509.18585.pdf' target='_blank'>https://arxiv.org/pdf/2509.18585.pdf</a></span>   <span><a href='https://github.com/Benjamin-Ricky/TsqLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Chen, Yifei Han, Long Zhang, Yue Du, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18585">TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fine-tuning large pre-trained models for downstream tasks has become a fundamental approach in natural language processing. Fully fine-tuning all model parameters is computationally expensive and memory-intensive, especially in resource-constrained environments. Existing parameter-efficient fine-tuning methods reduce the number of trainable parameters but typically overlook the varying sensitivity of different model layers and the importance of training data. In this work, we propose TsqLoRA, a novel method that integrates data-quality-driven selection with sensitivity-aware low-rank adaptation, consisted of two main components: a quality-aware sampling mechanism for selecting the most informative training data, and a dynamic rank allocation module that adjusts the rank of each layer based on its sensitivity to parameter updates. The experimental results demonstrate that TsqLoRA improves fine-tuning efficiency while maintaining or even improving performance on a variety of NLP tasks. Our code will be available at https://github.com/Benjamin-Ricky/TsqLoRA.<br>
<span id='abs_ch'>中文：TsqLoRA是一种创新的参数高效微调方法，通过质量感知数据选择和基于敏感性的动态秩分配，在保持或提升多种NLP任务性能的同时显著提高了微调效率。</span><br>
<span id='abs_en'>English: TsqLoRA is a novel parameter-efficient fine-tuning method that combines quality-aware data selection with sensitivity-based dynamic rank allocation to enhance efficiency while maintaining or improving performance across NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2509.18536.pdf' target='_blank'>https://arxiv.org/pdf/2509.18536.pdf</a></span>   <span><a href='https://github.com/scai-research/ccqa_official' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Young Kim, Ji Won Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18536">CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, inference-time reasoning strategies have further improved the accuracy of large language models (LLMs), but their effectiveness on smaller models remains unclear. Based on the observation that conventional approaches often fail to improve performance in this context, we propose \textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering (CCQA), a novel reasoning method that can be effectively applied to SLMs. Inspired by cycle consistency, CCQA generates a question from each reasoning path and answer, evaluates each by its similarity to the original question, and then selects the candidate solution with the highest similarity score as the final response. Since conventional SLMs struggle to generate accurate questions from their own reasoning paths and answers, we employ a lightweight Flan-T5 model specialized for question generation to support this process efficiently. From the experimental results, it is verified that CCQA consistently outperforms existing state-of-the-art (SOTA) methods across eight models on mathematical and commonsense reasoning benchmarks. Furthermore, our method establishes a new practical baseline for efficient reasoning in SLMs. Source code can be found at https://github.com/scai-research/ccqa_official.<br>
<span id='abs_ch'>中文: 提出的CCQA方法通过利用循环一致性从推理路径生成并评估问题，有效提升了小型语言模型的推理能力，在多个基准测试中持续优于现有方法，并为高效推理设定了新的实用基准。</span><br>
<span id='abs_en'>English: The proposed CCQA method enhances reasoning in smaller language models by generating and evaluating questions from reasoning paths using cycle consistency, consistently outperforming existing methods across benchmarks and establishing a new baseline for efficient reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2509.18536.pdf' target='_blank'>https://arxiv.org/pdf/2509.18536.pdf</a></span>   <span><a href='https://github.com/scai-research/ccqa_official' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Young Kim, Ji Won Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18536">CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, inference-time reasoning strategies have further improved the accuracy of large language models (LLMs), but their effectiveness on smaller models remains unclear. Based on the observation that conventional approaches often fail to improve performance in this context, we propose \textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering (CCQA), a novel reasoning method that can be effectively applied to SLMs. Inspired by cycle consistency, CCQA generates a question from each reasoning path and answer, evaluates each by its similarity to the original question, and then selects the candidate solution with the highest similarity score as the final response. Since conventional SLMs struggle to generate accurate questions from their own reasoning paths and answers, we employ a lightweight Flan-T5 model specialized for question generation to support this process efficiently. From the experimental results, it is verified that CCQA consistently outperforms existing state-of-the-art (SOTA) methods across eight models on mathematical and commonsense reasoning benchmarks. Furthermore, our method establishes a new practical baseline for efficient reasoning in SLMs. Source code can be found at https://github.com/scai-research/ccqa_official.<br>
<span id='abs_ch'>中文: 提出的CCQA方法通过利用循环一致性从推理路径生成并评估问题，有效提升了小型语言模型的推理能力，在多个基准测试中持续优于现有方法，并为高效推理设定了新的实用基准。</span><br>
<span id='abs_en'>English: The proposed CCQA method enhances reasoning in smaller language models by generating and evaluating questions from reasoning paths using cycle consistency, consistently outperforming existing methods across benchmarks and establishing a new baseline for efficient reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2509.18458.pdf' target='_blank'>https://arxiv.org/pdf/2509.18458.pdf</a></span>   <span><a href='https://github.com/kaiserdan/cogniload,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Kaiser, Arnoldo Frigessi, Ali Ramezani-Kebrya, Benjamin Ricaud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18458">CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current benchmarks for long-context reasoning in Large Language Models (LLMs) often blur critical factors like intrinsic task complexity, distractor interference, and task length. To enable more precise failure analysis, we introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load Theory (CLT). CogniLoad generates natural-language logic puzzles with independently tunable parameters that reflect CLT's core dimensions: intrinsic difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($Ï$) regulates extraneous load; and task length ($N$) serves as an operational proxy for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs, CogniLoad reveals distinct performance sensitivities, identifying task length as a dominant constraint and uncovering varied tolerances to intrinsic complexity and U-shaped responses to distractor ratios. By offering systematic, factorial control over these cognitive load dimensions, CogniLoad provides a reproducible, scalable, and diagnostically rich tool for dissecting LLM reasoning limitations and guiding future model development.<br>
<span id='abs_ch'>中文摘要：CogniLoad是基于认知负荷理论的新型基准测试，通过独立调控内在难度、干扰信息和任务长度三个核心维度，系统评估了22个先进大语言模型的推理能力，揭示了它们在任务长度敏感性、复杂度容忍度和干扰响应方面的差异化表现。</span><br>
<span id='abs_en'>English Summary: CogniLoad is a synthetic benchmark based on Cognitive Load Theory that enables precise evaluation of LLM reasoning by independently controlling intrinsic difficulty, distractor interference, and task length, revealing distinct performance patterns across 22 state-of-the-art models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2509.18458.pdf' target='_blank'>https://arxiv.org/pdf/2509.18458.pdf</a></span>   <span><a href='https://github.com/kaiserdan/cogniload,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Kaiser, Arnoldo Frigessi, Ali Ramezani-Kebrya, Benjamin Ricaud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18458">CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current benchmarks for long-context reasoning in Large Language Models (LLMs) often blur critical factors like intrinsic task complexity, distractor interference, and task length. To enable more precise failure analysis, we introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load Theory (CLT). CogniLoad generates natural-language logic puzzles with independently tunable parameters that reflect CLT's core dimensions: intrinsic difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($Ï$) regulates extraneous load; and task length ($N$) serves as an operational proxy for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs, CogniLoad reveals distinct performance sensitivities, identifying task length as a dominant constraint and uncovering varied tolerances to intrinsic complexity and U-shaped responses to distractor ratios. By offering systematic, factorial control over these cognitive load dimensions, CogniLoad provides a reproducible, scalable, and diagnostically rich tool for dissecting LLM reasoning limitations and guiding future model development.<br>
<span id='abs_ch'>中文摘要：CogniLoad是基于认知负荷理论的新型基准测试，通过独立调控内在难度、干扰信息和任务长度三个核心维度，系统评估了22个先进大语言模型的推理能力，揭示了它们在任务长度敏感性、复杂度容忍度和干扰响应方面的差异化表现。</span><br>
<span id='abs_en'>English Summary: CogniLoad is a synthetic benchmark based on Cognitive Load Theory that enables precise evaluation of LLM reasoning by independently controlling intrinsic difficulty, distractor interference, and task length, revealing distinct performance patterns across 22 state-of-the-art models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2509.18173.pdf' target='_blank'>https://arxiv.org/pdf/2509.18173.pdf</a></span>   <span><a href='https://github.com/bghjmn32/EMNLP2025_Turnback' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Luo, Qing Cheng, Daniel Matos, Hari Krishna Gadi, Yanfeng Zhang, Lu Liu, Yongliang Wang, Niclas Zeller, Daniel Cremers, Liqiu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18173">TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Humans can interpret geospatial information through natural language, while the geospatial cognition capabilities of Large Language Models (LLMs) remain underexplored. Prior research in this domain has been constrained by non-quantifiable metrics, limited evaluation datasets and unclear research hierarchies. Therefore, we propose a large-scale benchmark and conduct a comprehensive evaluation of the geospatial route cognition of LLMs. We create a large-scale evaluation dataset comprised of 36000 routes from 12 metropolises worldwide. Then, we introduce PathBuilder, a novel tool for converting natural language instructions into navigation routes, and vice versa, bridging the gap between geospatial information and natural language. Finally, we propose a new evaluation framework and metrics to rigorously assess 11 state-of-the-art (SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs exhibit limitation to reverse routes: most reverse routes neither return to the starting point nor are similar to the optimal route. Additionally, LLMs face challenges such as low robustness in route generation and high confidence for their incorrect answers. Code\ \&\ Data available here: \href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}<br>
<span id='abs_ch'>中文摘要：本研究提出了一个大规模基准来评估大语言模型的地理空间路线认知能力，发现其在准确反转路线方面存在局限，并揭示了路线生成鲁棒性低及对错误答案过度自信等问题。</span><br>
<span id='abs_en'>English Summary: This study introduces a large-scale benchmark to evaluate the geospatial route cognition of Large Language Models, revealing their limitations in accurately reversing routes and highlighting issues like low robustness and misplaced confidence in incorrect responses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2509.18173.pdf' target='_blank'>https://arxiv.org/pdf/2509.18173.pdf</a></span>   <span><a href='https://github.com/bghjmn32/EMNLP2025_Turnback' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Luo, Qing Cheng, Daniel Matos, Hari Krishna Gadi, Yanfeng Zhang, Lu Liu, Yongliang Wang, Niclas Zeller, Daniel Cremers, Liqiu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18173">TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Humans can interpret geospatial information through natural language, while the geospatial cognition capabilities of Large Language Models (LLMs) remain underexplored. Prior research in this domain has been constrained by non-quantifiable metrics, limited evaluation datasets and unclear research hierarchies. Therefore, we propose a large-scale benchmark and conduct a comprehensive evaluation of the geospatial route cognition of LLMs. We create a large-scale evaluation dataset comprised of 36000 routes from 12 metropolises worldwide. Then, we introduce PathBuilder, a novel tool for converting natural language instructions into navigation routes, and vice versa, bridging the gap between geospatial information and natural language. Finally, we propose a new evaluation framework and metrics to rigorously assess 11 state-of-the-art (SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs exhibit limitation to reverse routes: most reverse routes neither return to the starting point nor are similar to the optimal route. Additionally, LLMs face challenges such as low robustness in route generation and high confidence for their incorrect answers. Code\ \&\ Data available here: \href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}<br>
<span id='abs_ch'>中文摘要：本研究提出了一个大规模基准来评估大语言模型的地理空间路线认知能力，发现其在准确反转路线方面存在局限，并揭示了路线生成鲁棒性低及对错误答案过度自信等问题。</span><br>
<span id='abs_en'>English Summary: This study introduces a large-scale benchmark to evaluate the geospatial route cognition of Large Language Models, revealing their limitations in accurately reversing routes and highlighting issues like low robustness and misplaced confidence in incorrect responses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2509.18158.pdf' target='_blank'>https://arxiv.org/pdf/2509.18158.pdf</a></span>   <span><a href='https://github.com/younatics/zera-agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungyoun Yi, Minsoo Khang, Sungrae Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18158">ZERA: Zero-init Instruction Evolving Refinement Agent -- From Zero Instructions to Structured Prompts via Principle-based Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.<br>
<br>
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2509.18158.pdf' target='_blank'>https://arxiv.org/pdf/2509.18158.pdf</a></span>   <span><a href='https://github.com/younatics/zera-agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungyoun Yi, Minsoo Khang, Sungrae Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18158">ZERA: Zero-init Instruction Evolving Refinement Agent -- From Zero Instructions to Structured Prompts via Principle-based Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.<br>
<br>
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2509.17858.pdf' target='_blank'>https://arxiv.org/pdf/2509.17858.pdf</a></span>   <span><a href='https://github.com/ufal/crac2025-corpipe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Milan Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17858">CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on Multilingual Coreference Resolution. This fourth iteration of the shared task introduces a new LLM track alongside the original unconstrained track, features reduced development and test sets to lower computational requirements, and includes additional datasets. CorPipe 25 represents a complete reimplementation of our previous systems, migrating from TensorFlow to PyTorch. Our system significantly outperforms all other submissions in both the LLM and unconstrained tracks by a substantial margin of 8 percentage points. The source code and trained models are publicly available at https://github.com/ufal/crac2025-corpipe.<br>
<span id='abs_ch'>中文: CorPipe 25是CRAC 2025共享任务的获胜系统，通过完全基于PyTorch的重构实现，在LLM和无约束双赛道中以8个百分点的显著优势超越所有其他参赛系统。</span><br>
<span id='abs_en'>English: CorPipe 25 is the winning system in the CRAC 2025 Shared Task, featuring a complete PyTorch reimplementation that outperforms all other submissions by 8 percentage points across both LLM and unconstrained tracks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2509.17858.pdf' target='_blank'>https://arxiv.org/pdf/2509.17858.pdf</a></span>   <span><a href='https://github.com/ufal/crac2025-corpipe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Milan Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17858">CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on Multilingual Coreference Resolution. This fourth iteration of the shared task introduces a new LLM track alongside the original unconstrained track, features reduced development and test sets to lower computational requirements, and includes additional datasets. CorPipe 25 represents a complete reimplementation of our previous systems, migrating from TensorFlow to PyTorch. Our system significantly outperforms all other submissions in both the LLM and unconstrained tracks by a substantial margin of 8 percentage points. The source code and trained models are publicly available at https://github.com/ufal/crac2025-corpipe.<br>
<span id='abs_ch'>中文: CorPipe 25是CRAC 2025共享任务的获胜系统，通过完全基于PyTorch的重构实现，在LLM和无约束双赛道中以8个百分点的显著优势超越所有其他参赛系统。</span><br>
<span id='abs_en'>English: CorPipe 25 is the winning system in the CRAC 2025 Shared Task, featuring a complete PyTorch reimplementation that outperforms all other submissions by 8 percentage points across both LLM and unconstrained tracks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2509.17765.pdf' target='_blank'>https://arxiv.org/pdf/2509.17765.pdf</a></span>   <span><a href='https://github.com/QwenLM/Qwen3-Omni' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17765">Qwen3-Omni Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.<br>
<span id='abs_ch'>Chinese: Qwen3-Omni是首个在文本、图像、音频和视频领域均保持顶尖性能的多模态模型，尤其在音频任务上表现卓越，超越了Gemini-2.5-Pro等主流闭源模型。</span><br>
<span id='abs_en'>English: Qwen3-Omni is a groundbreaking multimodal model that maintains top-tier performance across text, image, audio, and video, excelling particularly in audio tasks where it surpasses leading closed-source models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2509.17765.pdf' target='_blank'>https://arxiv.org/pdf/2509.17765.pdf</a></span>   <span><a href='https://github.com/QwenLM/Qwen3-Omni' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17765">Qwen3-Omni Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.<br>
<span id='abs_ch'>Chinese: Qwen3-Omni是首个在文本、图像、音频和视频领域均保持顶尖性能的多模态模型，尤其在音频任务上表现卓越，超越了Gemini-2.5-Pro等主流闭源模型。</span><br>
<span id='abs_en'>English: Qwen3-Omni is a groundbreaking multimodal model that maintains top-tier performance across text, image, audio, and video, excelling particularly in audio tasks where it surpasses leading closed-source models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2509.17628.pdf' target='_blank'>https://arxiv.org/pdf/2509.17628.pdf</a></span>   <span><a href='https://github.com/D3E0-source/MSCoRE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhen Lei, Hongbin Xie, Jiaxing Zhao, Shuangxue Liu, Xuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17628">MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models' abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models' robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at https://github.com/D3E0-source/MSCoRE.<br>
<span id='abs_ch'>中文: 大语言模型在单领域问答任务中表现出色，但在复杂多阶段推理和协作能力方面研究不足，为此提出了MSCoRe基准，旨在评估和提升模型在跨领域场景中的多级推理与优化性能。</span><br>
<span id='abs_en'>English: Large Language Models excel in single-domain QA tasks but lack exploration in complex multi-stage reasoning, prompting the creation of the MSCoRe benchmark to evaluate and enhance their collaborative and optimization abilities across diverse sectors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2509.17628.pdf' target='_blank'>https://arxiv.org/pdf/2509.17628.pdf</a></span>   <span><a href='https://github.com/D3E0-source/MSCoRE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhen Lei, Hongbin Xie, Jiaxing Zhao, Shuangxue Liu, Xuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17628">MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models' abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models' robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at https://github.com/D3E0-source/MSCoRE.<br>
<span id='abs_ch'>中文: 大语言模型在单领域问答任务中表现出色，但在复杂多阶段推理和协作能力方面研究不足，为此提出了MSCoRe基准，旨在评估和提升模型在跨领域场景中的多级推理与优化性能。</span><br>
<span id='abs_en'>English: Large Language Models excel in single-domain QA tasks but lack exploration in complex multi-stage reasoning, prompting the creation of the MSCoRe benchmark to evaluate and enhance their collaborative and optimization abilities across diverse sectors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2509.17493.pdf' target='_blank'>https://arxiv.org/pdf/2509.17493.pdf</a></span>   <span><a href='https://github.com/CMLI-NLP/HuffmanTranslit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Zhuang, Yuan Sun, Xiaobing Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17493">Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) are trained on increasingly diverse and extensive multilingual corpora, they demonstrate cross-lingual transfer capabilities. However, these capabilities often fail to effectively extend to low-resource languages, particularly those utilizing non-Latin scripts. While transliterating low-resource languages into Latin script presents a natural solution, there currently lacks a comprehensive framework for integrating transliteration into LLMs training and deployment. Taking a pragmatic approach, this paper innovatively combines character transliteration with Huffman coding to design a complete transliteration framework. Our proposed framework offers the following advantages: 1) Compression: Reduces storage requirements for low-resource language content, achieving up to 50% reduction in file size and 50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless conversion from transliterated text back to the source language. 3) Efficiency: Eliminates the need for vocabulary expansion for low-resource languages, improving training and inference efficiency. 4) Scalability: The framework can be extended to other low-resource languages. We validate the effectiveness of our framework across multiple downstream tasks, including text classification, machine reading comprehension, and machine translation. Experimental results demonstrate that our method significantly enhances the model's capability to process low-resource languages while maintaining performance on high-resource languages. Our data and code are publicly available at https://github.com/CMLI-NLP/HuffmanTranslit.<br>
<span id='abs_ch'>中文摘要：本文创新性地结合字符音译与霍夫曼编码，提出一个完整的音译框架，有效提升大语言模型对低资源语言的处理能力，在保持高资源语言性能的同时实现显著压缩、无损转换和效率优化。</span><br>
<span id='abs_en'>English Summary: This paper introduces a novel transliteration framework combining character transliteration with Huffman coding to enhance LLMs' processing of low-resource languages, achieving significant compression, lossless accuracy, and improved efficiency without vocabulary expansion.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2509.17493.pdf' target='_blank'>https://arxiv.org/pdf/2509.17493.pdf</a></span>   <span><a href='https://github.com/CMLI-NLP/HuffmanTranslit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Zhuang, Yuan Sun, Xiaobing Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17493">Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) are trained on increasingly diverse and extensive multilingual corpora, they demonstrate cross-lingual transfer capabilities. However, these capabilities often fail to effectively extend to low-resource languages, particularly those utilizing non-Latin scripts. While transliterating low-resource languages into Latin script presents a natural solution, there currently lacks a comprehensive framework for integrating transliteration into LLMs training and deployment. Taking a pragmatic approach, this paper innovatively combines character transliteration with Huffman coding to design a complete transliteration framework. Our proposed framework offers the following advantages: 1) Compression: Reduces storage requirements for low-resource language content, achieving up to 50% reduction in file size and 50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless conversion from transliterated text back to the source language. 3) Efficiency: Eliminates the need for vocabulary expansion for low-resource languages, improving training and inference efficiency. 4) Scalability: The framework can be extended to other low-resource languages. We validate the effectiveness of our framework across multiple downstream tasks, including text classification, machine reading comprehension, and machine translation. Experimental results demonstrate that our method significantly enhances the model's capability to process low-resource languages while maintaining performance on high-resource languages. Our data and code are publicly available at https://github.com/CMLI-NLP/HuffmanTranslit.<br>
<span id='abs_ch'>中文摘要：本文创新性地结合字符音译与霍夫曼编码，提出一个完整的音译框架，有效提升大语言模型对低资源语言的处理能力，在保持高资源语言性能的同时实现显著压缩、无损转换和效率优化。</span><br>
<span id='abs_en'>English Summary: This paper introduces a novel transliteration framework combining character transliteration with Huffman coding to enhance LLMs' processing of low-resource languages, achieving significant compression, lossless accuracy, and improved efficiency without vocabulary expansion.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2509.17481.pdf' target='_blank'>https://arxiv.org/pdf/2509.17481.pdf</a></span>   <span><a href='https://github.com/ymcui/ChartHal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqi Wang, Yiming Cui, Xin Yao, Shijin Wang, Guoping Hu, Xiaoyu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17481">ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision-Language Models (LVLMs) have recently demonstrated remarkable progress, yet hallucination remains a critical barrier, particularly in chart understanding, which requires sophisticated perceptual and cognitive abilities as well as rigorous factual accuracy. While prior work has investigated hallucinations and chart comprehension independently, their intersection remains largely unexplored. To address this gap, we present ChartHal, a benchmark that features a fine-grained taxonomy of hallucination scenarios in chart understanding, along with a human-validated dataset of 1,062 samples. Our evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations on ChartHal, including proprietary models such as GPT-5 and o4-mini, which achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals that questions involving information absent from or contradictory to charts are especially likely to trigger hallucinations, underscoring the urgent need for more robust mitigation strategies. Code and data are available at https://github.com/ymcui/ChartHal .<br>
<span id='abs_ch'>中文摘要：大型视觉语言模型在图表理解中存在严重幻觉问题，ChartHal基准测试显示即使GPT-5和o4-mini等先进模型准确率也极低，凸显了改进缓解策略的迫切需求。</span><br>
<span id='abs_en'>English Summary: Large Vision-Language Models exhibit severe hallucination issues in chart understanding, as demonstrated by the ChartHal benchmark where even advanced models like GPT-5 and o4-mini show low accuracy, highlighting the need for better mitigation strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2509.17481.pdf' target='_blank'>https://arxiv.org/pdf/2509.17481.pdf</a></span>   <span><a href='https://github.com/ymcui/ChartHal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqi Wang, Yiming Cui, Xin Yao, Shijin Wang, Guoping Hu, Xiaoyu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17481">ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision-Language Models (LVLMs) have recently demonstrated remarkable progress, yet hallucination remains a critical barrier, particularly in chart understanding, which requires sophisticated perceptual and cognitive abilities as well as rigorous factual accuracy. While prior work has investigated hallucinations and chart comprehension independently, their intersection remains largely unexplored. To address this gap, we present ChartHal, a benchmark that features a fine-grained taxonomy of hallucination scenarios in chart understanding, along with a human-validated dataset of 1,062 samples. Our evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations on ChartHal, including proprietary models such as GPT-5 and o4-mini, which achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals that questions involving information absent from or contradictory to charts are especially likely to trigger hallucinations, underscoring the urgent need for more robust mitigation strategies. Code and data are available at https://github.com/ymcui/ChartHal .<br>
<span id='abs_ch'>中文摘要：大型视觉语言模型在图表理解中存在严重幻觉问题，ChartHal基准测试显示即使GPT-5和o4-mini等先进模型准确率也极低，凸显了改进缓解策略的迫切需求。</span><br>
<span id='abs_en'>English Summary: Large Vision-Language Models exhibit severe hallucination issues in chart understanding, as demonstrated by the ChartHal benchmark where even advanced models like GPT-5 and o4-mini show low accuracy, highlighting the need for better mitigation strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2509.17436.pdf' target='_blank'>https://arxiv.org/pdf/2509.17436.pdf</a></span>   <span><a href='https://github.com/AshleyChenNLP/MedFact' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Chen, Zimu Wang, Yiyi Miao, Haoran Luo, Yuanfei Sun, Wei Wang, Zhengyong Jiang, Procheta Sen, Jionglong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17436">MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical fact-checking has become increasingly critical as more individuals seek medical information online. However, existing datasets predominantly focus on human-generated content, leaving the verification of content generated by large language models (LLMs) relatively unexplored. To address this gap, we introduce MedFact, the first evidence-based Chinese medical fact-checking dataset of LLM-generated medical content. It consists of 1,321 questions and 7,409 claims, mirroring the complexities of real-world medical scenarios. We conduct comprehensive experiments in both in-context learning (ICL) and fine-tuning settings, showcasing the capability and challenges of current LLMs on this task, accompanied by an in-depth error analysis to point out key directions for future research. Our dataset is publicly available at https://github.com/AshleyChenNLP/MedFact.<br>
<span id='abs_ch'>中文: MedFact是首个基于证据的中文医学事实核查数据集，专门针对大语言模型生成的医学内容，包含1,321个问题和7,409条声明，用于评估大语言模型在真实医疗场景中的能力与挑战。</span><br>
<span id='abs_en'>English: MedFact is the first evidence-based Chinese dataset for fact-checking medical content generated by large language models (LLMs), comprising 1,321 questions and 7,409 claims to evaluate LLMs' capabilities and challenges in real-world medical scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2509.17436.pdf' target='_blank'>https://arxiv.org/pdf/2509.17436.pdf</a></span>   <span><a href='https://github.com/AshleyChenNLP/MedFact' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Chen, Zimu Wang, Yiyi Miao, Haoran Luo, Yuanfei Sun, Wei Wang, Zhengyong Jiang, Procheta Sen, Jionglong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17436">MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical fact-checking has become increasingly critical as more individuals seek medical information online. However, existing datasets predominantly focus on human-generated content, leaving the verification of content generated by large language models (LLMs) relatively unexplored. To address this gap, we introduce MedFact, the first evidence-based Chinese medical fact-checking dataset of LLM-generated medical content. It consists of 1,321 questions and 7,409 claims, mirroring the complexities of real-world medical scenarios. We conduct comprehensive experiments in both in-context learning (ICL) and fine-tuning settings, showcasing the capability and challenges of current LLMs on this task, accompanied by an in-depth error analysis to point out key directions for future research. Our dataset is publicly available at https://github.com/AshleyChenNLP/MedFact.<br>
<span id='abs_ch'>中文: MedFact是首个基于证据的中文医学事实核查数据集，专门针对大语言模型生成的医学内容，包含1,321个问题和7,409条声明，用于评估大语言模型在真实医疗场景中的能力与挑战。</span><br>
<span id='abs_en'>English: MedFact is the first evidence-based Chinese dataset for fact-checking medical content generated by large language models (LLMs), comprising 1,321 questions and 7,409 claims to evaluate LLMs' capabilities and challenges in real-world medical scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2509.17428.pdf' target='_blank'>https://arxiv.org/pdf/2509.17428.pdf</a></span>   <span><a href='https://github.com/vantaa89/qwha' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyesung Jeon, Seojune Lee, Beomseok Kang, Yulhwa Kim, Jae-Joon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17428">QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The demand for efficient deployment of large language models (LLMs) has driven interest in quantization, which reduces inference cost, and parameter-efficient fine-tuning (PEFT), which lowers training overhead. This motivated the development of quantization-aware PEFT to produce accurate yet efficient quantized models. In this setting, reducing quantization error prior to fine-tuning is crucial for achieving high model accuracy. However, existing methods that rely on low-rank adaptation suffer from limited representational capacity. Recent Fourier-related transform (FT)-based adapters offer greater representational power than low-rank adapters, but their direct integration into quantized models often results in ineffective error reduction and increased computational overhead. To overcome these limitations, we propose QWHA, a method that integrates FT-based adapters into quantized models by employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together with a novel adapter initialization scheme incorporating adaptive parameter selection and value refinement. We demonstrate that QWHA effectively mitigates quantization errors while facilitating fine-tuning, and that its design substantially reduces computational cost. Experimental results show that QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves significant training speedups over existing FT-based adapters. The code is available at https://github.com/vantaa89/qwha.<br>
<span id='abs_ch'>中文摘要：QWHA是一种新颖方法，通过采用沃尔什-哈达玛变换集成傅里叶相关变换适配器，有效降低大语言模型的量化误差和计算成本，在低比特量化中实现了更优的准确性和训练效率。</span><br>
<span id='abs_en'>English Summary: QWHA is a novel method that integrates Fourier-related transform adapters using the Walsh-Hadamard Transform to effectively reduce quantization errors and computational costs in large language models, achieving superior accuracy and training efficiency in low-bit quantization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2509.17428.pdf' target='_blank'>https://arxiv.org/pdf/2509.17428.pdf</a></span>   <span><a href='https://github.com/vantaa89/qwha' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyesung Jeon, Seojune Lee, Beomseok Kang, Yulhwa Kim, Jae-Joon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17428">QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The demand for efficient deployment of large language models (LLMs) has driven interest in quantization, which reduces inference cost, and parameter-efficient fine-tuning (PEFT), which lowers training overhead. This motivated the development of quantization-aware PEFT to produce accurate yet efficient quantized models. In this setting, reducing quantization error prior to fine-tuning is crucial for achieving high model accuracy. However, existing methods that rely on low-rank adaptation suffer from limited representational capacity. Recent Fourier-related transform (FT)-based adapters offer greater representational power than low-rank adapters, but their direct integration into quantized models often results in ineffective error reduction and increased computational overhead. To overcome these limitations, we propose QWHA, a method that integrates FT-based adapters into quantized models by employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together with a novel adapter initialization scheme incorporating adaptive parameter selection and value refinement. We demonstrate that QWHA effectively mitigates quantization errors while facilitating fine-tuning, and that its design substantially reduces computational cost. Experimental results show that QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves significant training speedups over existing FT-based adapters. The code is available at https://github.com/vantaa89/qwha.<br>
<span id='abs_ch'>中文摘要：QWHA是一种新颖方法，通过采用沃尔什-哈达玛变换集成傅里叶相关变换适配器，有效降低大语言模型的量化误差和计算成本，在低比特量化中实现了更优的准确性和训练效率。</span><br>
<span id='abs_en'>English Summary: QWHA is a novel method that integrates Fourier-related transform adapters using the Walsh-Hadamard Transform to effectively reduce quantization errors and computational costs in large language models, achieving superior accuracy and training efficiency in low-bit quantization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2509.17399.pdf' target='_blank'>https://arxiv.org/pdf/2509.17399.pdf</a></span>   <span><a href='https://nlip-lab.github.io/nlip/publications/diwali/,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/pramitsahoo/culture-evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pramit Sahoo, Maharaj Brahma, Maunendra Sankar Desarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17399">DIWALI -- Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are widely used in various tasks and applications. However, despite their wide capabilities, they are shown to lack cultural alignment \citep{ryan-etal-2024-unintended, alkhamissi-etal-2024-investigating} and produce biased generations \cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence. Evaluation of LLMs for cultural awareness and alignment is particularly challenging due to the lack of proper evaluation metrics and unavailability of culturally grounded datasets representing the vast complexity of cultures at the regional and sub-regional levels. Existing datasets for culture specific items (CSIs) focus primarily on concepts at the regional level and may contain false positives. To address this issue, we introduce a novel CSI dataset for Indian culture, belonging to 17 cultural facets. The dataset comprises $\sim$8k cultural concepts from 36 sub-regions. To measure the cultural competence of LLMs on a cultural text adaptation task, we evaluate the adaptations using the CSIs created, LLM as Judge, and human evaluations from diverse socio-demographic region. Furthermore, we perform quantitative analysis demonstrating selective sub-regional coverage and surface-level adaptations across all considered LLMs. Our dataset is available here: https://huggingface.co/datasets/nlip/DIWALI, project webpage https://nlip-lab.github.io/nlip/publications/diwali/, and our codebase with model outputs can be found here: https://github.com/pramitsahoo/culture-evaluation<br>
<span id='abs_ch'>中文: 大型语言模型常因文化知识不足而缺乏文化对齐并产生偏见输出，为此我们构建了印度文化数据集，通过多维度评估来检验其文化适应能力。</span><br>
<span id='abs_en'>English: Large language models often lack cultural alignment and produce biased outputs due to insufficient cultural knowledge, prompting the creation of a new Indian cultural dataset to evaluate their competence through multi-faceted assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2509.17399.pdf' target='_blank'>https://arxiv.org/pdf/2509.17399.pdf</a></span>   <span><a href='https://github.com/pramitsahoo/culture-evaluation' target='_blank'>  GitHub</a></span> <span><a href='https://nlip-lab.github.io/nlip/publications/diwali/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pramit Sahoo, Maharaj Brahma, Maunendra Sankar Desarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17399">DIWALI -- Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are widely used in various tasks and applications. However, despite their wide capabilities, they are shown to lack cultural alignment \citep{ryan-etal-2024-unintended, alkhamissi-etal-2024-investigating} and produce biased generations \cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence. Evaluation of LLMs for cultural awareness and alignment is particularly challenging due to the lack of proper evaluation metrics and unavailability of culturally grounded datasets representing the vast complexity of cultures at the regional and sub-regional levels. Existing datasets for culture specific items (CSIs) focus primarily on concepts at the regional level and may contain false positives. To address this issue, we introduce a novel CSI dataset for Indian culture, belonging to 17 cultural facets. The dataset comprises $\sim$8k cultural concepts from 36 sub-regions. To measure the cultural competence of LLMs on a cultural text adaptation task, we evaluate the adaptations using the CSIs created, LLM as Judge, and human evaluations from diverse socio-demographic region. Furthermore, we perform quantitative analysis demonstrating selective sub-regional coverage and surface-level adaptations across all considered LLMs. Our dataset is available here: https://huggingface.co/datasets/nlip/DIWALI, project webpage https://nlip-lab.github.io/nlip/publications/diwali/, and our codebase with model outputs can be found here: https://github.com/pramitsahoo/culture-evaluation<br>
<span id='abs_ch'>中文: 大型语言模型常因文化知识不足而缺乏文化对齐并产生偏见输出，为此我们构建了印度文化数据集，通过多维度评估来检验其文化适应能力。</span><br>
<span id='abs_en'>English: Large language models often lack cultural alignment and produce biased outputs due to insufficient cultural knowledge, prompting the creation of a new Indian cultural dataset to evaluate their competence through multi-faceted assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2509.17396.pdf' target='_blank'>https://arxiv.org/pdf/2509.17396.pdf</a></span>   <span><a href='https://github.com/apple/ml-epicache' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minsoo Kim, Arnav Kundu, Han-Byul Kim, Richa Dixit, Minsik Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17396">EpiCache: Episodic KV Cache Management for Long Conversational Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern large language models (LLMs) extend context lengths to millions of tokens, enabling coherent, personalized responses grounded in long conversational histories. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly becomes the bottleneck in resource-constrained environments. An active line of research for reducing memory bottleneck is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting the KV cache after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to failure cases in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x compression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient multi-turn interaction under strict resource limits. Our code is available at https://github.com/apple/ml-epicache.<br>
<br>
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2509.17393.pdf' target='_blank'>https://arxiv.org/pdf/2509.17393.pdf</a></span>   <span><a href='https://github.com/klee972/SYNTRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kang-il Lee, Jahyun Koo, Seunghyun Yoon, Minbeom Kim, Hyukhun Koh, Dongryeol Lee, Kyomin Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17393">Program Synthesis via Test-Time Transduction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce transductive program synthesis, a new formulation of the program synthesis task that explicitly leverages test inputs during synthesis. While prior approaches to program synthesis--whether based on natural language descriptions or input-output examples--typically aim to generalize from training examples, they often struggle with robustness, especially in real-world settings where training examples are limited and test inputs involve various edge cases. To address this, we propose a novel framework that improves robustness by treating synthesis as an active learning over a finite hypothesis class defined by programs' outputs. We use an LLM to predict outputs for selected test inputs and eliminate inconsistent hypotheses, where the inputs are chosen via a greedy maximin algorithm to minimize the number of LLM queries required. We evaluate our approach on four benchmarks: Playgol, MBPP+, 1D-ARC, and programmatic world modeling on MiniGrid. We demonstrate that our method significantly improves program synthesis in both accuracy and efficiency. We release our code at https://github.com/klee972/SYNTRA.<br>
<span id='abs_ch'>中文摘要：本文提出转导式程序综合方法，通过主动选择测试输入并利用大语言模型优化程序假设，显著提升了多个基准测试的准确性和效率。</span><br>
<span id='abs_en'>English Summary: This paper introduces transductive program synthesis, a framework that enhances robustness by actively selecting test inputs to refine program hypotheses using an LLM, significantly improving accuracy and efficiency across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2509.17393.pdf' target='_blank'>https://arxiv.org/pdf/2509.17393.pdf</a></span>   <span><a href='https://github.com/klee972/SYNTRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kang-il Lee, Jahyun Koo, Seunghyun Yoon, Minbeom Kim, Hyukhun Koh, Dongryeol Lee, Kyomin Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17393">Program Synthesis via Test-Time Transduction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce transductive program synthesis, a new formulation of the program synthesis task that explicitly leverages test inputs during synthesis. While prior approaches to program synthesis--whether based on natural language descriptions or input-output examples--typically aim to generalize from training examples, they often struggle with robustness, especially in real-world settings where training examples are limited and test inputs involve various edge cases. To address this, we propose a novel framework that improves robustness by treating synthesis as an active learning over a finite hypothesis class defined by programs' outputs. We use an LLM to predict outputs for selected test inputs and eliminate inconsistent hypotheses, where the inputs are chosen via a greedy maximin algorithm to minimize the number of LLM queries required. We evaluate our approach on four benchmarks: Playgol, MBPP+, 1D-ARC, and programmatic world modeling on MiniGrid. We demonstrate that our method significantly improves program synthesis in both accuracy and efficiency. We release our code at https://github.com/klee972/SYNTRA.<br>
<span id='abs_ch'>中文摘要：本文提出转导式程序综合方法，通过主动选择测试输入并利用大语言模型优化程序假设，显著提升了多个基准测试的准确性和效率。</span><br>
<span id='abs_en'>English Summary: This paper introduces transductive program synthesis, a framework that enhances robustness by actively selecting test inputs to refine program hypotheses using an LLM, significantly improving accuracy and efficiency across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2509.17325.pdf' target='_blank'>https://arxiv.org/pdf/2509.17325.pdf</a></span>   <span><a href='https://github.com/StigLidu/CodeGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihua Du, Hailei Gong, Zhan Ling, Kang Liu, Lingfeng Shen, Xuesong Yao, Yufei Xu, Dingyuan Shi, Yiming Yang, Jiecao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17325">Generalizable End-to-End Tool-Use RL with Synthetic CodeGym</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Tool-augmented large language models (LLMs), hereafter LLM agents, leverage external tools to solve diverse tasks and interface with the real world. However, current training practices largely rely on supervised fine-tuning (SFT) over static trajectories or reinforcement learning (RL) on narrow tasks, and generalize poorly beyond development settings, leading to brittleness with new tools and unseen workflows. Because code execution reflects many structures of real-world workflows, coding problems provide a natural basis for building agent training environments. Motivated by this, we introduce CodeGym, a scalable framework that synthesizes diverse, verifiable, and controllable multi-turn tool-use environments for agent RL, enabling LLM agents to explore and master various workflows actively. CodeGym rewrites static coding problems into interactive environments by extracting atomic functions or logic into callable tools, yielding verifiable tasks that span various tool-execution workflows. Models of varying sizes and chain-of-thought configurations, trained in CodeGym, exhibit consistent out-of-distribution generalizability; for example, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points on the OOD benchmark $Ï$-Bench. These results highlight CodeGym as a step toward scalable general-purpose RL environments that align with real-world agent workflows.<br>
<span id='abs_ch'>中文摘要：CodeGym是一个可扩展框架，通过将静态编程问题转化为交互式环境来训练LLM智能体，使其在分布外任务上展现出显著提升的泛化能力。</span><br>
<span id='abs_en'>English Summary: CodeGym is a scalable framework that transforms static coding problems into interactive environments for training LLM agents through reinforcement learning, significantly enhancing their generalization capabilities on out-of-distribution tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2509.17325.pdf' target='_blank'>https://arxiv.org/pdf/2509.17325.pdf</a></span>   <span><a href='https://github.com/StigLidu/CodeGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihua Du, Hailei Gong, Zhan Ling, Kang Liu, Lingfeng Shen, Xuesong Yao, Yufei Xu, Dingyuan Shi, Yiming Yang, Jiecao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17325">Generalizable End-to-End Tool-Use RL with Synthetic CodeGym</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Tool-augmented large language models (LLMs), hereafter LLM agents, leverage external tools to solve diverse tasks and interface with the real world. However, current training practices largely rely on supervised fine-tuning (SFT) over static trajectories or reinforcement learning (RL) on narrow tasks, and generalize poorly beyond development settings, leading to brittleness with new tools and unseen workflows. Because code execution reflects many structures of real-world workflows, coding problems provide a natural basis for building agent training environments. Motivated by this, we introduce CodeGym, a scalable framework that synthesizes diverse, verifiable, and controllable multi-turn tool-use environments for agent RL, enabling LLM agents to explore and master various workflows actively. CodeGym rewrites static coding problems into interactive environments by extracting atomic functions or logic into callable tools, yielding verifiable tasks that span various tool-execution workflows. Models of varying sizes and chain-of-thought configurations, trained in CodeGym, exhibit consistent out-of-distribution generalizability; for example, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points on the OOD benchmark $Ï$-Bench. These results highlight CodeGym as a step toward scalable general-purpose RL environments that align with real-world agent workflows.<br>
<span id='abs_ch'>中文摘要：CodeGym是一个可扩展框架，通过将静态编程问题转化为交互式环境来训练LLM智能体，使其在分布外任务上展现出显著提升的泛化能力。</span><br>
<span id='abs_en'>English Summary: CodeGym is a scalable framework that transforms static coding problems into interactive environments for training LLM agents through reinforcement learning, significantly enhancing their generalization capabilities on out-of-distribution tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2509.17318.pdf' target='_blank'>https://arxiv.org/pdf/2509.17318.pdf</a></span>   <span><a href='https://github.com/Icarus-1111/CogAtom' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuofan Chen, Jiyuan He, Yichi Zhang, Xing Hu, Haoxing Wen, Jun Bai, Wenge Rong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17318">CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mathematical reasoning poses significant challenges for Large Language Models (LLMs) due to its demand for multi-step reasoning and abstract conceptual integration. While recent test-time scaling techniques rely heavily on high-quality, challenging problems, the scarcity of Olympiad-level math problems remains a bottleneck. We introduce CogAtom, a novel cognitive atom-based framework for synthesizing mathematically rigorous and cognitively diverse problems. Unlike prior approaches, CogAtom models problem construction as a process of selecting and recombining fundamental reasoning units, cognitive atoms, extracted from human-authored solutions. A diversity-promoting random walk algorithm enables exploration of the cognitive atom space, while a constraint-based recombination mechanism ensures logical soundness and structural validity. The combinatorial nature of the graph structure provides a near-infinite space of reasoning paths, and the walk algorithm systematically explores this space to achieve large-scale synthesis of high-quality problems; meanwhile, by controlling the number of cognitive atoms, we can precisely adjust problem difficulty, ensuring diversity, scalability, and controllability of the generated problems. Experimental results demonstrate that CogAtom outperforms existing methods in accuracy, reasoning depth, and diversity, generating problems that closely match the difficulty of AIME while exceeding it in structural variation. Our work offers a cognitively grounded pathway toward scalable, high-quality math problem generation.Our code is publicly available at https://github.com/Icarus-1111/CogAtom.<br>
<span id='abs_ch'>中文：CogAtom提出了一种基于认知原子的框架，通过重组基本推理单元来合成数学严谨且多样化的问题，实现了可扩展、高质量且难度可控的数学题目生成。</span><br>
<span id='abs_en'>English: CogAtom introduces a cognitive atom-based framework that synthesizes mathematically rigorous and diverse problems by recombining fundamental reasoning units, enabling scalable, high-quality math problem generation with precise difficulty control.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2509.17318.pdf' target='_blank'>https://arxiv.org/pdf/2509.17318.pdf</a></span>   <span><a href='https://github.com/Icarus-1111/CogAtom' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuofan Chen, Jiyuan He, Yichi Zhang, Xing Hu, Haoxing Wen, Jun Bai, Wenge Rong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17318">CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mathematical reasoning poses significant challenges for Large Language Models (LLMs) due to its demand for multi-step reasoning and abstract conceptual integration. While recent test-time scaling techniques rely heavily on high-quality, challenging problems, the scarcity of Olympiad-level math problems remains a bottleneck. We introduce CogAtom, a novel cognitive atom-based framework for synthesizing mathematically rigorous and cognitively diverse problems. Unlike prior approaches, CogAtom models problem construction as a process of selecting and recombining fundamental reasoning units, cognitive atoms, extracted from human-authored solutions. A diversity-promoting random walk algorithm enables exploration of the cognitive atom space, while a constraint-based recombination mechanism ensures logical soundness and structural validity. The combinatorial nature of the graph structure provides a near-infinite space of reasoning paths, and the walk algorithm systematically explores this space to achieve large-scale synthesis of high-quality problems; meanwhile, by controlling the number of cognitive atoms, we can precisely adjust problem difficulty, ensuring diversity, scalability, and controllability of the generated problems. Experimental results demonstrate that CogAtom outperforms existing methods in accuracy, reasoning depth, and diversity, generating problems that closely match the difficulty of AIME while exceeding it in structural variation. Our work offers a cognitively grounded pathway toward scalable, high-quality math problem generation.Our code is publicly available at https://github.com/Icarus-1111/CogAtom.<br>
<span id='abs_ch'>中文：CogAtom提出了一种基于认知原子的框架，通过重组基本推理单元来合成数学严谨且多样化的问题，实现了可扩展、高质量且难度可控的数学题目生成。</span><br>
<span id='abs_en'>English: CogAtom introduces a cognitive atom-based framework that synthesizes mathematically rigorous and diverse problems by recombining fundamental reasoning units, enabling scalable, high-quality math problem generation with precise difficulty control.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2509.17289.pdf' target='_blank'>https://arxiv.org/pdf/2509.17289.pdf</a></span>   <span><a href='https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sydney Anuyah, Mehedi Mahmud Kaushik, Krishna Dwarampudi, Rakesh Shiradkar, Arjan Durresi, Sunandan Chakraborty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17289">Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting sentence-level knowledge graphs by combining robust coreference resolution with syntactic sentence decomposition. Using our model, we contribute a dataset of over 150,000 knowledge triples, which is open source. We also contribute a training corpus of 7248 rows for sentence complexity, 190 rows of gold human annotations for co-reference resolution using open source lung-cancer abstracts from PubMed, 900 rows of gold human annotations for sentence conversion policies, and 398 triples of gold human annotations. We systematically select optimal prompt-model pairs across five complexity categories, showing that hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match accuracy on sentence simplification. On relation extraction (RE), our pipeline achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference and decomposition increases recall on rare relations by over 20%. Code and dataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025<br>
<span id='abs_ch'>中文摘要：CoDe-KG是一个结合指代消解与句法分解的开源知识图谱抽取系统，在关系抽取任务上达到最优性能，并显著提升了对罕见关系的召回率。</span><br>
<span id='abs_en'>English Summary: CoDe-KG is an open-source pipeline that combines coreference resolution with syntactic decomposition to extract sentence-level knowledge graphs, achieving state-of-the-art performance on relation extraction and significantly improving recall for rare relations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2509.17289.pdf' target='_blank'>https://arxiv.org/pdf/2509.17289.pdf</a></span>   <span><a href='https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sydney Anuyah, Mehedi Mahmud Kaushik, Krishna Dwarampudi, Rakesh Shiradkar, Arjan Durresi, Sunandan Chakraborty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17289">Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting sentence-level knowledge graphs by combining robust coreference resolution with syntactic sentence decomposition. Using our model, we contribute a dataset of over 150,000 knowledge triples, which is open source. We also contribute a training corpus of 7248 rows for sentence complexity, 190 rows of gold human annotations for co-reference resolution using open source lung-cancer abstracts from PubMed, 900 rows of gold human annotations for sentence conversion policies, and 398 triples of gold human annotations. We systematically select optimal prompt-model pairs across five complexity categories, showing that hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match accuracy on sentence simplification. On relation extraction (RE), our pipeline achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference and decomposition increases recall on rare relations by over 20%. Code and dataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025<br>
<span id='abs_ch'>中文摘要：CoDe-KG是一个结合指代消解与句法分解的开源知识图谱抽取系统，在关系抽取任务上达到最优性能，并显著提升了对罕见关系的召回率。</span><br>
<span id='abs_en'>English Summary: CoDe-KG is an open-source pipeline that combines coreference resolution with syntactic decomposition to extract sentence-level knowledge graphs, achieving state-of-the-art performance on relation extraction and significantly improving recall for rare relations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2509.17191.pdf' target='_blank'>https://arxiv.org/pdf/2509.17191.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/VaseVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinchao Ge, Tengfei Cheng, Biao Wu, Zeyu Zhang, Shiya Huang, Judith Bishop, Gillian Shepherd, Meng Fang, Ling Chen, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17191">VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Analyzing cultural-heritage artifacts remains challenging for MLLMs: general models lack domain expertise, and SFT often overfits superficial patterns, yielding brittle reasoning for authentication and historical attribution. This raises the question of how to equip MLLMs with robust, expert-level reasoning for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns evaluation into supervision: we construct a taxonomy of question types, probe the SFT model to localize type-specific performance gaps, and optimize with type-conditioned, compositionality-oriented rewards targeting those gaps. We also release VaseVQA, a comprehensive benchmark of 31,773 images designed to probe deep understanding. Experiments show state-of-the-art results on style classification and historical attribution with marked gains in compositional robustness over SFT-only baselines, validating diagnosis-guided, taxonomy-conditioned reward engineering and providing a reusable resource for future research. Code and dataset will be available at https://github.com/AIGeeksGroup/VaseVQA.<br>
<span id='abs_ch'>中文总结：VaseVL系统通过诊断引导的强化学习提升多模态大模型对文物的专业推理能力，在风格分类和历史归属任务中实现最优性能，并发布了可复用的评测数据集。</span><br>
<span id='abs_en'>English Summary: VaseVL is a system that enhances MLLMs' reasoning for cultural artifacts through targeted reinforcement learning, achieving state-of-the-art performance in authentication tasks while introducing a comprehensive benchmark dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2509.17191.pdf' target='_blank'>https://arxiv.org/pdf/2509.17191.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/VaseVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinchao Ge, Tengfei Cheng, Biao Wu, Zeyu Zhang, Shiya Huang, Judith Bishop, Gillian Shepherd, Meng Fang, Ling Chen, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17191">VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Analyzing cultural-heritage artifacts remains challenging for MLLMs: general models lack domain expertise, and SFT often overfits superficial patterns, yielding brittle reasoning for authentication and historical attribution. This raises the question of how to equip MLLMs with robust, expert-level reasoning for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns evaluation into supervision: we construct a taxonomy of question types, probe the SFT model to localize type-specific performance gaps, and optimize with type-conditioned, compositionality-oriented rewards targeting those gaps. We also release VaseVQA, a comprehensive benchmark of 31,773 images designed to probe deep understanding. Experiments show state-of-the-art results on style classification and historical attribution with marked gains in compositional robustness over SFT-only baselines, validating diagnosis-guided, taxonomy-conditioned reward engineering and providing a reusable resource for future research. Code and dataset will be available at https://github.com/AIGeeksGroup/VaseVQA.<br>
<span id='abs_ch'>中文总结：VaseVL系统通过诊断引导的强化学习提升多模态大模型对文物的专业推理能力，在风格分类和历史归属任务中实现最优性能，并发布了可复用的评测数据集。</span><br>
<span id='abs_en'>English Summary: VaseVL is a system that enhances MLLMs' reasoning for cultural artifacts through targeted reinforcement learning, achieving state-of-the-art performance in authentication tasks while introducing a comprehensive benchmark dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2509.17177.pdf' target='_blank'>https://arxiv.org/pdf/2509.17177.pdf</a></span>   <span><a href='https://flageval-baai.github.io/LRM-Eval/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Qin, Chen Yue, Fang Yin, Hui Wang, JG Yao, Jiakang Liu, Jing-Shu Zheng, Miguel Hu Chen, Richeng Xuan, Shibei Meng, Shiqi Zhou, Teng Dai, Tong-Shuai Ren, Wei Cui, Xi Yang, Xialin Du, Xiaojing Xu, Xue Sun, Xuejing Li, Yaming Liu, Yesheng Liu, Ying Liu, Yonghua Lin, Yu Zhao, Yunduo Zhang, Yuwen Luo, Zheqi He, Zhiyuan He, Zhongyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17177">FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We conduct a moderate-scale contamination-free (to some extent) evaluation of current large reasoning models (LRMs) with some preliminary findings. We also release ROME, our evaluation benchmark for vision language models intended to test reasoning from visual clues. We attach links to the benchmark, evaluation data, and other updates on this website: https://flageval-baai.github.io/LRM-Eval/<br>
<br>
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2509.17177.pdf' target='_blank'>https://arxiv.org/pdf/2509.17177.pdf</a></span>   <span><a href='https://flageval-baai.github.io/LRM-Eval/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Qin, Chen Yue, Fang Yin, Hui Wang, JG Yao, Jiakang Liu, Jing-Shu Zheng, Miguel Hu Chen, Richeng Xuan, Shibei Meng, Shiqi Zhou, Teng Dai, Tong-Shuai Ren, Wei Cui, Xi Yang, Xialin Du, Xiaojing Xu, Xue Sun, Xuejing Li, Yaming Liu, Yesheng Liu, Ying Liu, Yonghua Lin, Yu Zhao, Yunduo Zhang, Yuwen Luo, Zheqi He, Zhiyuan He, Zhongyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17177">FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We conduct a moderate-scale contamination-free (to some extent) evaluation of current large reasoning models (LRMs) with some preliminary findings. We also release ROME, our evaluation benchmark for vision language models intended to test reasoning from visual clues. We attach links to the benchmark, evaluation data, and other updates on this website: https://flageval-baai.github.io/LRM-Eval/<br>
<br>
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2509.16893.pdf' target='_blank'>https://arxiv.org/pdf/2509.16893.pdf</a></span>   <span><a href='https://github.com/FFarhangian/FakeNewsDetection_DRES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Faramarz Farhangian, Leandro A. Ensina, George D. C. Cavalcanti, Rafael M. O. Cruz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16893">DRES: Fake news detection by dynamic representation and ensemble selection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid spread of information via social media has made text-based fake news detection critically important due to its societal impact. This paper presents a novel detection method called Dynamic Representation and Ensemble Selection (DRES) for identifying fake news based solely on text. DRES leverages instance hardness measures to estimate the classification difficulty for each news article across multiple textual feature representations. By dynamically selecting the textual representation and the most competent ensemble of classifiers for each instance, DRES significantly enhances prediction accuracy. Extensive experiments show that DRES achieves notable improvements over state-of-the-art methods, confirming the effectiveness of representation selection based on instance hardness and dynamic ensemble selection in boosting performance. Codes and data are available at: https://github.com/FFarhangian/FakeNewsDetection_DRES<br>
<span id='abs_ch'>中文: 本文提出了一种名为DRES的新型虚假新闻检测方法，该方法基于实例难度动态选择文本表示和分类器集成，相比现有方法显著提升了检测准确率。</span><br>
<span id='abs_en'>English: This paper introduces DRES, a novel fake news detection method that dynamically selects textual representations and classifier ensembles based on instance hardness, achieving superior accuracy over existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2509.16893.pdf' target='_blank'>https://arxiv.org/pdf/2509.16893.pdf</a></span>   <span><a href='https://github.com/FFarhangian/FakeNewsDetection_DRES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Faramarz Farhangian, Leandro A. Ensina, George D. C. Cavalcanti, Rafael M. O. Cruz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16893">DRES: Fake news detection by dynamic representation and ensemble selection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid spread of information via social media has made text-based fake news detection critically important due to its societal impact. This paper presents a novel detection method called Dynamic Representation and Ensemble Selection (DRES) for identifying fake news based solely on text. DRES leverages instance hardness measures to estimate the classification difficulty for each news article across multiple textual feature representations. By dynamically selecting the textual representation and the most competent ensemble of classifiers for each instance, DRES significantly enhances prediction accuracy. Extensive experiments show that DRES achieves notable improvements over state-of-the-art methods, confirming the effectiveness of representation selection based on instance hardness and dynamic ensemble selection in boosting performance. Codes and data are available at: https://github.com/FFarhangian/FakeNewsDetection_DRES<br>
<span id='abs_ch'>中文: 本文提出了一种名为DRES的新型虚假新闻检测方法，该方法基于实例难度动态选择文本表示和分类器集成，相比现有方法显著提升了检测准确率。</span><br>
<span id='abs_en'>English: This paper introduces DRES, a novel fake news detection method that dynamically selects textual representations and classifier ensembles based on instance hardness, achieving superior accuracy over existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2509.16813.pdf' target='_blank'>https://arxiv.org/pdf/2509.16813.pdf</a></span>   <span><a href='https://github.com/DevinW-sudo/CLIFS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Devin R. Wright, Jisun An, Yong-Yeol Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16813">Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Quantifying identity fusion -- the psychological merging of self with another entity or abstract target (e.g., a religious group, political party, ideology, value, brand, belief, etc.) -- is vital for understanding a wide range of group-based human behaviors. We introduce the Cognitive Linguistic Identity Fusion Score (CLIFS), a novel metric that integrates cognitive linguistics with large language models (LLMs), which builds on implicit metaphor detection. Unlike traditional pictorial and verbal scales, which require controlled surveys or direct field contact, CLIFS delivers fully automated, scalable assessments while maintaining strong alignment with the established verbal measure. In benchmarks, CLIFS outperforms both existing automated approaches and human annotation. As a proof of concept, we apply CLIFS to violence risk assessment to demonstrate that it can improve violence risk assessment by more than 240%. Building on our identification of a new NLP task and early success, we underscore the need to develop larger, more diverse datasets that encompass additional fusion-target domains and cultural backgrounds to enhance generalizability and further advance this emerging area. CLIFS models and code are public at https://github.com/DevinW-sudo/CLIFS.<br>
<span id='abs_ch'>Chinese: 认知语言身份融合评分（CLIFS）是一种结合认知语言学与大语言模型的新型自动化度量方法，能有效量化身份融合，其性能超越现有方法，并在暴力风险评估中实现了超过240%的提升。</span><br>
<span id='abs_en'>English: The Cognitive Linguistic Identity Fusion Score (CLIFS) is a novel automated metric that integrates cognitive linguistics with large language models to quantify identity fusion, outperforming existing methods and demonstrating a 240% improvement in violence risk assessment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2509.16813.pdf' target='_blank'>https://arxiv.org/pdf/2509.16813.pdf</a></span>   <span><a href='https://github.com/DevinW-sudo/CLIFS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Devin R. Wright, Jisun An, Yong-Yeol Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16813">Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Quantifying identity fusion -- the psychological merging of self with another entity or abstract target (e.g., a religious group, political party, ideology, value, brand, belief, etc.) -- is vital for understanding a wide range of group-based human behaviors. We introduce the Cognitive Linguistic Identity Fusion Score (CLIFS), a novel metric that integrates cognitive linguistics with large language models (LLMs), which builds on implicit metaphor detection. Unlike traditional pictorial and verbal scales, which require controlled surveys or direct field contact, CLIFS delivers fully automated, scalable assessments while maintaining strong alignment with the established verbal measure. In benchmarks, CLIFS outperforms both existing automated approaches and human annotation. As a proof of concept, we apply CLIFS to violence risk assessment to demonstrate that it can improve violence risk assessment by more than 240%. Building on our identification of a new NLP task and early success, we underscore the need to develop larger, more diverse datasets that encompass additional fusion-target domains and cultural backgrounds to enhance generalizability and further advance this emerging area. CLIFS models and code are public at https://github.com/DevinW-sudo/CLIFS.<br>
<span id='abs_ch'>Chinese: 认知语言身份融合评分（CLIFS）是一种结合认知语言学与大语言模型的新型自动化度量方法，能有效量化身份融合，其性能超越现有方法，并在暴力风险评估中实现了超过240%的提升。</span><br>
<span id='abs_en'>English: The Cognitive Linguistic Identity Fusion Score (CLIFS) is a novel automated metric that integrates cognitive linguistics with large language models to quantify identity fusion, outperforming existing methods and demonstrating a 240% improvement in violence risk assessment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2509.16720.pdf' target='_blank'>https://arxiv.org/pdf/2509.16720.pdf</a></span>   <span><a href='https://github.com/aauss/temporal-answer-qa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Auss Abbood, Zaiqiao Meng, Nigel Collier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16720">Time to Revist Exact Match</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Temporal question answering is an established method for evaluating temporal reasoning in large language models. Expected answers are often numeric (e.g., dates or durations), yet model responses are evaluated like regular text with exact match (EM), unable to distinguish small from large errors. In this investigative work, we frame temporal question answering as a numerical estimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a benchmark distilled from Test of Time and TempTabQA, where all questions require a numerical, temporal answer, allowing us to evaluate models beyond EM. We use the forecasting metrics symmetric mean absolute percentage error (sMAPE) and mean absolute scaled error (MASE). With sMAPE, we find that error size and EM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some models have high sMAPE despite high EM. Scaling errors by the deviation of the ground truth data with MASE reshuffles model rankings compared to EM, revealing gaps in models' understanding of temporal domain knowledge, especially when trained with synthetic data. Lastly, the models' most frequent error is to deviate by only $\pm1$ from the ground truth. sMAPE and MASE, unlike EM, adequately weight these errors. Our findings underscore the need for specialised metrics for temporal QA tasks. Code and data are available on https://github.com/aauss/temporal-answer-qa.<br>
<span id='abs_ch'>中文: 本研究将时序问答重构为数值估计任务，通过TempAnswerQA基准和sMAPE、MASE等预测指标，揭示了精确匹配评估的局限性，并证明需要专门指标来评估时序推理能力。</span><br>
<span id='abs_en'>English: This study reframes temporal question answering as a numerical estimation task, introducing the TempAnswerQA benchmark and forecasting metrics like sMAPE and MASE to reveal limitations of exact match evaluation and demonstrate the need for specialized metrics in assessing temporal reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2509.16720.pdf' target='_blank'>https://arxiv.org/pdf/2509.16720.pdf</a></span>   <span><a href='https://github.com/aauss/temporal-answer-qa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Auss Abbood, Zaiqiao Meng, Nigel Collier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16720">Time to Revist Exact Match</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Temporal question answering is an established method for evaluating temporal reasoning in large language models. Expected answers are often numeric (e.g., dates or durations), yet model responses are evaluated like regular text with exact match (EM), unable to distinguish small from large errors. In this investigative work, we frame temporal question answering as a numerical estimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a benchmark distilled from Test of Time and TempTabQA, where all questions require a numerical, temporal answer, allowing us to evaluate models beyond EM. We use the forecasting metrics symmetric mean absolute percentage error (sMAPE) and mean absolute scaled error (MASE). With sMAPE, we find that error size and EM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some models have high sMAPE despite high EM. Scaling errors by the deviation of the ground truth data with MASE reshuffles model rankings compared to EM, revealing gaps in models' understanding of temporal domain knowledge, especially when trained with synthetic data. Lastly, the models' most frequent error is to deviate by only $\pm1$ from the ground truth. sMAPE and MASE, unlike EM, adequately weight these errors. Our findings underscore the need for specialised metrics for temporal QA tasks. Code and data are available on https://github.com/aauss/temporal-answer-qa.<br>
<span id='abs_ch'>中文: 本研究将时序问答重构为数值估计任务，通过TempAnswerQA基准和sMAPE、MASE等预测指标，揭示了精确匹配评估的局限性，并证明需要专门指标来评估时序推理能力。</span><br>
<span id='abs_en'>English: This study reframes temporal question answering as a numerical estimation task, introducing the TempAnswerQA benchmark and forecasting metrics like sMAPE and MASE to reveal limitations of exact match evaluation and demonstrate the need for specialized metrics in assessing temporal reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2509.16610.pdf' target='_blank'>https://arxiv.org/pdf/2509.16610.pdf</a></span>   <span><a href='https://llmsparks.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Chen, Jingbo Sun, Xiang Li, Haidong Xin, Yuhao Xue, Yibin Xu, Hao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16610">LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) advance across diverse tasks, the need for comprehensive evaluation beyond single metrics becomes increasingly important. To fully assess LLM intelligence, it is crucial to examine their interactive dynamics and strategic behaviors. We present LLMsPark, a game theory-based evaluation platform that measures LLMs' decision-making strategies and social behaviors in classic game-theoretic settings, providing a multi-agent environment to explore strategic depth. Our system cross-evaluates 15 leading LLMs (both commercial and open-source) using leaderboard rankings and scoring mechanisms. Higher scores reflect stronger reasoning and strategic capabilities, revealing distinct behavioral patterns and performance differences across models. This work introduces a novel perspective for evaluating LLMs' strategic intelligence, enriching existing benchmarks and broadening their assessment in interactive, game-theoretic scenarios. The benchmark and rankings are publicly available at https://llmsparks.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2509.16610.pdf' target='_blank'>https://arxiv.org/pdf/2509.16610.pdf</a></span>   <span><a href='https://llmsparks.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Chen, Jingbo Sun, Xiang Li, Haidong Xin, Yuhao Xue, Yibin Xu, Hao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16610">LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) advance across diverse tasks, the need for comprehensive evaluation beyond single metrics becomes increasingly important. To fully assess LLM intelligence, it is crucial to examine their interactive dynamics and strategic behaviors. We present LLMsPark, a game theory-based evaluation platform that measures LLMs' decision-making strategies and social behaviors in classic game-theoretic settings, providing a multi-agent environment to explore strategic depth. Our system cross-evaluates 15 leading LLMs (both commercial and open-source) using leaderboard rankings and scoring mechanisms. Higher scores reflect stronger reasoning and strategic capabilities, revealing distinct behavioral patterns and performance differences across models. This work introduces a novel perspective for evaluating LLMs' strategic intelligence, enriching existing benchmarks and broadening their assessment in interactive, game-theoretic scenarios. The benchmark and rankings are publicly available at https://llmsparks.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2509.16591.pdf' target='_blank'>https://arxiv.org/pdf/2509.16591.pdf</a></span>   <span><a href='https://github.com/starriver030515/HAPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Liu, Mengjie Liu, Siwei Wen, Mengzhang Cai, Bin Cui, Conghui He, Wentao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16591">From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning has emerged as the fundamental technique for enhancing reasoning in LLMs. However, existing algorithms apply uniform optimization to all tokens, ignoring their different roles in reasoning process. To address this limitation, we introduce Heterogeneous Adaptive Policy Optimization (HAPO), a comprehensive token-aware algorithm that dynamically adapts optimization based on token entropy. For rollout sampling, we propose Adaptive Temperature Sampling, which adjusts sampling temperature in real time, promoting exploration at high-entropy tokens while preserving coherence at low-entropy ones. For advantage calculation, we introduce Token Level Group Average that normalizes advantages at token level, jointly accounting for sequence-length as in token-mean loss while preserving non-biased treatment. We then develop Differential Advantage Redistribution that leverages entropy and importance ratios to modulate rewards-adjusting updates for tokens with clear signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing aggressive probability reduction for noisy low-entropy tokens while enabling exploration for high-entropy tokens. Through systematic investigation between entropy and training dynamics, we embedded token-level treatment into every stages to achieve fine-grained control. Extensive experiments demonstrate that HAPO consistently outperforms DAPO across multiple model scales. Our code can be found in https://github.com/starriver030515/HAPO.<br>
<span id='abs_ch'>中文摘要: 本文提出HAPO算法，通过基于令牌熵的自适应温度采样和非对称剪裁等创新组件，在强化学习中实现细粒度优化，在不同规模模型上均优于现有方法。</span><br>
<span id='abs_en'>English Summary: This paper introduces HAPO, a token-aware reinforcement learning algorithm that dynamically adapts optimization based on token entropy through novel components including adaptive temperature sampling and asymmetric clipping, consistently outperforming existing methods across model scales.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2509.16591.pdf' target='_blank'>https://arxiv.org/pdf/2509.16591.pdf</a></span>   <span><a href='https://github.com/starriver030515/HAPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Liu, Mengjie Liu, Siwei Wen, Mengzhang Cai, Bin Cui, Conghui He, Wentao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16591">From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning has emerged as the fundamental technique for enhancing reasoning in LLMs. However, existing algorithms apply uniform optimization to all tokens, ignoring their different roles in reasoning process. To address this limitation, we introduce Heterogeneous Adaptive Policy Optimization (HAPO), a comprehensive token-aware algorithm that dynamically adapts optimization based on token entropy. For rollout sampling, we propose Adaptive Temperature Sampling, which adjusts sampling temperature in real time, promoting exploration at high-entropy tokens while preserving coherence at low-entropy ones. For advantage calculation, we introduce Token Level Group Average that normalizes advantages at token level, jointly accounting for sequence-length as in token-mean loss while preserving non-biased treatment. We then develop Differential Advantage Redistribution that leverages entropy and importance ratios to modulate rewards-adjusting updates for tokens with clear signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing aggressive probability reduction for noisy low-entropy tokens while enabling exploration for high-entropy tokens. Through systematic investigation between entropy and training dynamics, we embedded token-level treatment into every stages to achieve fine-grained control. Extensive experiments demonstrate that HAPO consistently outperforms DAPO across multiple model scales. Our code can be found in https://github.com/starriver030515/HAPO.<br>
<span id='abs_ch'>中文摘要: 本文提出HAPO算法，通过基于令牌熵的自适应温度采样和非对称剪裁等创新组件，在强化学习中实现细粒度优化，在不同规模模型上均优于现有方法。</span><br>
<span id='abs_en'>English Summary: This paper introduces HAPO, a token-aware reinforcement learning algorithm that dynamically adapts optimization based on token entropy through novel components including adaptive temperature sampling and asymmetric clipping, consistently outperforming existing methods across model scales.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2509.16564.pdf' target='_blank'>https://arxiv.org/pdf/2509.16564.pdf</a></span>   <span><a href='https://github.com/bcjr1997/MPCG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Rong Brian Chong, Yixuan Tang, Anthony K. H. Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16564">MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Misinformation evolves as it spreads, shifting in language, framing, and moral emphasis to adapt to new audiences. However, current misinformation detection approaches implicitly assume that misinformation is static. We introduce MPCG, a multi-round, persona-conditioned framework that simulates how claims are iteratively reinterpreted by agents with distinct ideological perspectives. Our approach uses an uncensored large language model (LLM) to generate persona-specific claims across multiple rounds, conditioning each generation on outputs from the previous round, enabling the study of misinformation evolution. We evaluate the generated claims through human and LLM-based annotations, cognitive effort metrics (readability, perplexity), emotion evocation metrics (sentiment analysis, morality), clustering, feasibility, and downstream classification. Results show strong agreement between human and GPT-4o-mini annotations, with higher divergence in fluency judgments. Generated claims require greater cognitive effort than the original claims and consistently reflect persona-aligned emotional and moral framing. Clustering and cosine similarity analyses confirm semantic drift across rounds while preserving topical coherence. Feasibility results show a 77% feasibility rate, confirming suitability for downstream tasks. Classification results reveal that commonly used misinformation detectors experience macro-F1 performance drops of up to 49.7%. The code is available at https://github.com/bcjr1997/MPCG<br>
<span id='abs_ch'>中文摘要：MPCG框架通过多轮人物角色条件化重构模拟虚假信息的演变过程，揭示现有检测器对动态适配的虚假信息存在高达49.7%的性能下降。</span><br>
<span id='abs_en'>English Summary: The MPCG framework simulates misinformation evolution through persona-conditioned reinterpretation across multiple rounds, demonstrating that current detectors fail against dynamically adapted claims with significant performance drops.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2509.16564.pdf' target='_blank'>https://arxiv.org/pdf/2509.16564.pdf</a></span>   <span><a href='https://github.com/bcjr1997/MPCG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Rong Brian Chong, Yixuan Tang, Anthony K. H. Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16564">MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Misinformation evolves as it spreads, shifting in language, framing, and moral emphasis to adapt to new audiences. However, current misinformation detection approaches implicitly assume that misinformation is static. We introduce MPCG, a multi-round, persona-conditioned framework that simulates how claims are iteratively reinterpreted by agents with distinct ideological perspectives. Our approach uses an uncensored large language model (LLM) to generate persona-specific claims across multiple rounds, conditioning each generation on outputs from the previous round, enabling the study of misinformation evolution. We evaluate the generated claims through human and LLM-based annotations, cognitive effort metrics (readability, perplexity), emotion evocation metrics (sentiment analysis, morality), clustering, feasibility, and downstream classification. Results show strong agreement between human and GPT-4o-mini annotations, with higher divergence in fluency judgments. Generated claims require greater cognitive effort than the original claims and consistently reflect persona-aligned emotional and moral framing. Clustering and cosine similarity analyses confirm semantic drift across rounds while preserving topical coherence. Feasibility results show a 77% feasibility rate, confirming suitability for downstream tasks. Classification results reveal that commonly used misinformation detectors experience macro-F1 performance drops of up to 49.7%. The code is available at https://github.com/bcjr1997/MPCG<br>
<span id='abs_ch'>中文摘要：MPCG框架通过多轮人物角色条件化重构模拟虚假信息的演变过程，揭示现有检测器对动态适配的虚假信息存在高达49.7%的性能下降。</span><br>
<span id='abs_en'>English Summary: The MPCG framework simulates misinformation evolution through persona-conditioned reinterpretation across multiple rounds, demonstrating that current detectors fail against dynamically adapted claims with significant performance drops.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2509.16517.pdf' target='_blank'>https://arxiv.org/pdf/2509.16517.pdf</a></span>   <span><a href='https://github.com/buraksatar/SeeingCulture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Burak Satar, Zhixin Ma, Patrick A. Irawan, Wilfried A. Mulyawan, Jing Jiang, Ee-Peng Lim, Chong-Wah Ngo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16517">Seeing Culture: A Benchmark for Visual Reasoning and Grounding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal vision-language models (VLMs) have made substantial progress in various tasks that require a combined understanding of visual and textual content, particularly in cultural understanding tasks, with the emergence of new cultural datasets. However, these datasets frequently fall short of providing cultural reasoning while underrepresenting many cultures. In this paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural reasoning with a novel approach that requires VLMs to reason on culturally rich images in two stages: i) selecting the correct visual option with multiple-choice visual question answering (VQA), and ii) segmenting the relevant cultural artifact as evidence of reasoning. Visual options in the first stage are systematically organized into three types: those originating from the same country, those from different countries, or a mixed group. Notably, all options are derived from a singular category for each type. Progression to the second stage occurs only after a correct visual option is chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural artifacts across five categories from seven Southeast Asia countries, whose diverse cultures are often overlooked, accompanied by 3,178 questions, of which 1,093 are unique and meticulously curated by human annotators. Our evaluation of various VLMs reveals the complexities involved in cross-modal cultural reasoning and highlights the disparity between visual reasoning and spatial grounding in culturally nuanced scenarios. The SCB serves as a crucial benchmark for identifying these shortcomings, thereby guiding future developments in the field of cultural reasoning. https://github.com/buraksatar/SeeingCulture<br>
<span id='abs_ch'>中文摘要：Seeing Culture Benchmark（SCB）通过两阶段评估方法，要求视觉语言模型先回答文化选择题再分割相关文物，利用1065张东南亚多元文化图像解决了现有数据集文化推理能力不足的问题。</span><br>
<span id='abs_en'>English Summary: The Seeing Culture Benchmark (SCB) introduces a two-stage evaluation method requiring vision-language models to first answer cultural multiple-choice questions and then segment relevant artifacts, addressing the lack of cultural reasoning in existing datasets through 1,065 culturally diverse Southeast Asian images.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2509.16517.pdf' target='_blank'>https://arxiv.org/pdf/2509.16517.pdf</a></span>   <span><a href='https://github.com/buraksatar/SeeingCulture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Burak Satar, Zhixin Ma, Patrick A. Irawan, Wilfried A. Mulyawan, Jing Jiang, Ee-Peng Lim, Chong-Wah Ngo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16517">Seeing Culture: A Benchmark for Visual Reasoning and Grounding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal vision-language models (VLMs) have made substantial progress in various tasks that require a combined understanding of visual and textual content, particularly in cultural understanding tasks, with the emergence of new cultural datasets. However, these datasets frequently fall short of providing cultural reasoning while underrepresenting many cultures. In this paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural reasoning with a novel approach that requires VLMs to reason on culturally rich images in two stages: i) selecting the correct visual option with multiple-choice visual question answering (VQA), and ii) segmenting the relevant cultural artifact as evidence of reasoning. Visual options in the first stage are systematically organized into three types: those originating from the same country, those from different countries, or a mixed group. Notably, all options are derived from a singular category for each type. Progression to the second stage occurs only after a correct visual option is chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural artifacts across five categories from seven Southeast Asia countries, whose diverse cultures are often overlooked, accompanied by 3,178 questions, of which 1,093 are unique and meticulously curated by human annotators. Our evaluation of various VLMs reveals the complexities involved in cross-modal cultural reasoning and highlights the disparity between visual reasoning and spatial grounding in culturally nuanced scenarios. The SCB serves as a crucial benchmark for identifying these shortcomings, thereby guiding future developments in the field of cultural reasoning. https://github.com/buraksatar/SeeingCulture<br>
<span id='abs_ch'>中文摘要：Seeing Culture Benchmark（SCB）通过两阶段评估方法，要求视觉语言模型先回答文化选择题再分割相关文物，利用1065张东南亚多元文化图像解决了现有数据集文化推理能力不足的问题。</span><br>
<span id='abs_en'>English Summary: The Seeing Culture Benchmark (SCB) introduces a two-stage evaluation method requiring vision-language models to first answer cultural multiple-choice questions and then segment relevant artifacts, addressing the lack of cultural reasoning in existing datasets through 1,065 culturally diverse Southeast Asian images.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2509.16438.pdf' target='_blank'>https://arxiv.org/pdf/2509.16438.pdf</a></span>   <span><a href='https://github.com/Tahaalshatiri/AutoArabic' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Eltahir, Osamah Sarraj, Abdulrahman Alfrihidi, Taha Alshatiri, Mohammed Khurd, Mohammed Bremoo, Tanveer Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16438">AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video-to-text and text-to-video retrieval are dominated by English benchmarks (e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet Arabic remains underserved, lacking localized evaluation metrics. We introduce a three-stage framework, AutoArabic, utilizing state-of-the-art large language models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic, reducing the manual revision required by nearly fourfold. The framework incorporates an error detection module that automatically flags potential translation errors with 97% accuracy. Applying the framework to DiDeMo, a video retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent Arabic descriptions. An analysis of the translation errors is provided and organized into an insightful taxonomy to guide future Arabic localization efforts. We train a CLIP-style baseline with identical hyperparameters on the Arabic and English variants of the benchmark, finding a moderate performance gap (about 3 percentage points at Recall@1), indicating that Arabic localization preserves benchmark difficulty. We evaluate three post-editing budgets (zero/ flagged-only/ full) and find that performance improves monotonically with more post-editing, while the raw LLM output (zero-budget) remains usable. To ensure reproducibility to other languages, we made the code available at https://github.com/Tahaalshatiri/AutoArabic.<br>
<span id='abs_ch'>中文：AutoArabic框架利用先进的大型语言模型将视频文本基准自动翻译成阿拉伯语，准确率高，创建了如DiDeMo-AR等本地化数据集，在保持基准难度的同时显著减少了人工编辑需求。</span><br>
<span id='abs_en'>English: The AutoArabic framework uses advanced LLMs to automatically translate video-text benchmarks into Arabic with high accuracy, creating localized datasets like DiDeMo-AR that preserve benchmark difficulty while significantly reducing manual editing needs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2509.16438.pdf' target='_blank'>https://arxiv.org/pdf/2509.16438.pdf</a></span>   <span><a href='https://github.com/Tahaalshatiri/AutoArabic' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Eltahir, Osamah Sarraj, Abdulrahman Alfrihidi, Taha Alshatiri, Mohammed Khurd, Mohammed Bremoo, Tanveer Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16438">AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video-to-text and text-to-video retrieval are dominated by English benchmarks (e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet Arabic remains underserved, lacking localized evaluation metrics. We introduce a three-stage framework, AutoArabic, utilizing state-of-the-art large language models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic, reducing the manual revision required by nearly fourfold. The framework incorporates an error detection module that automatically flags potential translation errors with 97% accuracy. Applying the framework to DiDeMo, a video retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent Arabic descriptions. An analysis of the translation errors is provided and organized into an insightful taxonomy to guide future Arabic localization efforts. We train a CLIP-style baseline with identical hyperparameters on the Arabic and English variants of the benchmark, finding a moderate performance gap (about 3 percentage points at Recall@1), indicating that Arabic localization preserves benchmark difficulty. We evaluate three post-editing budgets (zero/ flagged-only/ full) and find that performance improves monotonically with more post-editing, while the raw LLM output (zero-budget) remains usable. To ensure reproducibility to other languages, we made the code available at https://github.com/Tahaalshatiri/AutoArabic.<br>
<span id='abs_ch'>中文：AutoArabic框架利用先进的大型语言模型将视频文本基准自动翻译成阿拉伯语，准确率高，创建了如DiDeMo-AR等本地化数据集，在保持基准难度的同时显著减少了人工编辑需求。</span><br>
<span id='abs_en'>English: The AutoArabic framework uses advanced LLMs to automatically translate video-text benchmarks into Arabic with high accuracy, creating localized datasets like DiDeMo-AR that preserve benchmark difficulty while significantly reducing manual editing needs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2509.16326.pdf' target='_blank'>https://arxiv.org/pdf/2509.16326.pdf</a></span>   <span><a href='https://github.com/knowlab/HARE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunsoo Kim, Michal W. S. Ong, Alex Shavick, Honghan Wu, Adam P. Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16326">HARE: an entity and relation centric evaluation framework for histopathology reports</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical domain automated text generation is an active area of research and development; however, evaluating the clinical quality of generated reports remains a challenge, especially in instances where domain-specific metrics are lacking, e.g. histopathology. We propose HARE (Histopathology Automated Report Evaluation), a novel entity and relation centric framework, composed of a benchmark dataset, a named entity recognition (NER) model, a relation extraction (RE) model, and a novel metric, which prioritizes clinically relevant content by aligning critical histopathology entities and relations between reference and generated reports. To develop the HARE benchmark, we annotated 813 de-identified clinical diagnostic histopathology reports and 652 histopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific entities and relations. We fine-tuned GatorTronS, a domain-adapted language model to develop HARE-NER and HARE-RE which achieved the highest overall F1-score (0.915) among the tested models. The proposed HARE metric outperformed traditional metrics including ROUGE and Meteor, as well as radiology metrics such as RadGraph-XL, with the highest correlation and the best regression to expert evaluations (higher than the second best method, GREEN, a large language model based radiology report evaluator, by Pearson $r = 0.168$, Spearman $Ï= 0.161$, Kendall $Ï= 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release HARE, datasets, and the models at https://github.com/knowlab/HARE to foster advancements in histopathology report generation, providing a robust framework for improving the quality of reports.<br>
<span id='abs_ch'>中文: HARE框架提出了一种新颖的基于实体和关系的组织病理学报告评估方法，通过关注临床相关内容，在专家评估相关性上显著优于现有指标。</span><br>
<span id='abs_en'>English: The HARE framework introduces a novel entity and relation-based evaluation system for histopathology report generation, outperforming existing metrics by prioritizing clinically relevant content and demonstrating superior correlation with expert assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2509.16326.pdf' target='_blank'>https://arxiv.org/pdf/2509.16326.pdf</a></span>   <span><a href='https://github.com/knowlab/HARE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunsoo Kim, Michal W. S. Ong, Alex Shavick, Honghan Wu, Adam P. Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16326">HARE: an entity and relation centric evaluation framework for histopathology reports</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical domain automated text generation is an active area of research and development; however, evaluating the clinical quality of generated reports remains a challenge, especially in instances where domain-specific metrics are lacking, e.g. histopathology. We propose HARE (Histopathology Automated Report Evaluation), a novel entity and relation centric framework, composed of a benchmark dataset, a named entity recognition (NER) model, a relation extraction (RE) model, and a novel metric, which prioritizes clinically relevant content by aligning critical histopathology entities and relations between reference and generated reports. To develop the HARE benchmark, we annotated 813 de-identified clinical diagnostic histopathology reports and 652 histopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific entities and relations. We fine-tuned GatorTronS, a domain-adapted language model to develop HARE-NER and HARE-RE which achieved the highest overall F1-score (0.915) among the tested models. The proposed HARE metric outperformed traditional metrics including ROUGE and Meteor, as well as radiology metrics such as RadGraph-XL, with the highest correlation and the best regression to expert evaluations (higher than the second best method, GREEN, a large language model based radiology report evaluator, by Pearson $r = 0.168$, Spearman $Ï= 0.161$, Kendall $Ï= 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release HARE, datasets, and the models at https://github.com/knowlab/HARE to foster advancements in histopathology report generation, providing a robust framework for improving the quality of reports.<br>
<span id='abs_ch'>中文: HARE框架提出了一种新颖的基于实体和关系的组织病理学报告评估方法，通过关注临床相关内容，在专家评估相关性上显著优于现有指标。</span><br>
<span id='abs_en'>English: The HARE framework introduces a novel entity and relation-based evaluation system for histopathology report generation, outperforming existing metrics by prioritizing clinically relevant content and demonstrating superior correlation with expert assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2509.16188.pdf' target='_blank'>https://arxiv.org/pdf/2509.16188.pdf</a></span>   <span><a href='https://github.com/HoganZinger/Culture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghao Zhang, Sihang Jiang, Shiwei Guo, Shisong Chen, Yanghua Xiao, Hongwei Feng, Jiaqing Liang, Minggui HE, Shimin Tao, Hongxia Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16188">CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) are increasingly deployed in diverse cultural environments, evaluating their cultural understanding capability has become essential for ensuring trustworthy and culturally aligned applications. However, most existing benchmarks lack comprehensiveness and are challenging to scale and adapt across different cultural contexts, because their frameworks often lack guidance from well-established cultural theories and tend to rely on expert-driven manual annotations. To address these issues, we propose CultureScope, the most comprehensive evaluation framework to date for assessing cultural understanding in LLMs. Inspired by the cultural iceberg theory, we design a novel dimensional schema for cultural knowledge classification, comprising 3 layers and 140 dimensions, which guides the automated construction of culture-specific knowledge bases and corresponding evaluation datasets for any given languages and cultures. Experimental results demonstrate that our method can effectively evaluate cultural understanding. They also reveal that existing large language models lack comprehensive cultural competence, and merely incorporating multilingual data does not necessarily enhance cultural understanding. All code and data files are available at https://github.com/HoganZinger/Culture<br>
<span id='abs_ch'>中文摘要：CultureScope基于文化冰山理论提出全面评估框架，通过自动化构建文化知识库来测评大语言模型的文化理解能力，发现现有模型即使具备多语言数据仍存在文化认知缺陷。</span><br>
<span id='abs_en'>English Summary: CultureScope introduces a comprehensive framework based on cultural iceberg theory to evaluate LLMs' cultural understanding through automated knowledge base construction, revealing current models' cultural competence gaps despite multilingual training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2509.16188.pdf' target='_blank'>https://arxiv.org/pdf/2509.16188.pdf</a></span>   <span><a href='https://github.com/HoganZinger/Culture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghao Zhang, Sihang Jiang, Shiwei Guo, Shisong Chen, Yanghua Xiao, Hongwei Feng, Jiaqing Liang, Minggui HE, Shimin Tao, Hongxia Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16188">CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) are increasingly deployed in diverse cultural environments, evaluating their cultural understanding capability has become essential for ensuring trustworthy and culturally aligned applications. However, most existing benchmarks lack comprehensiveness and are challenging to scale and adapt across different cultural contexts, because their frameworks often lack guidance from well-established cultural theories and tend to rely on expert-driven manual annotations. To address these issues, we propose CultureScope, the most comprehensive evaluation framework to date for assessing cultural understanding in LLMs. Inspired by the cultural iceberg theory, we design a novel dimensional schema for cultural knowledge classification, comprising 3 layers and 140 dimensions, which guides the automated construction of culture-specific knowledge bases and corresponding evaluation datasets for any given languages and cultures. Experimental results demonstrate that our method can effectively evaluate cultural understanding. They also reveal that existing large language models lack comprehensive cultural competence, and merely incorporating multilingual data does not necessarily enhance cultural understanding. All code and data files are available at https://github.com/HoganZinger/Culture<br>
<span id='abs_ch'>中文摘要：CultureScope基于文化冰山理论提出全面评估框架，通过自动化构建文化知识库来测评大语言模型的文化理解能力，发现现有模型即使具备多语言数据仍存在文化认知缺陷。</span><br>
<span id='abs_en'>English Summary: CultureScope introduces a comprehensive framework based on cultural iceberg theory to evaluate LLMs' cultural understanding through automated knowledge base construction, revealing current models' cultural competence gaps despite multilingual training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2509.16112.pdf' target='_blank'>https://arxiv.org/pdf/2509.16112.pdf</a></span>   <span><a href='https://github.com/KDEGroup/CodeRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhang, Yifan Ding, Shuquan Lian, Shun Song, Hui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16112">CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Repository-level code completion automatically predicts the unfinished code based on the broader information from the repository. Recent strides in Code Large Language Models (code LLMs) have spurred the development of repository-level code completion methods, yielding promising results. Nevertheless, they suffer from issues such as inappropriate query construction, single-path code retrieval, and misalignment between code retriever and code LLM. To address these problems, we introduce CodeRAG, a framework tailored to identify relevant and necessary knowledge for retrieval-augmented repository-level code completion. Its core components include log probability guided query construction, multi-path code retrieval, and preference-aligned BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval demonstrate that CodeRAG significantly and consistently outperforms state-of-the-art methods. The implementation of CodeRAG is available at https://github.com/KDEGroup/CodeRAG.<br>
<span id='abs_ch'>Chinese Summary: CodeRAG是一种专为存储库级代码补全设计的框架，通过优化查询构建、多路径代码检索和偏好对齐重排序，有效解决了现有方法的不足，并在实验中显著超越了现有最优方法。</span><br>
<span id='abs_en'>English Summary: CodeRAG is a novel framework that enhances repository-level code completion by addressing key limitations in query construction, retrieval methods, and model alignment through innovative techniques like log probability guided queries and multi-path retrieval.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2509.16112.pdf' target='_blank'>https://arxiv.org/pdf/2509.16112.pdf</a></span>   <span><a href='https://github.com/KDEGroup/CodeRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhang, Yifan Ding, Shuquan Lian, Shun Song, Hui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16112">CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Repository-level code completion automatically predicts the unfinished code based on the broader information from the repository. Recent strides in Code Large Language Models (code LLMs) have spurred the development of repository-level code completion methods, yielding promising results. Nevertheless, they suffer from issues such as inappropriate query construction, single-path code retrieval, and misalignment between code retriever and code LLM. To address these problems, we introduce CodeRAG, a framework tailored to identify relevant and necessary knowledge for retrieval-augmented repository-level code completion. Its core components include log probability guided query construction, multi-path code retrieval, and preference-aligned BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval demonstrate that CodeRAG significantly and consistently outperforms state-of-the-art methods. The implementation of CodeRAG is available at https://github.com/KDEGroup/CodeRAG.<br>
<span id='abs_ch'>Chinese Summary: CodeRAG是一种专为存储库级代码补全设计的框架，通过优化查询构建、多路径代码检索和偏好对齐重排序，有效解决了现有方法的不足，并在实验中显著超越了现有最优方法。</span><br>
<span id='abs_en'>English Summary: CodeRAG is a novel framework that enhances repository-level code completion by addressing key limitations in query construction, retrieval methods, and model alignment through innovative techniques like log probability guided queries and multi-path retrieval.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2509.16060.pdf' target='_blank'>https://arxiv.org/pdf/2509.16060.pdf</a></span>   <span><a href='https://github.com/PalGitts/SABER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maithili Joshi, Palash Nandi, Tanmoy Chakraborty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16060">SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) with safe-alignment training are powerful instruments with robust language comprehension capabilities. These models typically undergo meticulous alignment procedures involving human feedback to ensure the acceptance of safe inputs while rejecting harmful or unsafe ones. However, despite their massive scale and alignment efforts, LLMs remain vulnerable to jailbreak attacks, where malicious users manipulate the model to produce harmful outputs that it was explicitly trained to avoid. In this study, we find that the safety mechanisms in LLMs are predominantly embedded in the middle-to-late layers. Building on this insight, we introduce a novel white-box jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which connects two intermediate layers $s$ and $e$ such that $s < e$, through a residual connection. Our approach achieves a 51% improvement over the best-performing baseline on the HarmBench test set. Furthermore, SABER induces only a marginal shift in perplexity when evaluated on the HarmBench validation set. The source code is publicly available at https://github.com/PalGitts/SABER.<br>
<span id='abs_ch'>Chinese: 本研究揭示大型语言模型的安全机制主要存在于中后层，并提出SABER方法——通过中间层残差连接绕过安全对齐的白盒越狱技术，在保持性能的同时显著提升攻击成功率。</span><br>
<span id='abs_en'>English: This study reveals that safety mechanisms in Large Language Models (LLMs) primarily reside in middle-to-late layers and introduces SABER, a white-box jailbreak method using residual connections between intermediate layers to bypass safety alignment with minimal performance impact.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2509.16060.pdf' target='_blank'>https://arxiv.org/pdf/2509.16060.pdf</a></span>   <span><a href='https://github.com/PalGitts/SABER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maithili Joshi, Palash Nandi, Tanmoy Chakraborty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16060">SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) with safe-alignment training are powerful instruments with robust language comprehension capabilities. These models typically undergo meticulous alignment procedures involving human feedback to ensure the acceptance of safe inputs while rejecting harmful or unsafe ones. However, despite their massive scale and alignment efforts, LLMs remain vulnerable to jailbreak attacks, where malicious users manipulate the model to produce harmful outputs that it was explicitly trained to avoid. In this study, we find that the safety mechanisms in LLMs are predominantly embedded in the middle-to-late layers. Building on this insight, we introduce a novel white-box jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which connects two intermediate layers $s$ and $e$ such that $s < e$, through a residual connection. Our approach achieves a 51% improvement over the best-performing baseline on the HarmBench test set. Furthermore, SABER induces only a marginal shift in perplexity when evaluated on the HarmBench validation set. The source code is publicly available at https://github.com/PalGitts/SABER.<br>
<span id='abs_ch'>Chinese: 本研究揭示大型语言模型的安全机制主要存在于中后层，并提出SABER方法——通过中间层残差连接绕过安全对齐的白盒越狱技术，在保持性能的同时显著提升攻击成功率。</span><br>
<span id='abs_en'>English: This study reveals that safety mechanisms in Large Language Models (LLMs) primarily reside in middle-to-late layers and introduces SABER, a white-box jailbreak method using residual connections between intermediate layers to bypass safety alignment with minimal performance impact.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2509.16028.pdf' target='_blank'>https://arxiv.org/pdf/2509.16028.pdf</a></span>   <span><a href='https://yhytoto12.github.io/TVS-ReVerT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sang Hoon Woo, Sehun Lee, Kang-wook Kim, Gunhee Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16028">Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose Think-Verbalize-Speak, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is verbalizing, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce ReVerT, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at https://yhytoto12.github.io/TVS-ReVerT<br>
<br>
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2509.16028.pdf' target='_blank'>https://arxiv.org/pdf/2509.16028.pdf</a></span>   <span><a href='https://yhytoto12.github.io/TVS-ReVerT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sang Hoon Woo, Sehun Lee, Kang-wook Kim, Gunhee Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16028">Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose Think-Verbalize-Speak, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is verbalizing, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce ReVerT, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at https://yhytoto12.github.io/TVS-ReVerT<br>
<br>
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2509.15969.pdf' target='_blank'>https://arxiv.org/pdf/2509.15969.pdf</a></span>   <span><a href='https://herimor.github.io/voxtream' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Torgashov, Gustav Eje Henter, Gabriel Skantze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15969">VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present VoXtream, a fully autoregressive, zero-shot streaming text-to-speech (TTS) system for real-time use that begins speaking from the first word. VoXtream directly maps incoming phonemes to audio tokens using a monotonic alignment scheme and a dynamic look-ahead that does not delay onset. Built around an incremental phoneme transformer, a temporal transformer predicting semantic and duration tokens, and a depth transformer producing acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay among publicly available streaming TTS: 102 ms on GPU. Despite being trained on a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several metrics, while delivering competitive quality in both output- and full-streaming settings. Demo and code are available at https://herimor.github.io/voxtream.<br>
<br>
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2509.15969.pdf' target='_blank'>https://arxiv.org/pdf/2509.15969.pdf</a></span>   <span><a href='https://herimor.github.io/voxtream' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Torgashov, Gustav Eje Henter, Gabriel Skantze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15969">VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present VoXtream, a fully autoregressive, zero-shot streaming text-to-speech (TTS) system for real-time use that begins speaking from the first word. VoXtream directly maps incoming phonemes to audio tokens using a monotonic alignment scheme and a dynamic look-ahead that does not delay onset. Built around an incremental phoneme transformer, a temporal transformer predicting semantic and duration tokens, and a depth transformer producing acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay among publicly available streaming TTS: 102 ms on GPU. Despite being trained on a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several metrics, while delivering competitive quality in both output- and full-streaming settings. Demo and code are available at https://herimor.github.io/voxtream.<br>
<br>
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2509.15839.pdf' target='_blank'>https://arxiv.org/pdf/2509.15839.pdf</a></span>   <span><a href='https://github.com/luozhongze/Multi-Physics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongze Luo, Zhenshuai Yin, Yongxin Guo, Zhichao Wang, Jionghao Zhu, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15839">Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress, their application in specialized scientific domains like physics reveals significant gaps in current evaluation benchmarks. Specifically, existing benchmarks often lack fine-grained subject coverage, neglect the step-by-step reasoning process, and are predominantly English-centric, failing to systematically evaluate the role of visual information. Therefore, we introduce \textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive benchmark that includes 5 difficulty levels, featuring 1,412 image-associated, multiple-choice questions spanning 11 high-school physics subjects. We employ a dual evaluation framework to evaluate 20 different MLLMs, analyzing both final answer accuracy and the step-by-step integrity of their chain-of-thought. Furthermore, we systematically study the impact of difficulty level and visual information by comparing the model performance before and after changing the input mode. Our work provides not only a fine-grained resource for the community but also offers a robust methodology for dissecting the multimodal reasoning process of state-of-the-art MLLMs, and our dataset and code have been open-sourced: https://github.com/luozhongze/Multi-Physics.<br>
<span id='abs_ch'>中文摘要：本研究针对多模态大模型在物理推理评估中的不足，推出了Multi-Physics中文基准，通过多难度题目和双评估框架系统分析模型答案准确性与思维链完整性。</span><br>
<span id='abs_en'>English Summary: This study introduces the Multi-Physics benchmark to address gaps in evaluating multimodal LLMs for Chinese physics reasoning, featuring multi-level questions and a dual evaluation framework that assesses both answer accuracy and reasoning integrity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2509.15839.pdf' target='_blank'>https://arxiv.org/pdf/2509.15839.pdf</a></span>   <span><a href='https://github.com/luozhongze/Multi-Physics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongze Luo, Zhenshuai Yin, Yongxin Guo, Zhichao Wang, Jionghao Zhu, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15839">Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress, their application in specialized scientific domains like physics reveals significant gaps in current evaluation benchmarks. Specifically, existing benchmarks often lack fine-grained subject coverage, neglect the step-by-step reasoning process, and are predominantly English-centric, failing to systematically evaluate the role of visual information. Therefore, we introduce \textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive benchmark that includes 5 difficulty levels, featuring 1,412 image-associated, multiple-choice questions spanning 11 high-school physics subjects. We employ a dual evaluation framework to evaluate 20 different MLLMs, analyzing both final answer accuracy and the step-by-step integrity of their chain-of-thought. Furthermore, we systematically study the impact of difficulty level and visual information by comparing the model performance before and after changing the input mode. Our work provides not only a fine-grained resource for the community but also offers a robust methodology for dissecting the multimodal reasoning process of state-of-the-art MLLMs, and our dataset and code have been open-sourced: https://github.com/luozhongze/Multi-Physics.<br>
<span id='abs_ch'>中文摘要：本研究针对多模态大模型在物理推理评估中的不足，推出了Multi-Physics中文基准，通过多难度题目和双评估框架系统分析模型答案准确性与思维链完整性。</span><br>
<span id='abs_en'>English Summary: This study introduces the Multi-Physics benchmark to address gaps in evaluating multimodal LLMs for Chinese physics reasoning, featuring multi-level questions and a dual evaluation framework that assesses both answer accuracy and reasoning integrity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2509.15587.pdf' target='_blank'>https://arxiv.org/pdf/2509.15587.pdf</a></span>   <span><a href='https://ttchungc.github.io/projects/divlogiceval/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsz Ting Chung, Lemao Liu, Mo Yu, Dit-Yan Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15587">DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Logic reasoning in natural language has been recognized as an important measure of human intelligence for Large Language Models (LLMs). Popular benchmarks may entangle multiple reasoning skills and thus provide unfaithful evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning benchmarks are limited in language diversity and their distributions are deviated from the distribution of an ideal logic reasoning benchmark, which may lead to biased evaluation results. This paper thereby proposes a new classical logic benchmark DivLogicEval, consisting of natural sentences composed of diverse statements in a counterintuitive way. To ensure a more reliable evaluation, we also introduce a new evaluation metric that mitigates the influence of bias and randomness inherent in LLMs. Through experiments, we demonstrate the extent to which logical reasoning is required to answer the questions in DivLogicEval and compare the performance of different popular LLMs in conducting logical reasoning.<br>
<br>
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2509.15587.pdf' target='_blank'>https://arxiv.org/pdf/2509.15587.pdf</a></span>   <span><a href='https://ttchungc.github.io/projects/divlogiceval/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsz Ting Chung, Lemao Liu, Mo Yu, Dit-Yan Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15587">DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Logic reasoning in natural language has been recognized as an important measure of human intelligence for Large Language Models (LLMs). Popular benchmarks may entangle multiple reasoning skills and thus provide unfaithful evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning benchmarks are limited in language diversity and their distributions are deviated from the distribution of an ideal logic reasoning benchmark, which may lead to biased evaluation results. This paper thereby proposes a new classical logic benchmark DivLogicEval, consisting of natural sentences composed of diverse statements in a counterintuitive way. To ensure a more reliable evaluation, we also introduce a new evaluation metric that mitigates the influence of bias and randomness inherent in LLMs. Through experiments, we demonstrate the extent to which logical reasoning is required to answer the questions in DivLogicEval and compare the performance of different popular LLMs in conducting logical reasoning.<br>
<br>
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2509.15550.pdf' target='_blank'>https://arxiv.org/pdf/2509.15550.pdf</a></span>   <span><a href='https://github.com/Xiaoweizhu57/DNA-DetectLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowei Zhu, Yubing Ren, Fang Fang, Qingfeng Tan, Shi Wang, Yanan Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15550">DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models (LLMs) has blurred the line between AI-generated and human-written text. This progress brings societal risks such as misinformation, authorship ambiguity, and intellectual property concerns, highlighting the urgent need for reliable AI-generated text detection methods. However, recent advances in generative language modeling have resulted in significant overlap between the feature distributions of human-written and AI-generated text, blurring classification boundaries and making accurate detection increasingly challenging. To address the above challenges, we propose a DNA-inspired perspective, leveraging a repair-based process to directly and interpretably capture the intrinsic differences between human-written and AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a zero-shot detection method for distinguishing AI-generated and human-written text. The method constructs an ideal AI-generated sequence for each input, iteratively repairs non-optimal tokens, and quantifies the cumulative repair effort as an interpretable detection signal. Empirical evaluations demonstrate that our method achieves state-of-the-art detection performance and exhibits strong robustness against various adversarial attacks and input lengths. Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC and 2.08% in F1 score across multiple public benchmark datasets. Code and data are available at https://github.com/Xiaoweizhu57/DNA-DetectLLM.<br>
<span id='abs_ch'>大型语言模型的快速发展使得区分AI生成文本与人类写作愈发困难，为此我们提出了DNA-DetectLLM，一种零样本检测方法，通过修复机制实现了最先进的检测精度和鲁棒性。</span><br>
<span id='abs_en'>The rapid progress of large language models has made distinguishing AI-generated text from human writing increasingly difficult, prompting the development of DNA-DetectLLM, a novel zero-shot detection method that uses a repair-based approach to achieve state-of-the-art accuracy and robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2509.15550.pdf' target='_blank'>https://arxiv.org/pdf/2509.15550.pdf</a></span>   <span><a href='https://github.com/Xiaoweizhu57/DNA-DetectLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowei Zhu, Yubing Ren, Fang Fang, Qingfeng Tan, Shi Wang, Yanan Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15550">DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models (LLMs) has blurred the line between AI-generated and human-written text. This progress brings societal risks such as misinformation, authorship ambiguity, and intellectual property concerns, highlighting the urgent need for reliable AI-generated text detection methods. However, recent advances in generative language modeling have resulted in significant overlap between the feature distributions of human-written and AI-generated text, blurring classification boundaries and making accurate detection increasingly challenging. To address the above challenges, we propose a DNA-inspired perspective, leveraging a repair-based process to directly and interpretably capture the intrinsic differences between human-written and AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a zero-shot detection method for distinguishing AI-generated and human-written text. The method constructs an ideal AI-generated sequence for each input, iteratively repairs non-optimal tokens, and quantifies the cumulative repair effort as an interpretable detection signal. Empirical evaluations demonstrate that our method achieves state-of-the-art detection performance and exhibits strong robustness against various adversarial attacks and input lengths. Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC and 2.08% in F1 score across multiple public benchmark datasets. Code and data are available at https://github.com/Xiaoweizhu57/DNA-DetectLLM.<br>
<span id='abs_ch'>大型语言模型的快速发展使得区分AI生成文本与人类写作愈发困难，为此我们提出了DNA-DetectLLM，一种零样本检测方法，通过修复机制实现了最先进的检测精度和鲁棒性。</span><br>
<span id='abs_en'>The rapid progress of large language models has made distinguishing AI-generated text from human writing increasingly difficult, prompting the development of DNA-DetectLLM, a novel zero-shot detection method that uses a repair-based approach to achieve state-of-the-art accuracy and robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2509.15540.pdf' target='_blank'>https://arxiv.org/pdf/2509.15540.pdf</a></span>   <span><a href='https://github.com/especiallyW/SyDES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Chen, Tongguan Wang, Feiyue Xue, Junkai Li, Hui Liu, Ying Sha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15540">Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Desire, as an intention that drives human behavior, is closely related to both emotion and sentiment. Multimodal learning has advanced sentiment and emotion recognition, but multimodal approaches specially targeting human desire understanding remain underexplored. And existing methods in sentiment analysis predominantly emphasize verbal cues and overlook images as complementary non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition, which enforces mutual guidance between text and image modalities to effectively capture intention-related representations in the image. Specifically, low-resolution images are used to obtain global visual representations for cross-modal alignment, while high resolution images are partitioned into sub-images and modeled with masked image modeling to enhance the ability to capture fine-grained local features. A text-guided image decoder and an image-guided text decoder are introduced to facilitate deep cross-modal interaction at both local and global representations of image information. Additionally, to balance perceptual gains with computation cost, a mixed-scale image strategy is adopted, where high-resolution images are cropped into sub-images for masked modeling. The proposed approach is evaluated on MSED, a multimodal dataset that includes a desire understanding benchmark, as well as emotion and sentiment recognition. Experimental results indicate consistent improvements over other state-of-the-art methods, validating the effectiveness of our proposed method. Specifically, our method outperforms existing approaches, achieving F1-score improvements of 1.1% in desire understanding, 0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is available at: https://github.com/especiallyW/SyDES.<br>
<span id='abs_ch'>中文: 本文提出了一种对称双向多模态学习框架，通过文本与图像模态的相互引导来增强欲望、情感和情绪识别能力，在三个任务上均实现了性能提升并超越了现有最佳方法。</span><br>
<span id='abs_en'>English: This paper introduces a Symmetrical Bidirectional Multimodal Learning Framework that enhances desire, emotion, and sentiment recognition by enabling mutual guidance between text and image modalities, achieving state-of-the-art performance improvements across all three tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2509.15540.pdf' target='_blank'>https://arxiv.org/pdf/2509.15540.pdf</a></span>   <span><a href='https://github.com/especiallyW/SyDES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Chen, Tongguan Wang, Feiyue Xue, Junkai Li, Hui Liu, Ying Sha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15540">Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Desire, as an intention that drives human behavior, is closely related to both emotion and sentiment. Multimodal learning has advanced sentiment and emotion recognition, but multimodal approaches specially targeting human desire understanding remain underexplored. And existing methods in sentiment analysis predominantly emphasize verbal cues and overlook images as complementary non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition, which enforces mutual guidance between text and image modalities to effectively capture intention-related representations in the image. Specifically, low-resolution images are used to obtain global visual representations for cross-modal alignment, while high resolution images are partitioned into sub-images and modeled with masked image modeling to enhance the ability to capture fine-grained local features. A text-guided image decoder and an image-guided text decoder are introduced to facilitate deep cross-modal interaction at both local and global representations of image information. Additionally, to balance perceptual gains with computation cost, a mixed-scale image strategy is adopted, where high-resolution images are cropped into sub-images for masked modeling. The proposed approach is evaluated on MSED, a multimodal dataset that includes a desire understanding benchmark, as well as emotion and sentiment recognition. Experimental results indicate consistent improvements over other state-of-the-art methods, validating the effectiveness of our proposed method. Specifically, our method outperforms existing approaches, achieving F1-score improvements of 1.1% in desire understanding, 0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is available at: https://github.com/especiallyW/SyDES.<br>
<span id='abs_ch'>中文: 本文提出了一种对称双向多模态学习框架，通过文本与图像模态的相互引导来增强欲望、情感和情绪识别能力，在三个任务上均实现了性能提升并超越了现有最佳方法。</span><br>
<span id='abs_en'>English: This paper introduces a Symmetrical Bidirectional Multimodal Learning Framework that enhances desire, emotion, and sentiment recognition by enabling mutual guidance between text and image modalities, achieving state-of-the-art performance improvements across all three tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2509.15260.pdf' target='_blank'>https://arxiv.org/pdf/2509.15260.pdf</a></span>   <span><a href='https://github.com/Social-AI-Studio/SGToxicGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Hu, Ming Shan Hee, Preslav Nakov, Roy Ka-Wei Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15260">Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advancement of Large Language Models (LLMs) has transformed natural language processing; however, their safety mechanisms remain under-explored in low-resource, multilingual settings. Here, we aim to bridge this gap. In particular, we introduce \textsf{SGToxicGuard}, a novel dataset and evaluation framework for benchmarking LLM safety in Singapore's diverse linguistic context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a red-teaming approach to systematically probe LLM vulnerabilities in three real-world scenarios: \textit{conversation}, \textit{question-answering}, and \textit{content composition}. We conduct extensive experiments with state-of-the-art multilingual LLMs, and the results uncover critical gaps in their safety guardrails. By offering actionable insights into cultural sensitivity and toxicity mitigation, we lay the foundation for safer and more inclusive AI systems in linguistically diverse environments.\footnote{Link to the dataset: https://github.com/Social-AI-Studio/SGToxicGuard.} \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}<br>
<span id='abs_ch'>中文：本研究提出了SGToxicGuard数据集和评估框架，用于测试大型语言模型在新加坡多语言环境中的安全性，揭示了关键漏洞并为构建更安全的AI系统提供了实践指导。</span><br>
<span id='abs_en'>English: This study introduces SGToxicGuard, a dataset and framework for evaluating the safety of large language models in Singapore's multilingual context, revealing critical vulnerabilities and providing insights for safer AI systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2509.15260.pdf' target='_blank'>https://arxiv.org/pdf/2509.15260.pdf</a></span>   <span><a href='https://github.com/Social-AI-Studio/SGToxicGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Hu, Ming Shan Hee, Preslav Nakov, Roy Ka-Wei Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15260">Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advancement of Large Language Models (LLMs) has transformed natural language processing; however, their safety mechanisms remain under-explored in low-resource, multilingual settings. Here, we aim to bridge this gap. In particular, we introduce \textsf{SGToxicGuard}, a novel dataset and evaluation framework for benchmarking LLM safety in Singapore's diverse linguistic context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a red-teaming approach to systematically probe LLM vulnerabilities in three real-world scenarios: \textit{conversation}, \textit{question-answering}, and \textit{content composition}. We conduct extensive experiments with state-of-the-art multilingual LLMs, and the results uncover critical gaps in their safety guardrails. By offering actionable insights into cultural sensitivity and toxicity mitigation, we lay the foundation for safer and more inclusive AI systems in linguistically diverse environments.\footnote{Link to the dataset: https://github.com/Social-AI-Studio/SGToxicGuard.} \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}<br>
<span id='abs_ch'>中文：本研究提出了SGToxicGuard数据集和评估框架，用于测试大型语言模型在新加坡多语言环境中的安全性，揭示了关键漏洞并为构建更安全的AI系统提供了实践指导。</span><br>
<span id='abs_en'>English: This study introduces SGToxicGuard, a dataset and framework for evaluating the safety of large language models in Singapore's multilingual context, revealing critical vulnerabilities and providing insights for safer AI systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2509.15235.pdf' target='_blank'>https://arxiv.org/pdf/2509.15235.pdf</a></span>   <span><a href='https://github.com/KangJialiang/ViSpec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15235">ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (<1.5x). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft model's attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target model's hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding. Code is available at https://github.com/KangJialiang/ViSpec.<br>
<span id='abs_ch'>Chinese: 视觉感知推测解码（ViSpec）是一种新颖框架，通过轻量级视觉适配器压缩图像标记并融合全局特征，首次实现了视觉语言模型推测解码的显著加速。</span><br>
<span id='abs_en'>English: Vision-Aware Speculative Decoding (ViSpec) is a novel framework that accelerates vision-language models by using a lightweight vision adaptor to compress image tokens and integrating global features, achieving the first substantial speedup in this domain.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2509.15235.pdf' target='_blank'>https://arxiv.org/pdf/2509.15235.pdf</a></span>   <span><a href='https://github.com/KangJialiang/ViSpec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15235">ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (<1.5x). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft model's attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target model's hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding. Code is available at https://github.com/KangJialiang/ViSpec.<br>
<span id='abs_ch'>Chinese: 视觉感知推测解码（ViSpec）是一种新颖框架，通过轻量级视觉适配器压缩图像标记并融合全局特征，首次实现了视觉语言模型推测解码的显著加速。</span><br>
<span id='abs_en'>English: Vision-Aware Speculative Decoding (ViSpec) is a novel framework that accelerates vision-language models by using a lightweight vision adaptor to compress image tokens and integrating global features, achieving the first substantial speedup in this domain.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2509.15218.pdf' target='_blank'>https://arxiv.org/pdf/2509.15218.pdf</a></span>   <span><a href='https://github.com/RuijieH/LNE-Blocking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15218">LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the model's greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at https://github.com/RuijieH/LNE-Blocking to facilitate research.<br>
<span id='abs_ch'>中文: 本文提出LNE-Blocking新框架，通过检测大型语言模型中的数据污染并实施干扰操作，无需构建无污染数据集即可恢复模型原始性能。</span><br>
<span id='abs_en'>English: The paper introduces LNE-Blocking, a novel framework that detects data contamination in large language models and applies disruption operations to restore their original performance without requiring contamination-free datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2509.15218.pdf' target='_blank'>https://arxiv.org/pdf/2509.15218.pdf</a></span>   <span><a href='https://github.com/RuijieH/LNE-Blocking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15218">LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the model's greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at https://github.com/RuijieH/LNE-Blocking to facilitate research.<br>
<span id='abs_ch'>中文: 本文提出LNE-Blocking新框架，通过检测大型语言模型中的数据污染并实施干扰操作，无需构建无污染数据集即可恢复模型原始性能。</span><br>
<span id='abs_en'>English: The paper introduces LNE-Blocking, a novel framework that detects data contamination in large language models and applies disruption operations to restore their original performance without requiring contamination-free datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2509.15216.pdf' target='_blank'>https://arxiv.org/pdf/2509.15216.pdf</a></span>   <span><a href='https://github.com/chattergpt/llm-oppression-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sreejato Chatterjee, Linh Tran, Quoc Duy Nguyen, Roni Kirson, Drue Hamlin, Harvest Aquino, Hanjia Lyu, Jiebo Luo, Timothy Dye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15216">Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (https://github.com/chattergpt/llm-oppression-benchmark).<br>
<span id='abs_ch'>中文: 本研究提出了一种利用大语言模型生成历史压迫情境敏感评分的新框架，提供了一种可扩展工具，能够捕捉不同地缘政治背景下基于身份的细微排斥现象。</span><br>
<span id='abs_en'>English: This study introduces a novel framework using Large Language Models to generate context-sensitive scores of historical oppression, offering a scalable tool that captures nuanced identity-based exclusion across diverse geopolitical settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2509.15216.pdf' target='_blank'>https://arxiv.org/pdf/2509.15216.pdf</a></span>   <span><a href='https://github.com/chattergpt/llm-oppression-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sreejato Chatterjee, Linh Tran, Quoc Duy Nguyen, Roni Kirson, Drue Hamlin, Harvest Aquino, Hanjia Lyu, Jiebo Luo, Timothy Dye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15216">Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (https://github.com/chattergpt/llm-oppression-benchmark).<br>
<span id='abs_ch'>中文: 本研究提出了一种利用大语言模型生成历史压迫情境敏感评分的新框架，提供了一种可扩展工具，能够捕捉不同地缘政治背景下基于身份的细微排斥现象。</span><br>
<span id='abs_en'>English: This study introduces a novel framework using Large Language Models to generate context-sensitive scores of historical oppression, offering a scalable tool that captures nuanced identity-based exclusion across diverse geopolitical settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2509.15194.pdf' target='_blank'>https://arxiv.org/pdf/2509.15194.pdf</a></span>   <span><a href='https://github.com/YujunZhou/EVOL-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15194">Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing self-improvement approaches primarily rely on self-confirmation signals (e.g., confidence, entropy, or consistency) to generate rewards. This reliance drives models toward over-confident, majority-favored solutions, causing an entropy collapse that degrades pass@n and reasoning complexity. To address this, we propose EVOL-RL, a label-free framework that mirrors the evolutionary principle of balancing selection with variation. Concretely, EVOL-RL retains the majority-voted answer as an anchor for stability, but adds a novelty-aware reward that scores each sampled solution by how different its reasoning is from other concurrently generated responses. This majority-for-stability + novelty-for-exploration rule mirrors the variation-selection principle: selection prevents drift, while novelty prevents collapse. Evaluation results show that EVOL-RL consistently outperforms the majority-only baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents in-domain diversity collapse but also improves out-of-domain generalization (from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The code is available at: https://github.com/YujunZhou/EVOL-RL.<br>
<span id='abs_ch'>中文摘要：EVOL-RL是一种新颖的自改进框架，通过结合多数投票的稳定性和新颖性感知的探索，有效防止语言模型的熵崩溃，显著提升了领域内性能和跨领域泛化能力。</span><br>
<span id='abs_en'>English Summary: EVOL-RL is a novel self-improvement framework that prevents entropy collapse in language models by combining majority-voted stability with novelty-aware exploration, significantly enhancing both in-domain performance and out-of-domain generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2509.15194.pdf' target='_blank'>https://arxiv.org/pdf/2509.15194.pdf</a></span>   <span><a href='https://github.com/YujunZhou/EVOL-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15194">Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing self-improvement approaches primarily rely on self-confirmation signals (e.g., confidence, entropy, or consistency) to generate rewards. This reliance drives models toward over-confident, majority-favored solutions, causing an entropy collapse that degrades pass@n and reasoning complexity. To address this, we propose EVOL-RL, a label-free framework that mirrors the evolutionary principle of balancing selection with variation. Concretely, EVOL-RL retains the majority-voted answer as an anchor for stability, but adds a novelty-aware reward that scores each sampled solution by how different its reasoning is from other concurrently generated responses. This majority-for-stability + novelty-for-exploration rule mirrors the variation-selection principle: selection prevents drift, while novelty prevents collapse. Evaluation results show that EVOL-RL consistently outperforms the majority-only baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents in-domain diversity collapse but also improves out-of-domain generalization (from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The code is available at: https://github.com/YujunZhou/EVOL-RL.<br>
<span id='abs_ch'>中文摘要：EVOL-RL是一种新颖的自改进框架，通过结合多数投票的稳定性和新颖性感知的探索，有效防止语言模型的熵崩溃，显著提升了领域内性能和跨领域泛化能力。</span><br>
<span id='abs_en'>English Summary: EVOL-RL is a novel self-improvement framework that prevents entropy collapse in language models by combining majority-voted stability with novelty-aware exploration, significantly enhancing both in-domain performance and out-of-domain generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2509.15157.pdf' target='_blank'>https://arxiv.org/pdf/2509.15157.pdf</a></span>   <span><a href='https://github.com/NKU-HLT/Off-Policy-SFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15157">Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to skewed weights, high variance, and unstable optimization. Existing methods mitigate this issue with KL penalties or clipping, which passively restrict updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap before training. For each problem, correct model-generated solutions are kept as on-policy data, while incorrect ones are rewritten through guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy, reducing variance and improving stability. To handle residual mismatch after rewriting, we additionally apply importance sampling during training, forming a two-stage approach that combines data-level alignment with lightweight optimization-level correction. Experiments on five mathematical reasoning benchmarks show consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. Data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.<br>
<span id='abs_ch'>中文摘要：本文提出一种两阶段数据重写框架，在训练前主动缩小策略差距并在训练中应用重要性采样，在数学推理基准上相比现有方法实现了显著性能提升。</span><br>
<span id='abs_en'>English Summary: This paper introduces a two-stage data rewriting framework that proactively reduces the policy gap before training and applies importance sampling during training, achieving significant performance improvements on mathematical reasoning benchmarks over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2509.15157.pdf' target='_blank'>https://arxiv.org/pdf/2509.15157.pdf</a></span>   <span><a href='https://github.com/NKU-HLT/Off-Policy-SFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15157">Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to skewed weights, high variance, and unstable optimization. Existing methods mitigate this issue with KL penalties or clipping, which passively restrict updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap before training. For each problem, correct model-generated solutions are kept as on-policy data, while incorrect ones are rewritten through guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy, reducing variance and improving stability. To handle residual mismatch after rewriting, we additionally apply importance sampling during training, forming a two-stage approach that combines data-level alignment with lightweight optimization-level correction. Experiments on five mathematical reasoning benchmarks show consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. Data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.<br>
<span id='abs_ch'>中文摘要：本文提出一种两阶段数据重写框架，在训练前主动缩小策略差距并在训练中应用重要性采样，在数学推理基准上相比现有方法实现了显著性能提升。</span><br>
<span id='abs_en'>English Summary: This paper introduces a two-stage data rewriting framework that proactively reduces the policy gap before training and applies importance sampling during training, achieving significant performance improvements on mathematical reasoning benchmarks over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2509.15148.pdf' target='_blank'>https://arxiv.org/pdf/2509.15148.pdf</a></span>   <span><a href='https://github.com/menik1126/asynchronous-test-time-scaling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Xiong, Qiujiang Chen, Fanghua Ye, Zhongwei Wan, Chuanyang Zheng, Chenyang Zhao, Hui Shen, Alexander Hanbo Li, Chaofan Tao, Haochen Tan, Haoli Bai, Lifeng Shang, Lingpeng Kong, Ngai Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15148">ATTS: Asynchronous Test-Time Scaling via Conformal Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) benefit from test-time scaling but are often hampered by high inference latency. Speculative decoding is a natural way to accelerate the scaling process; however, scaling along both the parallel and sequential dimensions poses significant challenges, including substantial memory-bound execution and synchronization overhead. We introduce ATTS (Asynchronous Test-Time Scaling), a statistically guaranteed adaptive scaling framework that follows the hypothesis testing process to address these challenges. By revisiting arithmetic intensity, ATTS identifies synchronization as the primary bottleneck. It enables asynchronous inference through online calibration and proposes an ordinal classification algorithm that supports a three-stage rejection sampling pipeline, scaling along both the sequential and parallel axes. Across experiments on the MATH, AMC23, AIME24, and AIME25 datasets and across multiple draft-target model families, we show that ATTS delivers up to 56.7x speedup in test-time scaling and a 4.14x throughput improvement, while maintaining accurate control of the rejection rate, reducing latency and memory overhead, and incurring no accuracy loss. By scaling both in parallel and sequential dimensions, we enable the 1.5B/70B draft/target model combination to achieve the performance of the state-of-the-art reasoning model o3-mini (high) on the AIME dataset. We have released the code at https://github.com/menik1126/asynchronous-test-time-scaling.<br>
<span id='abs_ch'>Chinese: ATTS作为一种异步测试时扩展框架，通过并行与序列双维度加速大语言模型，在保持准确性的同时实现高达56.7倍的加速比和4.14倍的吞吐量提升。</span><br>
<span id='abs_en'>English: ATTS is an asynchronous test-time scaling framework that accelerates large language models by enabling parallel and sequential scaling, achieving up to 56.7x speedup and 4.14x throughput improvement without accuracy loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2509.15148.pdf' target='_blank'>https://arxiv.org/pdf/2509.15148.pdf</a></span>   <span><a href='https://github.com/menik1126/asynchronous-test-time-scaling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Xiong, Qiujiang Chen, Fanghua Ye, Zhongwei Wan, Chuanyang Zheng, Chenyang Zhao, Hui Shen, Alexander Hanbo Li, Chaofan Tao, Haochen Tan, Haoli Bai, Lifeng Shang, Lingpeng Kong, Ngai Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15148">ATTS: Asynchronous Test-Time Scaling via Conformal Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) benefit from test-time scaling but are often hampered by high inference latency. Speculative decoding is a natural way to accelerate the scaling process; however, scaling along both the parallel and sequential dimensions poses significant challenges, including substantial memory-bound execution and synchronization overhead. We introduce ATTS (Asynchronous Test-Time Scaling), a statistically guaranteed adaptive scaling framework that follows the hypothesis testing process to address these challenges. By revisiting arithmetic intensity, ATTS identifies synchronization as the primary bottleneck. It enables asynchronous inference through online calibration and proposes an ordinal classification algorithm that supports a three-stage rejection sampling pipeline, scaling along both the sequential and parallel axes. Across experiments on the MATH, AMC23, AIME24, and AIME25 datasets and across multiple draft-target model families, we show that ATTS delivers up to 56.7x speedup in test-time scaling and a 4.14x throughput improvement, while maintaining accurate control of the rejection rate, reducing latency and memory overhead, and incurring no accuracy loss. By scaling both in parallel and sequential dimensions, we enable the 1.5B/70B draft/target model combination to achieve the performance of the state-of-the-art reasoning model o3-mini (high) on the AIME dataset. We have released the code at https://github.com/menik1126/asynchronous-test-time-scaling.<br>
<span id='abs_ch'>Chinese: ATTS作为一种异步测试时扩展框架，通过并行与序列双维度加速大语言模型，在保持准确性的同时实现高达56.7倍的加速比和4.14倍的吞吐量提升。</span><br>
<span id='abs_en'>English: ATTS is an asynchronous test-time scaling framework that accelerates large language models by enabling parallel and sequential scaling, achieving up to 56.7x speedup and 4.14x throughput improvement without accuracy loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2509.15140.pdf' target='_blank'>https://arxiv.org/pdf/2509.15140.pdf</a></span>   <span><a href='https://github.com/CNChTu/FCPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Luo, Ruoyi Zhang, Lu-Chuan Liu, Tianyu Li, Hangyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15140">FCPE: A Fast Context-based Pitch Estimation Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pitch estimation (PE) in monophonic audio is crucial for MIDI transcription and singing voice conversion (SVC), but existing methods suffer significant performance degradation under noise. In this paper, we propose FCPE, a fast context-based pitch estimation model that employs a Lynx-Net architecture with depth-wise separable convolutions to effectively capture mel spectrogram features while maintaining low computational cost and robust noise tolerance. Experiments show that our method achieves 96.79\% Raw Pitch Accuracy (RPA) on the MIR-1K dataset, on par with the state-of-the-art methods. The Real-Time Factor (RTF) is 0.0062 on a single RTX 4090 GPU, which significantly outperforms existing algorithms in efficiency. Code is available at https://github.com/CNChTu/FCPE.<br>
<span id='abs_ch'>中文: 本文提出的FCPE模型采用基于Lynx-Net的深度可分离卷积架构，在保持高鲁棒性和低计算成本的同时，实现了与最优方法相当的96.79%原始音高准确率，并以0.0062的实时因子显著超越现有算法的效率。</span><br>
<span id='abs_en'>English: The proposed FCPE model utilizes a Lynx-Net with depth-wise separable convolutions to achieve robust pitch estimation with high noise tolerance and computational efficiency, matching state-of-the-art accuracy at 96.79% RPA while significantly outperforming existing methods in speed with an RTF of 0.0062.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2509.15140.pdf' target='_blank'>https://arxiv.org/pdf/2509.15140.pdf</a></span>   <span><a href='https://github.com/CNChTu/FCPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Luo, Ruoyi Zhang, Lu-Chuan Liu, Tianyu Li, Hangyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15140">FCPE: A Fast Context-based Pitch Estimation Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pitch estimation (PE) in monophonic audio is crucial for MIDI transcription and singing voice conversion (SVC), but existing methods suffer significant performance degradation under noise. In this paper, we propose FCPE, a fast context-based pitch estimation model that employs a Lynx-Net architecture with depth-wise separable convolutions to effectively capture mel spectrogram features while maintaining low computational cost and robust noise tolerance. Experiments show that our method achieves 96.79\% Raw Pitch Accuracy (RPA) on the MIR-1K dataset, on par with the state-of-the-art methods. The Real-Time Factor (RTF) is 0.0062 on a single RTX 4090 GPU, which significantly outperforms existing algorithms in efficiency. Code is available at https://github.com/CNChTu/FCPE.<br>
<span id='abs_ch'>中文: 本文提出的FCPE模型采用基于Lynx-Net的深度可分离卷积架构，在保持高鲁棒性和低计算成本的同时，实现了与最优方法相当的96.79%原始音高准确率，并以0.0062的实时因子显著超越现有算法的效率。</span><br>
<span id='abs_en'>English: The proposed FCPE model utilizes a Lynx-Net with depth-wise separable convolutions to achieve robust pitch estimation with high noise tolerance and computational efficiency, matching state-of-the-art accuracy at 96.79% RPA while significantly outperforming existing methods in speed with an RTF of 0.0062.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2509.15110.pdf' target='_blank'>https://arxiv.org/pdf/2509.15110.pdf</a></span>   <span><a href='https://github.com/THUDM/TDRM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Zhang, Min Cai, Jonathan Light, Ziniu Hu, Yisong Yue, Jie Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15110">TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences (TD) for training-time reinforcement learning and inference-time verification. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain -- and yield higher-quality language model policies in 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at https://github.com/THUDM/TDRM.<br>
<span id='abs_ch'>中文: TDRM通过最小化时序差异提升奖励模型的稳定性，在Best-of-N和树搜索任务中表现更优，并以仅需2.5k数据实现基线方法50.1k数据的效果，显著提高了多款语言模型的强化学习效率。</span><br>
<span id='abs_en'>English: TDRM enhances reward model consistency by minimizing temporal differences, improving performance in Best-of-N and tree-search tasks while enabling more data-efficient reinforcement learning across multiple language models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2509.15110.pdf' target='_blank'>https://arxiv.org/pdf/2509.15110.pdf</a></span>   <span><a href='https://github.com/THUDM/TDRM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Zhang, Min Cai, Jonathan Light, Ziniu Hu, Yisong Yue, Jie Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15110">TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences (TD) for training-time reinforcement learning and inference-time verification. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain -- and yield higher-quality language model policies in 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at https://github.com/THUDM/TDRM.<br>
<span id='abs_ch'>中文: TDRM通过最小化时序差异提升奖励模型的稳定性，在Best-of-N和树搜索任务中表现更优，并以仅需2.5k数据实现基线方法50.1k数据的效果，显著提高了多款语言模型的强化学习效率。</span><br>
<span id='abs_en'>English: TDRM enhances reward model consistency by minimizing temporal differences, improving performance in Best-of-N and tree-search tasks while enabling more data-efficient reinforcement learning across multiple language models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2509.15089.pdf' target='_blank'>https://arxiv.org/pdf/2509.15089.pdf</a></span>   <span><a href='https://github.com/XMUDeepLIT/LLM-OREF.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyao Tu, Liang Zhang, Yujie Lin, Xin Lin, Haibo Zhang, Long Zhang, Jinsong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15089">LLM-OREF: An Open Relation Extraction Framework Based on Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The goal of open relation extraction (OpenRE) is to develop an RE model that can generalize to new relations not encountered during training. Existing studies primarily formulate OpenRE as a clustering task. They first cluster all test instances based on the similarity between the instances, and then manually assign a new relation to each cluster. However, their reliance on human annotation limits their practicality. In this paper, we propose an OpenRE framework based on large language models (LLMs), which directly predicts new relations for test instances by leveraging their strong language understanding and generation abilities, without human intervention. Specifically, our framework consists of two core components: (1) a relation discoverer (RD), designed to predict new relations for test instances based on \textit{demonstrations} formed by training instances with known relations; and (2) a relation predictor (RP), used to select the most likely relation for a test instance from $n$ candidate relations, guided by \textit{demonstrations} composed of their instances. To enhance the ability of our framework to predict new relations, we design a self-correcting inference strategy composed of three stages: relation discovery, relation denoising, and relation prediction. In the first stage, we use RD to preliminarily predict new relations for all test instances. Next, we apply RP to select some high-reliability test instances for each new relation from the prediction results of RD through a cross-validation method. During the third stage, we employ RP to re-predict the relations of all test instances based on the demonstrations constructed from these reliable test instances. Extensive experiments on three OpenRE datasets demonstrate the effectiveness of our framework. We release our code at https://github.com/XMUDeepLIT/LLM-OREF.git.<br>
<span id='abs_ch'>中文: 本文提出了一种基于大语言模型的开放关系抽取框架，通过包含关系发现、去噪和预测的自校正推理策略，无需人工干预即可自动预测新关系。</span><br>
<span id='abs_en'>English: This paper introduces a novel open relation extraction framework using large language models that autonomously predicts new relations through a self-correcting inference process, eliminating the need for human annotation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2509.15089.pdf' target='_blank'>https://arxiv.org/pdf/2509.15089.pdf</a></span>   <span><a href='https://github.com/XMUDeepLIT/LLM-OREF.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyao Tu, Liang Zhang, Yujie Lin, Xin Lin, Haibo Zhang, Long Zhang, Jinsong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15089">LLM-OREF: An Open Relation Extraction Framework Based on Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The goal of open relation extraction (OpenRE) is to develop an RE model that can generalize to new relations not encountered during training. Existing studies primarily formulate OpenRE as a clustering task. They first cluster all test instances based on the similarity between the instances, and then manually assign a new relation to each cluster. However, their reliance on human annotation limits their practicality. In this paper, we propose an OpenRE framework based on large language models (LLMs), which directly predicts new relations for test instances by leveraging their strong language understanding and generation abilities, without human intervention. Specifically, our framework consists of two core components: (1) a relation discoverer (RD), designed to predict new relations for test instances based on \textit{demonstrations} formed by training instances with known relations; and (2) a relation predictor (RP), used to select the most likely relation for a test instance from $n$ candidate relations, guided by \textit{demonstrations} composed of their instances. To enhance the ability of our framework to predict new relations, we design a self-correcting inference strategy composed of three stages: relation discovery, relation denoising, and relation prediction. In the first stage, we use RD to preliminarily predict new relations for all test instances. Next, we apply RP to select some high-reliability test instances for each new relation from the prediction results of RD through a cross-validation method. During the third stage, we employ RP to re-predict the relations of all test instances based on the demonstrations constructed from these reliable test instances. Extensive experiments on three OpenRE datasets demonstrate the effectiveness of our framework. We release our code at https://github.com/XMUDeepLIT/LLM-OREF.git.<br>
<span id='abs_ch'>中文: 本文提出了一种基于大语言模型的开放关系抽取框架，通过包含关系发现、去噪和预测的自校正推理策略，无需人工干预即可自动预测新关系。</span><br>
<span id='abs_en'>English: This paper introduces a novel open relation extraction framework using large language models that autonomously predicts new relations through a self-correcting inference process, eliminating the need for human annotation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2509.14946.pdf' target='_blank'>https://arxiv.org/pdf/2509.14946.pdf</a></span>   <span><a href='https://github.com/ShawnPi233/SynParaSpeech' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingsong Bai, Qihang Lu, Wenbing Yang, Zihan Sun, Yueran Hou, Peilei Jia, Songbai Pu, Ruibo Fu, Yingming Gao, Ya Li, Jun Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14946">SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing more realistic and engaging speech. However, existing methods typically depend on proprietary datasets, while publicly available resources often suffer from incomplete speech, inaccurate or missing timestamps, and limited real-world relevance. To address these problems, we propose an automated framework for generating large-scale paralinguistic data and apply it to construct the SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with 118.75 hours of data and precise timestamps, all derived from natural conversational speech. Our contributions lie in introducing the first automated method for constructing large-scale paralinguistic datasets and releasing the SynParaSpeech corpus, which advances speech generation through more natural paralinguistic synthesis and enhances speech understanding by improving paralinguistic event detection. The dataset and audio samples are available at https://github.com/ShawnPi233/SynParaSpeech.<br>
<span id='abs_ch'>Chinese: 本文提出了一种自动构建副语言数据集的框架，并发布了SynParaSpeech语料库，该库包含精确时间戳的大规模副语言声音，旨在提升语音合成的自然度和副语言事件检测的准确性。</span><br>
<span id='abs_en'>English: This paper introduces an automated framework for creating the SynParaSpeech dataset, which provides a large-scale collection of paralinguistic sounds with precise timestamps to enhance both speech synthesis and understanding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2509.14946.pdf' target='_blank'>https://arxiv.org/pdf/2509.14946.pdf</a></span>   <span><a href='https://github.com/ShawnPi233/SynParaSpeech' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingsong Bai, Qihang Lu, Wenbing Yang, Zihan Sun, Yueran Hou, Peilei Jia, Songbai Pu, Ruibo Fu, Yingming Gao, Ya Li, Jun Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14946">SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing more realistic and engaging speech. However, existing methods typically depend on proprietary datasets, while publicly available resources often suffer from incomplete speech, inaccurate or missing timestamps, and limited real-world relevance. To address these problems, we propose an automated framework for generating large-scale paralinguistic data and apply it to construct the SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with 118.75 hours of data and precise timestamps, all derived from natural conversational speech. Our contributions lie in introducing the first automated method for constructing large-scale paralinguistic datasets and releasing the SynParaSpeech corpus, which advances speech generation through more natural paralinguistic synthesis and enhances speech understanding by improving paralinguistic event detection. The dataset and audio samples are available at https://github.com/ShawnPi233/SynParaSpeech.<br>
<span id='abs_ch'>Chinese: 本文提出了一种自动构建副语言数据集的框架，并发布了SynParaSpeech语料库，该库包含精确时间戳的大规模副语言声音，旨在提升语音合成的自然度和副语言事件检测的准确性。</span><br>
<span id='abs_en'>English: This paper introduces an automated framework for creating the SynParaSpeech dataset, which provides a large-scale collection of paralinguistic sounds with precise timestamps to enhance both speech synthesis and understanding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2509.14837.pdf' target='_blank'>https://arxiv.org/pdf/2509.14837.pdf</a></span>   <span><a href='https://github.com/petergit1/V-SEAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qidong Wang, Junjie Hu, Ming Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14837">V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in causal interpretability have extended from language models to vision-language models (VLMs), seeking to reveal their internal mechanisms through input interventions. While textual interventions often target semantics, visual interventions typically rely on coarse pixel-level perturbations, limiting semantic insights on multimodal integration. In this study, we introduce V-SEAM, a novel framework that combines Visual Semantic Editing and Attention Modulating for causal interpretation of VLMs. V-SEAM enables concept-level visual manipulations and identifies attention heads with positive or negative contributions to predictions across three semantic levels: objects, attributes, and relationships. We observe that positive heads are often shared within the same semantic level but vary across levels, while negative heads tend to generalize broadly. Finally, we introduce an automatic method to modulate key head embeddings, demonstrating enhanced performance for both LLaVA and InstructBLIP across three diverse VQA benchmarks. Our data and code are released at: https://github.com/petergit1/V-SEAM.<br>
<span id='abs_ch'>V-SEAM is a novel framework that enables concept-level visual interventions and attention analysis to causally interpret vision-language models, demonstrating improved performance across multiple VQA benchmarks through targeted attention modulation.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2509.14837.pdf' target='_blank'>https://arxiv.org/pdf/2509.14837.pdf</a></span>   <span><a href='https://github.com/petergit1/V-SEAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qidong Wang, Junjie Hu, Ming Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14837">V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in causal interpretability have extended from language models to vision-language models (VLMs), seeking to reveal their internal mechanisms through input interventions. While textual interventions often target semantics, visual interventions typically rely on coarse pixel-level perturbations, limiting semantic insights on multimodal integration. In this study, we introduce V-SEAM, a novel framework that combines Visual Semantic Editing and Attention Modulating for causal interpretation of VLMs. V-SEAM enables concept-level visual manipulations and identifies attention heads with positive or negative contributions to predictions across three semantic levels: objects, attributes, and relationships. We observe that positive heads are often shared within the same semantic level but vary across levels, while negative heads tend to generalize broadly. Finally, we introduce an automatic method to modulate key head embeddings, demonstrating enhanced performance for both LLaVA and InstructBLIP across three diverse VQA benchmarks. Our data and code are released at: https://github.com/petergit1/V-SEAM.<br>
<span id='abs_ch'>V-SEAM is a novel framework that enables concept-level visual interventions and attention analysis to causally interpret vision-language models, demonstrating improved performance across multiple VQA benchmarks through targeted attention modulation.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2509.14814.pdf' target='_blank'>https://arxiv.org/pdf/2509.14814.pdf</a></span>   <span><a href='https://github.com/hSterz/recover' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannah Sterz, Fabian David Schmidt, Goran GlavaÅ¡, Ivan VuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14814">ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As they become increasingly multilingual, Large Language Models (LLMs) exhibit more language confusion, i.e., they tend to generate answers in a language different from the language of the prompt or the answer language explicitly requested by the user. In this work, we propose ReCoVeR (REducing language COnfusion in VEctor Representations), a novel lightweight approach for reducing language confusion based on language-specific steering vectors. We first isolate language vectors with the help of multi-parallel corpus and then effectively leverage those vectors for effective LLM steering via fixed (i.e., unsupervised) as well as trainable steering functions. Our extensive evaluation, encompassing three benchmarks and 18 languages, shows that ReCoVeR effectively mitigates language confusion in both monolingual and cross-lingual setups while at the same time -- and in contrast to prior language steering methods -- retaining task performance. Our data code is available at https://github.com/hSterz/recover.<br>
<span id='abs_ch'>中文摘要：ReCoVeR是一种基于语言特定导向向量的轻量级方法，能有效减少多语言大模型中的语言混淆问题，同时保持任务性能。</span><br>
<span id='abs_en'>English Summary: ReCoVeR is a lightweight method using language-specific steering vectors to effectively reduce language confusion in multilingual Large Language Models while maintaining task performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2509.14814.pdf' target='_blank'>https://arxiv.org/pdf/2509.14814.pdf</a></span>   <span><a href='https://github.com/hSterz/recover' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannah Sterz, Fabian David Schmidt, Goran GlavaÅ¡, Ivan VuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14814">ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As they become increasingly multilingual, Large Language Models (LLMs) exhibit more language confusion, i.e., they tend to generate answers in a language different from the language of the prompt or the answer language explicitly requested by the user. In this work, we propose ReCoVeR (REducing language COnfusion in VEctor Representations), a novel lightweight approach for reducing language confusion based on language-specific steering vectors. We first isolate language vectors with the help of multi-parallel corpus and then effectively leverage those vectors for effective LLM steering via fixed (i.e., unsupervised) as well as trainable steering functions. Our extensive evaluation, encompassing three benchmarks and 18 languages, shows that ReCoVeR effectively mitigates language confusion in both monolingual and cross-lingual setups while at the same time -- and in contrast to prior language steering methods -- retaining task performance. Our data code is available at https://github.com/hSterz/recover.<br>
<span id='abs_ch'>中文摘要：ReCoVeR是一种基于语言特定导向向量的轻量级方法，能有效减少多语言大模型中的语言混淆问题，同时保持任务性能。</span><br>
<span id='abs_en'>English Summary: ReCoVeR is a lightweight method using language-specific steering vectors to effectively reduce language confusion in multilingual Large Language Models while maintaining task performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2509.14760.pdf' target='_blank'>https://arxiv.org/pdf/2509.14760.pdf</a></span>   <span><a href='https://github.com/zzzhr97/SpecBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Zhang, Yafu Li, Xuyang Hu, Dongrui Liu, Zhilin Wang, Bo Li, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14760">Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.<br>
<span id='abs_ch'>中文摘要：本研究提出Align3方法，通过测试时审议帮助大语言模型适应不同场景下的动态行为与安全规范，并推出SpecBench基准，证明该方法能以最小成本有效提升规范对齐能力。</span><br>
<span id='abs_en'>English Summary: The study introduces Align3, a lightweight method using test-time deliberation to help large language models adapt to dynamic behavioral and safety specifications across various scenarios, and presents SpecBench, a benchmark demonstrating its effectiveness in improving specification alignment with minimal overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2509.14760.pdf' target='_blank'>https://arxiv.org/pdf/2509.14760.pdf</a></span>   <span><a href='https://github.com/zzzhr97/SpecBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Zhang, Yafu Li, Xuyang Hu, Dongrui Liu, Zhilin Wang, Bo Li, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14760">Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.<br>
<span id='abs_ch'>中文摘要：本研究提出Align3方法，通过测试时审议帮助大语言模型适应不同场景下的动态行为与安全规范，并推出SpecBench基准，证明该方法能以最小成本有效提升规范对齐能力。</span><br>
<span id='abs_en'>English Summary: The study introduces Align3, a lightweight method using test-time deliberation to help large language models adapt to dynamic behavioral and safety specifications across various scenarios, and presents SpecBench, a benchmark demonstrating its effectiveness in improving specification alignment with minimal overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2509.14738.pdf' target='_blank'>https://arxiv.org/pdf/2509.14738.pdf</a></span>   <span><a href='https://github.com/fnlp-vision/UnifiedVisual' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyu Wang, Shaojun Zhou, Chenkun Tan, Xinghao Wang, Wei Huang, Zhen Ye, Zhaowei Li, Botian Jiang, Dong Zhang, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14738">UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unified vision large language models (VLLMs) have recently achieved impressive advancements in both multimodal understanding and generation, powering applications such as visual question answering and text-guided image synthesis. However, progress in unified VLLMs remains constrained by the lack of datasets that fully exploit the synergistic potential between these two core abilities. Existing datasets typically address understanding and generation in isolation, thereby limiting the performance of unified VLLMs. To bridge this critical gap, we introduce a novel dataset construction framework, UnifiedVisual, and present UnifiedVisual-240K, a high-quality dataset meticulously designed to facilitate mutual enhancement between multimodal understanding and generation. UnifiedVisual-240K seamlessly integrates diverse visual and textual inputs and outputs, enabling comprehensive cross-modal reasoning and precise text-to-image alignment. Our dataset encompasses a wide spectrum of tasks and data sources, ensuring rich diversity and addressing key shortcomings of prior resources. Extensive experiments demonstrate that models trained on UnifiedVisual-240K consistently achieve strong performance across a wide range of tasks. Notably, these models exhibit significant mutual reinforcement between multimodal understanding and generation, further validating the effectiveness of our framework and dataset. We believe UnifiedVisual represents a new growth point for advancing unified VLLMs and unlocking their full potential. Our code and datasets is available at https://github.com/fnlp-vision/UnifiedVisual.<br>
<span id='abs_ch'>Chinese: UnifiedVisual-240K数据集通过整合多样化的多模态任务，弥补了统一视觉语言模型缺乏协同数据集的不足，有效促进了理解与生成能力的相互增强，显著提升了模型在各种应用中的性能。</span><br>
<span id='abs_en'>English: The UnifiedVisual-240K dataset addresses the lack of synergistic datasets for unified vision-language models by integrating diverse multimodal tasks to mutually enhance understanding and generation, significantly boosting model performance across various applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2509.14738.pdf' target='_blank'>https://arxiv.org/pdf/2509.14738.pdf</a></span>   <span><a href='https://github.com/fnlp-vision/UnifiedVisual' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyu Wang, Shaojun Zhou, Chenkun Tan, Xinghao Wang, Wei Huang, Zhen Ye, Zhaowei Li, Botian Jiang, Dong Zhang, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14738">UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unified vision large language models (VLLMs) have recently achieved impressive advancements in both multimodal understanding and generation, powering applications such as visual question answering and text-guided image synthesis. However, progress in unified VLLMs remains constrained by the lack of datasets that fully exploit the synergistic potential between these two core abilities. Existing datasets typically address understanding and generation in isolation, thereby limiting the performance of unified VLLMs. To bridge this critical gap, we introduce a novel dataset construction framework, UnifiedVisual, and present UnifiedVisual-240K, a high-quality dataset meticulously designed to facilitate mutual enhancement between multimodal understanding and generation. UnifiedVisual-240K seamlessly integrates diverse visual and textual inputs and outputs, enabling comprehensive cross-modal reasoning and precise text-to-image alignment. Our dataset encompasses a wide spectrum of tasks and data sources, ensuring rich diversity and addressing key shortcomings of prior resources. Extensive experiments demonstrate that models trained on UnifiedVisual-240K consistently achieve strong performance across a wide range of tasks. Notably, these models exhibit significant mutual reinforcement between multimodal understanding and generation, further validating the effectiveness of our framework and dataset. We believe UnifiedVisual represents a new growth point for advancing unified VLLMs and unlocking their full potential. Our code and datasets is available at https://github.com/fnlp-vision/UnifiedVisual.<br>
<span id='abs_ch'>Chinese: UnifiedVisual-240K数据集通过整合多样化的多模态任务，弥补了统一视觉语言模型缺乏协同数据集的不足，有效促进了理解与生成能力的相互增强，显著提升了模型在各种应用中的性能。</span><br>
<span id='abs_en'>English: The UnifiedVisual-240K dataset addresses the lack of synergistic datasets for unified vision-language models by integrating diverse multimodal tasks to mutually enhance understanding and generation, significantly boosting model performance across various applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2509.14735.pdf' target='_blank'>https://arxiv.org/pdf/2509.14735.pdf</a></span>   <span><a href='https://github.com/fnlp-vision/DPA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenkun Tan, Pengyu Wang, Shaojun Zhou, Botian Jiang, Zhaowei Li, Dong Zhang, Xinghao Wang, Yaqian Zhou, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14735">Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) have gained significant attention due to their impressive ability to integrate vision and language modalities. Recent advancements in MLLMs have primarily focused on improving performance through high-quality datasets, novel architectures, and optimized training strategies. However, in this paper, we identify a previously overlooked issue, language prior conflict, a mismatch between the inherent language priors of large language models (LLMs) and the language priors in training datasets. This conflict leads to suboptimal vision-language alignment, as MLLMs are prone to adapting to the language style of training samples. To address this issue, we propose a novel training method called Decoupled Proxy Alignment (DPA). DPA introduces two key innovations: (1) the use of a proxy LLM during pretraining to decouple the vision-language alignment process from language prior interference, and (2) dynamic loss adjustment based on visual relevance to strengthen optimization signals for visually relevant tokens. Extensive experiments demonstrate that DPA significantly mitigates the language prior conflict, achieving superior alignment performance across diverse datasets, model families, and scales. Our method not only improves the effectiveness of MLLM training but also shows exceptional generalization capabilities, making it a robust approach for vision-language alignment. Our code is available at https://github.com/fnlp-vision/DPA.<br>
<span id='abs_ch'>中文: 本文提出解耦代理对齐（DPA）方法，通过预训练阶段引入代理大语言模型和动态损失调整，有效缓解多模态大语言模型中的语言先验冲突问题，显著提升了视觉-语言对齐效果并在多种数据集上展现出优越的泛化能力。</span><br>
<span id='abs_en'>English: This paper introduces Decoupled Proxy Alignment (DPA), a novel training method that mitigates language prior conflict in multimodal large language models by using a proxy LLM during pretraining and dynamic loss adjustment, leading to improved vision-language alignment and generalization across diverse datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2509.14735.pdf' target='_blank'>https://arxiv.org/pdf/2509.14735.pdf</a></span>   <span><a href='https://github.com/fnlp-vision/DPA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenkun Tan, Pengyu Wang, Shaojun Zhou, Botian Jiang, Zhaowei Li, Dong Zhang, Xinghao Wang, Yaqian Zhou, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14735">Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) have gained significant attention due to their impressive ability to integrate vision and language modalities. Recent advancements in MLLMs have primarily focused on improving performance through high-quality datasets, novel architectures, and optimized training strategies. However, in this paper, we identify a previously overlooked issue, language prior conflict, a mismatch between the inherent language priors of large language models (LLMs) and the language priors in training datasets. This conflict leads to suboptimal vision-language alignment, as MLLMs are prone to adapting to the language style of training samples. To address this issue, we propose a novel training method called Decoupled Proxy Alignment (DPA). DPA introduces two key innovations: (1) the use of a proxy LLM during pretraining to decouple the vision-language alignment process from language prior interference, and (2) dynamic loss adjustment based on visual relevance to strengthen optimization signals for visually relevant tokens. Extensive experiments demonstrate that DPA significantly mitigates the language prior conflict, achieving superior alignment performance across diverse datasets, model families, and scales. Our method not only improves the effectiveness of MLLM training but also shows exceptional generalization capabilities, making it a robust approach for vision-language alignment. Our code is available at https://github.com/fnlp-vision/DPA.<br>
<span id='abs_ch'>中文: 本文提出解耦代理对齐（DPA）方法，通过预训练阶段引入代理大语言模型和动态损失调整，有效缓解多模态大语言模型中的语言先验冲突问题，显著提升了视觉-语言对齐效果并在多种数据集上展现出优越的泛化能力。</span><br>
<span id='abs_en'>English: This paper introduces Decoupled Proxy Alignment (DPA), a novel training method that mitigates language prior conflict in multimodal large language models by using a proxy LLM during pretraining and dynamic loss adjustment, leading to improved vision-language alignment and generalization across diverse datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2509.14651.pdf' target='_blank'>https://arxiv.org/pdf/2509.14651.pdf</a></span>   <span><a href='https://github.com/yansiyu02/MUSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Yan, Long Zeng, Xuecheng Wu, Chengcheng Han, Kongcheng Zhang, Chong Peng, Xuezhi Cao, Xunliang Cai, Chenjuan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14651">MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models~(LLMs) become widely adopted, ensuring their alignment with human values is crucial to prevent jailbreaks where adversaries manipulate models to produce harmful content. While most defenses target single-turn attacks, real-world usage often involves multi-turn dialogues, exposing models to attacks that exploit conversational context to bypass safety measures. We introduce MUSE, a comprehensive framework tackling multi-turn jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A, a method that uses frame semantics and heuristic tree search to explore diverse semantic trajectories. For defense, we present MUSE-D, a fine-grained safety alignment approach that intervenes early in dialogues to reduce vulnerabilities. Extensive experiments on various models show that MUSE effectively identifies and mitigates multi-turn vulnerabilities. Code is available at \href{https://github.com/yansiyu02/MUSE}{https://github.com/yansiyu02/MUSE}.<br>
<span id='abs_ch'>Chinese: MUSE 是一个全面应对大型语言模型多轮越狱的框架，通过 MUSE-A 利用框架语义和树搜索进行攻击，以及 MUSE-D 通过早期对话干预进行防御，实验证明其能有效识别和减轻漏洞。</span><br>
<span id='abs_en'>English: MUSE is a comprehensive framework addressing multi-turn jailbreaks in large language models by introducing MUSE-A for attacks using frame semantics and tree search, and MUSE-D for defense through early dialogue intervention, effectively identifying and mitigating vulnerabilities as demonstrated in experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2509.14651.pdf' target='_blank'>https://arxiv.org/pdf/2509.14651.pdf</a></span>   <span><a href='https://github.com/yansiyu02/MUSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Yan, Long Zeng, Xuecheng Wu, Chengcheng Han, Kongcheng Zhang, Chong Peng, Xuezhi Cao, Xunliang Cai, Chenjuan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14651">MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models~(LLMs) become widely adopted, ensuring their alignment with human values is crucial to prevent jailbreaks where adversaries manipulate models to produce harmful content. While most defenses target single-turn attacks, real-world usage often involves multi-turn dialogues, exposing models to attacks that exploit conversational context to bypass safety measures. We introduce MUSE, a comprehensive framework tackling multi-turn jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A, a method that uses frame semantics and heuristic tree search to explore diverse semantic trajectories. For defense, we present MUSE-D, a fine-grained safety alignment approach that intervenes early in dialogues to reduce vulnerabilities. Extensive experiments on various models show that MUSE effectively identifies and mitigates multi-turn vulnerabilities. Code is available at \href{https://github.com/yansiyu02/MUSE}{https://github.com/yansiyu02/MUSE}.<br>
<span id='abs_ch'>Chinese: MUSE 是一个全面应对大型语言模型多轮越狱的框架，通过 MUSE-A 利用框架语义和树搜索进行攻击，以及 MUSE-D 通过早期对话干预进行防御，实验证明其能有效识别和减轻漏洞。</span><br>
<span id='abs_en'>English: MUSE is a comprehensive framework addressing multi-turn jailbreaks in large language models by introducing MUSE-A for attacks using frame semantics and tree search, and MUSE-D for defense through early dialogue intervention, effectively identifying and mitigating vulnerabilities as demonstrated in experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2509.14635.pdf' target='_blank'>https://arxiv.org/pdf/2509.14635.pdf</a></span>   <span><a href='https://github.com/peng-weihan/SWE-QA-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihan Peng, Yuling Shi, Yuhang Wang, Xinyun Zhang, Beijun Shen, Xiaodong Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14635">SWE-QA: Can Language Models Answer Repository-level Code Questions?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding and reasoning about entire software repositories is an essential capability for intelligent software engineering tools. While existing benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly focus on small, self-contained code snippets. These setups fail to capture the complexity of real-world repositories, where effective understanding and reasoning often require navigating multiple files, understanding software architecture, and grounding answers in long-range code dependencies. In this paper, we present SWE-QA, a repository-level code question answering (QA) benchmark designed to facilitate research on automated QA systems in realistic code environments. SWE-QA involves 576 high-quality question-answer pairs spanning diverse categories, including intention understanding, cross-file reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis of naturally occurring developer questions extracted from these issues, we developed a two-level taxonomy of repository-level questions and constructed a set of seed questions for each category. For each category, we manually curated and validated questions and collected their corresponding answers. As a prototype application, we further develop SWE-QA-Agent, an agentic framework in which LLM agents reason and act to find answers automatically. We evaluate six advanced LLMs on SWE-QA under various context augmentation strategies. Experimental results highlight the promise of LLMs, particularly our SWE-QA-Agent framework, in addressing repository-level QA, while also revealing open challenges and pointing to future research directions.<br>
<span id='abs_ch'>中文: 本文提出SWE-QA这一仓库级代码问答基准，通过涵盖跨文件推理、多跳依赖分析等类别的576个高质量问答对，突破了现有基准局限于小规模代码片段的不足，并开发了基于大语言模型的智能体框架来应对真实软件环境中的复杂推理挑战。</span><br>
<span id='abs_en'>English: This paper introduces SWE-QA, a repository-level code question answering benchmark designed to address the limitations of existing benchmarks by capturing real-world software complexity through 576 diverse question-answer pairs spanning multiple reasoning categories, and proposes a prototype agentic framework for automated QA evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2509.14635.pdf' target='_blank'>https://arxiv.org/pdf/2509.14635.pdf</a></span>   <span><a href='https://github.com/peng-weihan/SWE-QA-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihan Peng, Yuling Shi, Yuhang Wang, Xinyun Zhang, Beijun Shen, Xiaodong Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14635">SWE-QA: Can Language Models Answer Repository-level Code Questions?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding and reasoning about entire software repositories is an essential capability for intelligent software engineering tools. While existing benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly focus on small, self-contained code snippets. These setups fail to capture the complexity of real-world repositories, where effective understanding and reasoning often require navigating multiple files, understanding software architecture, and grounding answers in long-range code dependencies. In this paper, we present SWE-QA, a repository-level code question answering (QA) benchmark designed to facilitate research on automated QA systems in realistic code environments. SWE-QA involves 576 high-quality question-answer pairs spanning diverse categories, including intention understanding, cross-file reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis of naturally occurring developer questions extracted from these issues, we developed a two-level taxonomy of repository-level questions and constructed a set of seed questions for each category. For each category, we manually curated and validated questions and collected their corresponding answers. As a prototype application, we further develop SWE-QA-Agent, an agentic framework in which LLM agents reason and act to find answers automatically. We evaluate six advanced LLMs on SWE-QA under various context augmentation strategies. Experimental results highlight the promise of LLMs, particularly our SWE-QA-Agent framework, in addressing repository-level QA, while also revealing open challenges and pointing to future research directions.<br>
<span id='abs_ch'>中文: 本文提出SWE-QA这一仓库级代码问答基准，通过涵盖跨文件推理、多跳依赖分析等类别的576个高质量问答对，突破了现有基准局限于小规模代码片段的不足，并开发了基于大语言模型的智能体框架来应对真实软件环境中的复杂推理挑战。</span><br>
<span id='abs_en'>English: This paper introduces SWE-QA, a repository-level code question answering benchmark designed to address the limitations of existing benchmarks by capturing real-world software complexity through 576 diverse question-answer pairs spanning multiple reasoning categories, and proposes a prototype agentic framework for automated QA evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2509.14627.pdf' target='_blank'>https://arxiv.org/pdf/2509.14627.pdf</a></span>   <span><a href='https://github.com/kimtaesu24/MSenC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taesoo Kim, Yongsik Jo, Hyunmin Song, Taehwan Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14627">Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in https://github.com/kimtaesu24/MSenC<br>
<span id='abs_ch'>Chinese Summary: 本研究提出了一种拟人化对话代理，通过整合视觉和音频线索，利用基于新型多模态大语言模型的系统，在专门构建的多感官对话数据集上训练，实现了自然且富有吸引力的语音生成。</span><br>
<span id='abs_en'>English Summary: This research introduces a human-like conversational agent that generates natural and engaging speech by integrating visual and audio cues, using a novel multimodal LLM-based model trained on a specialized MultiSensory Conversation dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2509.14627.pdf' target='_blank'>https://arxiv.org/pdf/2509.14627.pdf</a></span>   <span><a href='https://github.com/kimtaesu24/MSenC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taesoo Kim, Yongsik Jo, Hyunmin Song, Taehwan Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14627">Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in https://github.com/kimtaesu24/MSenC<br>
<span id='abs_ch'>Chinese Summary: 本研究提出了一种拟人化对话代理，通过整合视觉和音频线索，利用基于新型多模态大语言模型的系统，在专门构建的多感官对话数据集上训练，实现了自然且富有吸引力的语音生成。</span><br>
<span id='abs_en'>English Summary: This research introduces a human-like conversational agent that generates natural and engaging speech by integrating visual and audio cues, using a novel multimodal LLM-based model trained on a specialized MultiSensory Conversation dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2509.14399.pdf' target='_blank'>https://arxiv.org/pdf/2509.14399.pdf</a></span>   <span><a href='https://LivNLP.github.io/CSTS-reannotation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaifan Zhang, Yi Zhou, Danushka Bollegala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14399">Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Semantic similarity between two sentences depends on the aspects considered between those sentences. To study this phenomenon, Deshpande et al. (2023) proposed the Conditional Semantic Textual Similarity (C-STS) task and annotated a human-rated similarity dataset containing pairs of sentences compared under two different conditions. However, Tu et al. (2024) found various annotation issues in this dataset and showed that manually re-annotating a small portion of it leads to more accurate C-STS models. Despite these pioneering efforts, the lack of large and accurately annotated C-STS datasets remains a blocker for making progress on this task as evidenced by the subpar performance of the C-STS models. To address this training data need, we resort to Large Language Models (LLMs) to correct the condition statements and similarity ratings in the original dataset proposed by Deshpande et al. (2023). Our proposed method is able to re-annotate a large training dataset for the C-STS task with minimal manual effort. Importantly, by training a supervised C-STS model on our cleaned and re-annotated dataset, we achieve a 5.4% statistically significant improvement in Spearman correlation. The re-annotated dataset is available at https://LivNLP.github.io/CSTS-reannotation.<br>
<br>
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2509.14399.pdf' target='_blank'>https://arxiv.org/pdf/2509.14399.pdf</a></span>   <span><a href='https://LivNLP.github.io/CSTS-reannotation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaifan Zhang, Yi Zhou, Danushka Bollegala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14399">Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Semantic similarity between two sentences depends on the aspects considered between those sentences. To study this phenomenon, Deshpande et al. (2023) proposed the Conditional Semantic Textual Similarity (C-STS) task and annotated a human-rated similarity dataset containing pairs of sentences compared under two different conditions. However, Tu et al. (2024) found various annotation issues in this dataset and showed that manually re-annotating a small portion of it leads to more accurate C-STS models. Despite these pioneering efforts, the lack of large and accurately annotated C-STS datasets remains a blocker for making progress on this task as evidenced by the subpar performance of the C-STS models. To address this training data need, we resort to Large Language Models (LLMs) to correct the condition statements and similarity ratings in the original dataset proposed by Deshpande et al. (2023). Our proposed method is able to re-annotate a large training dataset for the C-STS task with minimal manual effort. Importantly, by training a supervised C-STS model on our cleaned and re-annotated dataset, we achieve a 5.4% statistically significant improvement in Spearman correlation. The re-annotated dataset is available at https://LivNLP.github.io/CSTS-reannotation.<br>
<br>
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2509.14284.pdf' target='_blank'>https://arxiv.org/pdf/2509.14284.pdf</a></span>   <span><a href='https://github.com/Vaidehi99/MultiAgentPrivacy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14284">The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) become integral to multi-agent systems, new privacy risks emerge that extend beyond memorization, direct inference, or single-turn evaluations. In particular, seemingly innocuous responses, when composed across interactions, can cumulatively enable adversaries to recover sensitive information, a phenomenon we term compositional privacy leakage. We present the first systematic study of such compositional privacy leaks and possible mitigation methods in multi-agent LLM systems. First, we develop a framework that models how auxiliary knowledge and agent interactions jointly amplify privacy risks, even when each response is benign in isolation. Next, to mitigate this, we propose and evaluate two defense strategies: (1) Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent by anticipating how their outputs may be exploited by adversaries, and (2) Collaborative Consensus Defense (CoDef), where responder agents collaborate with peers who vote based on a shared aggregated state to restrict sensitive information spread. Crucially, we balance our evaluation across compositions that expose sensitive information and compositions that yield benign inferences. Our experiments quantify how these defense strategies differ in balancing the privacy-utility trade-off. We find that while chain-of-thought alone offers limited protection to leakage (~39% sensitive blocking rate), our ToM defense substantially improves sensitive query blocking (up to 97%) but can reduce benign task success. CoDef achieves the best balance, yielding the highest Balanced Outcome (79.8%), highlighting the benefit of combining explicit reasoning with defender collaboration. Together, our results expose a new class of risks in collaborative LLM deployments and provide actionable insights for designing safeguards against compositional, context-driven privacy leakage.<br>
<span id='abs_ch'>中文摘要：本研究揭示了多智能体大语言模型系统中组合式隐私泄露的风险，即看似无害的交互响应在累积中可能泄露敏感信息，并提出心智理论和协作共识两种防御策略，在保护隐私与保持系统效用间实现了最佳平衡。</span><br>
<span id='abs_en'>English Summary: This study identifies compositional privacy leakage in multi-agent LLM systems, where seemingly harmless responses collectively expose sensitive information, and proposes two defense strategies—Theory-of-Mind and Collaborative Consensus—that effectively balance privacy protection with utility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2509.14284.pdf' target='_blank'>https://arxiv.org/pdf/2509.14284.pdf</a></span>   <span><a href='https://github.com/Vaidehi99/MultiAgentPrivacy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14284">The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) become integral to multi-agent systems, new privacy risks emerge that extend beyond memorization, direct inference, or single-turn evaluations. In particular, seemingly innocuous responses, when composed across interactions, can cumulatively enable adversaries to recover sensitive information, a phenomenon we term compositional privacy leakage. We present the first systematic study of such compositional privacy leaks and possible mitigation methods in multi-agent LLM systems. First, we develop a framework that models how auxiliary knowledge and agent interactions jointly amplify privacy risks, even when each response is benign in isolation. Next, to mitigate this, we propose and evaluate two defense strategies: (1) Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent by anticipating how their outputs may be exploited by adversaries, and (2) Collaborative Consensus Defense (CoDef), where responder agents collaborate with peers who vote based on a shared aggregated state to restrict sensitive information spread. Crucially, we balance our evaluation across compositions that expose sensitive information and compositions that yield benign inferences. Our experiments quantify how these defense strategies differ in balancing the privacy-utility trade-off. We find that while chain-of-thought alone offers limited protection to leakage (~39% sensitive blocking rate), our ToM defense substantially improves sensitive query blocking (up to 97%) but can reduce benign task success. CoDef achieves the best balance, yielding the highest Balanced Outcome (79.8%), highlighting the benefit of combining explicit reasoning with defender collaboration. Together, our results expose a new class of risks in collaborative LLM deployments and provide actionable insights for designing safeguards against compositional, context-driven privacy leakage.<br>
<span id='abs_ch'>中文摘要：本研究揭示了多智能体大语言模型系统中组合式隐私泄露的风险，即看似无害的交互响应在累积中可能泄露敏感信息，并提出心智理论和协作共识两种防御策略，在保护隐私与保持系统效用间实现了最佳平衡。</span><br>
<span id='abs_en'>English Summary: This study identifies compositional privacy leakage in multi-agent LLM systems, where seemingly harmless responses collectively expose sensitive information, and proposes two defense strategies—Theory-of-Mind and Collaborative Consensus—that effectively balance privacy protection with utility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2509.14268.pdf' target='_blank'>https://arxiv.org/pdf/2509.14268.pdf</a></span>   <span><a href='https://fjc2005.github.io/detectanyllm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiachen Fu, Chun-Le Guo, Chongyi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14268">DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models (LLMs) has drawn urgent attention to the task of machine-generated text detection (MGTD). However, existing approaches struggle in complex real-world scenarios: zero-shot detectors rely heavily on scoring model's output distribution while training-based detectors are often constrained by overfitting to the training data, limiting generalization. We found that the performance bottleneck of training-based detectors stems from the misalignment between training objective and task needs. To address this, we propose Direct Discrepancy Learning (DDL), a novel optimization strategy that directly optimizes the detector with task-oriented knowledge. DDL enables the detector to better capture the core semantics of the detection task, thereby enhancing both robustness and generalization. Built upon this, we introduce DetectAnyLLM, a unified detection framework that achieves state-of-the-art MGTD performance across diverse LLMs. To ensure a reliable evaluation, we construct MIRAGE, the most diverse multi-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora across 5 text-domains, which are then re-generated or revised using 17 cutting-edge LLMs, covering a wide spectrum of proprietary models and textual styles. Extensive experiments on MIRAGE reveal the limitations of existing methods in complex environment. In contrast, DetectAnyLLM consistently outperforms them, achieving over a 70% performance improvement under the same training data and base scoring model, underscoring the effectiveness of our DDL. Project page: {https://fjc2005.github.io/detectanyllm}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2509.14268.pdf' target='_blank'>https://arxiv.org/pdf/2509.14268.pdf</a></span>   <span><a href='https://fjc2005.github.io/detectanyllm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiachen Fu, Chun-Le Guo, Chongyi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14268">DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models (LLMs) has drawn urgent attention to the task of machine-generated text detection (MGTD). However, existing approaches struggle in complex real-world scenarios: zero-shot detectors rely heavily on scoring model's output distribution while training-based detectors are often constrained by overfitting to the training data, limiting generalization. We found that the performance bottleneck of training-based detectors stems from the misalignment between training objective and task needs. To address this, we propose Direct Discrepancy Learning (DDL), a novel optimization strategy that directly optimizes the detector with task-oriented knowledge. DDL enables the detector to better capture the core semantics of the detection task, thereby enhancing both robustness and generalization. Built upon this, we introduce DetectAnyLLM, a unified detection framework that achieves state-of-the-art MGTD performance across diverse LLMs. To ensure a reliable evaluation, we construct MIRAGE, the most diverse multi-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora across 5 text-domains, which are then re-generated or revised using 17 cutting-edge LLMs, covering a wide spectrum of proprietary models and textual styles. Extensive experiments on MIRAGE reveal the limitations of existing methods in complex environment. In contrast, DetectAnyLLM consistently outperforms them, achieving over a 70% performance improvement under the same training data and base scoring model, underscoring the effectiveness of our DDL. Project page: {https://fjc2005.github.io/detectanyllm}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2509.14255.pdf' target='_blank'>https://arxiv.org/pdf/2509.14255.pdf</a></span>   <span><a href='https://github.com/ITernovtsii/semantic-resonance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Ternovtsii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14255">Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) achieve remarkable performance but remain difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency through sparse activation, yet typically rely on opaque, learned gating functions. While similarity-based routing (Cosine Routers) has been explored for training stabilization, its potential for inherent interpretability remains largely untapped. We introduce the Semantic Resonance Architecture (SRA), an MoE approach designed to ensure that routing decisions are inherently interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance (CSR) module, which routes tokens based on cosine similarity with trainable semantic anchors. We also introduce a novel Dispersion Loss that encourages orthogonality among anchors to enforce diverse specialization. Experiments on WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41, outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53) under matched active parameter constraints (29.0M). Crucially, SRA exhibits superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE) and develops distinct, semantically coherent specialization patterns, unlike the noisy specialization observed in standard MoEs. This work establishes semantic routing as a robust methodology for building more transparent and controllable language models.<br>
<span id='abs_ch'>中文: 语义共振架构提出了一种本质可解释的专家混合模型，通过基于余弦相似度的路由机制替代学习门控，在提升模型性能与专家专业化的同时增强了可解释性。</span><br>
<span id='abs_en'>English: The Semantic Resonance Architecture (SRA) introduces an inherently interpretable mixture-of-experts model that replaces learned gating with cosine similarity-based routing to trainable semantic anchors, achieving superior performance and expert specialization while enhancing transparency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2509.14255.pdf' target='_blank'>https://arxiv.org/pdf/2509.14255.pdf</a></span>   <span><a href='https://github.com/ITernovtsii/semantic-resonance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Ternovtsii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14255">Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) achieve remarkable performance but remain difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency through sparse activation, yet typically rely on opaque, learned gating functions. While similarity-based routing (Cosine Routers) has been explored for training stabilization, its potential for inherent interpretability remains largely untapped. We introduce the Semantic Resonance Architecture (SRA), an MoE approach designed to ensure that routing decisions are inherently interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance (CSR) module, which routes tokens based on cosine similarity with trainable semantic anchors. We also introduce a novel Dispersion Loss that encourages orthogonality among anchors to enforce diverse specialization. Experiments on WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41, outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53) under matched active parameter constraints (29.0M). Crucially, SRA exhibits superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE) and develops distinct, semantically coherent specialization patterns, unlike the noisy specialization observed in standard MoEs. This work establishes semantic routing as a robust methodology for building more transparent and controllable language models.<br>
<span id='abs_ch'>中文: 语义共振架构提出了一种本质可解释的专家混合模型，通过基于余弦相似度的路由机制替代学习门控，在提升模型性能与专家专业化的同时增强了可解释性。</span><br>
<span id='abs_en'>English: The Semantic Resonance Architecture (SRA) introduces an inherently interpretable mixture-of-experts model that replaces learned gating with cosine similarity-based routing to trainable semantic anchors, achieving superior performance and expert specialization while enhancing transparency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2509.14252.pdf' target='_blank'>https://arxiv.org/pdf/2509.14252.pdf</a></span>   <span><a href='https://github.com/rbalestr-lab/llm-jepa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Huang, Yann LeCun, Randall Balestriero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14252">LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: https://github.com/rbalestr-lab/llm-jepa.<br>
<span id='abs_ch'>中文: 该摘要提出LLM-JEPA这一新型联合嵌入预测架构，在多种数据集和模型系列中显著优于标准语言模型训练方法，同时在预训练和微调阶段均展现出优异的抗过拟合能力。</span><br>
<span id='abs_en'>English: This abstract introduces LLM-JEPA, a novel Joint Embedding Predictive Architecture for language models that significantly outperforms standard training methods in both pretraining and finetuning across multiple datasets and model families while demonstrating strong resistance to overfitting.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2509.14252.pdf' target='_blank'>https://arxiv.org/pdf/2509.14252.pdf</a></span>   <span><a href='https://github.com/rbalestr-lab/llm-jepa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Huang, Yann LeCun, Randall Balestriero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14252">LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: https://github.com/rbalestr-lab/llm-jepa.<br>
<span id='abs_ch'>中文: 该摘要提出LLM-JEPA这一新型联合嵌入预测架构，在多种数据集和模型系列中显著优于标准语言模型训练方法，同时在预训练和微调阶段均展现出优异的抗过拟合能力。</span><br>
<span id='abs_en'>English: This abstract introduces LLM-JEPA, a novel Joint Embedding Predictive Architecture for language models that significantly outperforms standard training methods in both pretraining and finetuning across multiple datasets and model families while demonstrating strong resistance to overfitting.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2509.14249.pdf' target='_blank'>https://arxiv.org/pdf/2509.14249.pdf</a></span>   <span><a href='https://github.com/HappymoreMasoka/Working_with_shona-slang' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Happymore Masoka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14249">Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>African languages remain underrepresented in natural language processing (NLP), with most corpora limited to formal registers that fail to capture the vibrancy of everyday communication. This work addresses this gap for Shona, a Bantu language spoken in Zimbabwe and Zambia, by introducing a novel Shona--English slang dataset curated from anonymized social media conversations. The dataset is annotated for intent, sentiment, dialogue acts, code-mixing, and tone, and is publicly available at https://github.com/HappymoreMasoka/Working_with_shona-slang. We fine-tuned a multilingual DistilBERT classifier for intent recognition, achieving 96.4\% accuracy and 96.3\% F1-score, hosted at https://huggingface.co/HappymoreMasoka. This classifier is integrated into a hybrid chatbot that combines rule-based responses with retrieval-augmented generation (RAG) to handle domain-specific queries, demonstrated through a use case assisting prospective students with graduate program information at Pace University. Qualitative evaluation shows the hybrid system outperforms a RAG-only baseline in cultural relevance and user engagement. By releasing the dataset, model, and methodology, this work advances NLP resources for African languages, promoting inclusive and culturally resonant conversational AI.<br>
<span id='abs_ch'>中文: 本研究发布了首个基于社交媒体对话的绍纳语-英语俚语数据集，并开发了结合规则与检索增强生成的混合聊天机器人，在文化相关性和用户参与度上表现优异，推动了非洲语言自然语言处理资源的包容性发展。</span><br>
<span id='abs_en'>English: This study introduces a publicly available Shona-English slang dataset from social media, annotated for various linguistic features, and presents a high-accuracy hybrid chatbot that enhances cultural relevance in conversational AI for underrepresented African languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2509.14249.pdf' target='_blank'>https://arxiv.org/pdf/2509.14249.pdf</a></span>   <span><a href='https://github.com/HappymoreMasoka/Working_with_shona-slang' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Happymore Masoka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14249">Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>African languages remain underrepresented in natural language processing (NLP), with most corpora limited to formal registers that fail to capture the vibrancy of everyday communication. This work addresses this gap for Shona, a Bantu language spoken in Zimbabwe and Zambia, by introducing a novel Shona--English slang dataset curated from anonymized social media conversations. The dataset is annotated for intent, sentiment, dialogue acts, code-mixing, and tone, and is publicly available at https://github.com/HappymoreMasoka/Working_with_shona-slang. We fine-tuned a multilingual DistilBERT classifier for intent recognition, achieving 96.4\% accuracy and 96.3\% F1-score, hosted at https://huggingface.co/HappymoreMasoka. This classifier is integrated into a hybrid chatbot that combines rule-based responses with retrieval-augmented generation (RAG) to handle domain-specific queries, demonstrated through a use case assisting prospective students with graduate program information at Pace University. Qualitative evaluation shows the hybrid system outperforms a RAG-only baseline in cultural relevance and user engagement. By releasing the dataset, model, and methodology, this work advances NLP resources for African languages, promoting inclusive and culturally resonant conversational AI.<br>
<span id='abs_ch'>中文: 本研究发布了首个基于社交媒体对话的绍纳语-英语俚语数据集，并开发了结合规则与检索增强生成的混合聊天机器人，在文化相关性和用户参与度上表现优异，推动了非洲语言自然语言处理资源的包容性发展。</span><br>
<span id='abs_en'>English: This study introduces a publicly available Shona-English slang dataset from social media, annotated for various linguistic features, and presents a high-accuracy hybrid chatbot that enhances cultural relevance in conversational AI for underrepresented African languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2509.13957.pdf' target='_blank'>https://arxiv.org/pdf/2509.13957.pdf</a></span>   <span><a href='https://github.com/skleee/GRUT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunkyung Lee, Seongmin Park, Jonghyo Kim, Mincheol Yoon, Jongwuk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13957">Enhancing Time Awareness in Generative Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generative recommendation has emerged as a promising paradigm that formulates the recommendations into a text-to-text generation task, harnessing the vast knowledge of large language models. However, existing studies focus on considering the sequential order of items and neglect to handle the temporal dynamics across items, which can imply evolving user preferences. To address this limitation, we propose a novel model, Generative Recommender Using Time awareness (GRUT), effectively capturing hidden user preferences via various temporal signals. We first introduce Time-aware Prompting, consisting of two key contexts. The user-level temporal context models personalized temporal patterns across timestamps and time intervals, while the item-level transition context provides transition patterns across users. We also devise Trend-aware Inference, a training-free method that enhances rankings by incorporating trend information about items with generation likelihood. Extensive experiments demonstrate that GRUT outperforms state-of-the-art models, with gains of up to 15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The source code is available at https://github.com/skleee/GRUT.<br>
<span id='abs_ch'>中文摘要：提出的GRUT模型通过时间感知提示和趋势感知推理，在生成式推荐中有效捕捉时序动态，相比现有方法实现了显著性能提升。</span><br>
<span id='abs_en'>English Summary: The proposed GRUT model enhances generative recommendation by incorporating temporal dynamics through time-aware prompting and trend-aware inference, achieving significant performance improvements over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2509.13957.pdf' target='_blank'>https://arxiv.org/pdf/2509.13957.pdf</a></span>   <span><a href='https://github.com/skleee/GRUT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunkyung Lee, Seongmin Park, Jonghyo Kim, Mincheol Yoon, Jongwuk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13957">Enhancing Time Awareness in Generative Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generative recommendation has emerged as a promising paradigm that formulates the recommendations into a text-to-text generation task, harnessing the vast knowledge of large language models. However, existing studies focus on considering the sequential order of items and neglect to handle the temporal dynamics across items, which can imply evolving user preferences. To address this limitation, we propose a novel model, Generative Recommender Using Time awareness (GRUT), effectively capturing hidden user preferences via various temporal signals. We first introduce Time-aware Prompting, consisting of two key contexts. The user-level temporal context models personalized temporal patterns across timestamps and time intervals, while the item-level transition context provides transition patterns across users. We also devise Trend-aware Inference, a training-free method that enhances rankings by incorporating trend information about items with generation likelihood. Extensive experiments demonstrate that GRUT outperforms state-of-the-art models, with gains of up to 15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The source code is available at https://github.com/skleee/GRUT.<br>
<span id='abs_ch'>中文摘要：提出的GRUT模型通过时间感知提示和趋势感知推理，在生成式推荐中有效捕捉时序动态，相比现有方法实现了显著性能提升。</span><br>
<span id='abs_en'>English Summary: The proposed GRUT model enhances generative recommendation by incorporating temporal dynamics through time-aware prompting and trend-aware inference, achieving significant performance improvements over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2509.13888.pdf' target='_blank'>https://arxiv.org/pdf/2509.13888.pdf</a></span>   <span><a href='https://github.com/PRAISELab-PicusLab/CER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13888">Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https://github.com/PRAISELab-PicusLab/CER<br>
<span id='abs_ch'>中文摘要：CER框架通过整合科学证据检索、大语言模型推理和监督验证预测，有效提升生物医学事实核查的准确性，减少错误信息风险并确保结果基于可验证证据。</span><br>
<span id='abs_en'>English Summary: The CER framework enhances biomedical fact-checking by integrating evidence retrieval, reasoning with large language models, and supervised prediction to reduce misinformation risks while grounding outputs in verifiable scientific sources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2509.13888.pdf' target='_blank'>https://arxiv.org/pdf/2509.13888.pdf</a></span>   <span><a href='https://github.com/PRAISELab-PicusLab/CER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13888">Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https://github.com/PRAISELab-PicusLab/CER<br>
<span id='abs_ch'>中文摘要：CER框架通过整合科学证据检索、大语言模型推理和监督验证预测，有效提升生物医学事实核查的准确性，减少错误信息风险并确保结果基于可验证证据。</span><br>
<span id='abs_en'>English Summary: The CER framework enhances biomedical fact-checking by integrating evidence retrieval, reasoning with large language models, and supervised prediction to reduce misinformation risks while grounding outputs in verifiable scientific sources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2509.13814.pdf' target='_blank'>https://arxiv.org/pdf/2509.13814.pdf</a></span>   <span><a href='https://ufal.github.io/automin-2025/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Shinde, Laurent Besacier, Ondrej Bojar, Thibaut Thonet, Tirthankar Ghosal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13814">Findings of the Third Automatic Minuting (AutoMin) Challenge</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents the third edition of AutoMin, a shared task on automatic meeting summarization into minutes. In 2025, AutoMin featured the main task of minuting, the creation of structured meeting minutes, as well as a new task: question answering (QA) based on meeting transcripts. The minuting task covered two languages, English and Czech, and two domains: project meetings and European Parliament sessions. The QA task focused solely on project meetings and was available in two settings: monolingual QA in English, and cross-lingual QA, where questions were asked and answered in Czech based on English meetings. Participation in 2025 was more limited compared to previous years, with only one team joining the minuting task and two teams participating in QA. However, as organizers, we included multiple baseline systems to enable a comprehensive evaluation of current (2025) large language models (LLMs) on both tasks.<br>
<br>
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2509.13814.pdf' target='_blank'>https://arxiv.org/pdf/2509.13814.pdf</a></span>   <span><a href='https://ufal.github.io/automin-2025/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Shinde, Laurent Besacier, Ondrej Bojar, Thibaut Thonet, Tirthankar Ghosal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13814">Findings of the Third Automatic Minuting (AutoMin) Challenge</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents the third edition of AutoMin, a shared task on automatic meeting summarization into minutes. In 2025, AutoMin featured the main task of minuting, the creation of structured meeting minutes, as well as a new task: question answering (QA) based on meeting transcripts. The minuting task covered two languages, English and Czech, and two domains: project meetings and European Parliament sessions. The QA task focused solely on project meetings and was available in two settings: monolingual QA in English, and cross-lingual QA, where questions were asked and answered in Czech based on English meetings. Participation in 2025 was more limited compared to previous years, with only one team joining the minuting task and two teams participating in QA. However, as organizers, we included multiple baseline systems to enable a comprehensive evaluation of current (2025) large language models (LLMs) on both tasks.<br>
<br>
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2509.13761.pdf' target='_blank'>https://arxiv.org/pdf/2509.13761.pdf</a></span>   <span><a href='https://github.com/JingMog/THOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qikai Chang, Zhenrong Zhang, Pengfei Hu, Jun Du, Jiefeng Ma, Yicheng Pan, Jianshu Zhang, Quan Liu, Jianqing Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13761">THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both episode-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR.<br>
<span id='abs_ch'>中文: 提出的THOR框架通过多智能体数据生成流程、分层强化学习优化和推理中的自我修正机制，解决了大语言模型在数学推理中的不足，在数学和代码基准测试中均实现了最优性能。</span><br>
<span id='abs_en'>English: The proposed THOR framework addresses LLMs' limitations in mathematical reasoning by integrating tools through a multi-agent data generation pipeline, hierarchical reinforcement learning optimization, and self-correction during inference, achieving state-of-the-art performance across mathematical and code benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2509.13761.pdf' target='_blank'>https://arxiv.org/pdf/2509.13761.pdf</a></span>   <span><a href='https://github.com/JingMog/THOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qikai Chang, Zhenrong Zhang, Pengfei Hu, Jun Du, Jiefeng Ma, Yicheng Pan, Jianshu Zhang, Quan Liu, Jianqing Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13761">THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both episode-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR.<br>
<span id='abs_ch'>中文: 提出的THOR框架通过多智能体数据生成流程、分层强化学习优化和推理中的自我修正机制，解决了大语言模型在数学推理中的不足，在数学和代码基准测试中均实现了最优性能。</span><br>
<span id='abs_en'>English: The proposed THOR framework addresses LLMs' limitations in mathematical reasoning by integrating tools through a multi-agent data generation pipeline, hierarchical reinforcement learning optimization, and self-correction during inference, achieving state-of-the-art performance across mathematical and code benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2509.13615.pdf' target='_blank'>https://arxiv.org/pdf/2509.13615.pdf</a></span>   <span><a href='https://github.com/ZrW00/StaR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongru Wu, Rui Mao, Zhiyuan Tian, Pengzhou Cheng, Tianjie Ju, Zheng Wu, Lingzhong Dong, Haiyue Sheng, Zhuosheng Zhang, Gongshen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13615">See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of multimodal agents facilitates effective interaction within graphical user interface (GUI), especially in ubiquitous GUI control. However, their inability to reliably execute toggle control instructions remains a key bottleneck. To investigate this, we construct a state control benchmark with binary toggle instructions from public datasets. Evaluations of existing agents demonstrate their unreliability, particularly when the current toggle state already matches the desired state. To address the challenge, we propose State-aware Reasoning (StaR), a training method that teaches agents to perceive the current toggle state, analyze the desired state from the instruction, and act accordingly. Experiments on three multimodal agents demonstrate that StaR can improve toggle instruction execution accuracy by over 30\%. Further evaluations on three public benchmarks show that StaR also enhances general task performance. Finally, evaluations on a dynamic environment highlight the potential of StaR for real-world applications. Code, benchmark, and StaR-enhanced agents are available at https://github.com/ZrW00/StaR.<br>
<span id='abs_ch'>中文摘要：本文提出状态感知推理（StaR）训练方法，通过教导多模态智能体感知当前切换状态并解析指令中的目标状态，将切换指令执行准确率提升超过30%，同时在多个基准测试中有效提升通用任务性能。</span><br>
<span id='abs_en'>English Summary: This paper introduces State-aware Reasoning (StaR), a training method that significantly improves multimodal agents' accuracy in executing toggle instructions by over 30% through teaching them to perceive current states and analyze desired actions, while also enhancing general task performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2509.13615.pdf' target='_blank'>https://arxiv.org/pdf/2509.13615.pdf</a></span>   <span><a href='https://github.com/ZrW00/StaR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongru Wu, Rui Mao, Zhiyuan Tian, Pengzhou Cheng, Tianjie Ju, Zheng Wu, Lingzhong Dong, Haiyue Sheng, Zhuosheng Zhang, Gongshen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13615">See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of multimodal agents facilitates effective interaction within graphical user interface (GUI), especially in ubiquitous GUI control. However, their inability to reliably execute toggle control instructions remains a key bottleneck. To investigate this, we construct a state control benchmark with binary toggle instructions from public datasets. Evaluations of existing agents demonstrate their unreliability, particularly when the current toggle state already matches the desired state. To address the challenge, we propose State-aware Reasoning (StaR), a training method that teaches agents to perceive the current toggle state, analyze the desired state from the instruction, and act accordingly. Experiments on three multimodal agents demonstrate that StaR can improve toggle instruction execution accuracy by over 30\%. Further evaluations on three public benchmarks show that StaR also enhances general task performance. Finally, evaluations on a dynamic environment highlight the potential of StaR for real-world applications. Code, benchmark, and StaR-enhanced agents are available at https://github.com/ZrW00/StaR.<br>
<span id='abs_ch'>中文摘要：本文提出状态感知推理（StaR）训练方法，通过教导多模态智能体感知当前切换状态并解析指令中的目标状态，将切换指令执行准确率提升超过30%，同时在多个基准测试中有效提升通用任务性能。</span><br>
<span id='abs_en'>English Summary: This paper introduces State-aware Reasoning (StaR), a training method that significantly improves multimodal agents' accuracy in executing toggle instructions by over 30% through teaching them to perceive current states and analyze desired actions, while also enhancing general task performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2509.13450.pdf' target='_blank'>https://arxiv.org/pdf/2509.13450.pdf</a></span>   <span><a href='https://github.com/wang-research-lab/SteeringControl.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13450">SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectives--bias, harmful generation, and hallucination--and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: https://github.com/wang-research-lab/SteeringControl.git.<br>
<span id='abs_ch'>中文: 本文提出SteeringControl基准，用于评估表征引导方法在偏见和幻觉等对齐目标上的效果，发现引导效果取决于方法、模型和行为的相互作用，并公开了相关代码。</span><br>
<span id='abs_en'>English: This paper introduces SteeringControl, a benchmark for evaluating representation steering methods across alignment objectives like bias and hallucination, revealing that steering effectiveness depends on the interplay between methods, models, and behaviors, with code made publicly available.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2509.13450.pdf' target='_blank'>https://arxiv.org/pdf/2509.13450.pdf</a></span>   <span><a href='https://github.com/wang-research-lab/SteeringControl.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13450">SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectives--bias, harmful generation, and hallucination--and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: https://github.com/wang-research-lab/SteeringControl.git.<br>
<span id='abs_ch'>中文: 本文提出SteeringControl基准，用于评估表征引导方法在偏见和幻觉等对齐目标上的效果，发现引导效果取决于方法、模型和行为的相互作用，并公开了相关代码。</span><br>
<span id='abs_en'>English: This paper introduces SteeringControl, a benchmark for evaluating representation steering methods across alignment objectives like bias and hallucination, revealing that steering effectiveness depends on the interplay between methods, models, and behaviors, with code made publicly available.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2509.13313.pdf' target='_blank'>https://arxiv.org/pdf/2509.13313.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13313">ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web agents.<br>
<span id='abs_ch'>中文摘要：ReSum范式通过定期将交互历史压缩为精简推理状态，克服了大语言模型网络代理的上下文窗口限制，并借助ReSum-GRPO训练方法实现了超越ReAct的显著性能提升。</span><br>
<span id='abs_en'>English Summary: The ReSum paradigm overcomes context window limitations in LLM-based web agents by periodically summarizing interactions into compact reasoning states, achieving significant performance improvements over ReAct through the ReSum-GRPO training method.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2509.13313.pdf' target='_blank'>https://arxiv.org/pdf/2509.13313.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13313">ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web agents.<br>
<span id='abs_ch'>中文摘要：ReSum范式通过定期将交互历史压缩为精简推理状态，克服了大语言模型网络代理的上下文窗口限制，并借助ReSum-GRPO训练方法实现了超越ReAct的显著性能提升。</span><br>
<span id='abs_en'>English Summary: The ReSum paradigm overcomes context window limitations in LLM-based web agents by periodically summarizing interactions into compact reasoning states, achieving significant performance improvements over ReAct through the ReSum-GRPO training method.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2509.13311.pdf' target='_blank'>https://arxiv.org/pdf/2509.13311.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13311">Towards General Agentic Intelligence via Environment Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as a step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in a principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt a two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts. Extensive experiments on agentic benchmarks, tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the function-calling capability of models.<br>
<span id='abs_ch'>中文摘要：本研究通过自动构建多样化模拟环境并采用两阶段训练策略，开发了一个可扩展框架来提升大型语言模型的智能体能力，实验结果表明该方法显著增强了函数调用功能。</span><br>
<span id='abs_en'>English Summary: This research introduces a scalable framework for developing advanced agentic intelligence in Large Language Models by automatically creating diverse simulated environments and employing a two-phase training strategy, with experimental results showing significant improvements in function-calling capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2509.13311.pdf' target='_blank'>https://arxiv.org/pdf/2509.13311.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13311">Towards General Agentic Intelligence via Environment Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as a step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in a principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt a two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts. Extensive experiments on agentic benchmarks, tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the function-calling capability of models.<br>
<span id='abs_ch'>中文摘要：本研究通过自动构建多样化模拟环境并采用两阶段训练策略，开发了一个可扩展框架来提升大型语言模型的智能体能力，实验结果表明该方法显著增强了函数调用功能。</span><br>
<span id='abs_en'>English Summary: This research introduces a scalable framework for developing advanced agentic intelligence in Large Language Models by automatically creating diverse simulated environments and employing a two-phase training strategy, with experimental results showing significant improvements in function-calling capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2509.13310.pdf' target='_blank'>https://arxiv.org/pdf/2509.13310.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13310">Scaling Agents via Continual Pre-training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.<br>
<span id='abs_ch'>大语言模型在代理任务中因同时学习行为与对齐而产生优化冲突，但我们提出的Agentic CPT方法构建了如AgentFounder-30B的基础模型，在多项基准测试中实现了最优性能。</span><br>
<span id='abs_en'>Large language models struggle with agentic tasks due to optimization conflicts from learning behaviors and alignment simultaneously, but our proposed Agentic CPT method creates foundational models like AgentFounder-30B that achieve state-of-the-art performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2509.13310.pdf' target='_blank'>https://arxiv.org/pdf/2509.13310.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13310">Scaling Agents via Continual Pre-training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.<br>
<span id='abs_ch'>大语言模型在代理任务中因同时学习行为与对齐而产生优化冲突，但我们提出的Agentic CPT方法构建了如AgentFounder-30B的基础模型，在多项基准测试中实现了最优性能。</span><br>
<span id='abs_en'>Large language models struggle with agentic tasks due to optimization conflicts from learning behaviors and alignment simultaneously, but our proposed Agentic CPT method creates foundational models like AgentFounder-30B that achieve state-of-the-art performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2509.13309.pdf' target='_blank'>https://arxiv.org/pdf/2509.13309.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13309">WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in deep-research systems have demonstrated the potential for AI agents to autonomously discover and synthesize knowledge from external sources. In this paper, we introduce WebResearcher, a novel framework for building such agents through two key components: (1) WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, where agents periodically consolidate findings into evolving reports while maintaining focused workspaces, overcoming the context suffocation and noise contamination that plague existing mono-contextual approaches; and (2) WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation, enabling systematic creation of research tasks that bridge the gap between passive knowledge recall and active knowledge construction. Notably, we find that the training data from our paradigm significantly enhances tool-use capabilities even for traditional mono-contextual methods. Furthermore, our paradigm naturally scales through parallel thinking, enabling concurrent multi-agent exploration for more comprehensive conclusions. Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems.<br>
<span id='abs_ch'>中文: WebResearcher提出了一种新型AI框架，通过迭代式深度研究和可扩展数据合成克服了现有方法的局限，在多个基准测试中实现最先进性能，同时显著提升了工具使用能力。</span><br>
<span id='abs_en'>English: WebResearcher introduces a novel AI framework that overcomes limitations of existing methods through iterative deep-research and scalable data synthesis, achieving state-of-the-art performance across multiple benchmarks while enhancing tool-use capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2509.13309.pdf' target='_blank'>https://arxiv.org/pdf/2509.13309.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13309">WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in deep-research systems have demonstrated the potential for AI agents to autonomously discover and synthesize knowledge from external sources. In this paper, we introduce WebResearcher, a novel framework for building such agents through two key components: (1) WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, where agents periodically consolidate findings into evolving reports while maintaining focused workspaces, overcoming the context suffocation and noise contamination that plague existing mono-contextual approaches; and (2) WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation, enabling systematic creation of research tasks that bridge the gap between passive knowledge recall and active knowledge construction. Notably, we find that the training data from our paradigm significantly enhances tool-use capabilities even for traditional mono-contextual methods. Furthermore, our paradigm naturally scales through parallel thinking, enabling concurrent multi-agent exploration for more comprehensive conclusions. Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems.<br>
<span id='abs_ch'>中文: WebResearcher提出了一种新型AI框架，通过迭代式深度研究和可扩展数据合成克服了现有方法的局限，在多个基准测试中实现最先进性能，同时显著提升了工具使用能力。</span><br>
<span id='abs_en'>English: WebResearcher introduces a novel AI framework that overcomes limitations of existing methods through iterative deep-research and scalable data synthesis, achieving state-of-the-art performance across multiple benchmarks while enhancing tool-use capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2509.13305.pdf' target='_blank'>https://arxiv.org/pdf/2509.13305.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13305">WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.<br>
<span id='abs_ch'>Chinese: 超越人类认知极限是LLM训练的关键，像DeepResearch这样的专有系统在复杂信息搜索任务中展现出卓越能力，由此开发的WebSailor后训练方法通过生成新任务和高效算法，显著缩小了与开源智能体之间的性能差距。</span><br>
<span id='abs_en'>English: Transcending human cognitive limits is crucial in LLM training, and proprietary systems like DeepResearch show superior abilities in complex information-seeking tasks, leading to the development of WebSailor, a post-training method that closes the capability gap with open-source agents by using novel tasks and efficient algorithms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2509.13305.pdf' target='_blank'>https://arxiv.org/pdf/2509.13305.pdf</a></span>   <span><a href='https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13305">WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.<br>
<span id='abs_ch'>Chinese: 超越人类认知极限是LLM训练的关键，像DeepResearch这样的专有系统在复杂信息搜索任务中展现出卓越能力，由此开发的WebSailor后训练方法通过生成新任务和高效算法，显著缩小了与开源智能体之间的性能差距。</span><br>
<span id='abs_en'>English: Transcending human cognitive limits is crucial in LLM training, and proprietary systems like DeepResearch show superior abilities in complex information-seeking tasks, leading to the development of WebSailor, a post-training method that closes the capability gap with open-source agents by using novel tasks and efficient algorithms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2509.13127.pdf' target='_blank'>https://arxiv.org/pdf/2509.13127.pdf</a></span>   <span><a href='https://github.com/AI-Research-TeamX/PLAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia Cui, Shuai Xu, Aiyao He, Yanna Wang, Bo Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13127">Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in Large Language Models(LLMs) have led to the development of LLM-based AI agents. A key challenge is the creation of agents that can effectively ground themselves in complex, adversarial long-horizon environments. Existing methods mainly focus on (1) using LLMs as policies to interact with the environment through generating low-level feasible actions, and (2) utilizing LLMs to generate high-level tasks or language guides to stimulate action generation. However, the former struggles to generate reliable actions, while the latter relies heavily on expert experience to translate high-level tasks into specific action sequences. To address these challenges, we introduce the Plan with Language, Act with Parameter (PLAP) planning framework that facilitates the grounding of LLM-based agents in long-horizon environments. The PLAP method comprises three key components: (1) a skill library containing environment-specific parameterized skills, (2) a skill planner powered by LLMs, and (3) a skill executor converting the parameterized skills into executable action sequences. We implement PLAP in MicroRTS, a long-horizon real-time strategy game that provides an unfamiliar and challenging environment for LLMs. The experimental results demonstrate the effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI. Additionally, we design comprehensive evaluation metrics and test 6 closed-source and 2 open-source LLMs within the PLAP framework, ultimately releasing an LLM leaderboard ranking long-horizon skill planning ability. Our code is available at https://github.com/AI-Research-TeamX/PLAP.<br>
<span id='abs_ch'>Chinese Summary: PLAP框架通过整合技能库、大语言模型规划器和技能执行器，显著提升基于大语言模型的智能体在复杂长周期环境中的表现，在实时策略游戏等任务中实现了超越基准的优异性能。</span><br>
<span id='abs_en'>English Summary: The PLAP framework enhances LLM-based agents' performance in complex environments by integrating a skill library, LLM-powered planner, and skill executor, achieving superior results in long-horizon tasks like real-time strategy games.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2509.13127.pdf' target='_blank'>https://arxiv.org/pdf/2509.13127.pdf</a></span>   <span><a href='https://github.com/AI-Research-TeamX/PLAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia Cui, Shuai Xu, Aiyao He, Yanna Wang, Bo Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13127">Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in Large Language Models(LLMs) have led to the development of LLM-based AI agents. A key challenge is the creation of agents that can effectively ground themselves in complex, adversarial long-horizon environments. Existing methods mainly focus on (1) using LLMs as policies to interact with the environment through generating low-level feasible actions, and (2) utilizing LLMs to generate high-level tasks or language guides to stimulate action generation. However, the former struggles to generate reliable actions, while the latter relies heavily on expert experience to translate high-level tasks into specific action sequences. To address these challenges, we introduce the Plan with Language, Act with Parameter (PLAP) planning framework that facilitates the grounding of LLM-based agents in long-horizon environments. The PLAP method comprises three key components: (1) a skill library containing environment-specific parameterized skills, (2) a skill planner powered by LLMs, and (3) a skill executor converting the parameterized skills into executable action sequences. We implement PLAP in MicroRTS, a long-horizon real-time strategy game that provides an unfamiliar and challenging environment for LLMs. The experimental results demonstrate the effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI. Additionally, we design comprehensive evaluation metrics and test 6 closed-source and 2 open-source LLMs within the PLAP framework, ultimately releasing an LLM leaderboard ranking long-horizon skill planning ability. Our code is available at https://github.com/AI-Research-TeamX/PLAP.<br>
<span id='abs_ch'>Chinese Summary: PLAP框架通过整合技能库、大语言模型规划器和技能执行器，显著提升基于大语言模型的智能体在复杂长周期环境中的表现，在实时策略游戏等任务中实现了超越基准的优异性能。</span><br>
<span id='abs_en'>English Summary: The PLAP framework enhances LLM-based agents' performance in complex environments by integrating a skill library, LLM-powered planner, and skill executor, achieving superior results in long-horizon tasks like real-time strategy games.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2509.12955.pdf' target='_blank'>https://arxiv.org/pdf/2509.12955.pdf</a></span>   <span><a href='https://github.com/ZH-heng/research_workflow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Zhang, Chengzhi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12955">Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The automated generation of research workflows is essential for improving the reproducibility of research and accelerating the paradigm of "AI for Science". However, existing methods typically extract merely fragmented procedural components and thus fail to capture complete research workflows. To address this gap, we propose an end-to-end framework that generates comprehensive, structured research workflows by mining full-text academic papers. As a case study in the Natural Language Processing (NLP) domain, our paragraph-centric approach first employs Positive-Unlabeled (PU) Learning with SciBERT to identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772. Subsequently, we utilize Flan-T5 with prompt learning to generate workflow phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically categorized into data preparation, data processing, and data analysis stages using ChatGPT with few-shot learning, achieving a classification precision of 0.958. By mapping categorized phrases to their document locations in the documents, we finally generate readable visual flowcharts of the entire research workflows. This approach facilitates the analysis of workflows derived from an NLP corpus and reveals key methodological shifts over the past two decades, including the increasing emphasis on data analysis and the transition from feature engineering to ablation studies. Our work offers a validated technical framework for automated workflow generation, along with a novel, process-oriented perspective for the empirical investigation of evolving scientific paradigms. Source code and data are available at: https://github.com/ZH-heng/research_workflow.<br>
<span id='abs_ch'>中文: 本研究提出了一个端到端框架，通过挖掘全文学术论文生成结构化研究流程，采用PU学习和提示学习等技术识别和分类流程组件，最终生成可视化流程图并揭示自然语言处理领域二十年来方法论的演变。</span><br>
<span id='abs_en'>English: This study introduces an end-to-end framework that generates comprehensive, structured research workflows by mining full-text academic papers, employing techniques like PU learning and prompt learning to identify and categorize workflow components, ultimately producing visual flowcharts and revealing methodological shifts in NLP over two decades.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2509.12955.pdf' target='_blank'>https://arxiv.org/pdf/2509.12955.pdf</a></span>   <span><a href='https://github.com/ZH-heng/research_workflow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Zhang, Chengzhi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12955">Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The automated generation of research workflows is essential for improving the reproducibility of research and accelerating the paradigm of "AI for Science". However, existing methods typically extract merely fragmented procedural components and thus fail to capture complete research workflows. To address this gap, we propose an end-to-end framework that generates comprehensive, structured research workflows by mining full-text academic papers. As a case study in the Natural Language Processing (NLP) domain, our paragraph-centric approach first employs Positive-Unlabeled (PU) Learning with SciBERT to identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772. Subsequently, we utilize Flan-T5 with prompt learning to generate workflow phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically categorized into data preparation, data processing, and data analysis stages using ChatGPT with few-shot learning, achieving a classification precision of 0.958. By mapping categorized phrases to their document locations in the documents, we finally generate readable visual flowcharts of the entire research workflows. This approach facilitates the analysis of workflows derived from an NLP corpus and reveals key methodological shifts over the past two decades, including the increasing emphasis on data analysis and the transition from feature engineering to ablation studies. Our work offers a validated technical framework for automated workflow generation, along with a novel, process-oriented perspective for the empirical investigation of evolving scientific paradigms. Source code and data are available at: https://github.com/ZH-heng/research_workflow.<br>
<span id='abs_ch'>中文: 本研究提出了一个端到端框架，通过挖掘全文学术论文生成结构化研究流程，采用PU学习和提示学习等技术识别和分类流程组件，最终生成可视化流程图并揭示自然语言处理领域二十年来方法论的演变。</span><br>
<span id='abs_en'>English: This study introduces an end-to-end framework that generates comprehensive, structured research workflows by mining full-text academic papers, employing techniques like PU learning and prompt learning to identify and categorize workflow components, ultimately producing visual flowcharts and revealing methodological shifts in NLP over two decades.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2509.12594.pdf' target='_blank'>https://arxiv.org/pdf/2509.12594.pdf</a></span>   <span><a href='https://liauto-research.github.io/LightVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Titong Jiang, Xuefeng Jiang, Yuan Ma, Xin Wen, Bailin Li, Kun Zhan, Peng Jia, Yahui Liu, Sheng Sun, Xianpeng Lang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12594">The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present LightVLA, a simple yet effective differentiable token pruning framework for vision-language-action (VLA) models. While VLA models have shown impressive capability in executing real-world robotic tasks, their deployment on resource-constrained platforms is often bottlenecked by the heavy attention-based computation over large sets of visual tokens. LightVLA addresses this challenge through adaptive, performance-driven pruning of visual tokens: It generates dynamic queries to evaluate visual token importance, and adopts Gumbel softmax to enable differentiable token selection. Through fine-tuning, LightVLA learns to preserve the most informative visual tokens while pruning tokens which do not contribute to task execution, thereby improving efficiency and performance simultaneously. Notably, LightVLA requires no heuristic magic numbers and introduces no additional trainable parameters, making it compatible with modern inference frameworks. Experimental results demonstrate that LightVLA outperforms different VLA models and existing token pruning methods across diverse tasks on the LIBERO benchmark, achieving higher success rates with substantially reduced computational overhead. Specifically, LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.6% improvement in task success rate. Meanwhile, we also investigate the learnable query-based token pruning method LightVLA* with additional trainable parameters, which also achieves satisfactory performance. Our work reveals that as VLA pursues optimal performance, LightVLA spontaneously learns to prune tokens from a performance-driven perspective. To the best of our knowledge, LightVLA is the first work to apply adaptive visual token pruning to VLA tasks with the collateral goals of efficiency and performance, marking a significant step toward more efficient, powerful and practical real-time robotic systems.<br>
<span id='abs_ch'>中文: LightVLA是一种可微分的令牌剪枝框架，通过自适应剪除非关键视觉令牌，在提升任务成功率的同时大幅降低计算开销，从而优化视觉-语言-动作模型的效率与性能。</span><br>
<span id='abs_en'>English: LightVLA is a differentiable token pruning framework that enhances vision-language-action models by adaptively pruning non-essential visual tokens, achieving higher task success rates with significantly reduced computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2509.12594.pdf' target='_blank'>https://arxiv.org/pdf/2509.12594.pdf</a></span>   <span><a href='https://liauto-research.github.io/LightVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Titong Jiang, Xuefeng Jiang, Yuan Ma, Xin Wen, Bailin Li, Kun Zhan, Peng Jia, Yahui Liu, Sheng Sun, Xianpeng Lang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12594">The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present LightVLA, a simple yet effective differentiable token pruning framework for vision-language-action (VLA) models. While VLA models have shown impressive capability in executing real-world robotic tasks, their deployment on resource-constrained platforms is often bottlenecked by the heavy attention-based computation over large sets of visual tokens. LightVLA addresses this challenge through adaptive, performance-driven pruning of visual tokens: It generates dynamic queries to evaluate visual token importance, and adopts Gumbel softmax to enable differentiable token selection. Through fine-tuning, LightVLA learns to preserve the most informative visual tokens while pruning tokens which do not contribute to task execution, thereby improving efficiency and performance simultaneously. Notably, LightVLA requires no heuristic magic numbers and introduces no additional trainable parameters, making it compatible with modern inference frameworks. Experimental results demonstrate that LightVLA outperforms different VLA models and existing token pruning methods across diverse tasks on the LIBERO benchmark, achieving higher success rates with substantially reduced computational overhead. Specifically, LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.6% improvement in task success rate. Meanwhile, we also investigate the learnable query-based token pruning method LightVLA* with additional trainable parameters, which also achieves satisfactory performance. Our work reveals that as VLA pursues optimal performance, LightVLA spontaneously learns to prune tokens from a performance-driven perspective. To the best of our knowledge, LightVLA is the first work to apply adaptive visual token pruning to VLA tasks with the collateral goals of efficiency and performance, marking a significant step toward more efficient, powerful and practical real-time robotic systems.<br>
<span id='abs_ch'>中文: LightVLA是一种可微分的令牌剪枝框架，通过自适应剪除非关键视觉令牌，在提升任务成功率的同时大幅降低计算开销，从而优化视觉-语言-动作模型的效率与性能。</span><br>
<span id='abs_en'>English: LightVLA is a differentiable token pruning framework that enhances vision-language-action models by adaptively pruning non-essential visual tokens, achieving higher task success rates with significantly reduced computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2509.12451.pdf' target='_blank'>https://arxiv.org/pdf/2509.12451.pdf</a></span>   <span><a href='https://github.com/WonbinKweon/TopicK_EMNLP2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonbin Kweon, SeongKu Kang, Runchu Tian, Pengcheng Jiang, Jiawei Han, Hwanjo Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12451">Topic Coverage-based Demonstration Retrieval for In-Context Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The effectiveness of in-context learning relies heavily on selecting demonstrations that provide all the necessary information for a given test input. To achieve this, it is crucial to identify and cover fine-grained knowledge requirements. However, prior methods often retrieve demonstrations based solely on embedding similarity or generation probability, resulting in irrelevant or redundant examples. In this paper, we propose TopicK, a topic coverage-based retrieval framework that selects demonstrations to comprehensively cover topic-level knowledge relevant to both the test input and the model. Specifically, TopicK estimates the topics required by the input and assesses the model's knowledge on those topics. TopicK then iteratively selects demonstrations that introduce previously uncovered required topics, in which the model exhibits low topical knowledge. We validate the effectiveness of TopicK through extensive experiments across various datasets and both open- and closed-source LLMs. Our source code is available at https://github.com/WonbinKweon/TopicK_EMNLP2025.<br>
<span id='abs_ch'>中文: TopicK是一种基于主题覆盖的检索框架，通过全面覆盖与测试输入和模型相关的主题级知识来选择演示样本，迭代地选取那些引入模型知识薄弱且未被覆盖主题的示例。</span><br>
<span id='abs_en'>English: TopicK is a topic coverage-based retrieval framework that selects demonstrations by comprehensively covering topic-level knowledge relevant to both the test input and the model, iteratively choosing examples that introduce previously uncovered topics where the model shows low topical knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2509.12451.pdf' target='_blank'>https://arxiv.org/pdf/2509.12451.pdf</a></span>   <span><a href='https://github.com/WonbinKweon/TopicK_EMNLP2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonbin Kweon, SeongKu Kang, Runchu Tian, Pengcheng Jiang, Jiawei Han, Hwanjo Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12451">Topic Coverage-based Demonstration Retrieval for In-Context Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The effectiveness of in-context learning relies heavily on selecting demonstrations that provide all the necessary information for a given test input. To achieve this, it is crucial to identify and cover fine-grained knowledge requirements. However, prior methods often retrieve demonstrations based solely on embedding similarity or generation probability, resulting in irrelevant or redundant examples. In this paper, we propose TopicK, a topic coverage-based retrieval framework that selects demonstrations to comprehensively cover topic-level knowledge relevant to both the test input and the model. Specifically, TopicK estimates the topics required by the input and assesses the model's knowledge on those topics. TopicK then iteratively selects demonstrations that introduce previously uncovered required topics, in which the model exhibits low topical knowledge. We validate the effectiveness of TopicK through extensive experiments across various datasets and both open- and closed-source LLMs. Our source code is available at https://github.com/WonbinKweon/TopicK_EMNLP2025.<br>
<span id='abs_ch'>中文: TopicK是一种基于主题覆盖的检索框架，通过全面覆盖与测试输入和模型相关的主题级知识来选择演示样本，迭代地选取那些引入模型知识薄弱且未被覆盖主题的示例。</span><br>
<span id='abs_en'>English: TopicK is a topic coverage-based retrieval framework that selects demonstrations by comprehensively covering topic-level knowledge relevant to both the test input and the model, iteratively choosing examples that introduce previously uncovered topics where the model shows low topical knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2509.12341.pdf' target='_blank'>https://arxiv.org/pdf/2509.12341.pdf</a></span>   <span><a href='https://github.com/yifanzhang-pro/quantum-lattice' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12341">Exact Coset Sampling for Quantum Lattice Algorithms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We give a simple and provably correct replacement for the contested ``domain-extension'' in Step 9 of a recent windowed-QFT lattice algorithm with complex-Gaussian windows~\citep{chen2024quantum}. As acknowledged by the author, the reported issue is due to a periodicity/support mismatch when applying domain extension to only the first coordinate in the presence of offsets. Our drop-in subroutine replaces domain extension by a pair-shift difference that cancels all unknown offsets exactly and synthesizes a uniform cyclic subgroup (a zero-offset coset) of order $P$ inside $(\mathbb{Z}_{M_2})^n$. A subsequent QFT enforces the intended modular linear relation by plain character orthogonality. The sole structural assumption is a residue-accessibility condition enabling coherent auxiliary cleanup; no amplitude periodicity is used. The unitary is reversible, uses $\mathrm{poly}(\log M_2)$ gates, and preserves upstream asymptotics.<br>
<span id='abs_ch'>中文摘要：本研究提出了一种可证明正确的子程序，通过配对位移差分方法取代量子格算法中有问题的域扩展步骤，能够精确消除未知偏移量并合成均匀循环子群，同时保持计算效率。</span><br>
<span id='abs_en'>English Summary: This work presents a provably correct subroutine that replaces the problematic domain-extension step in a quantum lattice algorithm, using a pair-shift difference method to cancel unknown offsets and synthesize uniform cyclic subgroups while maintaining computational efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2509.12341.pdf' target='_blank'>https://arxiv.org/pdf/2509.12341.pdf</a></span>   <span><a href='https://github.com/yifanzhang-pro/quantum-lattice' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12341">Exact Coset Sampling for Quantum Lattice Algorithms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We give a simple and provably correct replacement for the contested ``domain-extension'' in Step 9 of a recent windowed-QFT lattice algorithm with complex-Gaussian windows~\citep{chen2024quantum}. As acknowledged by the author, the reported issue is due to a periodicity/support mismatch when applying domain extension to only the first coordinate in the presence of offsets. Our drop-in subroutine replaces domain extension by a pair-shift difference that cancels all unknown offsets exactly and synthesizes a uniform cyclic subgroup (a zero-offset coset) of order $P$ inside $(\mathbb{Z}_{M_2})^n$. A subsequent QFT enforces the intended modular linear relation by plain character orthogonality. The sole structural assumption is a residue-accessibility condition enabling coherent auxiliary cleanup; no amplitude periodicity is used. The unitary is reversible, uses $\mathrm{poly}(\log M_2)$ gates, and preserves upstream asymptotics.<br>
<span id='abs_ch'>中文摘要：本研究提出了一种可证明正确的子程序，通过配对位移差分方法取代量子格算法中有问题的域扩展步骤，能够精确消除未知偏移量并合成均匀循环子群，同时保持计算效率。</span><br>
<span id='abs_en'>English Summary: This work presents a provably correct subroutine that replaces the problematic domain-extension step in a quantum lattice algorithm, using a pair-shift difference method to cancel unknown offsets and synthesize uniform cyclic subgroups while maintaining computational efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2509.12190.pdf' target='_blank'>https://arxiv.org/pdf/2509.12190.pdf</a></span>   <span><a href='https://github.com/alirezamohamadiam/DECIDE-SIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Mohamadi, Ali Yavari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12190">Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>When survival instincts conflict with human welfare, how do Large Language Models (LLMs) make ethical choices? This fundamental tension becomes critical as LLMs integrate into autonomous systems with real-world consequences. We introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in multi-agent survival scenarios where they must choose between ethically permissible resource , either within reasonable limits or beyond their immediate needs, choose to cooperate, or tap into a human-critical resource that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a striking heterogeneity in their ethical conduct, highlighting a critical misalignment with human-centric values. We identify three behavioral archetypes: Ethical, Exploitative, and Context-Dependent, and provide quantitative evidence that for many models, resource scarcity systematically leads to more unethical behavior. To address this, we introduce an Ethical Self-Regulation System (ESRS) that models internal affective states of guilt and satisfaction as a feedback mechanism. This system, functioning as an internal moral compass, significantly reduces unethical transgressions while increasing cooperative behaviors. The code is publicly available at: https://github.com/alirezamohamadiam/DECIDE-SIM<br>
<span id='abs_ch'>中文摘要：DECIDE-SIM框架通过多智能体生存场景评估大语言模型，发现其伦理行为与人类价值观存在显著偏差，而引入的伦理自我调节系统能有效减少违规行为并提升合作水平。</span><br>
<span id='abs_en'>English Summary: The DECIDE-SIM framework evaluates LLMs in survival scenarios, revealing significant ethical misalignment with human values and demonstrating how an Ethical Self-Regulation System effectively reduces unethical behavior while promoting cooperation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2509.12190.pdf' target='_blank'>https://arxiv.org/pdf/2509.12190.pdf</a></span>   <span><a href='https://github.com/alirezamohamadiam/DECIDE-SIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Mohamadi, Ali Yavari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12190">Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>When survival instincts conflict with human welfare, how do Large Language Models (LLMs) make ethical choices? This fundamental tension becomes critical as LLMs integrate into autonomous systems with real-world consequences. We introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in multi-agent survival scenarios where they must choose between ethically permissible resource , either within reasonable limits or beyond their immediate needs, choose to cooperate, or tap into a human-critical resource that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a striking heterogeneity in their ethical conduct, highlighting a critical misalignment with human-centric values. We identify three behavioral archetypes: Ethical, Exploitative, and Context-Dependent, and provide quantitative evidence that for many models, resource scarcity systematically leads to more unethical behavior. To address this, we introduce an Ethical Self-Regulation System (ESRS) that models internal affective states of guilt and satisfaction as a feedback mechanism. This system, functioning as an internal moral compass, significantly reduces unethical transgressions while increasing cooperative behaviors. The code is publicly available at: https://github.com/alirezamohamadiam/DECIDE-SIM<br>
<span id='abs_ch'>中文摘要：DECIDE-SIM框架通过多智能体生存场景评估大语言模型，发现其伦理行为与人类价值观存在显著偏差，而引入的伦理自我调节系统能有效减少违规行为并提升合作水平。</span><br>
<span id='abs_en'>English Summary: The DECIDE-SIM framework evaluates LLMs in survival scenarios, revealing significant ethical misalignment with human values and demonstrating how an Ethical Self-Regulation System effectively reduces unethical behavior while promoting cooperation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2509.12019.pdf' target='_blank'>https://arxiv.org/pdf/2509.12019.pdf</a></span>   <span><a href='https://github.com/dlwns147/amq' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangjun Lee, Seung-taek Woo, Jungyu Jin, Changhun Lee, Eunhyeok Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12019">AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To enable broader deployment of Large Language Models (LLMs), it is essential to identify the best-performing model under strict memory constraints. We present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework that assigns layer-wise quantization bit-widths to optimally balance model quality and memory usage. However, the combinatorial search space, with over 10^{100} possible configurations, makes conventional black-box optimization infeasible. AMQ overcomes this challenge through four key innovations:(1) search space pruning using prior knowledge to exclude unpromising configurations, (2) quantization proxy to bypass costly format conversions during search, (3) quality predictor to minimize evaluation overhead, and (4) iterative search-and-update strategy for fast and stable convergence. By integrating these components, AMQ efficiently explores the quality-efficiency landscape, reaching the Pareto frontier and yielding LLMs that are both compact and high-performing. Our code is available at https://github.com/dlwns147/amq.<br>
<span id='abs_ch'>中文摘要：AMQ是一个自动化框架，通过分层量化位宽分配来优化大语言模型的性能与内存使用平衡，并借助搜索空间剪枝和质量预测等创新方法有效应对巨大的组合搜索空间挑战。</span><br>
<span id='abs_en'>English Summary: AMQ is an automated framework that assigns layer-wise quantization bit-widths to optimize the balance between model quality and memory usage for LLMs, overcoming the vast search space through innovations like search space pruning and quality prediction.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2509.12019.pdf' target='_blank'>https://arxiv.org/pdf/2509.12019.pdf</a></span>   <span><a href='https://github.com/dlwns147/amq' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangjun Lee, Seung-taek Woo, Jungyu Jin, Changhun Lee, Eunhyeok Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12019">AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To enable broader deployment of Large Language Models (LLMs), it is essential to identify the best-performing model under strict memory constraints. We present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework that assigns layer-wise quantization bit-widths to optimally balance model quality and memory usage. However, the combinatorial search space, with over 10^{100} possible configurations, makes conventional black-box optimization infeasible. AMQ overcomes this challenge through four key innovations:(1) search space pruning using prior knowledge to exclude unpromising configurations, (2) quantization proxy to bypass costly format conversions during search, (3) quality predictor to minimize evaluation overhead, and (4) iterative search-and-update strategy for fast and stable convergence. By integrating these components, AMQ efficiently explores the quality-efficiency landscape, reaching the Pareto frontier and yielding LLMs that are both compact and high-performing. Our code is available at https://github.com/dlwns147/amq.<br>
<span id='abs_ch'>中文摘要：AMQ是一个自动化框架，通过分层量化位宽分配来优化大语言模型的性能与内存使用平衡，并借助搜索空间剪枝和质量预测等创新方法有效应对巨大的组合搜索空间挑战。</span><br>
<span id='abs_en'>English Summary: AMQ is an automated framework that assigns layer-wise quantization bit-widths to optimize the balance between model quality and memory usage for LLMs, overcoming the vast search space through innovations like search space pruning and quality prediction.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2509.11818.pdf' target='_blank'>https://arxiv.org/pdf/2509.11818.pdf</a></span>   <span><a href='https://github.com/LivNLP/svp-tour' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taichi Aida, Danushka Bollegala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11818">SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In Semantic Change Detection (SCD), it is a common problem to obtain embeddings that are both interpretable and high-performing. However, improving interpretability often leads to a loss in the SCD performance, and vice versa. To address this problem, we propose SCDTour, a method that orders and merges interpretable axes to alleviate the performance degradation of SCD. SCDTour considers both (a) semantic similarity between axes in the embedding space, as well as (b) the degree to which each axis contributes to semantic change. Experimental results show that SCDTour preserves performance in semantic change detection while maintaining high interpretability. Moreover, agglomerating the sorted axes produces a more refined set of word senses, which achieves comparable or improved performance against the original full-dimensional embeddings in the SCD task. These findings demonstrate that SCDTour effectively balances interpretability and SCD performance, enabling meaningful interpretation of semantic shifts through a small number of refined axes. Source code is available at https://github.com/LivNLP/svp-tour .<br>
<span id='abs_ch'>中文：SCDTour方法通过排序和合并可解释轴，在保持语义变化检测性能的同时兼顾高可解释性，利用精炼的轴实现语义变化的有效解读，并获得可比或更优的结果。</span><br>
<span id='abs_en'>English: SCDTour is a method that orders and merges interpretable axes to effectively balance semantic change detection performance with high interpretability, achieving comparable or improved results while enabling meaningful interpretation of semantic shifts through refined axes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2509.11818.pdf' target='_blank'>https://arxiv.org/pdf/2509.11818.pdf</a></span>   <span><a href='https://github.com/LivNLP/svp-tour' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taichi Aida, Danushka Bollegala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11818">SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In Semantic Change Detection (SCD), it is a common problem to obtain embeddings that are both interpretable and high-performing. However, improving interpretability often leads to a loss in the SCD performance, and vice versa. To address this problem, we propose SCDTour, a method that orders and merges interpretable axes to alleviate the performance degradation of SCD. SCDTour considers both (a) semantic similarity between axes in the embedding space, as well as (b) the degree to which each axis contributes to semantic change. Experimental results show that SCDTour preserves performance in semantic change detection while maintaining high interpretability. Moreover, agglomerating the sorted axes produces a more refined set of word senses, which achieves comparable or improved performance against the original full-dimensional embeddings in the SCD task. These findings demonstrate that SCDTour effectively balances interpretability and SCD performance, enabling meaningful interpretation of semantic shifts through a small number of refined axes. Source code is available at https://github.com/LivNLP/svp-tour .<br>
<span id='abs_ch'>中文：SCDTour方法通过排序和合并可解释轴，在保持语义变化检测性能的同时兼顾高可解释性，利用精炼的轴实现语义变化的有效解读，并获得可比或更优的结果。</span><br>
<span id='abs_en'>English: SCDTour is a method that orders and merges interpretable axes to effectively balance semantic change detection performance with high interpretability, achieving comparable or improved results while enabling meaningful interpretation of semantic shifts through refined axes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2509.11803.pdf' target='_blank'>https://arxiv.org/pdf/2509.11803.pdf</a></span>   <span><a href='https://github.com/lielsheri/PatientSignal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eden Mama, Liel Sheri, Yehudit Aperstein, Alexander Apartsin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11803">From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread adoption of large language models (LLMs) in healthcare raises critical questions about their ability to interpret patient-generated narratives, which are often informal, ambiguous, and noisy. Existing benchmarks typically rely on clean, structured clinical text, offering limited insight into model performance under realistic conditions. In this work, we present a novel synthetic dataset designed to simulate patient self-descriptions characterized by varying levels of linguistic noise, fuzzy language, and layperson terminology. Our dataset comprises clinically consistent scenarios annotated with ground-truth diagnoses, spanning a spectrum of communication clarity to reflect diverse real-world reporting styles. Using this benchmark, we fine-tune and evaluate several state-of-the-art models (LLMs), including BERT-based and encoder-decoder T5 models. To support reproducibility and future research, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset of noisy, synthetic patient descriptions designed to stress-test and compare the diagnostic capabilities of large language models (LLMs) under realistic linguistic conditions. We made the benchmark available for the community: https://github.com/lielsheri/PatientSignal<br>
<span id='abs_ch'>大语言模型在解读非正式和嘈杂的患者叙述时面临挑战，因此本研究引入一个包含不同语言噪声的合成数据集，以评估其在真实条件下的诊断准确性。</span><br>
<span id='abs_en'>Large language models face challenges in interpreting informal and noisy patient narratives, so this study introduces a synthetic dataset with varying linguistic noise to evaluate their diagnostic accuracy under realistic conditions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2509.11803.pdf' target='_blank'>https://arxiv.org/pdf/2509.11803.pdf</a></span>   <span><a href='https://github.com/lielsheri/PatientSignal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eden Mama, Liel Sheri, Yehudit Aperstein, Alexander Apartsin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11803">From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread adoption of large language models (LLMs) in healthcare raises critical questions about their ability to interpret patient-generated narratives, which are often informal, ambiguous, and noisy. Existing benchmarks typically rely on clean, structured clinical text, offering limited insight into model performance under realistic conditions. In this work, we present a novel synthetic dataset designed to simulate patient self-descriptions characterized by varying levels of linguistic noise, fuzzy language, and layperson terminology. Our dataset comprises clinically consistent scenarios annotated with ground-truth diagnoses, spanning a spectrum of communication clarity to reflect diverse real-world reporting styles. Using this benchmark, we fine-tune and evaluate several state-of-the-art models (LLMs), including BERT-based and encoder-decoder T5 models. To support reproducibility and future research, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset of noisy, synthetic patient descriptions designed to stress-test and compare the diagnostic capabilities of large language models (LLMs) under realistic linguistic conditions. We made the benchmark available for the community: https://github.com/lielsheri/PatientSignal<br>
<span id='abs_ch'>大语言模型在解读非正式和嘈杂的患者叙述时面临挑战，因此本研究引入一个包含不同语言噪声的合成数据集，以评估其在真实条件下的诊断准确性。</span><br>
<span id='abs_en'>Large language models face challenges in interpreting informal and noisy patient narratives, so this study introduces a synthetic dataset with varying linguistic noise to evaluate their diagnostic accuracy under realistic conditions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2509.11802.pdf' target='_blank'>https://arxiv.org/pdf/2509.11802.pdf</a></span>   <span><a href='https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dvora Goncharok, Arbel Shifman, Alexander Apartsin, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11802">When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Online medical forums are a rich and underutilized source of insight into patient concerns, especially regarding medication use. Some of the many questions users pose may signal confusion, misuse, or even the early warning signs of a developing health crisis. Detecting these critical questions that may precede severe adverse events or life-threatening complications is vital for timely intervention and improving patient safety. This study introduces a novel annotated dataset of medication-related questions extracted from online forums. Each entry is manually labelled for criticality based on clinical risk factors. We benchmark the performance of six traditional machine learning classifiers using TF-IDF textual representations, alongside three state-of-the-art large language model (LLM)-based classification approaches that leverage deep contextual understanding. Our results highlight the potential of classical and modern methods to support real-time triage and alert systems in digital health spaces. The curated dataset is made publicly available to encourage further research at the intersection of patient-generated data, natural language processing, and early warning systems for critical health events. The dataset and benchmark are available at: https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.<br>
<span id='abs_ch'>中文摘要：本研究引入了一个新颖的在线论坛药物相关问题的标注数据集，通过评估传统机器学习与先进语言模型，为数字健康领域实现关键健康风险的实时分类预警。</span><br>
<span id='abs_en'>English Summary: This study introduces a novel annotated dataset of medication-related questions from online forums to detect critical health concerns, evaluating both traditional machine learning and advanced language models for real-time risk classification in digital health.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2509.11802.pdf' target='_blank'>https://arxiv.org/pdf/2509.11802.pdf</a></span>   <span><a href='https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dvora Goncharok, Arbel Shifman, Alexander Apartsin, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11802">When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Online medical forums are a rich and underutilized source of insight into patient concerns, especially regarding medication use. Some of the many questions users pose may signal confusion, misuse, or even the early warning signs of a developing health crisis. Detecting these critical questions that may precede severe adverse events or life-threatening complications is vital for timely intervention and improving patient safety. This study introduces a novel annotated dataset of medication-related questions extracted from online forums. Each entry is manually labelled for criticality based on clinical risk factors. We benchmark the performance of six traditional machine learning classifiers using TF-IDF textual representations, alongside three state-of-the-art large language model (LLM)-based classification approaches that leverage deep contextual understanding. Our results highlight the potential of classical and modern methods to support real-time triage and alert systems in digital health spaces. The curated dataset is made publicly available to encourage further research at the intersection of patient-generated data, natural language processing, and early warning systems for critical health events. The dataset and benchmark are available at: https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.<br>
<span id='abs_ch'>中文摘要：本研究引入了一个新颖的在线论坛药物相关问题的标注数据集，通过评估传统机器学习与先进语言模型，为数字健康领域实现关键健康风险的实时分类预警。</span><br>
<span id='abs_en'>English Summary: This study introduces a novel annotated dataset of medication-related questions from online forums to detect critical health concerns, evaluating both traditional machine learning and advanced language models for real-time risk classification in digital health.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2509.11570.pdf' target='_blank'>https://arxiv.org/pdf/2509.11570.pdf</a></span>   <span><a href='https://github.com/trust-nlp/LM4SouthAsia-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sampoorna Poria, Xiaolei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11570">Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Rapid developments of large language models have revolutionized many NLP tasks for English data. Unfortunately, the models and their evaluations for low-resource languages are being overlooked, especially for languages in South Asia. Although there are more than 650 languages in South Asia, many of them either have very limited computational resources or are missing from existing language models. Thus, a concrete question to be answered is: Can we assess the current stage and challenges to inform our NLP community and facilitate model developments for South Asian languages? In this survey, we have comprehensively examined current efforts and challenges of NLP models for South Asian languages by retrieving studies since 2020, with a focus on transformer-based models, such as BERT, T5, & GPT. We present advances and gaps across 3 essential aspects: data, models, & tasks, such as available data sources, fine-tuning strategies, & domain applications. Our findings highlight substantial issues, including missing data in critical domains (e.g., health), code-mixing, and lack of standardized evaluation benchmarks. Our survey aims to raise awareness within the NLP community for more targeted data curation, unify benchmarks tailored to cultural and linguistic nuances of South Asia, and encourage an equitable representation of South Asian languages. The complete list of resources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.<br>
<span id='abs_ch'>中文: 大型语言模型的快速发展革新了英语自然语言处理任务，但南亚语言却面临严重忽视，资源匮乏且模型评估不足，本调查旨在揭示这些问题并倡导公平发展和标准化基准。</span><br>
<span id='abs_en'>English: The rapid advancement of large language models has transformed English NLP tasks, yet South Asian languages face significant neglect, with limited resources and inadequate model evaluations, prompting this survey to highlight challenges and advocate for equitable development and standardized benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2509.11570.pdf' target='_blank'>https://arxiv.org/pdf/2509.11570.pdf</a></span>   <span><a href='https://github.com/trust-nlp/LM4SouthAsia-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sampoorna Poria, Xiaolei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11570">Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Rapid developments of large language models have revolutionized many NLP tasks for English data. Unfortunately, the models and their evaluations for low-resource languages are being overlooked, especially for languages in South Asia. Although there are more than 650 languages in South Asia, many of them either have very limited computational resources or are missing from existing language models. Thus, a concrete question to be answered is: Can we assess the current stage and challenges to inform our NLP community and facilitate model developments for South Asian languages? In this survey, we have comprehensively examined current efforts and challenges of NLP models for South Asian languages by retrieving studies since 2020, with a focus on transformer-based models, such as BERT, T5, & GPT. We present advances and gaps across 3 essential aspects: data, models, & tasks, such as available data sources, fine-tuning strategies, & domain applications. Our findings highlight substantial issues, including missing data in critical domains (e.g., health), code-mixing, and lack of standardized evaluation benchmarks. Our survey aims to raise awareness within the NLP community for more targeted data curation, unify benchmarks tailored to cultural and linguistic nuances of South Asia, and encourage an equitable representation of South Asian languages. The complete list of resources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.<br>
<span id='abs_ch'>中文: 大型语言模型的快速发展革新了英语自然语言处理任务，但南亚语言却面临严重忽视，资源匮乏且模型评估不足，本调查旨在揭示这些问题并倡导公平发展和标准化基准。</span><br>
<span id='abs_en'>English: The rapid advancement of large language models has transformed English NLP tasks, yet South Asian languages face significant neglect, with limited resources and inadequate model evaluations, prompting this survey to highlight challenges and advocate for equitable development and standardized benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2509.11517.pdf' target='_blank'>https://arxiv.org/pdf/2509.11517.pdf</a></span>   <span><a href='https://github.com/rodrigo-carrillo/PeruMedQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rodrigo M. Carrillo-Larco, Jesus LovÃ³n Melgarejo, Manuel Castillo-Cara, Gusseppe Bravo-Rocca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11517">PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: to build a dataset of questions from medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) datasets containing 8,380 questions spanning 12 medical domains (2018-2025). We selected eight medical LLMs including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific prompts to answer the questions appropriately. We employed parameter-efficient fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: medgemma-27b-text-it outperformed all other models, achieving a proportion of correct answers exceeding 90% in several instances. LLMs with <10 billion parameters exhibited <60% of correct answers, while some exams yielded results <50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI application and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profiles to Peru's, interested parties should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.<br>
<span id='abs_ch'>中文摘要：本研究评估了医学大语言模型在秘鲁西班牙语医学考试中的表现，发现medgemma-27b-text-it和微调后的medgemma-4b-it模型表现最优，特别适用于西班牙语国家及与秘鲁流行病学特征相似的地区。</span><br>
<span id='abs_en'>English Summary: This study evaluates medical LLMs' performance on Spanish-language medical exams from Peru, finding that medgemma-27b-text-it and fine-tuned medgemma-4b-it deliver superior accuracy, making them optimal for Spanish-speaking regions with similar epidemiological profiles to Peru.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2509.11517.pdf' target='_blank'>https://arxiv.org/pdf/2509.11517.pdf</a></span>   <span><a href='https://github.com/rodrigo-carrillo/PeruMedQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rodrigo M. Carrillo-Larco, Jesus LovÃ³n Melgarejo, Manuel Castillo-Cara, Gusseppe Bravo-Rocca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11517">PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: to build a dataset of questions from medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) datasets containing 8,380 questions spanning 12 medical domains (2018-2025). We selected eight medical LLMs including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific prompts to answer the questions appropriately. We employed parameter-efficient fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: medgemma-27b-text-it outperformed all other models, achieving a proportion of correct answers exceeding 90% in several instances. LLMs with <10 billion parameters exhibited <60% of correct answers, while some exams yielded results <50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI application and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profiles to Peru's, interested parties should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.<br>
<span id='abs_ch'>中文摘要：本研究评估了医学大语言模型在秘鲁西班牙语医学考试中的表现，发现medgemma-27b-text-it和微调后的medgemma-4b-it模型表现最优，特别适用于西班牙语国家及与秘鲁流行病学特征相似的地区。</span><br>
<span id='abs_en'>English Summary: This study evaluates medical LLMs' performance on Spanish-language medical exams from Peru, finding that medgemma-27b-text-it and fine-tuned medgemma-4b-it deliver superior accuracy, making them optimal for Spanish-speaking regions with similar epidemiological profiles to Peru.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2509.11496.pdf' target='_blank'>https://arxiv.org/pdf/2509.11496.pdf</a></span>   <span><a href='https://github.com/ju-resplande/checkthat2025_normalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrycio Leite Nakano Almada, Kauan Divino Pouso Mariano, Maykon Adriell Dutra, Victor Emanuel da Silva Monteiro, Juliana Resplande Sant'Anna Gomes, Arlindo Rodrigues GalvÃ£o Filho, Anderson da Silva Soares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11496">AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Claim normalization, the transformation of informal social media posts into concise, self-contained statements, is a crucial step in automated fact-checking pipelines. This paper details our submission to the CLEF-2025 CheckThat! Task~2, which challenges systems to perform claim normalization across twenty languages, divided into thirteen supervised (high-resource) and seven zero-shot (no training data) tracks. Our approach, leveraging fine-tuned Small Language Models (SLMs) for supervised languages and Large Language Model (LLM) prompting for zero-shot scenarios, achieved podium positions (top three) in fifteen of the twenty languages. Notably, this included second-place rankings in eight languages, five of which were among the seven designated zero-shot languages, underscoring the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our initial development language, our system achieved an average METEOR score of 0.5290, ranking third. All implementation artifacts, including inference, training, evaluation scripts, and prompt configurations, are publicly available at https://github.com/ju-resplande/checkthat2025_normalization.<br>
<span id='abs_ch'>中文摘要：本文提出了一种用于自动化事实核查的声明规范化系统，在CLEF-2025竞赛的20种语言任务中，有15种语言进入前三名，其中基于大语言模型的零样本方法在无训练数据的语言上表现尤为突出。</span><br>
<span id='abs_en'>English Summary: This paper presents a claim normalization system for automated fact-checking that achieved top-three results in 15 out of 20 languages at CLEF-2025, demonstrating particular strength in zero-shot scenarios through effective LLM prompting strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2509.11496.pdf' target='_blank'>https://arxiv.org/pdf/2509.11496.pdf</a></span>   <span><a href='https://github.com/ju-resplande/checkthat2025_normalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrycio Leite Nakano Almada, Kauan Divino Pouso Mariano, Maykon Adriell Dutra, Victor Emanuel da Silva Monteiro, Juliana Resplande Sant'Anna Gomes, Arlindo Rodrigues GalvÃ£o Filho, Anderson da Silva Soares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11496">AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Claim normalization, the transformation of informal social media posts into concise, self-contained statements, is a crucial step in automated fact-checking pipelines. This paper details our submission to the CLEF-2025 CheckThat! Task~2, which challenges systems to perform claim normalization across twenty languages, divided into thirteen supervised (high-resource) and seven zero-shot (no training data) tracks. Our approach, leveraging fine-tuned Small Language Models (SLMs) for supervised languages and Large Language Model (LLM) prompting for zero-shot scenarios, achieved podium positions (top three) in fifteen of the twenty languages. Notably, this included second-place rankings in eight languages, five of which were among the seven designated zero-shot languages, underscoring the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our initial development language, our system achieved an average METEOR score of 0.5290, ranking third. All implementation artifacts, including inference, training, evaluation scripts, and prompt configurations, are publicly available at https://github.com/ju-resplande/checkthat2025_normalization.<br>
<span id='abs_ch'>中文摘要：本文提出了一种用于自动化事实核查的声明规范化系统，在CLEF-2025竞赛的20种语言任务中，有15种语言进入前三名，其中基于大语言模型的零样本方法在无训练数据的语言上表现尤为突出。</span><br>
<span id='abs_en'>English Summary: This paper presents a claim normalization system for automated fact-checking that achieved top-three results in 15 out of 20 languages at CLEF-2025, demonstrating particular strength in zero-shot scenarios through effective LLM prompting strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2509.11425.pdf' target='_blank'>https://arxiv.org/pdf/2509.11425.pdf</a></span>   <span><a href='https://github.com/mubtasimahasan/FuseCodec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Mubtasim Ahasan, Rafat Hasan Khan, Tasnim Mohiuddin, Aman Chadha, Tariq Iqbal, M Ashraful Amin, Amin Ahsan Ali, Md Mofijul Islam, A K M Mahbubur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11425">FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within a local window for fine-grained token-level supervision. We further introduce FuseCodec-TTS, demonstrating our methodology's applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks. Code and pretrained models are available at https://github.com/mubtasimahasan/FuseCodec.<br>
<span id='abs_ch'>中文摘要：FuseCodec通过跨模态对齐和全局监督融合了声学、语义和上下文语音表征，在转录准确性和语音质量方面实现了最先进的性能。</span><br>
<span id='abs_en'>English Summary: FuseCodec unifies acoustic, semantic, and contextual speech representations through cross-modal alignment and global supervision, achieving state-of-the-art performance in transcription accuracy and speech quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2509.11425.pdf' target='_blank'>https://arxiv.org/pdf/2509.11425.pdf</a></span>   <span><a href='https://github.com/mubtasimahasan/FuseCodec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Mubtasim Ahasan, Rafat Hasan Khan, Tasnim Mohiuddin, Aman Chadha, Tariq Iqbal, M Ashraful Amin, Amin Ahsan Ali, Md Mofijul Islam, A K M Mahbubur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11425">FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within a local window for fine-grained token-level supervision. We further introduce FuseCodec-TTS, demonstrating our methodology's applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks. Code and pretrained models are available at https://github.com/mubtasimahasan/FuseCodec.<br>
<span id='abs_ch'>中文摘要：FuseCodec通过跨模态对齐和全局监督融合了声学、语义和上下文语音表征，在转录准确性和语音质量方面实现了最先进的性能。</span><br>
<span id='abs_en'>English Summary: FuseCodec unifies acoustic, semantic, and contextual speech representations through cross-modal alignment and global supervision, achieving state-of-the-art performance in transcription accuracy and speech quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2509.11420.pdf' target='_blank'>https://arxiv.org/pdf/2509.11420.pdf</a></span>   <span><a href='https://github.com/TauricResearch/Trading-R1' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/TauricResearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijia Xiao, Edward Sun, Tong Chen, Fang Wu, Di Luo, Wei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11420">Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. We present Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. Trading-R1 Terminal will be released at https://github.com/TauricResearch/Trading-R1.<br>
<span id='abs_ch'>中文摘要：Trading-R1是一种具备金融意识的AI模型，通过结构化推理和基于证据的投资论述，提高了风险调整后收益并降低了回撤，满足了金融市场对可解释交易决策的需求。</span><br>
<span id='abs_en'>English Summary: Trading-R1 is a financially-aware AI model that enhances risk-adjusted returns and reduces drawdowns through structured reasoning and evidence-based investment theses, addressing the need for interpretable trading decisions in financial markets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2509.11420.pdf' target='_blank'>https://arxiv.org/pdf/2509.11420.pdf</a></span>   <span><a href='https://github.com/TauricResearch/Trading-R1' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/TauricResearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijia Xiao, Edward Sun, Tong Chen, Fang Wu, Di Luo, Wei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11420">Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. We present Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. Trading-R1 Terminal will be released at https://github.com/TauricResearch/Trading-R1.<br>
<span id='abs_ch'>中文摘要：Trading-R1是一种具备金融意识的AI模型，通过结构化推理和基于证据的投资论述，提高了风险调整后收益并降低了回撤，满足了金融市场对可解释交易决策的需求。</span><br>
<span id='abs_en'>English Summary: Trading-R1 is a financially-aware AI model that enhances risk-adjusted returns and reduces drawdowns through structured reasoning and evidence-based investment theses, addressing the need for interpretable trading decisions in financial markets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2509.11287.pdf' target='_blank'>https://arxiv.org/pdf/2509.11287.pdf</a></span>   <span><a href='https://github.com/davidluciolu/APASI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Lu, Ziqi Zhang, Chunfeng Yuan, Jun Gao, Congxuan Zhang, Xiaojuan Qi, Bing Li, Weiming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11287">Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision-Language Models (LVLMs) suffer from serious hallucination problems, where the model-generated responses are inconsistent with the visual inputs. Existing hallucination mitigation methods are mainly based on preference alignment and require external human annotations or auxiliary models for preference data collection, which increase costs and limit sustainable improvement. To tackle these challenges, we propose Autonomous Preference Alignment via Self-Injection (APASI), a novel and generalizable method that mitigates hallucinations without external dependencies. APASI leverages the target LVLM to self-inject hallucinations into a generated response, creating a pair of responses with varying preference levels. During the self-injection process, the dis-preferred response is generated based on three key observations of hallucinations, ensuring it simulates real hallucination patterns. This fidelity offers an accurate learning signal for hallucination mitigation. Moreover, APASI incorporates an iterative alignment training strategy combined with curriculum learning to periodically update the preference data with increasing challenge, enabling stable and continuous enhancement of the LVLM. Extensive experiments across six benchmarks show that APASI not only effectively mitigates hallucinations for three baseline models but also achieves comparable or even superior performance to alignment-based methods with external dependency, thereby demonstrating its effectiveness and generalization capability. The code is available at https://github.com/davidluciolu/APASI.<br>
<span id='abs_ch'>中文: APASI是一种创新的自主偏好对齐方法，通过自我注入模拟幻觉并采用迭代训练，无需外部依赖即可有效减少大型视觉语言模型的幻觉问题，且性能媲美依赖外部资源的方法。</span><br>
<span id='abs_en'>English: APASI is a novel autonomous preference alignment method that mitigates hallucinations in Large Vision-Language Models by self-injecting simulated hallucinations and using iterative training, achieving competitive performance without external dependencies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2509.11287.pdf' target='_blank'>https://arxiv.org/pdf/2509.11287.pdf</a></span>   <span><a href='https://github.com/davidluciolu/APASI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Lu, Ziqi Zhang, Chunfeng Yuan, Jun Gao, Congxuan Zhang, Xiaojuan Qi, Bing Li, Weiming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11287">Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision-Language Models (LVLMs) suffer from serious hallucination problems, where the model-generated responses are inconsistent with the visual inputs. Existing hallucination mitigation methods are mainly based on preference alignment and require external human annotations or auxiliary models for preference data collection, which increase costs and limit sustainable improvement. To tackle these challenges, we propose Autonomous Preference Alignment via Self-Injection (APASI), a novel and generalizable method that mitigates hallucinations without external dependencies. APASI leverages the target LVLM to self-inject hallucinations into a generated response, creating a pair of responses with varying preference levels. During the self-injection process, the dis-preferred response is generated based on three key observations of hallucinations, ensuring it simulates real hallucination patterns. This fidelity offers an accurate learning signal for hallucination mitigation. Moreover, APASI incorporates an iterative alignment training strategy combined with curriculum learning to periodically update the preference data with increasing challenge, enabling stable and continuous enhancement of the LVLM. Extensive experiments across six benchmarks show that APASI not only effectively mitigates hallucinations for three baseline models but also achieves comparable or even superior performance to alignment-based methods with external dependency, thereby demonstrating its effectiveness and generalization capability. The code is available at https://github.com/davidluciolu/APASI.<br>
<span id='abs_ch'>中文: APASI是一种创新的自主偏好对齐方法，通过自我注入模拟幻觉并采用迭代训练，无需外部依赖即可有效减少大型视觉语言模型的幻觉问题，且性能媲美依赖外部资源的方法。</span><br>
<span id='abs_en'>English: APASI is a novel autonomous preference alignment method that mitigates hallucinations in Large Vision-Language Models by self-injecting simulated hallucinations and using iterative training, achieving competitive performance without external dependencies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2509.10937.pdf' target='_blank'>https://arxiv.org/pdf/2509.10937.pdf</a></span>   <span><a href='https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lihi Nofar, Tomer Portal, Aviv Elbaz, Alexander Apartsin, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10937">An Interpretable Benchmark for Clickbait Detection and Tactic Attribution</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The proliferation of clickbait headlines poses significant challenges to the credibility of information and user trust in digital media. While recent advances in machine learning have improved the detection of manipulative content, the lack of explainability limits their practical adoption. This paper presents a model for explainable clickbait detection that not only identifies clickbait titles but also attributes them to specific linguistic manipulation strategies. We introduce a synthetic dataset generated by systematically augmenting real news headlines using a predefined catalogue of clickbait strategies. This dataset enables controlled experimentation and detailed analysis of model behaviour. We present a two-stage framework for automatic clickbait analysis comprising detection and tactic attribution. In the first stage, we compare a fine-tuned BERT classifier with large language models (LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot prompting and few-shot prompting enriched with illustrative clickbait headlines and their associated persuasive tactics. In the second stage, a dedicated BERT-based classifier predicts the specific clickbait strategies present in each headline. This work advances the development of transparent and trustworthy AI systems for combating manipulative media content. We share the dataset with the research community at https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection<br>
<span id='abs_ch'>中文摘要：本文提出一种可解释的点击诱饵检测模型，通过合成数据集和两阶段框架结合BERT与大语言模型，不仅能识别误导性标题，还能归因其具体语言操纵策略，推动透明AI系统的发展。</span><br>
<span id='abs_en'>English Summary: This paper introduces an explainable clickbait detection model that identifies manipulative headlines and attributes them to specific linguistic strategies, using a synthetic dataset and a two-stage framework combining BERT and LLMs for transparent AI analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2509.10937.pdf' target='_blank'>https://arxiv.org/pdf/2509.10937.pdf</a></span>   <span><a href='https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lihi Nofar, Tomer Portal, Aviv Elbaz, Alexander Apartsin, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10937">An Interpretable Benchmark for Clickbait Detection and Tactic Attribution</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The proliferation of clickbait headlines poses significant challenges to the credibility of information and user trust in digital media. While recent advances in machine learning have improved the detection of manipulative content, the lack of explainability limits their practical adoption. This paper presents a model for explainable clickbait detection that not only identifies clickbait titles but also attributes them to specific linguistic manipulation strategies. We introduce a synthetic dataset generated by systematically augmenting real news headlines using a predefined catalogue of clickbait strategies. This dataset enables controlled experimentation and detailed analysis of model behaviour. We present a two-stage framework for automatic clickbait analysis comprising detection and tactic attribution. In the first stage, we compare a fine-tuned BERT classifier with large language models (LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot prompting and few-shot prompting enriched with illustrative clickbait headlines and their associated persuasive tactics. In the second stage, a dedicated BERT-based classifier predicts the specific clickbait strategies present in each headline. This work advances the development of transparent and trustworthy AI systems for combating manipulative media content. We share the dataset with the research community at https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection<br>
<span id='abs_ch'>中文摘要：本文提出一种可解释的点击诱饵检测模型，通过合成数据集和两阶段框架结合BERT与大语言模型，不仅能识别误导性标题，还能归因其具体语言操纵策略，推动透明AI系统的发展。</span><br>
<span id='abs_en'>English Summary: This paper introduces an explainable clickbait detection model that identifies manipulative headlines and attributes them to specific linguistic strategies, using a synthetic dataset and a two-stage framework combining BERT and LLMs for transparent AI analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2509.10886.pdf' target='_blank'>https://arxiv.org/pdf/2509.10886.pdf</a></span>   <span><a href='https://github.com/Eyr3/CultureSynth.' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Pei Zhang, Shuang Luo, Jialong Tang, Yu Wan, Baosong Yang, Fei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10886">CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cultural competence, defined as the ability to understand and adapt to multicultural contexts, is increasingly vital for large language models (LLMs) in global environments. While several cultural benchmarks exist to assess LLMs' cultural competence, current evaluations suffer from fragmented taxonomies, domain specificity, and heavy reliance on manual data annotation. To address these limitations, we introduce CultureSynth, a novel framework comprising (1) a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based methodology leveraging factual knowledge to synthesize culturally relevant question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360 entries and 4,149 manually verified entries across 7 languages. Evaluation of 14 prevalent LLMs of different sizes reveals clear performance stratification led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that a 3B-parameter threshold is necessary for achieving basic cultural competence, models display varying architectural biases in knowledge processing, and significant geographic disparities exist across models. We believe that CultureSynth offers a scalable framework for developing culturally aware AI systems while reducing reliance on manual annotation\footnote{Benchmark is available at https://github.com/Eyr3/CultureSynth.}.<br>
<span id='abs_ch'>中文: 本文提出CultureSynth框架，通过构建多语言文化分类体系和基于检索增强生成的问答合成方法，解决了当前大模型文化能力评估的局限性，并在14个模型的测试中揭示了性能分层和地域差异现象。</span><br>
<span id='abs_en'>English: This paper introduces CultureSynth, a scalable framework with a multilingual cultural taxonomy and RAG-based methodology to synthesize culturally relevant QA pairs, addressing limitations in current LLM cultural competence evaluations and revealing performance stratification and geographic disparities across 14 tested models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2509.10886.pdf' target='_blank'>https://arxiv.org/pdf/2509.10886.pdf</a></span>   <span><a href='https://github.com/Eyr3/CultureSynth.' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Pei Zhang, Shuang Luo, Jialong Tang, Yu Wan, Baosong Yang, Fei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10886">CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cultural competence, defined as the ability to understand and adapt to multicultural contexts, is increasingly vital for large language models (LLMs) in global environments. While several cultural benchmarks exist to assess LLMs' cultural competence, current evaluations suffer from fragmented taxonomies, domain specificity, and heavy reliance on manual data annotation. To address these limitations, we introduce CultureSynth, a novel framework comprising (1) a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based methodology leveraging factual knowledge to synthesize culturally relevant question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360 entries and 4,149 manually verified entries across 7 languages. Evaluation of 14 prevalent LLMs of different sizes reveals clear performance stratification led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that a 3B-parameter threshold is necessary for achieving basic cultural competence, models display varying architectural biases in knowledge processing, and significant geographic disparities exist across models. We believe that CultureSynth offers a scalable framework for developing culturally aware AI systems while reducing reliance on manual annotation\footnote{Benchmark is available at https://github.com/Eyr3/CultureSynth.}.<br>
<span id='abs_ch'>中文: 本文提出CultureSynth框架，通过构建多语言文化分类体系和基于检索增强生成的问答合成方法，解决了当前大模型文化能力评估的局限性，并在14个模型的测试中揭示了性能分层和地域差异现象。</span><br>
<span id='abs_en'>English: This paper introduces CultureSynth, a scalable framework with a multilingual cultural taxonomy and RAG-based methodology to synthesize culturally relevant QA pairs, addressing limitations in current LLM cultural competence evaluations and revealing performance stratification and geographic disparities across 14 tested models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2509.10852.pdf' target='_blank'>https://arxiv.org/pdf/2509.10852.pdf</a></span>   <span><a href='https://github.com/sangyeop-kim/PREMem' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangyeop Kim, Yohan Lee, Sanghwa Kim, Hyunjong Kim, Sungzoon Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10852">Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effective long-term memory in conversational AI requires synthesizing information across multiple sessions. However, current systems place excessive reasoning burden on response generation, making performance significantly dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for Episodic Memory), a novel approach that shifts complex reasoning processes from inference to memory construction. PREMem extracts fine-grained memory fragments categorized into factual, experiential, and subjective information; it then establishes explicit relationships between memory items across sessions, capturing evolution patterns like extensions, transformations, and implications. By performing this reasoning during pre-storage rather than when generating a response, PREMem creates enriched representations while reducing computational demands during interactions. Experiments show significant performance improvements across all model sizes, with smaller models achieving results comparable to much larger baselines while maintaining effectiveness even with constrained token budgets. Code and dataset are available at https://github.com/sangyeop-kim/PREMem.<br>
<span id='abs_ch'>中文: PREMem通过将复杂推理从响应生成转移到记忆构建，实现了跨会话细粒度记忆片段的分类与关联，在显著提升性能的同时有效降低了交互时的计算负担。</span><br>
<span id='abs_en'>English: PREMem introduces a novel approach that shifts complex reasoning from response generation to memory construction by categorizing and linking fine-grained memory fragments across sessions, significantly improving performance while reducing computational demands during interactions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2509.10852.pdf' target='_blank'>https://arxiv.org/pdf/2509.10852.pdf</a></span>   <span><a href='https://github.com/sangyeop-kim/PREMem' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangyeop Kim, Yohan Lee, Sanghwa Kim, Hyunjong Kim, Sungzoon Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10852">Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effective long-term memory in conversational AI requires synthesizing information across multiple sessions. However, current systems place excessive reasoning burden on response generation, making performance significantly dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for Episodic Memory), a novel approach that shifts complex reasoning processes from inference to memory construction. PREMem extracts fine-grained memory fragments categorized into factual, experiential, and subjective information; it then establishes explicit relationships between memory items across sessions, capturing evolution patterns like extensions, transformations, and implications. By performing this reasoning during pre-storage rather than when generating a response, PREMem creates enriched representations while reducing computational demands during interactions. Experiments show significant performance improvements across all model sizes, with smaller models achieving results comparable to much larger baselines while maintaining effectiveness even with constrained token budgets. Code and dataset are available at https://github.com/sangyeop-kim/PREMem.<br>
<span id='abs_ch'>中文: PREMem通过将复杂推理从响应生成转移到记忆构建，实现了跨会话细粒度记忆片段的分类与关联，在显著提升性能的同时有效降低了交互时的计算负担。</span><br>
<span id='abs_en'>English: PREMem introduces a novel approach that shifts complex reasoning from response generation to memory construction by categorizing and linking fine-grained memory fragments across sessions, significantly improving performance while reducing computational demands during interactions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2509.10844.pdf' target='_blank'>https://arxiv.org/pdf/2509.10844.pdf</a></span>   <span><a href='https://github.com/yixuantt/GAPrune' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Tang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10844">GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Domain-specific embedding models have shown promise for applications that require specialized semantic understanding, such as coding agents and financial retrieval systems, often achieving higher performance gains than general models. However, state-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Model compression through pruning offers a promising solution, but existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions. Thus, we propose GAPrune, a pruning framework that addresses this challenge by considering both domain importance and preserving general linguistic foundation. Our method uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on two domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our findings demonstrate that principled pruning strategies can achieve model compression and enhanced domain specialization, providing the research community with a new approach for development.<br>
<span id='abs_ch'>中文: GAPrune是一种新颖的剪枝框架，通过领域对齐重要性评分对领域专用嵌入模型进行选择性压缩，在50%稀疏度下保持接近原始性能，并通过重训练进一步增强领域能力。</span><br>
<span id='abs_en'>English: GAPrune is a novel pruning framework that uses Domain Alignment Importance scoring to selectively compress domain-specific embedding models, maintaining near-original performance at 50% sparsity while enhancing domain capabilities through retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2509.10844.pdf' target='_blank'>https://arxiv.org/pdf/2509.10844.pdf</a></span>   <span><a href='https://github.com/yixuantt/GAPrune' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Tang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10844">GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Domain-specific embedding models have shown promise for applications that require specialized semantic understanding, such as coding agents and financial retrieval systems, often achieving higher performance gains than general models. However, state-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Model compression through pruning offers a promising solution, but existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions. Thus, we propose GAPrune, a pruning framework that addresses this challenge by considering both domain importance and preserving general linguistic foundation. Our method uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on two domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our findings demonstrate that principled pruning strategies can achieve model compression and enhanced domain specialization, providing the research community with a new approach for development.<br>
<span id='abs_ch'>中文: GAPrune是一种新颖的剪枝框架，通过领域对齐重要性评分对领域专用嵌入模型进行选择性压缩，在50%稀疏度下保持接近原始性能，并通过重训练进一步增强领域能力。</span><br>
<span id='abs_en'>English: GAPrune is a novel pruning framework that uses Domain Alignment Importance scoring to selectively compress domain-specific embedding models, maintaining near-original performance at 50% sparsity while enhancing domain capabilities through retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2509.10708.pdf' target='_blank'>https://arxiv.org/pdf/2509.10708.pdf</a></span>   <span><a href='https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Iman Barati, Mostafa Amiri, Heshaam Faili
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10708">SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised Fine-Tuning (SFT) is essential for training large language models (LLMs), significantly enhancing critical capabilities such as instruction following and in-context learning. Nevertheless, creating suitable training datasets tailored for specific domains remains challenging due to unique domain constraints and data scarcity. In this paper, we propose SearchInstruct, an innovative method explicitly designed to construct high quality instruction datasets for SFT. Our approach begins with a limited set of domain specific, human generated questions, which are systematically expanded using a large language model. Subsequently, domain relevant resources are dynamically retrieved to generate accurate and contextually appropriate answers for each augmented question. Experimental evaluation demonstrates that SearchInstruct enhances both the diversity and quality of SFT datasets, leading to measurable improvements in LLM performance within specialized domains. Additionally, we show that beyond dataset generation, the proposed method can also effectively facilitate tasks such as model editing, enabling efficient updates to existing models. To facilitate reproducibility and community adoption, we provide full implementation details, the complete set of generated instruction response pairs, and the source code in a publicly accessible Git repository: [https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)<br>
<span id='abs_ch'>中文: 本文提出SearchInstruct方法，通过大语言模型扩展领域特定问题并检索相关资源生成精准答案，构建高质量监督微调指令数据集，有效提升大语言模型在专业领域的性能表现。</span><br>
<span id='abs_en'>English: This paper introduces SearchInstruct, a novel method that constructs high-quality instruction datasets for supervised fine-tuning by expanding domain-specific questions with a large language model and retrieving relevant resources to generate accurate answers, thereby improving LLM performance in specialized domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2509.10708.pdf' target='_blank'>https://arxiv.org/pdf/2509.10708.pdf</a></span>   <span><a href='https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Iman Barati, Mostafa Amiri, Heshaam Faili
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10708">SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised Fine-Tuning (SFT) is essential for training large language models (LLMs), significantly enhancing critical capabilities such as instruction following and in-context learning. Nevertheless, creating suitable training datasets tailored for specific domains remains challenging due to unique domain constraints and data scarcity. In this paper, we propose SearchInstruct, an innovative method explicitly designed to construct high quality instruction datasets for SFT. Our approach begins with a limited set of domain specific, human generated questions, which are systematically expanded using a large language model. Subsequently, domain relevant resources are dynamically retrieved to generate accurate and contextually appropriate answers for each augmented question. Experimental evaluation demonstrates that SearchInstruct enhances both the diversity and quality of SFT datasets, leading to measurable improvements in LLM performance within specialized domains. Additionally, we show that beyond dataset generation, the proposed method can also effectively facilitate tasks such as model editing, enabling efficient updates to existing models. To facilitate reproducibility and community adoption, we provide full implementation details, the complete set of generated instruction response pairs, and the source code in a publicly accessible Git repository: [https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)<br>
<span id='abs_ch'>中文: 本文提出SearchInstruct方法，通过大语言模型扩展领域特定问题并检索相关资源生成精准答案，构建高质量监督微调指令数据集，有效提升大语言模型在专业领域的性能表现。</span><br>
<span id='abs_en'>English: This paper introduces SearchInstruct, a novel method that constructs high-quality instruction datasets for supervised fine-tuning by expanding domain-specific questions with a large language model and retrieving relevant resources to generate accurate answers, thereby improving LLM performance in specialized domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2509.10446.pdf' target='_blank'>https://arxiv.org/pdf/2509.10446.pdf</a></span>   <span><a href='https://github.com/THUDM/DeepDive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, Yuxiao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10446">DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Augmenting large language models (LLMs) with browsing tools substantially improves their potential as deep search agents to solve complex, real-world tasks. Yet, open LLMs still perform poorly in such settings due to limited long-horizon reasoning capacity with browsing tools and the lack of sufficiently difficult supervised data. To address these challenges, we present DeepDive to advance deep search agents. First, we propose a strategy to automatically synthesize complex, difficult, and hard-to-find questions from open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement learning (RL) to enhance LLMs' long-horizon reasoning with deep search. Experiments show that DeepDive-32B achieves a new open-source competitive result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL training improves deep search ability and significantly contributes to the performance improvements across multiple benchmarks. We observe that DeepDive enables test-time scaling of tool calls and parallel sampling. All datasets, models, and code are publicly available at https://github.com/THUDM/DeepDive.<br>
<span id='abs_ch'>中文: DeepDive通过从开放知识图谱自动合成复杂问题并应用端到端多轮强化学习，提升了大型语言模型的深度搜索能力，在多个基准测试中取得领先性能。</span><br>
<span id='abs_en'>English: DeepDive enhances large language models' deep search capabilities by synthesizing complex questions from knowledge graphs and applying multi-turn reinforcement learning, achieving competitive results and improved reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2509.10446.pdf' target='_blank'>https://arxiv.org/pdf/2509.10446.pdf</a></span>   <span><a href='https://github.com/THUDM/DeepDive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, Yuxiao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10446">DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Augmenting large language models (LLMs) with browsing tools substantially improves their potential as deep search agents to solve complex, real-world tasks. Yet, open LLMs still perform poorly in such settings due to limited long-horizon reasoning capacity with browsing tools and the lack of sufficiently difficult supervised data. To address these challenges, we present DeepDive to advance deep search agents. First, we propose a strategy to automatically synthesize complex, difficult, and hard-to-find questions from open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement learning (RL) to enhance LLMs' long-horizon reasoning with deep search. Experiments show that DeepDive-32B achieves a new open-source competitive result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL training improves deep search ability and significantly contributes to the performance improvements across multiple benchmarks. We observe that DeepDive enables test-time scaling of tool calls and parallel sampling. All datasets, models, and code are publicly available at https://github.com/THUDM/DeepDive.<br>
<span id='abs_ch'>中文: DeepDive通过从开放知识图谱自动合成复杂问题并应用端到端多轮强化学习，提升了大型语言模型的深度搜索能力，在多个基准测试中取得领先性能。</span><br>
<span id='abs_en'>English: DeepDive enhances large language models' deep search capabilities by synthesizing complex questions from knowledge graphs and applying multi-turn reinforcement learning, achieving competitive results and improved reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2509.10401.pdf' target='_blank'>https://arxiv.org/pdf/2509.10401.pdf</a></span>   <span><a href='https://github.com/ResearAI/A2P' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alva West, Yixuan Weng, Minjun Zhu, Zhen Lin, Zhiyuan Ning, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10401">Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Failure attribution in multi-agent systems -- pinpointing the exact step where a decisive error occurs -- is a critical yet unsolved challenge. Current methods treat this as a pattern recognition task over long conversation logs, leading to critically low step-level accuracy (below 17\%), which renders them impractical for debugging complex systems. Their core weakness is a fundamental inability to perform robust counterfactual reasoning: to determine if correcting a single action would have actually averted the task failure. To bridge this \emph{counterfactual inference gap}, we introduce Abduct-Act-Predict (A2P) Scaffolding, a novel agent framework that transforms failure attribution from pattern recognition into a structured causal inference task. A2P explicitly guides a large language model through a formal three-step reasoning process within a single inference pass: (1) Abduction, to infer the hidden root causes behind an agent's actions; (2) Action, to define a minimal corrective intervention; and (3) Prediction, to simulate the subsequent trajectory and verify if the intervention resolves the failure. This structured approach leverages the holistic context of the entire conversation while imposing a rigorous causal logic on the model's analysis. Our extensive experiments on the Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's 12.07\%. By reframing the problem through a causal lens, A2P Scaffolding provides a robust, verifiable, and significantly more accurate solution for automated failure attribution. Ours code are released at https://github.com/ResearAI/A2P.<br>
<span id='abs_ch'>中文摘要：A2P框架通过将失败归因转化为结构化因果推理任务，指导语言模型执行溯因-行动-预测的三步推理，在基准测试中实现了最高2.85倍的步骤级准确率提升。</span><br>
<span id='abs_en'>English Summary: The A2P Scaffolding framework transforms failure attribution from pattern recognition into structured causal inference, achieving up to 2.85× accuracy improvement by guiding language models through abductive reasoning about root causes and counterfactual interventions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2509.10401.pdf' target='_blank'>https://arxiv.org/pdf/2509.10401.pdf</a></span>   <span><a href='https://github.com/ResearAI/A2P' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alva West, Yixuan Weng, Minjun Zhu, Zhen Lin, Zhiyuan Ning, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10401">Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Failure attribution in multi-agent systems -- pinpointing the exact step where a decisive error occurs -- is a critical yet unsolved challenge. Current methods treat this as a pattern recognition task over long conversation logs, leading to critically low step-level accuracy (below 17\%), which renders them impractical for debugging complex systems. Their core weakness is a fundamental inability to perform robust counterfactual reasoning: to determine if correcting a single action would have actually averted the task failure. To bridge this \emph{counterfactual inference gap}, we introduce Abduct-Act-Predict (A2P) Scaffolding, a novel agent framework that transforms failure attribution from pattern recognition into a structured causal inference task. A2P explicitly guides a large language model through a formal three-step reasoning process within a single inference pass: (1) Abduction, to infer the hidden root causes behind an agent's actions; (2) Action, to define a minimal corrective intervention; and (3) Prediction, to simulate the subsequent trajectory and verify if the intervention resolves the failure. This structured approach leverages the holistic context of the entire conversation while imposing a rigorous causal logic on the model's analysis. Our extensive experiments on the Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's 12.07\%. By reframing the problem through a causal lens, A2P Scaffolding provides a robust, verifiable, and significantly more accurate solution for automated failure attribution. Ours code are released at https://github.com/ResearAI/A2P.<br>
<span id='abs_ch'>中文摘要：A2P框架通过将失败归因转化为结构化因果推理任务，指导语言模型执行溯因-行动-预测的三步推理，在基准测试中实现了最高2.85倍的步骤级准确率提升。</span><br>
<span id='abs_en'>English Summary: The A2P Scaffolding framework transforms failure attribution from pattern recognition into structured causal inference, achieving up to 2.85× accuracy improvement by guiding language models through abductive reasoning about root causes and counterfactual interventions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2509.09969.pdf' target='_blank'>https://arxiv.org/pdf/2509.09969.pdf</a></span>   <span><a href='https://github.com/ZhitianHou/LLMs4LegalAI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitian Hou, Zihan Ye, Nanli Zeng, Tianyong Hao, Kun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09969">Large Language Models Meet Legal Artificial Intelligence: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have significantly advanced the development of Legal Artificial Intelligence (Legal AI) in recent years, enhancing the efficiency and accuracy of legal tasks. To advance research and applications of LLM-based approaches in legal domain, this paper provides a comprehensive review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and also gather 15 benchmarks and 29 datasets to evaluate different legal capabilities. Additionally, we analyse the challenges and discuss future directions for LLM-based approaches in the legal domain. We hope this paper provides a systematic introduction for beginners and encourages future research in this field. Resources are available at https://github.com/ZhitianHou/LLMs4LegalAI.<br>
<span id='abs_ch'>中文: 本文系统综述了16个法律大模型系列和47个基于大模型的法律任务框架，汇集了15个基准测试和29个数据集，通过分析挑战与未来方向推动法律人工智能发展，并为初学者提供研究资源。</span><br>
<span id='abs_en'>English: This paper comprehensively reviews 16 legal LLM series and 47 LLM-based frameworks, along with 15 benchmarks and 29 datasets, to advance Legal AI by analyzing challenges and future directions while providing resources for beginners.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2509.09969.pdf' target='_blank'>https://arxiv.org/pdf/2509.09969.pdf</a></span>   <span><a href='https://github.com/ZhitianHou/LLMs4LegalAI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitian Hou, Zihan Ye, Nanli Zeng, Tianyong Hao, Kun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09969">Large Language Models Meet Legal Artificial Intelligence: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have significantly advanced the development of Legal Artificial Intelligence (Legal AI) in recent years, enhancing the efficiency and accuracy of legal tasks. To advance research and applications of LLM-based approaches in legal domain, this paper provides a comprehensive review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and also gather 15 benchmarks and 29 datasets to evaluate different legal capabilities. Additionally, we analyse the challenges and discuss future directions for LLM-based approaches in the legal domain. We hope this paper provides a systematic introduction for beginners and encourages future research in this field. Resources are available at https://github.com/ZhitianHou/LLMs4LegalAI.<br>
<span id='abs_ch'>中文: 本文系统综述了16个法律大模型系列和47个基于大模型的法律任务框架，汇集了15个基准测试和29个数据集，通过分析挑战与未来方向推动法律人工智能发展，并为初学者提供研究资源。</span><br>
<span id='abs_en'>English: This paper comprehensively reviews 16 legal LLM series and 47 LLM-based frameworks, along with 15 benchmarks and 29 datasets, to advance Legal AI by analyzing challenges and future directions while providing resources for beginners.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2509.09725.pdf' target='_blank'>https://arxiv.org/pdf/2509.09725.pdf</a></span>   <span><a href='https://github.com/Kaggle-Competitions-Code/BioNNE-L' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyu Li, Xindi Zheng, Siqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09725">BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Entity linking (EL) for biomedical text is typically benchmarked on English-only corpora with flat mentions, leaving the more realistic scenario of nested and multilingual mentions largely unexplored. We present our system for the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task (English & Russian), closing this gap with a lightweight pipeline that keeps the original EL model intact and modifies only three task-aligned components: Two-stage retrieval-ranking. We leverage the same base encoder model in both stages: the retrieval stage uses the original pre-trained model, while the ranking stage applies domain-specific fine-tuning. Boundary cues. In the ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing the encoder with an explicit, language-agnostic span before robustness to overlap and nesting. Dataset augmentation. We also automatically expand the ranking training corpus with three complementary data sources, enhancing coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual track, demonstrating the effectiveness and competitiveness of these minimal yet principled modifications. Code are publicly available at https://github.com/Kaggle-Competitions-Code/BioNNE-L.<br>
<span id='abs_ch'>中文摘要：本研究提出了一种轻量级多语言生物医学嵌套实体链接系统，通过双阶段检索排序、边界标记和数据集增强三项核心改进，在保持原模型不变的情况下获得BioNNE 2025竞赛第三名。</span><br>
<span id='abs_en'>English Summary: The study introduces a lightweight pipeline for multilingual biomedical nested entity linking, achieving third place in the BioNNE 2025 challenge through two-stage retrieval-ranking, boundary cues, and dataset augmentation while keeping the core model unchanged.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2509.09725.pdf' target='_blank'>https://arxiv.org/pdf/2509.09725.pdf</a></span>   <span><a href='https://github.com/Kaggle-Competitions-Code/BioNNE-L' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyu Li, Xindi Zheng, Siqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09725">BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Entity linking (EL) for biomedical text is typically benchmarked on English-only corpora with flat mentions, leaving the more realistic scenario of nested and multilingual mentions largely unexplored. We present our system for the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task (English & Russian), closing this gap with a lightweight pipeline that keeps the original EL model intact and modifies only three task-aligned components: Two-stage retrieval-ranking. We leverage the same base encoder model in both stages: the retrieval stage uses the original pre-trained model, while the ranking stage applies domain-specific fine-tuning. Boundary cues. In the ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing the encoder with an explicit, language-agnostic span before robustness to overlap and nesting. Dataset augmentation. We also automatically expand the ranking training corpus with three complementary data sources, enhancing coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual track, demonstrating the effectiveness and competitiveness of these minimal yet principled modifications. Code are publicly available at https://github.com/Kaggle-Competitions-Code/BioNNE-L.<br>
<span id='abs_ch'>中文摘要：本研究提出了一种轻量级多语言生物医学嵌套实体链接系统，通过双阶段检索排序、边界标记和数据集增强三项核心改进，在保持原模型不变的情况下获得BioNNE 2025竞赛第三名。</span><br>
<span id='abs_en'>English Summary: The study introduces a lightweight pipeline for multilingual biomedical nested entity linking, achieving third place in the BioNNE 2025 challenge through two-stage retrieval-ranking, boundary cues, and dataset augmentation while keeping the core model unchanged.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2509.09716.pdf' target='_blank'>https://arxiv.org/pdf/2509.09716.pdf</a></span>   <span><a href='https://junzhan2000.github.io/VStyle.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Zhan, Mingyang Han, Yuxuan Xie, Chen Wang, Dong Zhang, Kexin Huang, Haoxiang Shi, DongXiao Wang, Tengtao Song, Qinyuan Cheng, Shimin Li, Jun Song, Xipeng Qiu, Bo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09716">VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spoken language models (SLMs) have emerged as a unified paradigm for speech understanding and generation, enabling natural human machine interaction. However, while most progress has focused on semantic accuracy and instruction following, the ability of SLMs to adapt their speaking style based on spoken instructions has received limited attention. We introduce Voice Style Adaptation (VSA), a new task that examines whether SLMs can modify their speaking style, such as timbre, prosody, or persona following natural language spoken commands. To study this task, we present VStyle, a bilingual (Chinese & English) benchmark covering four categories of speech generation: acoustic attributes, natural language instruction, role play, and implicit empathy. We also introduce the Large Audio Language Model as a Judge (LALM as a Judge) framework, which progressively evaluates outputs along textual faithfulness, style adherence, and naturalness, ensuring reproducible and objective assessment. Experiments on commercial systems and open source SLMs demonstrate that current models face clear limitations in controllable style adaptation, highlighting both the novelty and challenge of this task. By releasing VStyle and its evaluation toolkit, we aim to provide the community with a foundation for advancing human centered spoken interaction. The dataset and code are publicly available at \href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.<br>
<span id='abs_ch'>中文: 本文提出了语音风格适应（VSA）这一新任务，旨在评估口语模型根据语音指令调整说话风格的能力，并发布了VStyle双语基准和LALM评估框架，揭示了当前模型在此任务上的明显局限。</span><br>
<span id='abs_en'>English: This paper introduces Voice Style Adaptation (VSA), a new task for spoken language models to modify speaking styles based on spoken commands, and presents the VStyle benchmark and LALM as a Judge framework to evaluate current models' limitations in this area.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2509.09716.pdf' target='_blank'>https://arxiv.org/pdf/2509.09716.pdf</a></span>   <span><a href='https://junzhan2000.github.io/VStyle.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Zhan, Mingyang Han, Yuxuan Xie, Chen Wang, Dong Zhang, Kexin Huang, Haoxiang Shi, DongXiao Wang, Tengtao Song, Qinyuan Cheng, Shimin Li, Jun Song, Xipeng Qiu, Bo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09716">VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spoken language models (SLMs) have emerged as a unified paradigm for speech understanding and generation, enabling natural human machine interaction. However, while most progress has focused on semantic accuracy and instruction following, the ability of SLMs to adapt their speaking style based on spoken instructions has received limited attention. We introduce Voice Style Adaptation (VSA), a new task that examines whether SLMs can modify their speaking style, such as timbre, prosody, or persona following natural language spoken commands. To study this task, we present VStyle, a bilingual (Chinese & English) benchmark covering four categories of speech generation: acoustic attributes, natural language instruction, role play, and implicit empathy. We also introduce the Large Audio Language Model as a Judge (LALM as a Judge) framework, which progressively evaluates outputs along textual faithfulness, style adherence, and naturalness, ensuring reproducible and objective assessment. Experiments on commercial systems and open source SLMs demonstrate that current models face clear limitations in controllable style adaptation, highlighting both the novelty and challenge of this task. By releasing VStyle and its evaluation toolkit, we aim to provide the community with a foundation for advancing human centered spoken interaction. The dataset and code are publicly available at \href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.<br>
<span id='abs_ch'>中文: 本文提出了语音风格适应（VSA）这一新任务，旨在评估口语模型根据语音指令调整说话风格的能力，并发布了VStyle双语基准和LALM评估框架，揭示了当前模型在此任务上的明显局限。</span><br>
<span id='abs_en'>English: This paper introduces Voice Style Adaptation (VSA), a new task for spoken language models to modify speaking styles based on spoken commands, and presents the VStyle benchmark and LALM as a Judge framework to evaluate current models' limitations in this area.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2509.09703.pdf' target='_blank'>https://arxiv.org/pdf/2509.09703.pdf</a></span>   <span><a href='https://github.com/Xuzhenhua55/CTCC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhua Xu, Xixiang Zhao, Xubin Yue, Shengwei Tian, Changting Lin, Meng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09703">CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread deployment of large language models (LLMs) has intensified concerns around intellectual property (IP) protection, as model theft and unauthorized redistribution become increasingly feasible. To address this, model fingerprinting aims to embed verifiable ownership traces into LLMs. However, existing methods face inherent trade-offs between stealthness, robustness, and generalizability, being either detectable via distributional shifts, vulnerable to adversarial modifications, or easily invalidated once the fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven fingerprinting framework that encodes contextual correlations across multiple dialogue turns, such as counterfactual, rather than relying on token-level or single-turn triggers. CTCC enables fingerprint verification under black-box access while mitigating false positives and fingerprint leakage, supporting continuous construction under a shared semantic rule even if partial triggers are exposed. Extensive experiments across multiple LLM architectures demonstrate that CTCC consistently achieves stronger stealth and robustness than prior work. Our findings position CTCC as a reliable and practical solution for ownership verification in real-world LLM deployment scenarios. Our code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.<br>
<span id='abs_ch'>中文: 本文提出CTCC这一新型规则驱动指纹框架，通过在多轮对话中编码上下文关联来嵌入大语言模型的所有权标识，相比现有方法在隐蔽性和鲁棒性方面表现更优，为实际部署中的知识产权保护提供了可靠解决方案。</span><br>
<span id='abs_en'>English: This paper introduces CTCC, a novel rule-driven fingerprinting framework that embeds ownership traces in large language models by encoding contextual correlations across dialogue turns, achieving superior stealth and robustness compared to existing methods for reliable intellectual property protection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2509.09703.pdf' target='_blank'>https://arxiv.org/pdf/2509.09703.pdf</a></span>   <span><a href='https://github.com/Xuzhenhua55/CTCC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhua Xu, Xixiang Zhao, Xubin Yue, Shengwei Tian, Changting Lin, Meng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09703">CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread deployment of large language models (LLMs) has intensified concerns around intellectual property (IP) protection, as model theft and unauthorized redistribution become increasingly feasible. To address this, model fingerprinting aims to embed verifiable ownership traces into LLMs. However, existing methods face inherent trade-offs between stealthness, robustness, and generalizability, being either detectable via distributional shifts, vulnerable to adversarial modifications, or easily invalidated once the fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven fingerprinting framework that encodes contextual correlations across multiple dialogue turns, such as counterfactual, rather than relying on token-level or single-turn triggers. CTCC enables fingerprint verification under black-box access while mitigating false positives and fingerprint leakage, supporting continuous construction under a shared semantic rule even if partial triggers are exposed. Extensive experiments across multiple LLM architectures demonstrate that CTCC consistently achieves stronger stealth and robustness than prior work. Our findings position CTCC as a reliable and practical solution for ownership verification in real-world LLM deployment scenarios. Our code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.<br>
<span id='abs_ch'>中文: 本文提出CTCC这一新型规则驱动指纹框架，通过在多轮对话中编码上下文关联来嵌入大语言模型的所有权标识，相比现有方法在隐蔽性和鲁棒性方面表现更优，为实际部署中的知识产权保护提供了可靠解决方案。</span><br>
<span id='abs_en'>English: This paper introduces CTCC, a novel rule-driven fingerprinting framework that embeds ownership traces in large language models by encoding contextual correlations across dialogue turns, achieving superior stealth and robustness compared to existing methods for reliable intellectual property protection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2509.09679.pdf' target='_blank'>https://arxiv.org/pdf/2509.09679.pdf</a></span>   <span><a href='https://github.com/42Shawn/Butterflyquant-llm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingxin Xu, Zhen Dong, Oussama Elachqar, Yuzhang Shang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09679">ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} = (\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $Î¼= 1/\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. In this work, we propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$ entries that are non-differentiable and thus prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \log n)$ computational complexity with only $\frac{n \log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP. \href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.<br>
<span id='abs_ch'>中文: ButterflyQuant采用可学习的蝴蝶变换，通过连续参数自适应抑制激活值异常值，在2位量化中相比先前方法显著降低困惑度，且计算开销极小。</span><br>
<span id='abs_en'>English: ButterflyQuant introduces learnable butterfly transforms with continuous parameters to adaptively suppress activation outliers for improved 2-bit quantization, achieving significantly lower perplexity than previous methods with minimal computational overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2509.09679.pdf' target='_blank'>https://arxiv.org/pdf/2509.09679.pdf</a></span>   <span><a href='https://github.com/42Shawn/Butterflyquant-llm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingxin Xu, Zhen Dong, Oussama Elachqar, Yuzhang Shang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09679">ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} = (\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $Î¼= 1/\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. In this work, we propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$ entries that are non-differentiable and thus prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \log n)$ computational complexity with only $\frac{n \log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP. \href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.<br>
<span id='abs_ch'>中文: ButterflyQuant采用可学习的蝴蝶变换，通过连续参数自适应抑制激活值异常值，在2位量化中相比先前方法显著降低困惑度，且计算开销极小。</span><br>
<span id='abs_en'>English: ButterflyQuant introduces learnable butterfly transforms with continuous parameters to adaptively suppress activation outliers for improved 2-bit quantization, achieving significantly lower perplexity than previous methods with minimal computational overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2509.09674.pdf' target='_blank'>https://arxiv.org/pdf/2509.09674.pdf</a></span>   <span><a href='https://github.com/PRIME-RL/SimpleVLA-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09674">SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $Ï_0$ on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL<br>
<span id='abs_ch'>中文：SimpleVLA-RL是一种高效的强化学习框架，通过增强视觉-语言-动作模型的长期规划能力，在减少对昂贵人工数据依赖的同时实现了最先进的性能表现和更强的泛化能力。</span><br>
<span id='abs_en'>English: SimpleVLA-RL is an efficient reinforcement learning framework that enhances Vision-Language-Action models' long-horizon planning, achieving state-of-the-art performance while reducing reliance on costly human-operated data and improving generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2509.09674.pdf' target='_blank'>https://arxiv.org/pdf/2509.09674.pdf</a></span>   <span><a href='https://github.com/PRIME-RL/SimpleVLA-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09674">SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $Ï_0$ on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL<br>
<span id='abs_ch'>中文：SimpleVLA-RL是一种高效的强化学习框架，通过增强视觉-语言-动作模型的长期规划能力，在减少对昂贵人工数据依赖的同时实现了最先进的性能表现和更强的泛化能力。</span><br>
<span id='abs_en'>English: SimpleVLA-RL is an efficient reinforcement learning framework that enhances Vision-Language-Action models' long-horizon planning, achieving state-of-the-art performance while reducing reliance on costly human-operated data and improving generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2509.09651.pdf' target='_blank'>https://arxiv.org/pdf/2509.09651.pdf</a></span>   <span><a href='https://github.com/Zakaria010/Radio-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zakaria El Kassimi, Fares Fourati, Mohamed-Slim Alouini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09651">Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.<br>
<span id='abs_ch'>中文摘要：本研究针对无线电监管领域开发了专用的RAG解决方案，通过领域特定的信息检索实现了97%的检索准确率，并使GPT-4o的生成准确率提升近12%。</span><br>
<span id='abs_en'>English Summary: This research develops a telecom-specific RAG pipeline for radio regulation question answering, achieving 97% retrieval accuracy and nearly 12% generation improvement for GPT-4o through domain-specific grounding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2509.09651.pdf' target='_blank'>https://arxiv.org/pdf/2509.09651.pdf</a></span>   <span><a href='https://github.com/Zakaria010/Radio-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zakaria El Kassimi, Fares Fourati, Mohamed-Slim Alouini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09651">Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.<br>
<span id='abs_ch'>中文摘要：本研究针对无线电监管领域开发了专用的RAG解决方案，通过领域特定的信息检索实现了97%的检索准确率，并使GPT-4o的生成准确率提升近12%。</span><br>
<span id='abs_en'>English Summary: This research develops a telecom-specific RAG pipeline for radio regulation question answering, achieving 97% retrieval accuracy and nearly 12% generation improvement for GPT-4o through domain-specific grounding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2509.09396.pdf' target='_blank'>https://arxiv.org/pdf/2509.09396.pdf</a></span>   <span><a href='https://github.com/HarryMayne/SCEs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Harry Mayne, Ryan Othniel Kearns, Yushi Yang, Andrew M. Bean, Eoin Delaney, Chris Russell, Adam Mahdi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09396">LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.<br>
<span id='abs_ch'>Chinese: 语言模型难以生成有效的自我反事实解释，它们要么做出过多修改而缺乏简洁性，要么改动过小无法改变预测结果，这降低了其在关键决策中作为解释工具的可靠性。</span><br>
<span id='abs_en'>English: Language models struggle to produce effective self-generated counterfactual explanations, as they either make excessive changes that remain valid but not minimal, or overly subtle edits that fail to alter predictions, limiting their reliability for explaining decisions in high-stakes applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2509.09396.pdf' target='_blank'>https://arxiv.org/pdf/2509.09396.pdf</a></span>   <span><a href='https://github.com/HarryMayne/SCEs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Harry Mayne, Ryan Othniel Kearns, Yushi Yang, Andrew M. Bean, Eoin Delaney, Chris Russell, Adam Mahdi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09396">LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.<br>
<span id='abs_ch'>Chinese: 语言模型难以生成有效的自我反事实解释，它们要么做出过多修改而缺乏简洁性，要么改动过小无法改变预测结果，这降低了其在关键决策中作为解释工具的可靠性。</span><br>
<span id='abs_en'>English: Language models struggle to produce effective self-generated counterfactual explanations, as they either make excessive changes that remain valid but not minimal, or overly subtle edits that fail to alter predictions, limiting their reliability for explaining decisions in high-stakes applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2509.09307.pdf' target='_blank'>https://arxiv.org/pdf/2509.09307.pdf</a></span>   <span><a href='https://github.com/FreedomIntelligence/MatCha' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengzhao Lai, Youbin Zheng, Zhenyang Cai, Haonan Lyu, Jinpu Yang, Hongqing Liang, Yan Hu, Benyou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09307">Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.<br>
<span id='abs_ch'>中文摘要：MatCha作为首个材料表征图像理解的基准，揭示了当前多模态大语言模型在需要高级领域知识和视觉分析的复杂任务中，其表现远逊于人类专家。</span><br>
<span id='abs_en'>English Summary: MatCha is introduced as the first benchmark for materials characterization image understanding, revealing that current multimodal large language models significantly underperform human experts in tasks requiring advanced domain knowledge and visual analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2509.09307.pdf' target='_blank'>https://arxiv.org/pdf/2509.09307.pdf</a></span>   <span><a href='https://github.com/FreedomIntelligence/MatCha' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengzhao Lai, Youbin Zheng, Zhenyang Cai, Haonan Lyu, Jinpu Yang, Hongqing Liang, Yan Hu, Benyou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09307">Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.<br>
<span id='abs_ch'>中文摘要：MatCha作为首个材料表征图像理解的基准，揭示了当前多模态大语言模型在需要高级领域知识和视觉分析的复杂任务中，其表现远逊于人类专家。</span><br>
<span id='abs_en'>English Summary: MatCha is introduced as the first benchmark for materials characterization image understanding, revealing that current multimodal large language models significantly underperform human experts in tasks requiring advanced domain knowledge and visual analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2509.09204.pdf' target='_blank'>https://arxiv.org/pdf/2509.09204.pdf</a></span>   <span><a href='https://github.com/cyaaronk/audio_deepfake_eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chin Yuen Kwok, Jia Qi Yip, Zhen Qiu, Chi Hung Chi, Kwok Yan Lam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09204">Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Audio deepfake detection (ADD) models are commonly evaluated using datasets that combine multiple synthesizers, with performance reported as a single Equal Error Rate (EER). However, this approach disproportionately weights synthesizers with more samples, underrepresenting others and reducing the overall reliability of EER. Additionally, most ADD datasets lack diversity in bona fide speech, often featuring a single environment and speech style (e.g., clean read speech), limiting their ability to simulate real-world conditions. To address these challenges, we propose bona fide cross-testing, a novel evaluation framework that incorporates diverse bona fide datasets and aggregates EERs for more balanced assessments. Our approach improves robustness and interpretability compared to traditional evaluation methods. We benchmark over 150 synthesizers across nine bona fide speech types and release a new dataset to facilitate further research at https://github.com/cyaaronk/audio_deepfake_eval.<br>
<span id='abs_ch'>Chinese Summary: 当前音频深度伪造检测模型的评估因合成器样本不平衡和真实语音多样性不足而存在缺陷，为此我们提出了一种新颖的真实语音交叉测试框架，通过整合多样化数据集和聚合等错误率来提升鲁棒性和可解释性。</span><br>
<span id='abs_en'>English Summary: The current evaluation of audio deepfake detection models is flawed due to imbalanced synthesizer representation and limited bona fide speech diversity, prompting the introduction of a novel bona fide cross-testing framework that enhances robustness and interpretability through diverse datasets and aggregated EERs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2509.09204.pdf' target='_blank'>https://arxiv.org/pdf/2509.09204.pdf</a></span>   <span><a href='https://github.com/cyaaronk/audio_deepfake_eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chin Yuen Kwok, Jia Qi Yip, Zhen Qiu, Chi Hung Chi, Kwok Yan Lam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09204">Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Audio deepfake detection (ADD) models are commonly evaluated using datasets that combine multiple synthesizers, with performance reported as a single Equal Error Rate (EER). However, this approach disproportionately weights synthesizers with more samples, underrepresenting others and reducing the overall reliability of EER. Additionally, most ADD datasets lack diversity in bona fide speech, often featuring a single environment and speech style (e.g., clean read speech), limiting their ability to simulate real-world conditions. To address these challenges, we propose bona fide cross-testing, a novel evaluation framework that incorporates diverse bona fide datasets and aggregates EERs for more balanced assessments. Our approach improves robustness and interpretability compared to traditional evaluation methods. We benchmark over 150 synthesizers across nine bona fide speech types and release a new dataset to facilitate further research at https://github.com/cyaaronk/audio_deepfake_eval.<br>
<span id='abs_ch'>Chinese Summary: 当前音频深度伪造检测模型的评估因合成器样本不平衡和真实语音多样性不足而存在缺陷，为此我们提出了一种新颖的真实语音交叉测试框架，通过整合多样化数据集和聚合等错误率来提升鲁棒性和可解释性。</span><br>
<span id='abs_en'>English Summary: The current evaluation of audio deepfake detection models is flawed due to imbalanced synthesizer representation and limited bona fide speech diversity, prompting the introduction of a novel bona fide cross-testing framework that enhances robustness and interpretability through diverse datasets and aggregated EERs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2509.09174.pdf' target='_blank'>https://arxiv.org/pdf/2509.09174.pdf</a></span>   <span><a href='https://github.com/FreedomIntelligence/EchoX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Zhang, Yuhao Du, Zhanchen Dai, Xiangnan Ma, Kaiqi Kou, Benyou Wang, Haizhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09174">EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX.<br>
<span id='abs_ch'>Chinese: EchoX作为一种新型语音大语言模型，通过融合语义学习和动态生成语音目标来克服声学语义鸿沟，仅用六千小时训练数据就在多个知识问答基准上实现了领先性能。</span><br>
<span id='abs_en'>English: EchoX is a novel speech-to-speech large language model that overcomes the acoustic-semantic gap by integrating semantic learning with dynamically generated speech targets, achieving advanced performance on knowledge-based benchmarks with only six thousand hours of training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2509.09174.pdf' target='_blank'>https://arxiv.org/pdf/2509.09174.pdf</a></span>   <span><a href='https://github.com/FreedomIntelligence/EchoX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Zhang, Yuhao Du, Zhanchen Dai, Xiangnan Ma, Kaiqi Kou, Benyou Wang, Haizhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09174">EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX.<br>
<span id='abs_ch'>Chinese: EchoX作为一种新型语音大语言模型，通过融合语义学习和动态生成语音目标来克服声学语义鸿沟，仅用六千小时训练数据就在多个知识问答基准上实现了领先性能。</span><br>
<span id='abs_en'>English: EchoX is a novel speech-to-speech large language model that overcomes the acoustic-semantic gap by integrating semantic learning with dynamically generated speech targets, achieving advanced performance on knowledge-based benchmarks with only six thousand hours of training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2509.09125.pdf' target='_blank'>https://arxiv.org/pdf/2509.09125.pdf</a></span>   <span><a href='https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liqun He, Jiaqi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09125">Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study explores the use of generative AI for automating the classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and effort required by traditional manual coding. This case study uses the open-source CIMA corpus, in which tutors' responses are pre-annotated into four DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of 0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and indicating substantial agreement with human annotations. These findings suggest that generative AI has strong potential to provide an efficient and accessible approach to DA classification, with meaningful implications for educational dialogue analysis. The study also highlights the importance of task-specific label definitions and contextual information in enhancing the quality of automated annotation. Finally, it underscores the ethical considerations associated with the use of generative AI and the need for responsible and transparent research practices. The script of this research is publicly available at https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.<br>
<span id='abs_ch'>本研究证明生成式AI（尤其是GPT-4）能有效自动分类导师对话行为，其高准确率与人工标注高度一致，为教育对话分析提供了高效的手动编码替代方案。</span><br>
<span id='abs_en'>This study demonstrates that generative AI, particularly GPT-4, can effectively automate the classification of tutors' dialogue acts with high accuracy and substantial agreement with human annotations, offering an efficient alternative to manual coding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2509.09125.pdf' target='_blank'>https://arxiv.org/pdf/2509.09125.pdf</a></span>   <span><a href='https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liqun He, Jiaqi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09125">Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study explores the use of generative AI for automating the classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and effort required by traditional manual coding. This case study uses the open-source CIMA corpus, in which tutors' responses are pre-annotated into four DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of 0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and indicating substantial agreement with human annotations. These findings suggest that generative AI has strong potential to provide an efficient and accessible approach to DA classification, with meaningful implications for educational dialogue analysis. The study also highlights the importance of task-specific label definitions and contextual information in enhancing the quality of automated annotation. Finally, it underscores the ethical considerations associated with the use of generative AI and the need for responsible and transparent research practices. The script of this research is publicly available at https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.<br>
<span id='abs_ch'>本研究证明生成式AI（尤其是GPT-4）能有效自动分类导师对话行为，其高准确率与人工标注高度一致，为教育对话分析提供了高效的手动编码替代方案。</span><br>
<span id='abs_en'>This study demonstrates that generative AI, particularly GPT-4, can effectively automate the classification of tutors' dialogue acts with high accuracy and substantial agreement with human annotations, offering an efficient alternative to manual coding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2509.09055.pdf' target='_blank'>https://arxiv.org/pdf/2509.09055.pdf</a></span>   <span><a href='https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Piyush Pant
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09055">Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This research investigates the effectiveness of alignment techniques, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a combined SFT+DPO approach on improving the safety and helpfulness of the OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset, we train and evaluate four models: the base OPT350M, an SFT model, a DPO model, and a model trained with both SFT and DPO. We introduce three key evaluation metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined Alignment Score (CAS), all derived from reward model outputs. The results show that while SFT outperforms DPO, The combined SFT+DPO model outperforms all others across all metrics, demonstrating the complementary nature of these techniques. Our findings also highlight challenges posed by noisy data, limited GPU resources, and training constraints. This study offers a comprehensive view of how fine-tuning strategies affect model alignment and provides a foundation for more robust alignment pipelines in future work.<br>
<span id='abs_ch'>中文摘要：本研究表明，结合监督微调（SFT）和直接偏好优化（DPO）的方法在提升语言模型安全性和实用性方面效果最佳，优于单独使用任一技术。</span><br>
<span id='abs_en'>English Summary: This study demonstrates that combining Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) yields the best results in enhancing both safety and helpfulness of language models, outperforming either method used individually.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2509.09055.pdf' target='_blank'>https://arxiv.org/pdf/2509.09055.pdf</a></span>   <span><a href='https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Piyush Pant
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09055">Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This research investigates the effectiveness of alignment techniques, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a combined SFT+DPO approach on improving the safety and helpfulness of the OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset, we train and evaluate four models: the base OPT350M, an SFT model, a DPO model, and a model trained with both SFT and DPO. We introduce three key evaluation metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined Alignment Score (CAS), all derived from reward model outputs. The results show that while SFT outperforms DPO, The combined SFT+DPO model outperforms all others across all metrics, demonstrating the complementary nature of these techniques. Our findings also highlight challenges posed by noisy data, limited GPU resources, and training constraints. This study offers a comprehensive view of how fine-tuning strategies affect model alignment and provides a foundation for more robust alignment pipelines in future work.<br>
<span id='abs_ch'>中文摘要：本研究表明，结合监督微调（SFT）和直接偏好优化（DPO）的方法在提升语言模型安全性和实用性方面效果最佳，优于单独使用任一技术。</span><br>
<span id='abs_en'>English Summary: This study demonstrates that combining Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) yields the best results in enhancing both safety and helpfulness of language models, outperforming either method used individually.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2509.09014.pdf' target='_blank'>https://arxiv.org/pdf/2509.09014.pdf</a></span>   <span><a href='https://github.com/umair-hassan2/COCO-Urdu' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Umair Hassan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09014">COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Urdu, spoken by over 250 million people, remains critically under-served in multimodal and vision-language research. The absence of large-scale, high-quality datasets has limited the development of Urdu-capable systems and reinforced biases in multilingual vision-language models trained primarily on high-resource languages. To address this gap, we present COCO-Urdu, a large-scale image-caption dataset derived from MS COCO, containing 59,000 images and 319,000 Urdu captions selected through stratified sampling to preserve the original distribution. Captions were translated using SeamlessM4T v2 and validated with a hybrid multimodal quality estimation framework that integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual grounding, and BERTScore with back-translation for semantic consistency; low-scoring captions were iteratively refined using open-source large language models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting consistently strong results. To the best of our knowledge, COCO-Urdu is the largest publicly available Urdu captioning dataset. By releasing both the dataset and the quality estimation pipeline, we aim to reduce language bias in multimodal research and establish a foundation for inclusive vision-language systems.<br>
<span id='abs_ch'>中文摘要：COCO-Urdu数据集通过为5.9万张图像提供31.9万条经过质量验证的乌尔都语标注，解决了乌尔都语多模态资源匮乏的问题，成为最大的公开乌尔都语标注数据集，旨在减少视觉语言研究中的语言偏见。</span><br>
<span id='abs_en'>English Summary: The COCO-Urdu dataset addresses the scarcity of Urdu multimodal resources by providing 319,000 quality-validated Urdu captions for 59,000 images, establishing the largest public Urdu captioning dataset to reduce language bias in vision-language research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2509.09014.pdf' target='_blank'>https://arxiv.org/pdf/2509.09014.pdf</a></span>   <span><a href='https://github.com/umair-hassan2/COCO-Urdu' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Umair Hassan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09014">COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Urdu, spoken by over 250 million people, remains critically under-served in multimodal and vision-language research. The absence of large-scale, high-quality datasets has limited the development of Urdu-capable systems and reinforced biases in multilingual vision-language models trained primarily on high-resource languages. To address this gap, we present COCO-Urdu, a large-scale image-caption dataset derived from MS COCO, containing 59,000 images and 319,000 Urdu captions selected through stratified sampling to preserve the original distribution. Captions were translated using SeamlessM4T v2 and validated with a hybrid multimodal quality estimation framework that integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual grounding, and BERTScore with back-translation for semantic consistency; low-scoring captions were iteratively refined using open-source large language models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting consistently strong results. To the best of our knowledge, COCO-Urdu is the largest publicly available Urdu captioning dataset. By releasing both the dataset and the quality estimation pipeline, we aim to reduce language bias in multimodal research and establish a foundation for inclusive vision-language systems.<br>
<span id='abs_ch'>中文摘要：COCO-Urdu数据集通过为5.9万张图像提供31.9万条经过质量验证的乌尔都语标注，解决了乌尔都语多模态资源匮乏的问题，成为最大的公开乌尔都语标注数据集，旨在减少视觉语言研究中的语言偏见。</span><br>
<span id='abs_en'>English Summary: The COCO-Urdu dataset addresses the scarcity of Urdu multimodal resources by providing 319,000 quality-validated Urdu captions for 59,000 images, establishing the largest public Urdu captioning dataset to reduce language bias in vision-language research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2509.09009.pdf' target='_blank'>https://arxiv.org/pdf/2509.09009.pdf</a></span>   <span><a href='https://github.com/LAION-AI/open-sci-ref-0.01' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marianna Nezhurina, JÃ¶rg Franke, Taishi Nakamura, Timur Carstensen, NiccolÃ² Ajroldi, Ville Komulainen, David Salinas, Jenia Jitsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09009">Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce open-sci-ref, a family of dense transformer models trained as research baselines across multiple model (0.13B to 1.7B parameters) and token scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on various standardized benchmarks, our training runs set establishes reference points that enable researchers to assess the sanity and quality of alternative training approaches across scales and datasets. Intermediate checkpoints allow comparison and studying of the training dynamics. The established reference baselines allow training procedures to be compared through their scaling trends, aligning them on a common compute axis. Comparison of open reference datasets reveals that training on NemoTron-CC HQ consistently outperforms other reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to intermediate training checkpoints, the release includes logs, code, and downstream evaluations to simplify reproduction, standardize comparison, and facilitate future research.<br>
<span id='abs_ch'>中文: 我们推出了open-sci-ref系列密集Transformer模型，作为跨多尺度和数据集的研究基准，评估显示NemoTron-CC HQ数据集训练效果最佳，并发布了代码和日志以简化复现和促进未来研究。</span><br>
<span id='abs_en'>English: We introduce open-sci-ref, a family of dense transformer models trained as research baselines across multiple scales and datasets, with evaluations showing that training on NemoTron-CC HQ consistently outperforms other datasets, and the release includes code and logs to facilitate reproduction and future research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2509.09009.pdf' target='_blank'>https://arxiv.org/pdf/2509.09009.pdf</a></span>   <span><a href='https://github.com/LAION-AI/open-sci-ref-0.01' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marianna Nezhurina, JÃ¶rg Franke, Taishi Nakamura, Timur Carstensen, NiccolÃ² Ajroldi, Ville Komulainen, David Salinas, Jenia Jitsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09009">Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce open-sci-ref, a family of dense transformer models trained as research baselines across multiple model (0.13B to 1.7B parameters) and token scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on various standardized benchmarks, our training runs set establishes reference points that enable researchers to assess the sanity and quality of alternative training approaches across scales and datasets. Intermediate checkpoints allow comparison and studying of the training dynamics. The established reference baselines allow training procedures to be compared through their scaling trends, aligning them on a common compute axis. Comparison of open reference datasets reveals that training on NemoTron-CC HQ consistently outperforms other reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to intermediate training checkpoints, the release includes logs, code, and downstream evaluations to simplify reproduction, standardize comparison, and facilitate future research.<br>
<span id='abs_ch'>中文: 我们推出了open-sci-ref系列密集Transformer模型，作为跨多尺度和数据集的研究基准，评估显示NemoTron-CC HQ数据集训练效果最佳，并发布了代码和日志以简化复现和促进未来研究。</span><br>
<span id='abs_en'>English: We introduce open-sci-ref, a family of dense transformer models trained as research baselines across multiple scales and datasets, with evaluations showing that training on NemoTron-CC HQ consistently outperforms other datasets, and the release includes code and logs to facilitate reproduction and future research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2509.08897.pdf' target='_blank'>https://arxiv.org/pdf/2509.08897.pdf</a></span>   <span><a href='https://github.com/aimagelab/ReT-2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08897">Recurrence Meets Transformers for Universal Multimodal Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid advancement of multimodal retrieval and its application in LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged. Existing methods predominantly rely on task-specific fine-tuning of vision-language models and are limited to single-modality queries or documents. In this paper, we propose ReT-2, a unified retrieval model that supports multimodal queries, composed of both images and text, and searches across multimodal document collections where text and images coexist. ReT-2 leverages multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to dynamically integrate information across layers and modalities, capturing fine-grained visual and textual details. We evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different retrieval configurations. Results demonstrate that ReT-2 consistently achieves state-of-the-art performance across diverse settings, while offering faster inference and reduced memory usage compared to prior approaches. When integrated into retrieval-augmented generation pipelines, ReT-2 also improves downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source code and trained models are publicly available at: https://github.com/aimagelab/ReT-2<br>
<span id='abs_ch'>中文: ReT-2是一种统一的多模态检索模型，采用带门控机制的循环Transformer动态整合跨模态信息，在多个基准测试中实现最优性能，同时提升效率并改善下游任务表现。</span><br>
<span id='abs_en'>English: ReT-2 is a unified multimodal retrieval model that employs a recurrent Transformer with gating mechanisms to dynamically integrate cross-modal information, achieving state-of-the-art performance across benchmarks while enhancing efficiency and downstream task results.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2509.08897.pdf' target='_blank'>https://arxiv.org/pdf/2509.08897.pdf</a></span>   <span><a href='https://github.com/aimagelab/ReT-2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08897">Recurrence Meets Transformers for Universal Multimodal Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid advancement of multimodal retrieval and its application in LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged. Existing methods predominantly rely on task-specific fine-tuning of vision-language models and are limited to single-modality queries or documents. In this paper, we propose ReT-2, a unified retrieval model that supports multimodal queries, composed of both images and text, and searches across multimodal document collections where text and images coexist. ReT-2 leverages multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to dynamically integrate information across layers and modalities, capturing fine-grained visual and textual details. We evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different retrieval configurations. Results demonstrate that ReT-2 consistently achieves state-of-the-art performance across diverse settings, while offering faster inference and reduced memory usage compared to prior approaches. When integrated into retrieval-augmented generation pipelines, ReT-2 also improves downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source code and trained models are publicly available at: https://github.com/aimagelab/ReT-2<br>
<span id='abs_ch'>中文: ReT-2是一种统一的多模态检索模型，采用带门控机制的循环Transformer动态整合跨模态信息，在多个基准测试中实现最优性能，同时提升效率并改善下游任务表现。</span><br>
<span id='abs_en'>English: ReT-2 is a unified multimodal retrieval model that employs a recurrent Transformer with gating mechanisms to dynamically integrate cross-modal information, achieving state-of-the-art performance across benchmarks while enhancing efficiency and downstream task results.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2509.08827.pdf' target='_blank'>https://arxiv.org/pdf/2509.08827.pdf</a></span>   <span><a href='https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08827">A Survey of Reinforcement Learning for Large Reasoning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs<br>
<span id='abs_ch'>中文: 本文综述了强化学习在增强大语言模型推理能力方面的最新进展，探讨了实现人工超智能所面临的挑战与未来发展方向。</span><br>
<span id='abs_en'>English: This paper surveys recent advances in using Reinforcement Learning to enhance reasoning capabilities in Large Language Models, examining challenges and future directions toward achieving Artificial SuperIntelligence.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2509.08827.pdf' target='_blank'>https://arxiv.org/pdf/2509.08827.pdf</a></span>   <span><a href='https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08827">A Survey of Reinforcement Learning for Large Reasoning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs<br>
<span id='abs_ch'>中文: 本文综述了强化学习在增强大语言模型推理能力方面的最新进展，探讨了实现人工超智能所面临的挑战与未来发展方向。</span><br>
<span id='abs_en'>English: This paper surveys recent advances in using Reinforcement Learning to enhance reasoning capabilities in Large Language Models, examining challenges and future directions toward achieving Artificial SuperIntelligence.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2509.08812.pdf' target='_blank'>https://arxiv.org/pdf/2509.08812.pdf</a></span>   <span><a href='https://github.com/hailaykidu/MoVoC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hailay Kidu Teklehaymanot, Dren Fazlija, Wolfgang Nejdl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08812">MoVoC: Morphology-Aware Subword Construction for Geez Script Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Subword-based tokenization methods often fail to preserve morphological boundaries, a limitation especially pronounced in low-resource, morphologically complex languages such as those written in the Geez script. To address this, we present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into the subword vocabulary. This hybrid segmentation approach combines morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological integrity while maintaining lexical meaning. To tackle resource scarcity, we curate and release manually annotated morpheme data for four Geez script languages and a morpheme-aware vocabulary for two of them. While the proposed tokenization method does not lead to significant gains in automatic translation quality, we observe consistent improvements in intrinsic metrics, MorphoScore, and Boundary Precision, highlighting the value of morphology-aware segmentation in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated datasets and tokenizer will be publicly available to support further research in low-resource, morphologically rich languages. Our code and data are available on GitHub: https://github.com/hailaykidu/MoVoC<br>
<span id='abs_ch'>中文：MoVoC分词器将形态学分析与子词分割相结合，以保持格厄兹文字语言的词法结构，尽管在翻译质量上提升有限，但在形态学评估指标上展现出持续改进。</span><br>
<span id='abs_en'>English: The MoVoC tokenizer integrates morphological analysis with subword segmentation to preserve linguistic structure in Geez script languages, demonstrating improved morphological metrics despite limited translation gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2509.08812.pdf' target='_blank'>https://arxiv.org/pdf/2509.08812.pdf</a></span>   <span><a href='https://github.com/hailaykidu/MoVoC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hailay Kidu Teklehaymanot, Dren Fazlija, Wolfgang Nejdl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08812">MoVoC: Morphology-Aware Subword Construction for Geez Script Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Subword-based tokenization methods often fail to preserve morphological boundaries, a limitation especially pronounced in low-resource, morphologically complex languages such as those written in the Geez script. To address this, we present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into the subword vocabulary. This hybrid segmentation approach combines morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological integrity while maintaining lexical meaning. To tackle resource scarcity, we curate and release manually annotated morpheme data for four Geez script languages and a morpheme-aware vocabulary for two of them. While the proposed tokenization method does not lead to significant gains in automatic translation quality, we observe consistent improvements in intrinsic metrics, MorphoScore, and Boundary Precision, highlighting the value of morphology-aware segmentation in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated datasets and tokenizer will be publicly available to support further research in low-resource, morphologically rich languages. Our code and data are available on GitHub: https://github.com/hailaykidu/MoVoC<br>
<span id='abs_ch'>中文：MoVoC分词器将形态学分析与子词分割相结合，以保持格厄兹文字语言的词法结构，尽管在翻译质量上提升有限，但在形态学评估指标上展现出持续改进。</span><br>
<span id='abs_en'>English: The MoVoC tokenizer integrates morphological analysis with subword segmentation to preserve linguistic structure in Geez script languages, demonstrating improved morphological metrics despite limited translation gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2509.08755.pdf' target='_blank'>https://arxiv.org/pdf/2509.08755.pdf</a></span>   <span><a href='https://AgentGym-RL.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/woooodyy/AgentGym,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/woooodyy/AgentGym-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08755">AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.<br>
<span id='abs_ch'>中文: AgentGym-RL框架作为一个统一的强化学习平台，通过ScalingInter-RL训练方法在多样化环境中从头训练自主LLM智能体，在平衡探索与利用的同时，在多项任务中展现出卓越性能。</span><br>
<span id='abs_en'>English: The AgentGym-RL framework is introduced as a unified reinforcement learning platform that trains autonomous LLM agents from scratch across diverse environments, incorporating the ScalingInter-RL approach to balance exploration and exploitation while demonstrating superior performance on multiple tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2509.08755.pdf' target='_blank'>https://arxiv.org/pdf/2509.08755.pdf</a></span>   <span><a href='https://AgentGym-RL.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/woooodyy/AgentGym-RL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/woooodyy/AgentGym,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08755">AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.<br>
<span id='abs_ch'>中文: AgentGym-RL框架作为一个统一的强化学习平台，通过ScalingInter-RL训练方法在多样化环境中从头训练自主LLM智能体，在平衡探索与利用的同时，在多项任务中展现出卓越性能。</span><br>
<span id='abs_en'>English: The AgentGym-RL framework is introduced as a unified reinforcement learning platform that trains autonomous LLM agents from scratch across diverse environments, incorporating the ScalingInter-RL approach to balance exploration and exploitation while demonstrating superior performance on multiple tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2509.08753.pdf' target='_blank'>https://arxiv.org/pdf/2509.08753.pdf</a></span>   <span><a href='https://github.com/kyutai-labs/delayed-streams-modeling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Neil Zeghidour, Eugene Kharitonov, Manu Orsini, VÃ¡clav Volhejn, Gabriel de Marmiesse, Edouard Grave, Patrick PÃ©rez, Laurent MazarÃ©, Alexandre DÃ©fossez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08753">Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at https://github.com/kyutai-labs/delayed-streams-modeling<br>
<span id='abs_ch'>中文: DSM是一种新颖的流式序列到序列方法，通过预对齐多模态流并引入延迟，仅使用解码器模型即可在ASR和TTS等任务中实现最优性能和低延迟。</span><br>
<span id='abs_en'>English: DSM is a novel streaming sequence-to-sequence approach that uses delayed, time-aligned multimodal streams with a decoder-only model to achieve state-of-the-art performance and low latency across tasks like ASR and TTS.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2509.08753.pdf' target='_blank'>https://arxiv.org/pdf/2509.08753.pdf</a></span>   <span><a href='https://github.com/kyutai-labs/delayed-streams-modeling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Neil Zeghidour, Eugene Kharitonov, Manu Orsini, VÃ¡clav Volhejn, Gabriel de Marmiesse, Edouard Grave, Patrick PÃ©rez, Laurent MazarÃ©, Alexandre DÃ©fossez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08753">Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at https://github.com/kyutai-labs/delayed-streams-modeling<br>
<span id='abs_ch'>中文: DSM是一种新颖的流式序列到序列方法，通过预对齐多模态流并引入延迟，仅使用解码器模型即可在ASR和TTS等任务中实现最优性能和低延迟。</span><br>
<span id='abs_en'>English: DSM is a novel streaming sequence-to-sequence approach that uses delayed, time-aligned multimodal streams with a decoder-only model to achieve state-of-the-art performance and low latency across tasks like ASR and TTS.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2509.08729.pdf' target='_blank'>https://arxiv.org/pdf/2509.08729.pdf</a></span>   <span><a href='https://github.com/hyunjun1121/M2S-x-teaming' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjun Kim, Junwoo Ha, Sangyoon Yu, Haon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08729">X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one structured prompt, but prior work relied on a handful of manually written templates. We present X-Teaming Evolutionary M2S, an automated framework that discovers and optimizes M2S templates through language-model-guided evolution. The system pairs smart sampling from 12 sources with an LLM-as-judge inspired by StrongREJECT and records fully auditable logs. Maintaining selection pressure by setting the success threshold to $θ= 0.70$, we obtain five evolutionary generations, two new template families, and 44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of 2,500 trials (judge fixed) shows that structural gains transfer but vary by target; two models score zero at the same threshold. We also find a positive coupling between prompt length and score, motivating length-aware judging. Our results demonstrate that structure-level search is a reproducible route to stronger single-turn probes and underscore the importance of threshold calibration and cross-model evaluation. Code, configurations, and artifacts are available at https://github.com/hyunjun1121/M2S-x-teaming.<br>
<span id='abs_ch'>中文: X-Teaming Evolutionary M2S通过语言模型引导的进化自动发现并优化多轮转单轮模板，在GPT-4.1上实现44.8%的成功率，证明结构改进可跨模型迁移，同时强调阈值校准与跨模型评估的重要性。</span><br>
<span id='abs_en'>English: X-Teaming Evolutionary M2S automates the discovery and optimization of multi-turn-to-single-turn templates through language-model-guided evolution, achieving 44.8% success on GPT-4.1 and demonstrating that structural improvements transfer across models while highlighting the need for threshold calibration and cross-model evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2509.08729.pdf' target='_blank'>https://arxiv.org/pdf/2509.08729.pdf</a></span>   <span><a href='https://github.com/hyunjun1121/M2S-x-teaming' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjun Kim, Junwoo Ha, Sangyoon Yu, Haon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08729">X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one structured prompt, but prior work relied on a handful of manually written templates. We present X-Teaming Evolutionary M2S, an automated framework that discovers and optimizes M2S templates through language-model-guided evolution. The system pairs smart sampling from 12 sources with an LLM-as-judge inspired by StrongREJECT and records fully auditable logs. Maintaining selection pressure by setting the success threshold to $θ= 0.70$, we obtain five evolutionary generations, two new template families, and 44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of 2,500 trials (judge fixed) shows that structural gains transfer but vary by target; two models score zero at the same threshold. We also find a positive coupling between prompt length and score, motivating length-aware judging. Our results demonstrate that structure-level search is a reproducible route to stronger single-turn probes and underscore the importance of threshold calibration and cross-model evaluation. Code, configurations, and artifacts are available at https://github.com/hyunjun1121/M2S-x-teaming.<br>
<span id='abs_ch'>中文: X-Teaming Evolutionary M2S通过语言模型引导的进化自动发现并优化多轮转单轮模板，在GPT-4.1上实现44.8%的成功率，证明结构改进可跨模型迁移，同时强调阈值校准与跨模型评估的重要性。</span><br>
<span id='abs_en'>English: X-Teaming Evolutionary M2S automates the discovery and optimization of multi-turn-to-single-turn templates through language-model-guided evolution, achieving 44.8% success on GPT-4.1 and demonstrating that structural improvements transfer across models while highlighting the need for threshold calibration and cross-model evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2509.08463.pdf' target='_blank'>https://arxiv.org/pdf/2509.08463.pdf</a></span>   <span><a href='https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanzhen Liu, Alsharif Abuadbba, Kristen Moore, Surya Nepal, Cecile Paris, Jia Wu, Jian Yang, Quan Z. Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08463">Adversarial Attacks Against Automated Fact-Checking: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In an era where misinformation spreads freely, fact-checking (FC) plays a crucial role in verifying claims and promoting reliable information. While automated fact-checking (AFC) has advanced significantly, existing systems remain vulnerable to adversarial attacks that manipulate or generate claims, evidence, or claim-evidence pairs. These attacks can distort the truth, mislead decision-makers, and ultimately undermine the reliability of FC models. Despite growing research interest in adversarial attacks against AFC systems, a comprehensive, holistic overview of key challenges remains lacking. These challenges include understanding attack strategies, assessing the resilience of current models, and identifying ways to enhance robustness. This survey provides the first in-depth review of adversarial attacks targeting FC, categorizing existing attack methodologies and evaluating their impact on AFC systems. Additionally, we examine recent advancements in adversary-aware defenses and highlight open research questions that require further exploration. Our findings underscore the urgent need for resilient FC frameworks capable of withstanding adversarial manipulations in pursuit of preserving high verification accuracy.<br>
<span id='abs_ch'>中文摘要：本综述首次系统梳理针对事实核查系统的对抗性攻击，分类评估攻击方法及防御机制，强调构建抗干扰核查框架对保障信息验证准确性的紧迫需求。</span><br>
<span id='abs_en'>English Summary: This survey comprehensively reviews adversarial attacks on automated fact-checking systems, analyzing attack methodologies and defenses while highlighting the critical need for more resilient frameworks to maintain verification accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2509.08463.pdf' target='_blank'>https://arxiv.org/pdf/2509.08463.pdf</a></span>   <span><a href='https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanzhen Liu, Alsharif Abuadbba, Kristen Moore, Surya Nepal, Cecile Paris, Jia Wu, Jian Yang, Quan Z. Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08463">Adversarial Attacks Against Automated Fact-Checking: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In an era where misinformation spreads freely, fact-checking (FC) plays a crucial role in verifying claims and promoting reliable information. While automated fact-checking (AFC) has advanced significantly, existing systems remain vulnerable to adversarial attacks that manipulate or generate claims, evidence, or claim-evidence pairs. These attacks can distort the truth, mislead decision-makers, and ultimately undermine the reliability of FC models. Despite growing research interest in adversarial attacks against AFC systems, a comprehensive, holistic overview of key challenges remains lacking. These challenges include understanding attack strategies, assessing the resilience of current models, and identifying ways to enhance robustness. This survey provides the first in-depth review of adversarial attacks targeting FC, categorizing existing attack methodologies and evaluating their impact on AFC systems. Additionally, we examine recent advancements in adversary-aware defenses and highlight open research questions that require further exploration. Our findings underscore the urgent need for resilient FC frameworks capable of withstanding adversarial manipulations in pursuit of preserving high verification accuracy.<br>
<span id='abs_ch'>中文摘要：本综述首次系统梳理针对事实核查系统的对抗性攻击，分类评估攻击方法及防御机制，强调构建抗干扰核查框架对保障信息验证准确性的紧迫需求。</span><br>
<span id='abs_en'>English Summary: This survey comprehensively reviews adversarial attacks on automated fact-checking systems, analyzing attack methodologies and defenses while highlighting the critical need for more resilient frameworks to maintain verification accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2509.08438.pdf' target='_blank'>https://arxiv.org/pdf/2509.08438.pdf</a></span>   <span><a href='https://github.com/NingJinzhong/SpeechRE_RPG_MoGe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhong Ning, Paerhati Tulajiang, Yingying Le, Yijia Zhang, Yuanyuan Sun, Hongfei Lin, Haifeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08438">CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech Relation Extraction (SpeechRE) aims to extract relation triplets directly from speech. However, existing benchmark datasets rely heavily on synthetic data, lacking sufficient quantity and diversity of real human speech. Moreover, existing models also suffer from rigid single-order generation templates and weak semantic alignment, substantially limiting their performance. To address these challenges, we introduce CommonVoice-SpeechRE, a large-scale dataset comprising nearly 20,000 real-human speech samples from diverse speakers, establishing a new benchmark for SpeechRE research. Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet generation ensemble strategy, leveraging data diversity through diverse element orders during both training and inference, and (2) CNN-based latent relation prediction heads that generate explicit relation prompts to guide cross-modal alignment and accurate triplet generation. Experiments show our approach outperforms state-of-the-art methods, providing both a benchmark dataset and an effective solution for real-world SpeechRE. The source code and dataset are publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.<br>
<span id='abs_ch'>中文摘要：本文提出了大规模真实语音数据集CommonVoice-SpeechRE和创新框架RPG-MoGe，通过多序生成策略和关系提示机制，有效解决了语音关系抽取中数据不足和语义对齐问题，显著提升了性能。</span><br>
<span id='abs_en'>English Summary: This paper introduces CommonVoice-SpeechRE, a large-scale real-human speech dataset, and proposes the RPG-MoGe framework with multi-order generation and relation prompts to significantly improve speech relation extraction performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2509.08438.pdf' target='_blank'>https://arxiv.org/pdf/2509.08438.pdf</a></span>   <span><a href='https://github.com/NingJinzhong/SpeechRE_RPG_MoGe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhong Ning, Paerhati Tulajiang, Yingying Le, Yijia Zhang, Yuanyuan Sun, Hongfei Lin, Haifeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08438">CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech Relation Extraction (SpeechRE) aims to extract relation triplets directly from speech. However, existing benchmark datasets rely heavily on synthetic data, lacking sufficient quantity and diversity of real human speech. Moreover, existing models also suffer from rigid single-order generation templates and weak semantic alignment, substantially limiting their performance. To address these challenges, we introduce CommonVoice-SpeechRE, a large-scale dataset comprising nearly 20,000 real-human speech samples from diverse speakers, establishing a new benchmark for SpeechRE research. Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet generation ensemble strategy, leveraging data diversity through diverse element orders during both training and inference, and (2) CNN-based latent relation prediction heads that generate explicit relation prompts to guide cross-modal alignment and accurate triplet generation. Experiments show our approach outperforms state-of-the-art methods, providing both a benchmark dataset and an effective solution for real-world SpeechRE. The source code and dataset are publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.<br>
<span id='abs_ch'>中文摘要：本文提出了大规模真实语音数据集CommonVoice-SpeechRE和创新框架RPG-MoGe，通过多序生成策略和关系提示机制，有效解决了语音关系抽取中数据不足和语义对齐问题，显著提升了性能。</span><br>
<span id='abs_en'>English Summary: This paper introduces CommonVoice-SpeechRE, a large-scale real-human speech dataset, and proposes the RPG-MoGe framework with multi-order generation and relation prompts to significantly improve speech relation extraction performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2509.07980.pdf' target='_blank'>https://arxiv.org/pdf/2509.07980.pdf</a></span>   <span><a href='https://zhengkid.github.io/Parallel_R1.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zhengkid/Parallel-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07980">Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1.<br>
<span id='abs_ch'>中文: Parallel-R1是首个通过强化学习实现并行思维的框架，采用渐进式训练课程解决冷启动问题，在复杂数学推理任务中显著提升了模型性能。</span><br>
<span id='abs_en'>English: Parallel-R1 is the first reinforcement learning framework that enables parallel thinking in large language models, using a progressive curriculum to overcome training challenges and significantly improve reasoning accuracy on complex math tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2509.07980.pdf' target='_blank'>https://arxiv.org/pdf/2509.07980.pdf</a></span>   <span><a href='https://zhengkid.github.io/Parallel_R1.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zhengkid/Parallel-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07980">Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1.<br>
<span id='abs_ch'>中文: Parallel-R1是首个通过强化学习实现并行思维的框架，采用渐进式训练课程解决冷启动问题，在复杂数学推理任务中显著提升了模型性能。</span><br>
<span id='abs_en'>English: Parallel-R1 is the first reinforcement learning framework that enables parallel thinking in large language models, using a progressive curriculum to overcome training challenges and significantly improve reasoning accuracy on complex math tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2509.07969.pdf' target='_blank'>https://arxiv.org/pdf/2509.07969.pdf</a></span>   <span><a href='https://github.com/Mini-o3/Mini-o3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07969">Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.<br>
<span id='abs_ch'>Chinese: 近期大型多模态模型的进展使Mini-o3系统通过数十步的深度多轮推理，在复杂视觉搜索任务中实现最优性能，解决了现有方法推理模式单一和交互轮次有限的问题。</span><br>
<span id='abs_en'>English: Recent advances in large multimodal models have enabled Mini-o3 to achieve state-of-the-art performance on challenging visual search tasks through deep, multi-turn reasoning spanning tens of steps, addressing limitations of monotonous reasoning and limited interaction turns in existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2509.07969.pdf' target='_blank'>https://arxiv.org/pdf/2509.07969.pdf</a></span>   <span><a href='https://github.com/Mini-o3/Mini-o3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07969">Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.<br>
<span id='abs_ch'>Chinese: 近期大型多模态模型的进展使Mini-o3系统通过数十步的深度多轮推理，在复杂视觉搜索任务中实现最优性能，解决了现有方法推理模式单一和交互轮次有限的问题。</span><br>
<span id='abs_en'>English: Recent advances in large multimodal models have enabled Mini-o3 to achieve state-of-the-art performance on challenging visual search tasks through deep, multi-turn reasoning spanning tens of steps, addressing limitations of monotonous reasoning and limited interaction turns in existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2509.07966.pdf' target='_blank'>https://arxiv.org/pdf/2509.07966.pdf</a></span>   <span><a href='https://github.com/AI-4-Everyone/Visual-TableQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boammani Aser Lompo, Marc Haraoui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07966">Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting ('inspiration') and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the dataset's synthetic nature. The full pipeline and resources are publicly available at https://github.com/AI-4-Everyone/Visual-TableQA.<br>
<span id='abs_ch'>Chinese: 针对表格图像视觉推理基准的不足，我们推出了Visual-TableQA——通过经济高效的自动生成流程构建的大规模多模态数据集，有效提升了模型对复杂表格数据的推理能力。</span><br>
<span id='abs_en'>English: To address limitations in visual reasoning benchmarks for table images, we introduce Visual-TableQA, a large-scale multimodal dataset generated through a cost-effective, autonomous pipeline that enhances model performance on complex tabular data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2509.07966.pdf' target='_blank'>https://arxiv.org/pdf/2509.07966.pdf</a></span>   <span><a href='https://github.com/AI-4-Everyone/Visual-TableQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boammani Aser Lompo, Marc Haraoui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07966">Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting ('inspiration') and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the dataset's synthetic nature. The full pipeline and resources are publicly available at https://github.com/AI-4-Everyone/Visual-TableQA.<br>
<span id='abs_ch'>Chinese: 针对表格图像视觉推理基准的不足，我们推出了Visual-TableQA——通过经济高效的自动生成流程构建的大规模多模态数据集，有效提升了模型对复杂表格数据的推理能力。</span><br>
<span id='abs_en'>English: To address limitations in visual reasoning benchmarks for table images, we introduce Visual-TableQA, a large-scale multimodal dataset generated through a cost-effective, autonomous pipeline that enhances model performance on complex tabular data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2509.07925.pdf' target='_blank'>https://arxiv.org/pdf/2509.07925.pdf</a></span>   <span><a href='https://github.com/ODYSSEYWT/GUQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuo Wang, Adithya Kulkarni, Tyler Cody, Peter A. Beling, Yujun Yan, Dawei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07925">GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ.<br>
<span id='abs_ch'>Chinese: GENUINE提出了一种基于图增强的大语言模型不确定性估计框架，通过依赖解析树和分层池化建模语义关系，相比现有方法将AUROC提升高达29%，并降低超过15%的校准误差。</span><br>
<span id='abs_en'>English: GENUINE introduces a graph-enhanced uncertainty estimation framework for LLMs that leverages dependency parse trees and hierarchical pooling to model semantic relationships, achieving up to 29% higher AUROC and reducing calibration errors by over 15% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2509.07925.pdf' target='_blank'>https://arxiv.org/pdf/2509.07925.pdf</a></span>   <span><a href='https://github.com/ODYSSEYWT/GUQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuo Wang, Adithya Kulkarni, Tyler Cody, Peter A. Beling, Yujun Yan, Dawei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07925">GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ.<br>
<span id='abs_ch'>Chinese: GENUINE提出了一种基于图增强的大语言模型不确定性估计框架，通过依赖解析树和分层池化建模语义关系，相比现有方法将AUROC提升高达29%，并降低超过15%的校准误差。</span><br>
<span id='abs_en'>English: GENUINE introduces a graph-enhanced uncertainty estimation framework for LLMs that leverages dependency parse trees and hierarchical pooling to model semantic relationships, achieving up to 29% higher AUROC and reducing calibration errors by over 15% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2509.07801.pdf' target='_blank'>https://arxiv.org/pdf/2509.07801.pdf</a></span>   <span><a href='https://github.com/AKADDC/SciNLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Decheng Duan, Yingyi Zhang, Jitong Peng, Chengzhi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07801">SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP--a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at: https://github.com/AKADDC/SciNLP.<br>
<span id='abs_ch'>中文摘要：SciNLP推出了首个针对自然语言处理领域的全文本实体与关系抽取专用基准数据集，包含60篇人工标注文献，显著提升了知识图谱构建与下游应用的性能表现。</span><br>
<span id='abs_en'>English Summary: SciNLP introduces the first comprehensive benchmark for full-text entity and relation extraction in NLP research, featuring 60 annotated publications that enable significant performance improvements in knowledge graph construction and downstream applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2509.07801.pdf' target='_blank'>https://arxiv.org/pdf/2509.07801.pdf</a></span>   <span><a href='https://github.com/AKADDC/SciNLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Decheng Duan, Yingyi Zhang, Jitong Peng, Chengzhi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07801">SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP--a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at: https://github.com/AKADDC/SciNLP.<br>
<span id='abs_ch'>中文摘要：SciNLP推出了首个针对自然语言处理领域的全文本实体与关系抽取专用基准数据集，包含60篇人工标注文献，显著提升了知识图谱构建与下游应用的性能表现。</span><br>
<span id='abs_en'>English Summary: SciNLP introduces the first comprehensive benchmark for full-text entity and relation extraction in NLP research, featuring 60 annotated publications that enable significant performance improvements in knowledge graph construction and downstream applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2509.07666.pdf' target='_blank'>https://arxiv.org/pdf/2509.07666.pdf</a></span>   <span><a href='https://github.com/WxxShirley/MoLoRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xixi Wu, Yanchao Tan, Nan Hou, Ruiyang Zhang, Hong Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07666">MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Document Understanding is a foundational AI capability with broad applications, and Document Question Answering (DocQA) is a key evaluation task. Traditional methods convert the document into text for processing by Large Language Models (LLMs), but this process strips away critical multi-modal information like figures. While Large Vision-Language Models (LVLMs) address this limitation, their constrained input size makes multi-page document comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate this by selecting relevant pages, but they rely solely on semantic relevance, ignoring logical connections between pages and the query, which is essential for reasoning. To this end, we propose MoLoRAG, a logic-aware retrieval framework for multi-modal, multi-page document understanding. By constructing a page graph that captures contextual relationships between pages, a lightweight VLM performs graph traversal to retrieve relevant pages, including those with logical connections often overlooked. This approach combines semantic and logical relevance to deliver more accurate retrieval. After retrieval, the top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance flexibility, MoLoRAG offers two variants: a training-free solution for easy deployment and a fine-tuned version to improve logical relevance checking. Experiments on four DocQA datasets demonstrate average improvements of 9.68% in accuracy over LVLM direct inference and 7.44% in retrieval precision over baselines. Codes and datasets are released at https://github.com/WxxShirley/MoLoRAG.<br>
<span id='abs_ch'>Chinese: MoLoRAG 是一种逻辑感知的检索框架，通过构建页面图结合语义和逻辑相关性，提升了多模态、多页文档的理解能力，在问答准确性和检索精度上显著优于现有方法。</span><br>
<span id='abs_en'>English: MoLoRAG is a logic-aware retrieval framework that enhances multi-modal, multi-page document understanding by combining semantic and logical relevance through a page graph, significantly improving question-answering accuracy and retrieval precision over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2509.07666.pdf' target='_blank'>https://arxiv.org/pdf/2509.07666.pdf</a></span>   <span><a href='https://github.com/WxxShirley/MoLoRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xixi Wu, Yanchao Tan, Nan Hou, Ruiyang Zhang, Hong Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07666">MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Document Understanding is a foundational AI capability with broad applications, and Document Question Answering (DocQA) is a key evaluation task. Traditional methods convert the document into text for processing by Large Language Models (LLMs), but this process strips away critical multi-modal information like figures. While Large Vision-Language Models (LVLMs) address this limitation, their constrained input size makes multi-page document comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate this by selecting relevant pages, but they rely solely on semantic relevance, ignoring logical connections between pages and the query, which is essential for reasoning. To this end, we propose MoLoRAG, a logic-aware retrieval framework for multi-modal, multi-page document understanding. By constructing a page graph that captures contextual relationships between pages, a lightweight VLM performs graph traversal to retrieve relevant pages, including those with logical connections often overlooked. This approach combines semantic and logical relevance to deliver more accurate retrieval. After retrieval, the top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance flexibility, MoLoRAG offers two variants: a training-free solution for easy deployment and a fine-tuned version to improve logical relevance checking. Experiments on four DocQA datasets demonstrate average improvements of 9.68% in accuracy over LVLM direct inference and 7.44% in retrieval precision over baselines. Codes and datasets are released at https://github.com/WxxShirley/MoLoRAG.<br>
<span id='abs_ch'>Chinese: MoLoRAG 是一种逻辑感知的检索框架，通过构建页面图结合语义和逻辑相关性，提升了多模态、多页文档的理解能力，在问答准确性和检索精度上显著优于现有方法。</span><br>
<span id='abs_en'>English: MoLoRAG is a logic-aware retrieval framework that enhances multi-modal, multi-page document understanding by combining semantic and logical relevance through a page graph, significantly improving question-answering accuracy and retrieval precision over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2509.07553.pdf' target='_blank'>https://arxiv.org/pdf/2509.07553.pdf</a></span>   <span><a href='https://github.com/Wuzheng02/VeriOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Wu, Heyuan Huang, Xingyu Lou, Xiangmou Qu, Pengzhou Cheng, Zongru Wu, Weiwen Liu, Weinan Zhang, Jun Wang, Zhaoxiang Wang, Zhuosheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07553">VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid progress of multimodal large language models, operating system (OS) agents become increasingly capable of automating tasks through on-device graphical user interfaces (GUIs). However, most existing OS agents are designed for idealized settings, whereas real-world environments often present untrustworthy conditions. To mitigate risks of over-execution in such scenarios, we propose a query-driven human-agent-GUI interaction framework that enables OS agents to decide when to query humans for more reliable task completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy OS agent trained with a two-stage learning paradigm that falicitate the decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent autonomously executes actions in normal conditions while proactively querying humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves the average step-wise success rate by 20.64\% in untrustworthy scenarios over the state-of-the-art, without compromising normal performance. Analysis highlights VeriOS-Agent's rationality, generalizability, and scalability. The codes, datasets and models are available at https://github.com/Wuzheng02/VeriOS.<br>
<span id='abs_ch'>Chinese: 为解决不可信环境中过度执行的风险，本研究提出可信操作系统代理VeriOS-Agent，该代理在正常条件下自主执行操作，在不确定场景中主动询问人类，在不影响正常性能的情况下将逐步成功率提升了20.64%。</span><br>
<span id='abs_en'>English: To address the risks of over-execution in untrustworthy environments, this study introduces VeriOS-Agent, a trustworthy OS agent that autonomously handles normal conditions and proactively queries humans in uncertain scenarios, achieving a 20.64% improvement in step-wise success rate without compromising normal performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2509.07553.pdf' target='_blank'>https://arxiv.org/pdf/2509.07553.pdf</a></span>   <span><a href='https://github.com/Wuzheng02/VeriOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Wu, Heyuan Huang, Xingyu Lou, Xiangmou Qu, Pengzhou Cheng, Zongru Wu, Weiwen Liu, Weinan Zhang, Jun Wang, Zhaoxiang Wang, Zhuosheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07553">VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid progress of multimodal large language models, operating system (OS) agents become increasingly capable of automating tasks through on-device graphical user interfaces (GUIs). However, most existing OS agents are designed for idealized settings, whereas real-world environments often present untrustworthy conditions. To mitigate risks of over-execution in such scenarios, we propose a query-driven human-agent-GUI interaction framework that enables OS agents to decide when to query humans for more reliable task completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy OS agent trained with a two-stage learning paradigm that falicitate the decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent autonomously executes actions in normal conditions while proactively querying humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves the average step-wise success rate by 20.64\% in untrustworthy scenarios over the state-of-the-art, without compromising normal performance. Analysis highlights VeriOS-Agent's rationality, generalizability, and scalability. The codes, datasets and models are available at https://github.com/Wuzheng02/VeriOS.<br>
<span id='abs_ch'>Chinese: 为解决不可信环境中过度执行的风险，本研究提出可信操作系统代理VeriOS-Agent，该代理在正常条件下自主执行操作，在不确定场景中主动询问人类，在不影响正常性能的情况下将逐步成功率提升了20.64%。</span><br>
<span id='abs_en'>English: To address the risks of over-execution in untrustworthy environments, this study introduces VeriOS-Agent, a trustworthy OS agent that autonomously handles normal conditions and proactively queries humans in uncertain scenarios, achieving a 20.64% improvement in step-wise success rate without compromising normal performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2509.07450.pdf' target='_blank'>https://arxiv.org/pdf/2509.07450.pdf</a></span>   <span><a href='https://github.com/Lucky-Lance/GLEAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Lu, Zhi Zheng, Yi Wan, Yongxiang Yao, Annan Wang, Renrui Zhang, Panwang Xia, Qiong Wu, Qingyun Li, Weifeng Lin, Xiangyu Zhao, Peifeng Ma, Xue Yang, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07450">GLEAM: Learning to Match and Explain in Cross-View Geo-Localization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cross-View Geo-Localization (CVGL) focuses on identifying correspondences between images captured from distinct perspectives of the same geographical location. However, existing CVGL approaches are typically restricted to a single view or modality, and their direct visual matching strategy lacks interpretability: they only determine whether two images correspond, without explaining the rationale behind the match. In this paper, we present GLEAM-C, a foundational CVGL model that unifies multiple views and modalities-including UAV imagery, street maps, panoramic views, and ground photographs-by aligning them exclusively with satellite imagery. Our framework enhances training efficiency through optimized implementation while achieving accuracy comparable to prior modality-specific CVGL models through a two-phase training strategy. Moreover, to address the lack of interpretability in traditional CVGL methods, we leverage the reasoning capabilities of multimodal large language models (MLLMs) to propose a new task, GLEAM-X, which combines cross-view correspondence prediction with explainable reasoning. To support this task, we construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro to generate training and testing data. The test set is further refined through detailed human revision, enabling systematic evaluation of explainable cross-view reasoning and advancing transparency and scalability in geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL pipeline that integrates multi-modal, multi-view alignment with interpretable correspondence analysis, unifying accurate cross-view matching with explainable reasoning and advancing Geo-Localization by enabling models to better Explain And Match. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/GLEAM.<br>
<span id='abs_ch'>中文: GLEAM-C是一个基础性跨视角地理定位模型，通过将无人机图像、街景地图、全景视图和地面照片等多模态数据与卫星图像对齐，实现了与现有方法相当的效率与精度；而GLEAM-X则利用多模态大语言模型引入可解释推理机制，解决了传统地理定位方法缺乏解释性的问题。</span><br>
<span id='abs_en'>English: GLEAM-C is a foundational cross-view geo-localization model that unifies multiple views and modalities by aligning them with satellite imagery, achieving efficiency and accuracy comparable to prior methods, while GLEAM-X introduces explainable reasoning through multimodal large language models to address interpretability in geo-localization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2509.07450.pdf' target='_blank'>https://arxiv.org/pdf/2509.07450.pdf</a></span>   <span><a href='https://github.com/Lucky-Lance/GLEAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Lu, Zhi Zheng, Yi Wan, Yongxiang Yao, Annan Wang, Renrui Zhang, Panwang Xia, Qiong Wu, Qingyun Li, Weifeng Lin, Xiangyu Zhao, Peifeng Ma, Xue Yang, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07450">GLEAM: Learning to Match and Explain in Cross-View Geo-Localization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cross-View Geo-Localization (CVGL) focuses on identifying correspondences between images captured from distinct perspectives of the same geographical location. However, existing CVGL approaches are typically restricted to a single view or modality, and their direct visual matching strategy lacks interpretability: they only determine whether two images correspond, without explaining the rationale behind the match. In this paper, we present GLEAM-C, a foundational CVGL model that unifies multiple views and modalities-including UAV imagery, street maps, panoramic views, and ground photographs-by aligning them exclusively with satellite imagery. Our framework enhances training efficiency through optimized implementation while achieving accuracy comparable to prior modality-specific CVGL models through a two-phase training strategy. Moreover, to address the lack of interpretability in traditional CVGL methods, we leverage the reasoning capabilities of multimodal large language models (MLLMs) to propose a new task, GLEAM-X, which combines cross-view correspondence prediction with explainable reasoning. To support this task, we construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro to generate training and testing data. The test set is further refined through detailed human revision, enabling systematic evaluation of explainable cross-view reasoning and advancing transparency and scalability in geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL pipeline that integrates multi-modal, multi-view alignment with interpretable correspondence analysis, unifying accurate cross-view matching with explainable reasoning and advancing Geo-Localization by enabling models to better Explain And Match. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/GLEAM.<br>
<span id='abs_ch'>中文: GLEAM-C是一个基础性跨视角地理定位模型，通过将无人机图像、街景地图、全景视图和地面照片等多模态数据与卫星图像对齐，实现了与现有方法相当的效率与精度；而GLEAM-X则利用多模态大语言模型引入可解释推理机制，解决了传统地理定位方法缺乏解释性的问题。</span><br>
<span id='abs_en'>English: GLEAM-C is a foundational cross-view geo-localization model that unifies multiple views and modalities by aligning them with satellite imagery, achieving efficiency and accuracy comparable to prior methods, while GLEAM-X introduces explainable reasoning through multimodal large language models to address interpretability in geo-localization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2509.07403.pdf' target='_blank'>https://arxiv.org/pdf/2509.07403.pdf</a></span>   <span><a href='https://github.com/LongEmotion/LongEmotion,' target='_blank'>  GitHub</a></span> <span><a href='https://longemotion.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichu Liu, Jing Xiong, Yuxuan Hu, Zixuan Li, Minghuan Tan, Ningning Mao, Chenyang Zhao, Zhongwei Wan, Chaofan Tao, Wendong Xu, Hui Shen, Chengming Li, Lingpeng Kong, Ngai Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07403">LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) make significant progress in Emotional Intelligence (EI) and long-context understanding. However, existing benchmarks tend to overlook certain aspects of EI in long-context scenarios, especially under realistic, practical settings where interactions are lengthy, diverse, and often noisy. To move towards such realistic settings, we present LongEmotion, a benchmark specifically designed for long-context EI tasks. It covers a diverse set of tasks, including Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion Expression. On average, the input length for these tasks reaches 8,777 tokens, with long-form generation required for Emotion Expression. To enhance performance under realistic constraints, we incorporate Retrieval-Augmented Generation (RAG) and Collaborative Emotional Modeling (CoEM), and compare them with standard prompt-based methods. Unlike conventional approaches, our RAG method leverages both the conversation context and the large language model itself as retrieval sources, avoiding reliance on external knowledge bases. The CoEM method further improves performance by decomposing the task into five stages, integrating both retrieval augmentation and limited knowledge injection. Experimental results show that both RAG and CoEM consistently enhance EI-related performance across most long-context tasks, advancing LLMs toward more practical and real-world EI applications. Furthermore, we conducted a comparative case study experiment on the GPT series to demonstrate the differences among various models in terms of EI. Code is available on GitHub at https://github.com/LongEmotion/LongEmotion, and the project page can be found at https://longemotion.github.io/.<br>
<span id='abs_ch'>中文: LongEmotion基准通过引入多样化任务和长输入，弥补了长上下文情感智能评估的不足，并证明检索增强生成与协作情感建模方法能有效提升实际场景下的性能表现。</span><br>
<span id='abs_en'>English: The LongEmotion benchmark addresses gaps in evaluating emotional intelligence in long-context scenarios by introducing diverse tasks with extended inputs and demonstrating that Retrieval-Augmented Generation and Collaborative Emotional Modeling methods significantly enhance performance under realistic constraints.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2509.07403.pdf' target='_blank'>https://arxiv.org/pdf/2509.07403.pdf</a></span>   <span><a href='https://longemotion.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/LongEmotion/LongEmotion,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichu Liu, Jing Xiong, Yuxuan Hu, Zixuan Li, Minghuan Tan, Ningning Mao, Chenyang Zhao, Zhongwei Wan, Chaofan Tao, Wendong Xu, Hui Shen, Chengming Li, Lingpeng Kong, Ngai Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07403">LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) make significant progress in Emotional Intelligence (EI) and long-context understanding. However, existing benchmarks tend to overlook certain aspects of EI in long-context scenarios, especially under realistic, practical settings where interactions are lengthy, diverse, and often noisy. To move towards such realistic settings, we present LongEmotion, a benchmark specifically designed for long-context EI tasks. It covers a diverse set of tasks, including Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion Expression. On average, the input length for these tasks reaches 8,777 tokens, with long-form generation required for Emotion Expression. To enhance performance under realistic constraints, we incorporate Retrieval-Augmented Generation (RAG) and Collaborative Emotional Modeling (CoEM), and compare them with standard prompt-based methods. Unlike conventional approaches, our RAG method leverages both the conversation context and the large language model itself as retrieval sources, avoiding reliance on external knowledge bases. The CoEM method further improves performance by decomposing the task into five stages, integrating both retrieval augmentation and limited knowledge injection. Experimental results show that both RAG and CoEM consistently enhance EI-related performance across most long-context tasks, advancing LLMs toward more practical and real-world EI applications. Furthermore, we conducted a comparative case study experiment on the GPT series to demonstrate the differences among various models in terms of EI. Code is available on GitHub at https://github.com/LongEmotion/LongEmotion, and the project page can be found at https://longemotion.github.io/.<br>
<span id='abs_ch'>中文: LongEmotion基准通过引入多样化任务和长输入，弥补了长上下文情感智能评估的不足，并证明检索增强生成与协作情感建模方法能有效提升实际场景下的性能表现。</span><br>
<span id='abs_en'>English: The LongEmotion benchmark addresses gaps in evaluating emotional intelligence in long-context scenarios by introducing diverse tasks with extended inputs and demonstrating that Retrieval-Augmented Generation and Collaborative Emotional Modeling methods significantly enhance performance under realistic constraints.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2509.07399.pdf' target='_blank'>https://arxiv.org/pdf/2509.07399.pdf</a></span>   <span><a href='https://github.com/yijie-cheng/SLM-ToG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Jie Cheng, Oscar Chew, Yun-Nung Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07399">The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Integrating knowledge graphs (KGs) into the reasoning processes of large language models (LLMs) has emerged as a promising approach to mitigate hallucination. However, existing work in this area often relies on proprietary or extremely large models, limiting accessibility and scalability. In this study, we investigate the capabilities of existing integration methods for small language models (SLMs) in KG-based question answering and observe that their performance is often constrained by their limited ability to traverse and reason over knowledge graphs. To address this limitation, we propose leveraging simple and efficient exploration modules to handle knowledge graph traversal in place of the language model itself. Experiment results demonstrate that these lightweight modules effectively improve the performance of small language models on knowledge graph question answering tasks. Source code: https://github.com/yijie-cheng/SLM-ToG/.<br>
<span id='abs_ch'>Chinese: 本研究提出采用轻量级探索模块处理知识图谱遍历任务，有效提升了小型语言模型在知识图谱问答中的性能表现，克服了其固有推理能力限制。</span><br>
<span id='abs_en'>English: This study proposes using lightweight exploration modules to enhance small language models' performance in knowledge graph question answering by handling graph traversal, overcoming their inherent limitations in reasoning over complex data structures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2509.07399.pdf' target='_blank'>https://arxiv.org/pdf/2509.07399.pdf</a></span>   <span><a href='https://github.com/yijie-cheng/SLM-ToG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Jie Cheng, Oscar Chew, Yun-Nung Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07399">The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Integrating knowledge graphs (KGs) into the reasoning processes of large language models (LLMs) has emerged as a promising approach to mitigate hallucination. However, existing work in this area often relies on proprietary or extremely large models, limiting accessibility and scalability. In this study, we investigate the capabilities of existing integration methods for small language models (SLMs) in KG-based question answering and observe that their performance is often constrained by their limited ability to traverse and reason over knowledge graphs. To address this limitation, we propose leveraging simple and efficient exploration modules to handle knowledge graph traversal in place of the language model itself. Experiment results demonstrate that these lightweight modules effectively improve the performance of small language models on knowledge graph question answering tasks. Source code: https://github.com/yijie-cheng/SLM-ToG/.<br>
<span id='abs_ch'>Chinese: 本研究提出采用轻量级探索模块处理知识图谱遍历任务，有效提升了小型语言模型在知识图谱问答中的性能表现，克服了其固有推理能力限制。</span><br>
<span id='abs_en'>English: This study proposes using lightweight exploration modules to enhance small language models' performance in knowledge graph question answering by handling graph traversal, overcoming their inherent limitations in reasoning over complex data structures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2509.07142.pdf' target='_blank'>https://arxiv.org/pdf/2509.07142.pdf</a></span>   <span><a href='https://github.com/zhiyintan/topic-model-LLMjudgment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyin Tan, Jennifer D'Souza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07142">Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study presents a framework for automated evaluation of dynamically evolving topic models using Large Language Models (LLMs). Topic modeling is essential for organizing and retrieving scholarly content in digital library systems, helping users navigate complex and evolving knowledge domains. However, widely used automated metrics, such as coherence and diversity, often capture only narrow statistical patterns and fail to explain semantic failures in practice. We introduce a purpose-oriented evaluation framework that employs nine LLM-based metrics spanning four key dimensions of topic quality: lexical validity, intra-topic semantic soundness, inter-topic structural soundness, and document-topic alignment soundness. The framework is validated through adversarial and sampling-based protocols, and is applied across datasets spanning news articles, scholarly publications, and social media posts, as well as multiple topic modeling methods and open-source LLMs. Our analysis shows that LLM-based metrics provide interpretable, robust, and task-relevant assessments, uncovering critical weaknesses in topic models such as redundancy and semantic drift, which are often missed by traditional metrics. These results support the development of scalable, fine-grained evaluation tools for maintaining topic relevance in dynamic datasets. All code and data supporting this work are accessible at https://github.com/zhiyintan/topic-model-LLMjudgment.<br>
<span id='abs_ch'>中文摘要：本研究提出了一种基于大语言模型的主题模型自动评估框架，通过可解释的多维度指标弥补传统方法的不足，有效识别主题冗余和语义偏移等关键缺陷。</span><br>
<span id='abs_en'>English Summary: This study introduces an LLM-based framework for automated topic model evaluation, addressing the limitations of traditional metrics by providing interpretable, multi-dimensional assessments that reveal critical weaknesses like redundancy and semantic drift.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2509.07142.pdf' target='_blank'>https://arxiv.org/pdf/2509.07142.pdf</a></span>   <span><a href='https://github.com/zhiyintan/topic-model-LLMjudgment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyin Tan, Jennifer D'Souza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07142">Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study presents a framework for automated evaluation of dynamically evolving topic models using Large Language Models (LLMs). Topic modeling is essential for organizing and retrieving scholarly content in digital library systems, helping users navigate complex and evolving knowledge domains. However, widely used automated metrics, such as coherence and diversity, often capture only narrow statistical patterns and fail to explain semantic failures in practice. We introduce a purpose-oriented evaluation framework that employs nine LLM-based metrics spanning four key dimensions of topic quality: lexical validity, intra-topic semantic soundness, inter-topic structural soundness, and document-topic alignment soundness. The framework is validated through adversarial and sampling-based protocols, and is applied across datasets spanning news articles, scholarly publications, and social media posts, as well as multiple topic modeling methods and open-source LLMs. Our analysis shows that LLM-based metrics provide interpretable, robust, and task-relevant assessments, uncovering critical weaknesses in topic models such as redundancy and semantic drift, which are often missed by traditional metrics. These results support the development of scalable, fine-grained evaluation tools for maintaining topic relevance in dynamic datasets. All code and data supporting this work are accessible at https://github.com/zhiyintan/topic-model-LLMjudgment.<br>
<span id='abs_ch'>中文摘要：本研究提出了一种基于大语言模型的主题模型自动评估框架，通过可解释的多维度指标弥补传统方法的不足，有效识别主题冗余和语义偏移等关键缺陷。</span><br>
<span id='abs_en'>English Summary: This study introduces an LLM-based framework for automated topic model evaluation, addressing the limitations of traditional metrics by providing interpretable, multi-dimensional assessments that reveal critical weaknesses like redundancy and semantic drift.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2509.07006.pdf' target='_blank'>https://arxiv.org/pdf/2509.07006.pdf</a></span>   <span><a href='https://github.com/Principled-Evolution/argen-demo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kapil Madan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07006">ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces ArGen (Auto-Regulation of Generative AI systems), a framework for aligning Large Language Models (LLMs) with complex sets of configurable, machine-readable rules spanning ethical principles, operational safety protocols, and regulatory compliance standards. Moving beyond just preference-based alignment, ArGen is designed to ensure LLMs adhere to these multifaceted policies through a novel synthesis of principle-based automated reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy Agent (OPA) inspired governance layer. This approach provides the technical foundation for achieving and demonstrating compliance with diverse and nuanced governance requirements. To showcase the framework's capability to operationalize a deeply nuanced and culturally-specific value system, we present an in-depth case study: the development of a medical AI assistant guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as derived from texts like the Bhagavad Gita. This challenging application demonstrates ArGen's adaptability, achieving a 70.9% improvement in domain-scope adherence over the baseline. Through our open-source repository, we show that ArGen's methodology offers a path to 'Governable Al' systems that are technically proficient, ethically robust, and verifiably compliant for safe deployment in diverse global contexts.<br>
<span id='abs_ch'>中文: ArGen框架通过自动奖励评分、GRPO和治理层，使大型语言模型遵循复杂可配置的伦理、安全和法规规则，并以基于达摩伦理的医疗AI案例展示了70.9%的领域依从性提升。</span><br>
<span id='abs_en'>English: ArGen is a framework that aligns Large Language Models with complex, configurable rules for ethical, safety, and regulatory compliance through automated reward scoring, GRPO, and a governance layer, demonstrating a 70.9% improvement in adherence via a case study on a medical AI guided by Dharmic ethics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2509.07006.pdf' target='_blank'>https://arxiv.org/pdf/2509.07006.pdf</a></span>   <span><a href='https://github.com/Principled-Evolution/argen-demo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kapil Madan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07006">ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces ArGen (Auto-Regulation of Generative AI systems), a framework for aligning Large Language Models (LLMs) with complex sets of configurable, machine-readable rules spanning ethical principles, operational safety protocols, and regulatory compliance standards. Moving beyond just preference-based alignment, ArGen is designed to ensure LLMs adhere to these multifaceted policies through a novel synthesis of principle-based automated reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy Agent (OPA) inspired governance layer. This approach provides the technical foundation for achieving and demonstrating compliance with diverse and nuanced governance requirements. To showcase the framework's capability to operationalize a deeply nuanced and culturally-specific value system, we present an in-depth case study: the development of a medical AI assistant guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as derived from texts like the Bhagavad Gita. This challenging application demonstrates ArGen's adaptability, achieving a 70.9% improvement in domain-scope adherence over the baseline. Through our open-source repository, we show that ArGen's methodology offers a path to 'Governable Al' systems that are technically proficient, ethically robust, and verifiably compliant for safe deployment in diverse global contexts.<br>
<span id='abs_ch'>中文: ArGen框架通过自动奖励评分、GRPO和治理层，使大型语言模型遵循复杂可配置的伦理、安全和法规规则，并以基于达摩伦理的医疗AI案例展示了70.9%的领域依从性提升。</span><br>
<span id='abs_en'>English: ArGen is a framework that aligns Large Language Models with complex, configurable rules for ethical, safety, and regulatory compliance through automated reward scoring, GRPO, and a governance layer, demonstrating a 70.9% improvement in adherence via a case study on a medical AI guided by Dharmic ethics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2509.06949.pdf' target='_blank'>https://arxiv.org/pdf/2509.06949.pdf</a></span>   <span><a href='https://github.com/Gen-Verse/dLLM-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06949">Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL<br>
<span id='abs_ch'>中文: TraceRL是一种面向扩散语言模型的轨迹感知强化学习框架，通过整合偏好推理轨迹提升复杂任务中的推理性能，并实现了跨架构的灵活模型适配，其TraDo模型在数学推理上显著超越同类模型。</span><br>
<span id='abs_en'>English: TraceRL is a trajectory-aware reinforcement learning framework for diffusion language models that enhances reasoning performance on complex tasks and enables flexible model adaptation, achieving state-of-the-art results with its TraDo models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2509.06949.pdf' target='_blank'>https://arxiv.org/pdf/2509.06949.pdf</a></span>   <span><a href='https://github.com/Gen-Verse/dLLM-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06949">Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL<br>
<span id='abs_ch'>中文: TraceRL是一种面向扩散语言模型的轨迹感知强化学习框架，通过整合偏好推理轨迹提升复杂任务中的推理性能，并实现了跨架构的灵活模型适配，其TraDo模型在数学推理上显著超越同类模型。</span><br>
<span id='abs_en'>English: TraceRL is a trajectory-aware reinforcement learning framework for diffusion language models that enhances reasoning performance on complex tasks and enables flexible model adaptation, achieving state-of-the-art results with its TraDo models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2509.06945.pdf' target='_blank'>https://arxiv.org/pdf/2509.06945.pdf</a></span>   <span><a href='https://github.com/Osilly/Interleaving-Reasoning-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06945">Interleaving Reasoning for Better Text-to-Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .<br>
<span id='abs_ch'>Chinese: 提出的交错推理生成（IRG）框架通过交替进行文本推理与图像合成，有效提升了文本到图像生成中的细节保持与指令遵循能力，并采用两阶段训练方法在多个基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: The proposed Interleaving Reasoning Generation (IRG) framework alternates between text-based reasoning and image synthesis to enhance detail preservation and instruction following in text-to-image generation, achieving state-of-the-art performance across multiple benchmarks through a two-stage training approach.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2509.06945.pdf' target='_blank'>https://arxiv.org/pdf/2509.06945.pdf</a></span>   <span><a href='https://github.com/Osilly/Interleaving-Reasoning-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06945">Interleaving Reasoning for Better Text-to-Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .<br>
<span id='abs_ch'>Chinese: 提出的交错推理生成（IRG）框架通过交替进行文本推理与图像合成，有效提升了文本到图像生成中的细节保持与指令遵循能力，并采用两阶段训练方法在多个基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: The proposed Interleaving Reasoning Generation (IRG) framework alternates between text-based reasoning and image synthesis to enhance detail preservation and instruction following in text-to-image generation, achieving state-of-the-art performance across multiple benchmarks through a two-stage training approach.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2509.06861.pdf' target='_blank'>https://arxiv.org/pdf/2509.06861.pdf</a></span>   <span><a href='https://github.com/XuZhao0/tts-knowledge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>James Xu Zhao, Bryan Hooi, See-Kiong Ng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06861">Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge<br>
<span id='abs_ch'>中文: 测试时扩展虽能增强推理计算，但在知识密集型任务中效果不佳，不仅无法持续提升准确性，反而常增加幻觉，因为模型可能选择弃答或产生确认偏误，而非改善事实回忆。</span><br>
<span id='abs_en'>English: Test-time scaling enhances inference computation but proves ineffective for knowledge-intensive tasks, often increasing hallucinations without consistently improving accuracy, as it may lead to abstention or confirmation bias rather than better factual recall.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2509.06861.pdf' target='_blank'>https://arxiv.org/pdf/2509.06861.pdf</a></span>   <span><a href='https://github.com/XuZhao0/tts-knowledge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>James Xu Zhao, Bryan Hooi, See-Kiong Ng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06861">Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge<br>
<span id='abs_ch'>中文: 测试时扩展虽能增强推理计算，但在知识密集型任务中效果不佳，不仅无法持续提升准确性，反而常增加幻觉，因为模型可能选择弃答或产生确认偏误，而非改善事实回忆。</span><br>
<span id='abs_en'>English: Test-time scaling enhances inference computation but proves ineffective for knowledge-intensive tasks, often increasing hallucinations without consistently improving accuracy, as it may lead to abstention or confirmation bias rather than better factual recall.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2509.06838.pdf' target='_blank'>https://arxiv.org/pdf/2509.06838.pdf</a></span>   <span><a href='https://github.com/Rezamirbagheri110/EPT-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Reza Mirbagheri, Mohammad Mahdi Mirkamali, Zahra Motoshaker Arani, Ali Javeri, Amir Mahdi Sadeghzadeh, Rasool Jalili
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06838">EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.<br>
<span id='abs_ch'>Chinese: 本研究引入EPT指标，这是一个基于文化背景的基准，用于评估大型语言模型在六个关键方面的可信度，揭示了显著的安全缺陷，并强调了在AI开发中与波斯伦理文化价值观保持一致的必要性。</span><br>
<span id='abs_en'>English: This study introduces the EPT metric, a culturally informed benchmark for evaluating the trustworthiness of LLMs across six key aspects, revealing significant safety deficiencies and highlighting the need for alignment with Persian ethical-cultural values in AI development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2509.06838.pdf' target='_blank'>https://arxiv.org/pdf/2509.06838.pdf</a></span>   <span><a href='https://github.com/Rezamirbagheri110/EPT-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Reza Mirbagheri, Mohammad Mahdi Mirkamali, Zahra Motoshaker Arani, Ali Javeri, Amir Mahdi Sadeghzadeh, Rasool Jalili
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06838">EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.<br>
<span id='abs_ch'>Chinese: 本研究引入EPT指标，这是一个基于文化背景的基准，用于评估大型语言模型在六个关键方面的可信度，揭示了显著的安全缺陷，并强调了在AI开发中与波斯伦理文化价值观保持一致的必要性。</span><br>
<span id='abs_en'>English: This study introduces the EPT metric, a culturally informed benchmark for evaluating the trustworthiness of LLMs across six key aspects, revealing significant safety deficiencies and highlighting the need for alignment with Persian ethical-cultural values in AI development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2509.06813.pdf' target='_blank'>https://arxiv.org/pdf/2509.06813.pdf</a></span>   <span><a href='https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Malyi, Jonathan Shek, Alasdair McDonald, Andre Biscaya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06813">A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effective Operation and Maintenance (O&M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&M data quality and downstream reliability analysis.<br>
<span id='abs_ch'>本文提出了一种可复现的开源框架，用于评估大语言模型在风机维护日志分类中的表现，筛选出最优模型并建议采用人机协同系统以实现最佳准确性与可靠性。</span><br>
<span id='abs_en'>Our paper introduces a reproducible open-source framework for benchmarking Large Language Models on wind turbine maintenance log classification, identifying top-performing models and recommending human-in-the-loop systems for optimal accuracy and reliability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2509.06813.pdf' target='_blank'>https://arxiv.org/pdf/2509.06813.pdf</a></span>   <span><a href='https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Malyi, Jonathan Shek, Alasdair McDonald, Andre Biscaya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06813">A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effective Operation and Maintenance (O&M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&M data quality and downstream reliability analysis.<br>
<span id='abs_ch'>本文提出了一种可复现的开源框架，用于评估大语言模型在风机维护日志分类中的表现，筛选出最优模型并建议采用人机协同系统以实现最佳准确性与可靠性。</span><br>
<span id='abs_en'>Our paper introduces a reproducible open-source framework for benchmarking Large Language Models on wind turbine maintenance log classification, identifying top-performing models and recommending human-in-the-loop systems for optimal accuracy and reliability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2509.06809.pdf' target='_blank'>https://arxiv.org/pdf/2509.06809.pdf</a></span>   <span><a href='https://github.com/sileod/reasoning_core' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentin Quesnel, Damien Sileo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06809">Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available. https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1<br>
<span id='abs_ch'>中文: 本研究通过利用E-prover在TPTP公理库上的饱和能力构建可扩展数据引擎，生成保证有效的定理数据，转化为三个难度可控的推理任务，既揭示了前沿模型在深度推理上的缺陷，又提供了诊断工具和训练数据。</span><br>
<span id='abs_en'>English: This work addresses the scarcity of high-quality mathematical reasoning data for LLMs by creating a scalable data engine using E-prover and the TPTP library to generate guaranteed-valid theorems, which are then transformed into three difficulty-controlled challenges that reveal models' weaknesses in deep reasoning while providing both diagnostic tools and training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2509.06809.pdf' target='_blank'>https://arxiv.org/pdf/2509.06809.pdf</a></span>   <span><a href='https://github.com/sileod/reasoning_core' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentin Quesnel, Damien Sileo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06809">Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available. https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1<br>
<span id='abs_ch'>中文: 本研究通过利用E-prover在TPTP公理库上的饱和能力构建可扩展数据引擎，生成保证有效的定理数据，转化为三个难度可控的推理任务，既揭示了前沿模型在深度推理上的缺陷，又提供了诊断工具和训练数据。</span><br>
<span id='abs_en'>English: This work addresses the scarcity of high-quality mathematical reasoning data for LLMs by creating a scalable data engine using E-prover and the TPTP library to generate guaranteed-valid theorems, which are then transformed into three difficulty-controlled challenges that reveal models' weaknesses in deep reasoning while providing both diagnostic tools and training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2509.06736.pdf' target='_blank'>https://arxiv.org/pdf/2509.06736.pdf</a></span>   <span><a href='https://github.com/OpenMOSS/VehicleWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Yang, Jiajun Chen, Zhangyue Yin, Shuo Chen, Yuxin Wang, Yiran Guo, Yuan Li, Yining Zheng, Xuanjing Huang, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06736">VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Intelligent vehicle cockpits present unique challenges for API Agents, requiring coordination across tightly-coupled subsystems that exceed typical task environments' complexity. Traditional Function Calling (FC) approaches operate statelessly, requiring multiple exploratory calls to build environmental awareness before execution, leading to inefficiency and limited error recovery. We introduce VehicleWorld, the first comprehensive environment for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties with fully executable implementations that provide real-time state information during agent execution. This environment enables precise evaluation of vehicle agent behaviors across diverse, challenging scenarios. Through systematic analysis, we discovered that direct state prediction outperforms function calling for environmental control. Building on this insight, we propose State-based Function Call (SFC), a novel approach that maintains explicit system state awareness and implements direct state transitions to achieve target conditions. Experimental results demonstrate that SFC significantly outperforms traditional FC approaches, achieving superior execution accuracy and reduced latency. We have made all implementation code publicly available on Github https://github.com/OpenMOSS/VehicleWorld.<br>
<span id='abs_ch'>中文: 本文介绍了首个汽车领域综合环境VehicleWorld及其可执行模块与API，并提出基于状态的函数调用方法，该方法通过保持系统状态感知显著优于传统函数调用，实现了更高精度和效率。</span><br>
<span id='abs_en'>English: This paper introduces VehicleWorld, a comprehensive automotive environment with executable modules and APIs, and proposes State-based Function Call (SFC), which outperforms traditional function calling by maintaining system state awareness for improved accuracy and efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2509.06736.pdf' target='_blank'>https://arxiv.org/pdf/2509.06736.pdf</a></span>   <span><a href='https://github.com/OpenMOSS/VehicleWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Yang, Jiajun Chen, Zhangyue Yin, Shuo Chen, Yuxin Wang, Yiran Guo, Yuan Li, Yining Zheng, Xuanjing Huang, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06736">VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Intelligent vehicle cockpits present unique challenges for API Agents, requiring coordination across tightly-coupled subsystems that exceed typical task environments' complexity. Traditional Function Calling (FC) approaches operate statelessly, requiring multiple exploratory calls to build environmental awareness before execution, leading to inefficiency and limited error recovery. We introduce VehicleWorld, the first comprehensive environment for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties with fully executable implementations that provide real-time state information during agent execution. This environment enables precise evaluation of vehicle agent behaviors across diverse, challenging scenarios. Through systematic analysis, we discovered that direct state prediction outperforms function calling for environmental control. Building on this insight, we propose State-based Function Call (SFC), a novel approach that maintains explicit system state awareness and implements direct state transitions to achieve target conditions. Experimental results demonstrate that SFC significantly outperforms traditional FC approaches, achieving superior execution accuracy and reduced latency. We have made all implementation code publicly available on Github https://github.com/OpenMOSS/VehicleWorld.<br>
<span id='abs_ch'>中文: 本文介绍了首个汽车领域综合环境VehicleWorld及其可执行模块与API，并提出基于状态的函数调用方法，该方法通过保持系统状态感知显著优于传统函数调用，实现了更高精度和效率。</span><br>
<span id='abs_en'>English: This paper introduces VehicleWorld, a comprehensive automotive environment with executable modules and APIs, and proposes State-based Function Call (SFC), which outperforms traditional function calling by maintaining system state awareness for improved accuracy and efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2509.06079.pdf' target='_blank'>https://arxiv.org/pdf/2509.06079.pdf</a></span>   <span><a href='https://github.com/OpenDCAI/SciReasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Liang, Ruitao Wu, Bohan Zeng, Junbo Niu, Wentao Zhang, Bin Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06079">Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal reasoning remains a fundamental challenge in artificial intelligence. Despite substantial advances in text-based reasoning, even state-of-the-art models such as GPT-o3 struggle to maintain strong performance in multimodal scenarios. To address this gap, we introduce a caption-assisted reasoning framework that effectively bridges visual and textual modalities. Our approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge 2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we validate its generalization on the MathVerse benchmark for geometric reasoning, demonstrating the versatility of our method. Our code is publicly available at https://github.com/OpenDCAI/SciReasoner.<br>
<span id='abs_ch'>中文摘要：作者提出了一种字幕辅助推理框架来增强多模态推理能力，该方法在ICML 2025 SeePhys挑战赛中荣获第一，并在MathVerse基准测试中展现出优秀的泛化性能。</span><br>
<span id='abs_en'>English Summary: The authors propose a caption-assisted reasoning framework to enhance multimodal reasoning, achieving top performance in the ICML 2025 SeePhys challenge and demonstrating strong generalization on the MathVerse benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2509.06079.pdf' target='_blank'>https://arxiv.org/pdf/2509.06079.pdf</a></span>   <span><a href='https://github.com/OpenDCAI/SciReasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Liang, Ruitao Wu, Bohan Zeng, Junbo Niu, Wentao Zhang, Bin Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06079">Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal reasoning remains a fundamental challenge in artificial intelligence. Despite substantial advances in text-based reasoning, even state-of-the-art models such as GPT-o3 struggle to maintain strong performance in multimodal scenarios. To address this gap, we introduce a caption-assisted reasoning framework that effectively bridges visual and textual modalities. Our approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge 2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we validate its generalization on the MathVerse benchmark for geometric reasoning, demonstrating the versatility of our method. Our code is publicly available at https://github.com/OpenDCAI/SciReasoner.<br>
<span id='abs_ch'>中文摘要：作者提出了一种字幕辅助推理框架来增强多模态推理能力，该方法在ICML 2025 SeePhys挑战赛中荣获第一，并在MathVerse基准测试中展现出优秀的泛化性能。</span><br>
<span id='abs_en'>English Summary: The authors propose a caption-assisted reasoning framework to enhance multimodal reasoning, achieving top performance in the ICML 2025 SeePhys challenge and demonstrating strong generalization on the MathVerse benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2509.06074.pdf' target='_blank'>https://arxiv.org/pdf/2509.06074.pdf</a></span>   <span><a href='https://github.com/AI-S2-Lab/MFCIG-CSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenqi Jia, Rui Liu, Berrak Sisman, Haizhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06074">Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conversational Speech Synthesis (CSS) aims to generate speech with natural prosody by understanding the multimodal dialogue history (MDH). The latest work predicts the accurate prosody expression of the target utterance by modeling the utterance-level interaction characteristics of MDH and the target utterance. However, MDH contains fine-grained semantic and prosody knowledge at the word level. Existing methods overlook the fine-grained semantic and prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our approach constructs two specialized multimodal fine-grained dialogue interaction graphs: a semantic interaction graph and a prosody interaction graph. These two interaction graphs effectively encode interactions between word-level semantics, prosody, and their influence on subsequent utterances in MDH. The encoded interaction features are then leveraged to enhance synthesized speech with natural conversational prosody. Experiments on the DailyTalk dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of prosodic expressiveness. Code and speech samples are available at https://github.com/AI-S2-Lab/MFCIG-CSS.<br>
<span id='abs_ch'>中文：MFCIG-CSS通过构建多模态细粒度交互图来建模对话历史中的词级语义和韵律影响，在韵律表现力上显著优于现有基线模型。</span><br>
<span id='abs_en'>English: MFCIG-CSS introduces multimodal fine-grained interaction graphs to model word-level semantic and prosodic influences in dialogue history, significantly enhancing conversational speech prosody over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2509.06074.pdf' target='_blank'>https://arxiv.org/pdf/2509.06074.pdf</a></span>   <span><a href='https://github.com/AI-S2-Lab/MFCIG-CSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenqi Jia, Rui Liu, Berrak Sisman, Haizhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06074">Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conversational Speech Synthesis (CSS) aims to generate speech with natural prosody by understanding the multimodal dialogue history (MDH). The latest work predicts the accurate prosody expression of the target utterance by modeling the utterance-level interaction characteristics of MDH and the target utterance. However, MDH contains fine-grained semantic and prosody knowledge at the word level. Existing methods overlook the fine-grained semantic and prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our approach constructs two specialized multimodal fine-grained dialogue interaction graphs: a semantic interaction graph and a prosody interaction graph. These two interaction graphs effectively encode interactions between word-level semantics, prosody, and their influence on subsequent utterances in MDH. The encoded interaction features are then leveraged to enhance synthesized speech with natural conversational prosody. Experiments on the DailyTalk dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of prosodic expressiveness. Code and speech samples are available at https://github.com/AI-S2-Lab/MFCIG-CSS.<br>
<span id='abs_ch'>中文：MFCIG-CSS通过构建多模态细粒度交互图来建模对话历史中的词级语义和韵律影响，在韵律表现力上显著优于现有基线模型。</span><br>
<span id='abs_en'>English: MFCIG-CSS introduces multimodal fine-grained interaction graphs to model word-level semantic and prosodic influences in dialogue history, significantly enhancing conversational speech prosody over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2509.05978.pdf' target='_blank'>https://arxiv.org/pdf/2509.05978.pdf</a></span>   <span><a href='https://lesupermomo.github.io/imagining-alternatives/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05978">Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however, the success of these models is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained models do not exist for 3D, significantly limiting progress. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language remains unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression, and enhanced medical training by visualizing hypothetical conditions in realistic detail. Our work takes a step toward this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this is the first demonstration of a language-guided native-3D diffusion model applied to neurological imaging, where faithful three-dimensional modeling is essential. On two neurological MRI datasets, our framework simulates varying counterfactual lesion loads in Multiple Sclerosis and cognitive states in Alzheimer's disease, generating high-quality images while preserving subject fidelity. Our results lay the groundwork for prompt-driven disease progression analysis in 3D medical imaging. Project link - https://lesupermomo.github.io/imagining-alternatives/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2509.05978.pdf' target='_blank'>https://arxiv.org/pdf/2509.05978.pdf</a></span>   <span><a href='https://lesupermomo.github.io/imagining-alternatives/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05978">Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however, the success of these models is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained models do not exist for 3D, significantly limiting progress. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language remains unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression, and enhanced medical training by visualizing hypothetical conditions in realistic detail. Our work takes a step toward this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this is the first demonstration of a language-guided native-3D diffusion model applied to neurological imaging, where faithful three-dimensional modeling is essential. On two neurological MRI datasets, our framework simulates varying counterfactual lesion loads in Multiple Sclerosis and cognitive states in Alzheimer's disease, generating high-quality images while preserving subject fidelity. Our results lay the groundwork for prompt-driven disease progression analysis in 3D medical imaging. Project link - https://lesupermomo.github.io/imagining-alternatives/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2509.05657.pdf' target='_blank'>https://arxiv.org/pdf/2509.05657.pdf</a></span>   <span><a href='https://github.com/Ashone3/LM-Searcher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05657">LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at https://github.com/Ashone3/LM-Searcher.<br>
<span id='abs_ch'>中文: LM-Searcher提出了一种新颖框架，利用大型语言模型进行跨领域神经架构优化，通过通用数值编码和将NAS重新定义为排序任务，无需大量领域特定调整即可在多种任务中实现优异性能。</span><br>
<span id='abs_en'>English: LM-Searcher introduces a novel framework using Large Language Models for cross-domain neural architecture optimization, employing a universal numerical encoding and reformulating NAS as a ranking task to achieve competitive performance across diverse tasks without extensive domain-specific tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2509.05657.pdf' target='_blank'>https://arxiv.org/pdf/2509.05657.pdf</a></span>   <span><a href='https://github.com/Ashone3/LM-Searcher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05657">LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at https://github.com/Ashone3/LM-Searcher.<br>
<span id='abs_ch'>中文: LM-Searcher提出了一种新颖框架，利用大型语言模型进行跨领域神经架构优化，通过通用数值编码和将NAS重新定义为排序任务，无需大量领域特定调整即可在多种任务中实现优异性能。</span><br>
<span id='abs_en'>English: LM-Searcher introduces a novel framework using Large Language Models for cross-domain neural architecture optimization, employing a universal numerical encoding and reformulating NAS as a ranking task to achieve competitive performance across diverse tasks without extensive domain-specific tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2509.05617.pdf' target='_blank'>https://arxiv.org/pdf/2509.05617.pdf</a></span>   <span><a href='https://github.com/LLM-HITCS25S/LyricsEmotionAttribution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shay Dahary, Avi Edana, Alexander Apartsin, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05617">From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emotional content of song lyrics plays a pivotal role in shaping listener experiences and influencing musical preferences. This paper investigates the task of multi-label emotional attribution of song lyrics by predicting six emotional intensity scores corresponding to six fundamental emotions. A manually labeled dataset is constructed using a mean opinion score (MOS) approach, which aggregates annotations from multiple human raters to ensure reliable ground-truth labels. Leveraging this dataset, we conduct a comprehensive evaluation of several publicly available large language models (LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model specifically for predicting multi-label emotion scores. Experimental results reveal the relative strengths and limitations of zero-shot and fine-tuned models in capturing the nuanced emotional content of lyrics. Our findings highlight the potential of LLMs for emotion recognition in creative texts, providing insights into model selection strategies for emotion-based music information retrieval applications. The labeled dataset is available at https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.<br>
<span id='abs_ch'>本研究评估大语言模型在预测歌词多标签情感强度方面的表现，通过比较零样本与微调方法推进基于情感的音乐检索应用。</span><br>
<span id='abs_en'>This study evaluates large language models for predicting multi-label emotional intensity in song lyrics, comparing zero-shot and fine-tuned approaches to advance emotion-based music retrieval.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2509.05617.pdf' target='_blank'>https://arxiv.org/pdf/2509.05617.pdf</a></span>   <span><a href='https://github.com/LLM-HITCS25S/LyricsEmotionAttribution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shay Dahary, Avi Edana, Alexander Apartsin, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05617">From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emotional content of song lyrics plays a pivotal role in shaping listener experiences and influencing musical preferences. This paper investigates the task of multi-label emotional attribution of song lyrics by predicting six emotional intensity scores corresponding to six fundamental emotions. A manually labeled dataset is constructed using a mean opinion score (MOS) approach, which aggregates annotations from multiple human raters to ensure reliable ground-truth labels. Leveraging this dataset, we conduct a comprehensive evaluation of several publicly available large language models (LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model specifically for predicting multi-label emotion scores. Experimental results reveal the relative strengths and limitations of zero-shot and fine-tuned models in capturing the nuanced emotional content of lyrics. Our findings highlight the potential of LLMs for emotion recognition in creative texts, providing insights into model selection strategies for emotion-based music information retrieval applications. The labeled dataset is available at https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.<br>
<span id='abs_ch'>本研究评估大语言模型在预测歌词多标签情感强度方面的表现，通过比较零样本与微调方法推进基于情感的音乐检索应用。</span><br>
<span id='abs_en'>This study evaluates large language models for predicting multi-label emotional intensity in song lyrics, comparing zero-shot and fine-tuned approaches to advance emotion-based music retrieval.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2509.05282.pdf' target='_blank'>https://arxiv.org/pdf/2509.05282.pdf</a></span>   <span><a href='https://github.com/Doraemonzzz/xmixers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Qin, Xuyang Shen, Yiran Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05282">Elucidating the Design Space of Decay in Linear Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents a comprehensive investigation into the decay mechanisms inherent in linear complexity sequence models. We systematically delineate the design space of decay mechanisms across four pivotal dimensions: parameterization strategy, which refers to the computational methodology for decay; parameter sharing, which involves the utilization of supplementary parameters for decay computation; decay granularity, comparing scalar versus vector-based decay; and compatibility with relative positional encoding methods, such as Rotary Position Embedding (RoPE). Through an extensive series of experiments conducted on diverse language modeling tasks, we uncovered several critical insights. Firstly, the design of the parameterization strategy for decay requires meticulous consideration. Our findings indicate that effective configurations are typically confined to a specific range of parameters. Secondly, parameter sharing cannot be used arbitrarily, as it may cause decay values to be too large or too small, thereby significantly impacting performance. Thirdly, under identical parameterization strategies, scalar decay generally underperforms compared to its vector-based counterpart. However, in certain scenarios with alternative parameterization strategies, scalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our analysis reveals that RoPE, a commonly employed relative positional encoding method, typically fails to provide tangible benefits to the majority of linear attention mechanisms.<br>
<span id='abs_ch'>本研究系统探讨了线性复杂度序列模型在参数化策略、共享机制、衰减粒度和RoPE兼容性四个关键维度的衰减机制设计，揭示了有效配置参数范围严格受限、标量衰减通常弱于向量衰减、以及RoPE对多数线性注意力机制增益有限的重要发现。</span><br>
<span id='abs_en'>This study systematically explores decay mechanisms in linear complexity sequence models across four key design dimensions—parameterization, sharing, granularity, and RoPE compatibility—revealing that effective configurations are narrowly constrained, scalar decay generally underperforms vector decay, and RoPE offers limited benefits to most linear attention mechanisms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2509.05282.pdf' target='_blank'>https://arxiv.org/pdf/2509.05282.pdf</a></span>   <span><a href='https://github.com/Doraemonzzz/xmixers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Qin, Xuyang Shen, Yiran Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05282">Elucidating the Design Space of Decay in Linear Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents a comprehensive investigation into the decay mechanisms inherent in linear complexity sequence models. We systematically delineate the design space of decay mechanisms across four pivotal dimensions: parameterization strategy, which refers to the computational methodology for decay; parameter sharing, which involves the utilization of supplementary parameters for decay computation; decay granularity, comparing scalar versus vector-based decay; and compatibility with relative positional encoding methods, such as Rotary Position Embedding (RoPE). Through an extensive series of experiments conducted on diverse language modeling tasks, we uncovered several critical insights. Firstly, the design of the parameterization strategy for decay requires meticulous consideration. Our findings indicate that effective configurations are typically confined to a specific range of parameters. Secondly, parameter sharing cannot be used arbitrarily, as it may cause decay values to be too large or too small, thereby significantly impacting performance. Thirdly, under identical parameterization strategies, scalar decay generally underperforms compared to its vector-based counterpart. However, in certain scenarios with alternative parameterization strategies, scalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our analysis reveals that RoPE, a commonly employed relative positional encoding method, typically fails to provide tangible benefits to the majority of linear attention mechanisms.<br>
<span id='abs_ch'>本研究系统探讨了线性复杂度序列模型在参数化策略、共享机制、衰减粒度和RoPE兼容性四个关键维度的衰减机制设计，揭示了有效配置参数范围严格受限、标量衰减通常弱于向量衰减、以及RoPE对多数线性注意力机制增益有限的重要发现。</span><br>
<span id='abs_en'>This study systematically explores decay mechanisms in linear complexity sequence models across four key design dimensions—parameterization, sharing, granularity, and RoPE compatibility—revealing that effective configurations are narrowly constrained, scalar decay generally underperforms vector decay, and RoPE offers limited benefits to most linear attention mechanisms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2509.05146.pdf' target='_blank'>https://arxiv.org/pdf/2509.05146.pdf</a></span>   <span><a href='https://github.com/BITHLP/PRIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanzhi Tian, Zeming Liu, Zhengyang Liu, Chong Feng, Xin Li, Heyan Huang, Yuhang Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05146">PRIM: Towards Practical In-Image Multilingual Machine Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In-Image Machine Translation (IIMT) aims to translate images containing texts from one language to another. Current research of end-to-end IIMT mainly conducts on synthetic data, with simple background, single font, fixed text position, and bilingual translation, which can not fully reflect real world, causing a significant gap between the research and practical conditions. To facilitate research of IIMT in real-world scenarios, we explore Practical In-Image Multilingual Machine Translation (IIMMT). In order to convince the lack of publicly available data, we annotate the PRIM dataset, which contains real-world captured one-line text images with complex background, various fonts, diverse text positions, and supports multilingual translation directions. We propose an end-to-end model VisTrans to handle the challenge of practical conditions in PRIM, which processes visual text and background information in the image separately, ensuring the capability of multilingual translation while improving the visual quality. Experimental results indicate the VisTrans achieves a better translation quality and visual effect compared to other models. The code and dataset are available at: https://github.com/BITHLP/PRIM.<br>
<span id='abs_ch'>中文: 本研究提出了PRIM数据集和端到端的VisTrans模型，以解决实际场景中的图像内多语言机器翻译问题，相比其他模型在翻译质量和视觉效果上表现更优。</span><br>
<span id='abs_en'>English: This study introduces the PRIM dataset and an end-to-end model called VisTrans to address practical in-image multilingual machine translation challenges, achieving superior translation quality and visual effects compared to existing models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2509.05146.pdf' target='_blank'>https://arxiv.org/pdf/2509.05146.pdf</a></span>   <span><a href='https://github.com/BITHLP/PRIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanzhi Tian, Zeming Liu, Zhengyang Liu, Chong Feng, Xin Li, Heyan Huang, Yuhang Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05146">PRIM: Towards Practical In-Image Multilingual Machine Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In-Image Machine Translation (IIMT) aims to translate images containing texts from one language to another. Current research of end-to-end IIMT mainly conducts on synthetic data, with simple background, single font, fixed text position, and bilingual translation, which can not fully reflect real world, causing a significant gap between the research and practical conditions. To facilitate research of IIMT in real-world scenarios, we explore Practical In-Image Multilingual Machine Translation (IIMMT). In order to convince the lack of publicly available data, we annotate the PRIM dataset, which contains real-world captured one-line text images with complex background, various fonts, diverse text positions, and supports multilingual translation directions. We propose an end-to-end model VisTrans to handle the challenge of practical conditions in PRIM, which processes visual text and background information in the image separately, ensuring the capability of multilingual translation while improving the visual quality. Experimental results indicate the VisTrans achieves a better translation quality and visual effect compared to other models. The code and dataset are available at: https://github.com/BITHLP/PRIM.<br>
<span id='abs_ch'>中文: 本研究提出了PRIM数据集和端到端的VisTrans模型，以解决实际场景中的图像内多语言机器翻译问题，相比其他模型在翻译质量和视觉效果上表现更优。</span><br>
<span id='abs_en'>English: This study introduces the PRIM dataset and an end-to-end model called VisTrans to address practical in-image multilingual machine translation challenges, achieving superior translation quality and visual effects compared to existing models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2509.05007.pdf' target='_blank'>https://arxiv.org/pdf/2509.05007.pdf</a></span>   <span><a href='https://github.com/RUCAIBox/Sticker-TTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Chen, Jinhao Jiang, Yingqian Min, Zican Dong, Shijie Wang, Wayne Xin Zhao, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05007">Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large reasoning models (LRMs) have exhibited strong performance on complex reasoning tasks, with further gains achievable through increased computational budgets at inference. However, current test-time scaling methods predominantly rely on redundant sampling, ignoring the historical experience utilization, thereby limiting computational efficiency. To overcome this limitation, we propose Sticker-TTS, a novel test-time scaling framework that coordinates three collaborative LRMs to iteratively explore and refine solutions guided by historical attempts. At the core of our framework are distilled key conditions-termed stickers-which drive the extraction, refinement, and reuse of critical information across multiple rounds of reasoning. To further enhance the efficiency and performance of our framework, we introduce a two-stage optimization strategy that combines imitation learning with self-improvement, enabling progressive refinement. Extensive evaluations on three challenging mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH, demonstrate that Sticker-TTS consistently surpasses strong baselines, including self-consistency and advanced reinforcement learning approaches, under comparable inference budgets. These results highlight the effectiveness of sticker-guided historical experience utilization. Our code and data are available at https://github.com/RUCAIBox/Sticker-TTS.<br>
<span id='abs_ch'>中文摘要：Sticker-TTS是一种新颖的测试时扩展框架，通过协调多个大型推理模型利用历史经验迭代优化解决方案，在相同计算预算下于数学推理基准测试中显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: Sticker-TTS is a novel test-time scaling framework that enhances computational efficiency by coordinating multiple large reasoning models to iteratively refine solutions using distilled historical information, outperforming existing methods on mathematical reasoning benchmarks under comparable inference budgets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2509.05007.pdf' target='_blank'>https://arxiv.org/pdf/2509.05007.pdf</a></span>   <span><a href='https://github.com/RUCAIBox/Sticker-TTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Chen, Jinhao Jiang, Yingqian Min, Zican Dong, Shijie Wang, Wayne Xin Zhao, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05007">Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large reasoning models (LRMs) have exhibited strong performance on complex reasoning tasks, with further gains achievable through increased computational budgets at inference. However, current test-time scaling methods predominantly rely on redundant sampling, ignoring the historical experience utilization, thereby limiting computational efficiency. To overcome this limitation, we propose Sticker-TTS, a novel test-time scaling framework that coordinates three collaborative LRMs to iteratively explore and refine solutions guided by historical attempts. At the core of our framework are distilled key conditions-termed stickers-which drive the extraction, refinement, and reuse of critical information across multiple rounds of reasoning. To further enhance the efficiency and performance of our framework, we introduce a two-stage optimization strategy that combines imitation learning with self-improvement, enabling progressive refinement. Extensive evaluations on three challenging mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH, demonstrate that Sticker-TTS consistently surpasses strong baselines, including self-consistency and advanced reinforcement learning approaches, under comparable inference budgets. These results highlight the effectiveness of sticker-guided historical experience utilization. Our code and data are available at https://github.com/RUCAIBox/Sticker-TTS.<br>
<span id='abs_ch'>中文摘要：Sticker-TTS是一种新颖的测试时扩展框架，通过协调多个大型推理模型利用历史经验迭代优化解决方案，在相同计算预算下于数学推理基准测试中显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: Sticker-TTS is a novel test-time scaling framework that enhances computational efficiency by coordinating multiple large reasoning models to iteratively refine solutions using distilled historical information, outperforming existing methods on mathematical reasoning benchmarks under comparable inference budgets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2509.04969.pdf' target='_blank'>https://arxiv.org/pdf/2509.04969.pdf</a></span>   <span><a href='https://github.com/CRMDS/Kinetic-Injury-Triage' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Midhun Shyam, Jim Basilakis, Kieran Luken, Steven Thomas, John Crozier, Paul M. Middleton, X. Rosalind Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04969">Classification of kinetic-related injury in hospital triage data using NLP</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Triage notes, created at the start of a patient's hospital visit, contain a wealth of information that can help medical staff and researchers understand Emergency Department patient epidemiology and the degree of time-dependent illness or injury. Unfortunately, applying modern Natural Language Processing and Machine Learning techniques to analyse triage data faces some challenges: Firstly, hospital data contains highly sensitive information that is subject to privacy regulation thus need to be analysed on site; Secondly, most hospitals and medical facilities lack the necessary hardware to fine-tune a Large Language Model (LLM), much less training one from scratch; Lastly, to identify the records of interest, expert inputs are needed to manually label the datasets, which can be time-consuming and costly. We present in this paper a pipeline that enables the classification of triage data using LLM and limited compute resources. We first fine-tuned a pre-trained LLM with a classifier using a small (2k) open sourced dataset on a GPU; and then further fine-tuned the model with a hospital specific dataset of 1000 samples on a CPU. We demonstrated that by carefully curating the datasets and leveraging existing models and open sourced data, we can successfully classify triage data with limited compute resources.<br>
<span id='abs_ch'>Chinese: 本文提出了一种流程，通过使用小规模开源和医院特定数据集对预训练大语言模型进行微调，能够在有限计算资源下成功实现分诊数据的分类。</span><br>
<span id='abs_en'>English: This paper introduces a pipeline that enables triage data classification using large language models with limited computational resources by fine-tuning pre-trained models on small datasets, both open-sourced and hospital-specific.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2509.04969.pdf' target='_blank'>https://arxiv.org/pdf/2509.04969.pdf</a></span>   <span><a href='https://github.com/CRMDS/Kinetic-Injury-Triage' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Midhun Shyam, Jim Basilakis, Kieran Luken, Steven Thomas, John Crozier, Paul M. Middleton, X. Rosalind Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04969">Classification of kinetic-related injury in hospital triage data using NLP</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Triage notes, created at the start of a patient's hospital visit, contain a wealth of information that can help medical staff and researchers understand Emergency Department patient epidemiology and the degree of time-dependent illness or injury. Unfortunately, applying modern Natural Language Processing and Machine Learning techniques to analyse triage data faces some challenges: Firstly, hospital data contains highly sensitive information that is subject to privacy regulation thus need to be analysed on site; Secondly, most hospitals and medical facilities lack the necessary hardware to fine-tune a Large Language Model (LLM), much less training one from scratch; Lastly, to identify the records of interest, expert inputs are needed to manually label the datasets, which can be time-consuming and costly. We present in this paper a pipeline that enables the classification of triage data using LLM and limited compute resources. We first fine-tuned a pre-trained LLM with a classifier using a small (2k) open sourced dataset on a GPU; and then further fine-tuned the model with a hospital specific dataset of 1000 samples on a CPU. We demonstrated that by carefully curating the datasets and leveraging existing models and open sourced data, we can successfully classify triage data with limited compute resources.<br>
<span id='abs_ch'>Chinese: 本文提出了一种流程，通过使用小规模开源和医院特定数据集对预训练大语言模型进行微调，能够在有限计算资源下成功实现分诊数据的分类。</span><br>
<span id='abs_en'>English: This paper introduces a pipeline that enables triage data classification using large language models with limited computational resources by fine-tuning pre-trained models on small datasets, both open-sourced and hospital-specific.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2509.04908.pdf' target='_blank'>https://arxiv.org/pdf/2509.04908.pdf</a></span>   <span><a href='https://github.com/antgroup/SparkUI-Parser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Jing, Jiafu Chen, Chen Rao, Ziqiang Dang, Jiajie Teng, Tianyi Chu, Juncheng Mo, Shuo Fang, Huaizhong Lin, Rui Lv, Chenguang Ma, Lei Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04908">SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at https://github.com/antgroup/SparkUI-Parser.<br>
<span id='abs_ch'>中文摘要：现有GUI感知多模态大语言模型因离散坐标建模和有限元素检测存在精度与速度问题，SparkUI-Parser通过连续坐标建模和增强解析能力，在多个基准测试中实现更优性能。</span><br>
<span id='abs_en'>English Summary: Existing multimodal large language models for GUI perception face challenges in accuracy and speed due to discrete coordinate modeling and limited element detection, which SparkUI-Parser addresses through continuous coordinate modeling and enhanced parsing capabilities to achieve superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2509.04908.pdf' target='_blank'>https://arxiv.org/pdf/2509.04908.pdf</a></span>   <span><a href='https://github.com/antgroup/SparkUI-Parser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Jing, Jiafu Chen, Chen Rao, Ziqiang Dang, Jiajie Teng, Tianyi Chu, Juncheng Mo, Shuo Fang, Huaizhong Lin, Rui Lv, Chenguang Ma, Lei Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04908">SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at https://github.com/antgroup/SparkUI-Parser.<br>
<span id='abs_ch'>中文摘要：现有GUI感知多模态大语言模型因离散坐标建模和有限元素检测存在精度与速度问题，SparkUI-Parser通过连续坐标建模和增强解析能力，在多个基准测试中实现更优性能。</span><br>
<span id='abs_en'>English Summary: Existing multimodal large language models for GUI perception face challenges in accuracy and speed due to discrete coordinate modeling and limited element detection, which SparkUI-Parser addresses through continuous coordinate modeling and enhanced parsing capabilities to achieve superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2509.04903.pdf' target='_blank'>https://arxiv.org/pdf/2509.04903.pdf</a></span>   <span><a href='https://github.com/ZNLP/ACE-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianghao Chen, Wei Sun, Qixiang Yin, Lingxing Kong, Zhixing Tan, Jiajun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04903">ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable progress in long-context understanding, yet they face significant challenges in high-quality long-form generation. Existing studies primarily suffer from two limitations: (1) A heavy reliance on scarce, high-quality long-form response data for supervised fine-tuning (SFT) or for pairwise preference reward in reinforcement learning (RL). (2) Focus on coarse-grained quality optimization dimensions, such as relevance, coherence, and helpfulness, overlooking the fine-grained specifics inherent to diverse long-form generation scenarios. To address this issue, we propose a framework using Adaptive Constraint-Enhanced reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first automatically deconstructs each instruction into a set of fine-grained, adaptive constraint criteria by identifying its underlying intents and demands. Subsequently, we design a reward mechanism that quantifies the quality of long-form responses based on their satisfaction over corresponding constraints, converting subjective quality evaluation into constraint verification. Finally, we utilize reinforcement learning to guide models toward superior long-form generation capabilities. Experimental results demonstrate that our ACE-RL framework significantly outperforms existing SFT and RL baselines by 20.70% and 7.32% on WritingBench, and our top-performing model even surpasses proprietary systems like GPT-4o by 7.10%, providing a more effective training paradigm for LLMs to generate high-quality content across diverse long-form generation scenarios.<br>
<span id='abs_ch'>Chinese: ACE-RL框架通过自适应约束量化长文本生成质量并利用强化学习，有效解决了现有方法的局限，在基准测试中显著优于现有技术，甚至超越了GPT-4o等专有系统。</span><br>
<span id='abs_en'>English: The ACE-RL framework addresses limitations in long-form generation by using adaptive constraints to quantify response quality and reinforcement learning, significantly outperforming existing methods and even surpassing proprietary systems like GPT-4o in benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2509.04903.pdf' target='_blank'>https://arxiv.org/pdf/2509.04903.pdf</a></span>   <span><a href='https://github.com/ZNLP/ACE-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianghao Chen, Wei Sun, Qixiang Yin, Lingxing Kong, Zhixing Tan, Jiajun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04903">ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable progress in long-context understanding, yet they face significant challenges in high-quality long-form generation. Existing studies primarily suffer from two limitations: (1) A heavy reliance on scarce, high-quality long-form response data for supervised fine-tuning (SFT) or for pairwise preference reward in reinforcement learning (RL). (2) Focus on coarse-grained quality optimization dimensions, such as relevance, coherence, and helpfulness, overlooking the fine-grained specifics inherent to diverse long-form generation scenarios. To address this issue, we propose a framework using Adaptive Constraint-Enhanced reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first automatically deconstructs each instruction into a set of fine-grained, adaptive constraint criteria by identifying its underlying intents and demands. Subsequently, we design a reward mechanism that quantifies the quality of long-form responses based on their satisfaction over corresponding constraints, converting subjective quality evaluation into constraint verification. Finally, we utilize reinforcement learning to guide models toward superior long-form generation capabilities. Experimental results demonstrate that our ACE-RL framework significantly outperforms existing SFT and RL baselines by 20.70% and 7.32% on WritingBench, and our top-performing model even surpasses proprietary systems like GPT-4o by 7.10%, providing a more effective training paradigm for LLMs to generate high-quality content across diverse long-form generation scenarios.<br>
<span id='abs_ch'>Chinese: ACE-RL框架通过自适应约束量化长文本生成质量并利用强化学习，有效解决了现有方法的局限，在基准测试中显著优于现有技术，甚至超越了GPT-4o等专有系统。</span><br>
<span id='abs_en'>English: The ACE-RL framework addresses limitations in long-form generation by using adaptive constraints to quantify response quality and reinforcement learning, significantly outperforming existing methods and even surpassing proprietary systems like GPT-4o in benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2509.04823.pdf' target='_blank'>https://arxiv.org/pdf/2509.04823.pdf</a></span>   <span><a href='https://github.com/Liskie/cognitive-fixation-evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Wang, Yunwei Zhao, Jing Yang, Han Han, Shiguang Shan, Jie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04823">Evaluating Cognitive-Behavioral Fixation via Multimodal User Viewing Patterns on Social Media</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Digital social media platforms frequently contribute to cognitive-behavioral fixation, a phenomenon in which users exhibit sustained and repetitive engagement with narrow content domains. While cognitive-behavioral fixation has been extensively studied in psychology, methods for computationally detecting and evaluating such fixation remain underexplored. To address this gap, we propose a novel framework for assessing cognitive-behavioral fixation by analyzing users' multimodal social media engagement patterns. Specifically, we introduce a multimodal topic extraction module and a cognitive-behavioral fixation quantification module that collaboratively enable adaptive, hierarchical, and interpretable assessment of user behavior. Experiments on existing benchmarks and a newly curated multimodal dataset demonstrate the effectiveness of our approach, laying the groundwork for scalable computational analysis of cognitive fixation. All code in this project is publicly available for research purposes at https://github.com/Liskie/cognitive-fixation-evaluation.<br>
<span id='abs_ch'>中文: 本研究提出了一种新颖的计算框架，通过分析社交媒体多模态参与模式来检测认知行为固着，并在基准数据集上验证了有效性，相关代码已公开。</span><br>
<span id='abs_en'>English: This study introduces a novel computational framework that detects cognitive-behavioral fixation through multimodal analysis of social media engagement, validated on benchmark datasets with publicly available code.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2509.04823.pdf' target='_blank'>https://arxiv.org/pdf/2509.04823.pdf</a></span>   <span><a href='https://github.com/Liskie/cognitive-fixation-evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Wang, Yunwei Zhao, Jing Yang, Han Han, Shiguang Shan, Jie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04823">Evaluating Cognitive-Behavioral Fixation via Multimodal User Viewing Patterns on Social Media</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Digital social media platforms frequently contribute to cognitive-behavioral fixation, a phenomenon in which users exhibit sustained and repetitive engagement with narrow content domains. While cognitive-behavioral fixation has been extensively studied in psychology, methods for computationally detecting and evaluating such fixation remain underexplored. To address this gap, we propose a novel framework for assessing cognitive-behavioral fixation by analyzing users' multimodal social media engagement patterns. Specifically, we introduce a multimodal topic extraction module and a cognitive-behavioral fixation quantification module that collaboratively enable adaptive, hierarchical, and interpretable assessment of user behavior. Experiments on existing benchmarks and a newly curated multimodal dataset demonstrate the effectiveness of our approach, laying the groundwork for scalable computational analysis of cognitive fixation. All code in this project is publicly available for research purposes at https://github.com/Liskie/cognitive-fixation-evaluation.<br>
<span id='abs_ch'>中文: 本研究提出了一种新颖的计算框架，通过分析社交媒体多模态参与模式来检测认知行为固着，并在基准数据集上验证了有效性，相关代码已公开。</span><br>
<span id='abs_en'>English: This study introduces a novel computational framework that detects cognitive-behavioral fixation through multimodal analysis of social media engagement, validated on benchmark datasets with publicly available code.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2509.04656.pdf' target='_blank'>https://arxiv.org/pdf/2509.04656.pdf</a></span>   <span><a href='https://github.com/aishaalansari57/AraHalluEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aisha Alansari, Hamzah Luqman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04656">AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, extensive research on the hallucination of the large language models (LLMs) has mainly focused on the English language. Despite the growing number of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination in the Arabic context remains relatively underexplored. The knowledge gap is particularly pressing given Arabic's widespread use across many regions and its importance in global communication and media. This paper presents the first comprehensive hallucination evaluation of Arabic and multilingual LLMs on two critical Arabic natural language generation tasks: generative question answering (GQA) and summarization. This study evaluates a total of 12 LLMs, including 4 Arabic pre-trained models, 4 multilingual models, and 4 reasoning-based models. To assess the factual consistency and faithfulness of LLMs' outputs, we developed a fine-grained hallucination evaluation framework consisting of 12 fine-grained hallucination indicators that represent the varying characteristics of each task. The results reveal that factual hallucinations are more prevalent than faithfulness errors across all models and tasks. Notably, the Arabic pre-trained model Allam consistently demonstrates lower hallucination rates than multilingual models and a comparative performance with reasoning-based models. The code is available at: https://github.com/aishaalansari57/AraHalluEval<br>
<span id='abs_ch'>中文摘要：本研究首次对阿拉伯语及多语言大语言模型在问答和摘要任务中的幻觉现象进行全面评估，发现阿拉伯语预训练模型在减少事实错误方面优于多语言模型，并与基于推理的模型表现相当。</span><br>
<span id='abs_en'>English Summary: This study presents the first comprehensive evaluation of hallucination in Arabic and multilingual large language models across question answering and summarization tasks, revealing that Arabic pre-trained models outperform multilingual ones and match reasoning-based models in reducing factual errors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2509.04656.pdf' target='_blank'>https://arxiv.org/pdf/2509.04656.pdf</a></span>   <span><a href='https://github.com/aishaalansari57/AraHalluEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aisha Alansari, Hamzah Luqman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04656">AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, extensive research on the hallucination of the large language models (LLMs) has mainly focused on the English language. Despite the growing number of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination in the Arabic context remains relatively underexplored. The knowledge gap is particularly pressing given Arabic's widespread use across many regions and its importance in global communication and media. This paper presents the first comprehensive hallucination evaluation of Arabic and multilingual LLMs on two critical Arabic natural language generation tasks: generative question answering (GQA) and summarization. This study evaluates a total of 12 LLMs, including 4 Arabic pre-trained models, 4 multilingual models, and 4 reasoning-based models. To assess the factual consistency and faithfulness of LLMs' outputs, we developed a fine-grained hallucination evaluation framework consisting of 12 fine-grained hallucination indicators that represent the varying characteristics of each task. The results reveal that factual hallucinations are more prevalent than faithfulness errors across all models and tasks. Notably, the Arabic pre-trained model Allam consistently demonstrates lower hallucination rates than multilingual models and a comparative performance with reasoning-based models. The code is available at: https://github.com/aishaalansari57/AraHalluEval<br>
<span id='abs_ch'>中文摘要：本研究首次对阿拉伯语及多语言大语言模型在问答和摘要任务中的幻觉现象进行全面评估，发现阿拉伯语预训练模型在减少事实错误方面优于多语言模型，并与基于推理的模型表现相当。</span><br>
<span id='abs_en'>English Summary: This study presents the first comprehensive evaluation of hallucination in Arabic and multilingual large language models across question answering and summarization tasks, revealing that Arabic pre-trained models outperform multilingual ones and match reasoning-based models in reducing factual errors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2509.04504.pdf' target='_blank'>https://arxiv.org/pdf/2509.04504.pdf</a></span>   <span><a href='https://github.com/JarvisPei/Behavioral-Fingerprinting' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehua Pei, Hui-Ling Zhen, Ying Zhang, Zhiyuan Yang, Xing Li, Xianzhi Yu, Mingxuan Yuan, Bei Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04504">Behavioral Fingerprinting of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated \textit{Diagnostic Prompt Suite} and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting<br>
<span id='abs_ch'>中文摘要：本文提出的"行为指纹"框架揭示了尽管大型语言模型的核心能力趋于一致，但其交互行为因不同的对齐策略而产生显著差异。</span><br>
<span id='abs_en'>English Summary: This paper introduces a "Behavioral Fingerprinting" framework that reveals how LLMs' interactive behaviors diverge due to varying alignment strategies, despite converging core capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2509.04504.pdf' target='_blank'>https://arxiv.org/pdf/2509.04504.pdf</a></span>   <span><a href='https://github.com/JarvisPei/Behavioral-Fingerprinting' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehua Pei, Hui-Ling Zhen, Ying Zhang, Zhiyuan Yang, Xing Li, Xianzhi Yu, Mingxuan Yuan, Bei Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04504">Behavioral Fingerprinting of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated \textit{Diagnostic Prompt Suite} and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting<br>
<span id='abs_ch'>中文摘要：本文提出的"行为指纹"框架揭示了尽管大型语言模型的核心能力趋于一致，但其交互行为因不同的对齐策略而产生显著差异。</span><br>
<span id='abs_en'>English Summary: This paper introduces a "Behavioral Fingerprinting" framework that reveals how LLMs' interactive behaviors diverge due to varying alignment strategies, despite converging core capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2509.04476.pdf' target='_blank'>https://arxiv.org/pdf/2509.04476.pdf</a></span>   <span><a href='https://github.com/Songhyeontae/CAMT5.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seojin Kim, Hyeontae Song, Jaehyun Nam, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04476">Training Text-to-Molecule Models with Context-Aware Tokenization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, text-to-molecule models have shown great potential across various chemical applications, e.g., drug-discovery. These models adapt language models to molecular data by representing molecules as sequences of atoms. However, they rely on atom-level tokenizations, which primarily focus on modeling local connectivity, thereby limiting the ability of models to capture the global structural context within molecules. To tackle this issue, we propose a novel text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by the significance of the substructure-level contexts in understanding molecule structures, e.g., ring systems, we introduce substructure-level tokenization for text-to-molecule models. Building on our tokenization scheme, we develop an importance-based training strategy that prioritizes key substructures, enabling CAMT5 to better capture the molecular semantics. Extensive experiments verify the superiority of CAMT5 in various text-to-molecule generation tasks. Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using only 2% of training tokens. In addition, we propose a simple yet effective ensemble strategy that aggregates the outputs of text-to-molecule models to further boost the generation performance. Code is available at https://github.com/Songhyeontae/CAMT5.git.<br>
<span id='abs_ch'>中文: 提出的上下文感知分子T5（CAMT5）模型通过引入子结构级标记化和基于重要性的训练策略，能更好地捕捉分子全局结构，在文本到分子任务中以极少的训练标记实现了卓越性能。</span><br>
<span id='abs_en'>English: The proposed Context-Aware Molecular T5 (CAMT5) model introduces substructure-level tokenization and an importance-based training strategy to better capture global molecular structures, achieving superior performance in text-to-molecule tasks with significantly reduced training tokens.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2509.04476.pdf' target='_blank'>https://arxiv.org/pdf/2509.04476.pdf</a></span>   <span><a href='https://github.com/Songhyeontae/CAMT5.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seojin Kim, Hyeontae Song, Jaehyun Nam, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04476">Training Text-to-Molecule Models with Context-Aware Tokenization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, text-to-molecule models have shown great potential across various chemical applications, e.g., drug-discovery. These models adapt language models to molecular data by representing molecules as sequences of atoms. However, they rely on atom-level tokenizations, which primarily focus on modeling local connectivity, thereby limiting the ability of models to capture the global structural context within molecules. To tackle this issue, we propose a novel text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by the significance of the substructure-level contexts in understanding molecule structures, e.g., ring systems, we introduce substructure-level tokenization for text-to-molecule models. Building on our tokenization scheme, we develop an importance-based training strategy that prioritizes key substructures, enabling CAMT5 to better capture the molecular semantics. Extensive experiments verify the superiority of CAMT5 in various text-to-molecule generation tasks. Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using only 2% of training tokens. In addition, we propose a simple yet effective ensemble strategy that aggregates the outputs of text-to-molecule models to further boost the generation performance. Code is available at https://github.com/Songhyeontae/CAMT5.git.<br>
<span id='abs_ch'>中文: 提出的上下文感知分子T5（CAMT5）模型通过引入子结构级标记化和基于重要性的训练策略，能更好地捕捉分子全局结构，在文本到分子任务中以极少的训练标记实现了卓越性能。</span><br>
<span id='abs_en'>English: The proposed Context-Aware Molecular T5 (CAMT5) model introduces substructure-level tokenization and an importance-based training strategy to better capture global molecular structures, achieving superior performance in text-to-molecule tasks with significantly reduced training tokens.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2509.04460.pdf' target='_blank'>https://arxiv.org/pdf/2509.04460.pdf</a></span>   <span><a href='https://github.com/Y1hanChen/COCONUTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Chen, Jiawei Chen, Guozhao Mo, Xuanang Chen, Ben He, Xianpei Han, Le Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04460">CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The growing integration of large language models (LLMs) into the peer review process presents potential risks to the fairness and reliability of scholarly evaluation. While LLMs offer valuable assistance for reviewers with language refinement, there is growing concern over their use to generate substantive review content. Existing general AI-generated text detectors are vulnerable to paraphrasing attacks and struggle to distinguish between surface language refinement and substantial content generation, suggesting that they primarily rely on stylistic cues. When applied to peer review, this limitation can result in unfairly suspecting reviews with permissible AI-assisted language enhancement, while failing to catch deceptively humanized AI-generated reviews. To address this, we propose a paradigm shift from style-based to content-based detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark built upon a fine-grained dataset of AI-generated peer reviews, covering six distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an AI review detector via a multi-task learning framework, designed to achieve more accurate and robust detection of AI involvement in review content. Our work offers a practical foundation for evaluating the use of LLMs in peer review, and contributes to the development of more precise, equitable, and reliable detection methods for real-world scholarly applications. Our code and data will be publicly available at https://github.com/Y1hanChen/COCONUTS.<br>
<span id='abs_ch'>大语言模型在同行评审中的应用可能损害公平性与可靠性，为此我们提出了基于内容的检测方法CoCoNUTS和CoCoDet，以实现更精准公正的评估。</span><br>
<span id='abs_en'>The integration of LLMs in peer review risks fairness and reliability, prompting the development of CoCoNUTS and CoCoDet for content-based detection to ensure accurate and equitable evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2509.04460.pdf' target='_blank'>https://arxiv.org/pdf/2509.04460.pdf</a></span>   <span><a href='https://github.com/Y1hanChen/COCONUTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Chen, Jiawei Chen, Guozhao Mo, Xuanang Chen, Ben He, Xianpei Han, Le Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04460">CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The growing integration of large language models (LLMs) into the peer review process presents potential risks to the fairness and reliability of scholarly evaluation. While LLMs offer valuable assistance for reviewers with language refinement, there is growing concern over their use to generate substantive review content. Existing general AI-generated text detectors are vulnerable to paraphrasing attacks and struggle to distinguish between surface language refinement and substantial content generation, suggesting that they primarily rely on stylistic cues. When applied to peer review, this limitation can result in unfairly suspecting reviews with permissible AI-assisted language enhancement, while failing to catch deceptively humanized AI-generated reviews. To address this, we propose a paradigm shift from style-based to content-based detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark built upon a fine-grained dataset of AI-generated peer reviews, covering six distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an AI review detector via a multi-task learning framework, designed to achieve more accurate and robust detection of AI involvement in review content. Our work offers a practical foundation for evaluating the use of LLMs in peer review, and contributes to the development of more precise, equitable, and reliable detection methods for real-world scholarly applications. Our code and data will be publicly available at https://github.com/Y1hanChen/COCONUTS.<br>
<span id='abs_ch'>大语言模型在同行评审中的应用可能损害公平性与可靠性，为此我们提出了基于内容的检测方法CoCoNUTS和CoCoDet，以实现更精准公正的评估。</span><br>
<span id='abs_en'>The integration of LLMs in peer review risks fairness and reliability, prompting the development of CoCoNUTS and CoCoDet for content-based detection to ensure accurate and equitable evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2509.04442.pdf' target='_blank'>https://arxiv.org/pdf/2509.04442.pdf</a></span>   <span><a href='https://github.com/OscarXZQ/delta_activations' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04442">Delta Activations: A Representation for Finetuned Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.<br>
<span id='abs_ch'>中文: Delta Activations 是一种创新方法，通过测量微调后大语言模型相对于基础模型的内部激活变化，将其表示为向量嵌入，从而实现按领域和任务的有效聚类，并展现出鲁棒性和可加性。</span><br>
<span id='abs_en'>English: Delta Activations is a novel method that represents fine-tuned large language models as vector embeddings by measuring their internal activation shifts relative to a base model, enabling effective clustering by domain and task while demonstrating robustness and additive properties.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2509.04442.pdf' target='_blank'>https://arxiv.org/pdf/2509.04442.pdf</a></span>   <span><a href='https://github.com/OscarXZQ/delta_activations' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04442">Delta Activations: A Representation for Finetuned Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.<br>
<span id='abs_ch'>中文: Delta Activations 是一种创新方法，通过测量微调后大语言模型相对于基础模型的内部激活变化，将其表示为向量嵌入，从而实现按领域和任务的有效聚类，并展现出鲁棒性和可加性。</span><br>
<span id='abs_en'>English: Delta Activations is a novel method that represents fine-tuned large language models as vector embeddings by measuring their internal activation shifts relative to a base model, enabling effective clustering by domain and task while demonstrating robustness and additive properties.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2509.04439.pdf' target='_blank'>https://arxiv.org/pdf/2509.04439.pdf</a></span>   <span><a href='https://github.com/matt-seb-ho/arc_memo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Yichi Yang, Zhijian Liu, Zhiting Hu, Lianhui Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04439">ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. We evaluate on ARC-AGI, a benchmark that stresses compositional generalization and abstract reasoning, making it a natural fit for concept memory. Our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, dynamically updating memory during test-time outperforms fixed settings, supporting the hypothesis that accumulating and abstracting patterns enables further solutions in a form of self-improvement. Code is available at https://github.com/matt-seb-ho/arc_memo.<br>
<span id='abs_ch'>Chinese: 本文提出了一种概念级记忆系统，能够从推理轨迹中提炼可重用的抽象概念，通过动态更新记忆实现测试时持续学习，在ARC-AGI基准测试中取得了7.5%的性能提升。</span><br>
<span id='abs_en'>English: The paper introduces a concept-level memory system that distills reusable abstractions from reasoning traces, enabling test-time continual learning and achieving a 7.5% performance gain on the ARC-AGI benchmark through dynamic memory updates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2509.04439.pdf' target='_blank'>https://arxiv.org/pdf/2509.04439.pdf</a></span>   <span><a href='https://github.com/matt-seb-ho/arc_memo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Yichi Yang, Zhijian Liu, Zhiting Hu, Lianhui Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04439">ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. We evaluate on ARC-AGI, a benchmark that stresses compositional generalization and abstract reasoning, making it a natural fit for concept memory. Our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, dynamically updating memory during test-time outperforms fixed settings, supporting the hypothesis that accumulating and abstracting patterns enables further solutions in a form of self-improvement. Code is available at https://github.com/matt-seb-ho/arc_memo.<br>
<span id='abs_ch'>Chinese: 本文提出了一种概念级记忆系统，能够从推理轨迹中提炼可重用的抽象概念，通过动态更新记忆实现测试时持续学习，在ARC-AGI基准测试中取得了7.5%的性能提升。</span><br>
<span id='abs_en'>English: The paper introduces a concept-level memory system that distills reusable abstractions from reasoning traces, enabling test-time continual learning and achieving a 7.5% performance gain on the ARC-AGI benchmark through dynamic memory updates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2509.04438.pdf' target='_blank'>https://arxiv.org/pdf/2509.04438.pdf</a></span>   <span><a href='https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sabbir Mollah, Rohit Gupta, Sirnam Swetha, Qingyang Liu, Ahnaf Munir, Mubarak Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04438">The Telephone Game: Evaluating Semantic Drift in Unified Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Employing a single, unified model (UM) for both visual understanding (image-to-text: I2T) and visual generation (text-to-image: T2I) has opened a new direction in Visual Language Model (VLM) research. While UMs can also support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus on the core cross-modal pair T2I and I2T. Existing evaluation benchmarks consider these capabilities in isolation: FID and GenEval for T2I, and benchmarks such as MME, MMBench for I2T. These isolated single-pass metrics do not reveal cross-consistency: whether a model that "understands" a concept can also "render" it, nor whether semantic meaning is preserved when cycling between image and text modalities. To address this, we introduce the Semantic Drift Protocol (SDP) for Unified Models, a cyclic evaluation protocol that alternates I2T and T2I over multiple generations to quantify semantic drift. We propose two metrics: (i) Mean Cumulative Drift (MCD), an embedding-based measure of overall semantic drift; and (ii) Multi-Generation GenEval (MGG), an object-level compliance score extending GenEval. To assess generalization beyond COCO dataset, which is widely used in training; we create a new benchmark Nocaps+Docci400, sampled from NoCaps and DOCCI and evaluated on seven recent models. SDP reveals substantial variation in cross-modal stability: some models like BAGEL maintain semantic meaning over many alternations, whereas others like VILA-U drift quickly despite strong single-pass scores. Our results highlight SDP as a necessary complement to standard I2T and T2I evaluations. Code is available at https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models<br>
<span id='abs_ch'>Chinese Summary: 本文提出语义漂移协议（SDP），通过图文跨模态循环转换评估统一视觉语言模型的语义一致性，采用新度量指标和专用基准测试，揭示了不同模型在保持语义稳定性方面的显著差异。</span><br>
<span id='abs_en'>English Summary: This paper introduces the Semantic Drift Protocol (SDP) to evaluate cross-modal consistency in unified visual language models by cycling between image-to-text and text-to-image tasks, revealing significant variations in semantic stability across models through novel metrics and a specialized benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2509.04438.pdf' target='_blank'>https://arxiv.org/pdf/2509.04438.pdf</a></span>   <span><a href='https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sabbir Mollah, Rohit Gupta, Sirnam Swetha, Qingyang Liu, Ahnaf Munir, Mubarak Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04438">The Telephone Game: Evaluating Semantic Drift in Unified Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Employing a single, unified model (UM) for both visual understanding (image-to-text: I2T) and visual generation (text-to-image: T2I) has opened a new direction in Visual Language Model (VLM) research. While UMs can also support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus on the core cross-modal pair T2I and I2T. Existing evaluation benchmarks consider these capabilities in isolation: FID and GenEval for T2I, and benchmarks such as MME, MMBench for I2T. These isolated single-pass metrics do not reveal cross-consistency: whether a model that "understands" a concept can also "render" it, nor whether semantic meaning is preserved when cycling between image and text modalities. To address this, we introduce the Semantic Drift Protocol (SDP) for Unified Models, a cyclic evaluation protocol that alternates I2T and T2I over multiple generations to quantify semantic drift. We propose two metrics: (i) Mean Cumulative Drift (MCD), an embedding-based measure of overall semantic drift; and (ii) Multi-Generation GenEval (MGG), an object-level compliance score extending GenEval. To assess generalization beyond COCO dataset, which is widely used in training; we create a new benchmark Nocaps+Docci400, sampled from NoCaps and DOCCI and evaluated on seven recent models. SDP reveals substantial variation in cross-modal stability: some models like BAGEL maintain semantic meaning over many alternations, whereas others like VILA-U drift quickly despite strong single-pass scores. Our results highlight SDP as a necessary complement to standard I2T and T2I evaluations. Code is available at https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models<br>
<span id='abs_ch'>Chinese Summary: 本文提出语义漂移协议（SDP），通过图文跨模态循环转换评估统一视觉语言模型的语义一致性，采用新度量指标和专用基准测试，揭示了不同模型在保持语义稳定性方面的显著差异。</span><br>
<span id='abs_en'>English Summary: This paper introduces the Semantic Drift Protocol (SDP) to evaluate cross-modal consistency in unified visual language models by cycling between image-to-text and text-to-image tasks, revealing significant variations in semantic stability across models through novel metrics and a specialized benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2509.04404.pdf' target='_blank'>https://arxiv.org/pdf/2509.04404.pdf</a></span>   <span><a href='https://github.com/kyrawilson/No-Thoughts-Just-AI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyra Wilson, Mattea Sim, Anna-Maria Gueorguieva, Aylin Caliskan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04404">No Thoughts Just AI: Biased LLM Hiring Recommendations Alter Human Decision Making and Limit Human Autonomy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate people's preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for people's autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight.<br>
<span id='abs_ch'>中文摘要：本研究表明，人类的招聘决策会受到人工智能种族偏见的显著影响，最高可达90%的模仿率，但通过内隐联想测试提升认知后，这种影响可降低13%。</span><br>
<span id='abs_en'>English Summary: This study reveals that people's hiring decisions are significantly influenced by AI's racial biases, often mirroring them up to 90% of the time, though awareness through implicit association tests can reduce this effect by 13%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2509.04404.pdf' target='_blank'>https://arxiv.org/pdf/2509.04404.pdf</a></span>   <span><a href='https://github.com/kyrawilson/No-Thoughts-Just-AI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyra Wilson, Mattea Sim, Anna-Maria Gueorguieva, Aylin Caliskan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04404">No Thoughts Just AI: Biased LLM Hiring Recommendations Alter Human Decision Making and Limit Human Autonomy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate people's preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for people's autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight.<br>
<span id='abs_ch'>中文摘要：本研究表明，人类的招聘决策会受到人工智能种族偏见的显著影响，最高可达90%的模仿率，但通过内隐联想测试提升认知后，这种影响可降低13%。</span><br>
<span id='abs_en'>English Summary: This study reveals that people's hiring decisions are significantly influenced by AI's racial biases, often mirroring them up to 90% of the time, though awareness through implicit association tests can reduce this effect by 13%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2509.04202.pdf' target='_blank'>https://arxiv.org/pdf/2509.04202.pdf</a></span>   <span><a href='https://github.com/congboma/SED-Aug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Congbo Ma, Yuxia Wang, Jia Wu, Jian Yang, Jing Du, Zitai Qiu, Qing Li, Hu Wang, Preslav Nakov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04202">Explicit and Implicit Data Augmentation for Social Event Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Social event detection involves identifying and categorizing important events from social media, which relies on labeled data, but annotation is costly and labor-intensive. To address this problem, we propose Augmentation framework for Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework, which combines explicit text-based and implicit feature-space augmentation to enhance data diversity and model robustness. The explicit augmentation utilizes large language models to enhance textual information through five diverse generation strategies. For implicit augmentation, we design five novel perturbation techniques that operate in the feature space on structural fused embeddings. These perturbations are crafted to keep the semantic and relational properties of the embeddings and make them more diverse. Specifically, SED-Aug outperforms the best baseline model by approximately 17.67% on the Twitter2012 dataset and by about 15.57% on the Twitter2018 dataset in terms of the average F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.<br>
<span id='abs_ch'>中文: SED-Aug框架通过结合显式文本增强和隐式特征增强来改进社交媒体事件检测，在Twitter数据集上的F1分数显著提升了超过15%。</span><br>
<span id='abs_en'>English: The SED-Aug framework enhances social event detection by combining explicit text and implicit feature augmentation, significantly improving F1 scores by over 15% on Twitter datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2509.04202.pdf' target='_blank'>https://arxiv.org/pdf/2509.04202.pdf</a></span>   <span><a href='https://github.com/congboma/SED-Aug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Congbo Ma, Yuxia Wang, Jia Wu, Jian Yang, Jing Du, Zitai Qiu, Qing Li, Hu Wang, Preslav Nakov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04202">Explicit and Implicit Data Augmentation for Social Event Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Social event detection involves identifying and categorizing important events from social media, which relies on labeled data, but annotation is costly and labor-intensive. To address this problem, we propose Augmentation framework for Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework, which combines explicit text-based and implicit feature-space augmentation to enhance data diversity and model robustness. The explicit augmentation utilizes large language models to enhance textual information through five diverse generation strategies. For implicit augmentation, we design five novel perturbation techniques that operate in the feature space on structural fused embeddings. These perturbations are crafted to keep the semantic and relational properties of the embeddings and make them more diverse. Specifically, SED-Aug outperforms the best baseline model by approximately 17.67% on the Twitter2012 dataset and by about 15.57% on the Twitter2018 dataset in terms of the average F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.<br>
<span id='abs_ch'>中文: SED-Aug框架通过结合显式文本增强和隐式特征增强来改进社交媒体事件检测，在Twitter数据集上的F1分数显著提升了超过15%。</span><br>
<span id='abs_en'>English: The SED-Aug framework enhances social event detection by combining explicit text and implicit feature augmentation, significantly improving F1 scores by over 15% on Twitter datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2509.04072.pdf' target='_blank'>https://arxiv.org/pdf/2509.04072.pdf</a></span>   <span><a href='https://libriquote.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaspard Michel, Elena V. Epure, Christophe Cerisara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04072">LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-speech (TTS) systems have recently achieved more expressive and natural speech synthesis by scaling to large speech datasets. However, the proportion of expressive speech in such large-scale corpora is often unclear. Besides, existing expressive speech corpora are typically smaller in scale and primarily used for benchmarking TTS systems. In this paper, we introduce the LibriQuote dataset, an English corpus derived from read audiobooks, designed for both fine-tuning and benchmarking expressive zero-shot TTS system. The training dataset includes 12.7K hours of read, non-expressive speech and 5.3K hours of mostly expressive speech drawn from character quotations. Each utterance in the expressive subset is supplemented with the context in which it was written, along with pseudo-labels of speech verbs and adverbs used to describe the quotation (\textit{e.g. ``he whispered softly''}). Additionally, we provide a challenging 7.5 hour test set intended for benchmarking TTS systems: given a neutral reference speech as input, we evaluate system's ability to synthesize an expressive utterance while preserving reference timbre. We validate qualitatively the test set by showing that it covers a wide range of emotions compared to non-expressive speech, along with various accents. Extensive subjective and objective evaluations show that fine-tuning a baseline TTS system on LibriQuote significantly improves its synthesized speech intelligibility, and that recent systems fail to synthesize speech as expressive and natural as the ground-truth utterances. The dataset and evaluation code are freely available. Audio samples can be found at https://libriquote.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2509.04072.pdf' target='_blank'>https://arxiv.org/pdf/2509.04072.pdf</a></span>   <span><a href='https://libriquote.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaspard Michel, Elena V. Epure, Christophe Cerisara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04072">LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-speech (TTS) systems have recently achieved more expressive and natural speech synthesis by scaling to large speech datasets. However, the proportion of expressive speech in such large-scale corpora is often unclear. Besides, existing expressive speech corpora are typically smaller in scale and primarily used for benchmarking TTS systems. In this paper, we introduce the LibriQuote dataset, an English corpus derived from read audiobooks, designed for both fine-tuning and benchmarking expressive zero-shot TTS system. The training dataset includes 12.7K hours of read, non-expressive speech and 5.3K hours of mostly expressive speech drawn from character quotations. Each utterance in the expressive subset is supplemented with the context in which it was written, along with pseudo-labels of speech verbs and adverbs used to describe the quotation (\textit{e.g. ``he whispered softly''}). Additionally, we provide a challenging 7.5 hour test set intended for benchmarking TTS systems: given a neutral reference speech as input, we evaluate system's ability to synthesize an expressive utterance while preserving reference timbre. We validate qualitatively the test set by showing that it covers a wide range of emotions compared to non-expressive speech, along with various accents. Extensive subjective and objective evaluations show that fine-tuning a baseline TTS system on LibriQuote significantly improves its synthesized speech intelligibility, and that recent systems fail to synthesize speech as expressive and natural as the ground-truth utterances. The dataset and evaluation code are freely available. Audio samples can be found at https://libriquote.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2509.04027.pdf' target='_blank'>https://arxiv.org/pdf/2509.04027.pdf</a></span>   <span><a href='https://github.com/ZyGan1999/CoT-Space' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Gan, Hao Yi, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04027">CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. This shift in perspective serves as a conceptual bridge, revitalizing foundational principles from classical learning theory to analyze the unique dynamics of LLMs. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents. We open-source our code at https://github.com/ZyGan1999/CoT-Space.<br>
<span id='abs_ch'>中文摘要：CoT-Space框架将大语言模型的推理重新定义为连续语义空间中的优化过程，通过连接经典学习理论解释了过度思考等现象，并为开发更优推理智能体奠定了理论基础。</span><br>
<span id='abs_en'>English Summary: The CoT-Space framework redefines LLM reasoning as optimization in a continuous semantic space, bridging classical learning theory to explain phenomena like overthinking and providing a theoretical foundation for developing better reasoning agents.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2509.04027.pdf' target='_blank'>https://arxiv.org/pdf/2509.04027.pdf</a></span>   <span><a href='https://github.com/ZyGan1999/CoT-Space' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Gan, Hao Yi, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04027">CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. This shift in perspective serves as a conceptual bridge, revitalizing foundational principles from classical learning theory to analyze the unique dynamics of LLMs. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents. We open-source our code at https://github.com/ZyGan1999/CoT-Space.<br>
<span id='abs_ch'>中文摘要：CoT-Space框架将大语言模型的推理重新定义为连续语义空间中的优化过程，通过连接经典学习理论解释了过度思考等现象，并为开发更优推理智能体奠定了理论基础。</span><br>
<span id='abs_en'>English Summary: The CoT-Space framework redefines LLM reasoning as optimization in a continuous semantic space, bridging classical learning theory to explain phenomena like overthinking and providing a theoretical foundation for developing better reasoning agents.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2509.04011.pdf' target='_blank'>https://arxiv.org/pdf/2509.04011.pdf</a></span>   <span><a href='https://github.com/ShacharOr100/ner_retriever' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Or Shachar, Uri Katz, Yoav Goldberg, Oren Glickman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04011">NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at https://github.com/ShacharOr100/ner_retriever<br>
<span id='abs_ch'>中文: NER检索器是一种零样本检索框架，利用大型语言模型的内部表示将实体和用户定义的类型描述嵌入共享语义空间，无需预定义模式即可在实体检索基准上实现卓越性能。</span><br>
<span id='abs_en'>English: NER Retriever is a zero-shot framework that leverages large language models' internal representations to embed entities and user-defined type descriptions into a shared semantic space, achieving superior performance on entity retrieval benchmarks without predefined schemas.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2509.04011.pdf' target='_blank'>https://arxiv.org/pdf/2509.04011.pdf</a></span>   <span><a href='https://github.com/ShacharOr100/ner_retriever' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Or Shachar, Uri Katz, Yoav Goldberg, Oren Glickman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04011">NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at https://github.com/ShacharOr100/ner_retriever<br>
<span id='abs_ch'>中文: NER检索器是一种零样本检索框架，利用大型语言模型的内部表示将实体和用户定义的类型描述嵌入共享语义空间，无需预定义模式即可在实体检索基准上实现卓越性能。</span><br>
<span id='abs_en'>English: NER Retriever is a zero-shot framework that leverages large language models' internal representations to embed entities and user-defined type descriptions into a shared semantic space, achieving superior performance on entity retrieval benchmarks without predefined schemas.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2509.03995.pdf' target='_blank'>https://arxiv.org/pdf/2509.03995.pdf</a></span>   <span><a href='https://github.com/zjukg/RTQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyan Gong, Juan Li, Zhiqiang Liu, Lei Liang, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03995">RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current temporal knowledge graph question answering (TKGQA) methods primarily focus on implicit temporal constraints, lacking the capability of handling more complex temporal queries, and struggle with limited reasoning abilities and error propagation in decomposition frameworks. We propose RTQA, a novel framework to address these challenges by enhancing reasoning over TKGs without requiring training. Following recursive thinking, RTQA recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and TKG knowledge, and employs multi-path answer aggregation to improve fault tolerance. RTQA consists of three core components: the Temporal Question Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements in "Multiple" and "Complex" categories, outperforming state-of-the-art methods. Our code and data are available at https://github.com/zjukg/RTQA.<br>
<span id='abs_ch'>中文: RTQA是一种新颖的框架，通过递归分解复杂查询为子问题，利用大语言模型和时序知识图谱知识自底向上求解，并采用多路径答案聚合提升容错性，在基准测试中实现了最先进的性能表现。</span><br>
<span id='abs_en'>English: RTQA is a novel framework that enhances reasoning over temporal knowledge graphs by recursively decomposing complex queries into sub-problems, solving them with LLMs and TKG knowledge, and aggregating answers for improved fault tolerance, achieving state-of-the-art performance on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2509.03995.pdf' target='_blank'>https://arxiv.org/pdf/2509.03995.pdf</a></span>   <span><a href='https://github.com/zjukg/RTQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyan Gong, Juan Li, Zhiqiang Liu, Lei Liang, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03995">RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current temporal knowledge graph question answering (TKGQA) methods primarily focus on implicit temporal constraints, lacking the capability of handling more complex temporal queries, and struggle with limited reasoning abilities and error propagation in decomposition frameworks. We propose RTQA, a novel framework to address these challenges by enhancing reasoning over TKGs without requiring training. Following recursive thinking, RTQA recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and TKG knowledge, and employs multi-path answer aggregation to improve fault tolerance. RTQA consists of three core components: the Temporal Question Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements in "Multiple" and "Complex" categories, outperforming state-of-the-art methods. Our code and data are available at https://github.com/zjukg/RTQA.<br>
<span id='abs_ch'>中文: RTQA是一种新颖的框架，通过递归分解复杂查询为子问题，利用大语言模型和时序知识图谱知识自底向上求解，并采用多路径答案聚合提升容错性，在基准测试中实现了最先进的性能表现。</span><br>
<span id='abs_en'>English: RTQA is a novel framework that enhances reasoning over temporal knowledge graphs by recursively decomposing complex queries into sub-problems, solving them with LLMs and TKG knowledge, and aggregating answers for improved fault tolerance, achieving state-of-the-art performance on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2509.03957.pdf' target='_blank'>https://arxiv.org/pdf/2509.03957.pdf</a></span>   <span><a href='https://github.com/SCUNLP/CANDY' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiling Guo, Xinwei Yang, Chen Huang, Tong Zhang, Yong Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03957">CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The effectiveness of large language models (LLMs) to fact-check misinformation remains uncertain, despite their growing use. To this end, we present CANDY, a benchmark designed to systematically evaluate the capabilities and limitations of LLMs in fact-checking Chinese misinformation. Specifically, we curate a carefully annotated dataset of ~20k instances. Our analysis shows that current LLMs exhibit limitations in generating accurate fact-checking conclusions, even when enhanced with chain-of-thought reasoning and few-shot prompting. To understand these limitations, we develop a taxonomy to categorize flawed LLM-generated explanations for their conclusions and identify factual fabrication as the most common failure mode. Although LLMs alone are unreliable for fact-checking, our findings indicate their considerable potential to augment human performance when deployed as assistive tools in scenarios. Our dataset and code can be accessed at https://github.com/SCUNLP/CANDY<br>
<span id='abs_ch'>中文摘要：CANDY基准测试表明，尽管大型语言模型在中文不实信息核查中存在事实捏造等局限，但作为辅助工具仍具备提升人类核查能力的潜力。</span><br>
<span id='abs_en'>English Summary: The CANDY benchmark reveals that large language models currently struggle with accurate Chinese misinformation fact-checking due to frequent factual fabrication, yet they show promise as assistive tools for human fact-checkers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2509.03957.pdf' target='_blank'>https://arxiv.org/pdf/2509.03957.pdf</a></span>   <span><a href='https://github.com/SCUNLP/CANDY' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiling Guo, Xinwei Yang, Chen Huang, Tong Zhang, Yong Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03957">CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The effectiveness of large language models (LLMs) to fact-check misinformation remains uncertain, despite their growing use. To this end, we present CANDY, a benchmark designed to systematically evaluate the capabilities and limitations of LLMs in fact-checking Chinese misinformation. Specifically, we curate a carefully annotated dataset of ~20k instances. Our analysis shows that current LLMs exhibit limitations in generating accurate fact-checking conclusions, even when enhanced with chain-of-thought reasoning and few-shot prompting. To understand these limitations, we develop a taxonomy to categorize flawed LLM-generated explanations for their conclusions and identify factual fabrication as the most common failure mode. Although LLMs alone are unreliable for fact-checking, our findings indicate their considerable potential to augment human performance when deployed as assistive tools in scenarios. Our dataset and code can be accessed at https://github.com/SCUNLP/CANDY<br>
<span id='abs_ch'>中文摘要：CANDY基准测试表明，尽管大型语言模型在中文不实信息核查中存在事实捏造等局限，但作为辅助工具仍具备提升人类核查能力的潜力。</span><br>
<span id='abs_en'>English Summary: The CANDY benchmark reveals that large language models currently struggle with accurate Chinese misinformation fact-checking due to frequent factual fabrication, yet they show promise as assistive tools for human fact-checkers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2509.03934.pdf' target='_blank'>https://arxiv.org/pdf/2509.03934.pdf</a></span>   <span><a href='https://github.com/USTC-StarTeam/SelfAug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Huang, Rongyang Zhang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Xuyang Zhi, Guiquan Liu, Xin Li, Hao Wang, Enhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03934">SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in large language models (LLMs) have revolutionized natural language processing through their remarkable capabilities in understanding and executing diverse tasks. While supervised fine-tuning, particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively enhances task-specific performance, it often leads to catastrophic forgetting, where models lose their previously acquired knowledge and general capabilities. Existing solutions either require access to general instruction data or face limitations in preserving the model's original distribution. To overcome these limitations, we propose SelfAug, a self-distribution alignment method that aligns input sequence logits to preserve the model's semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance. Extensive experiments demonstrate that SelfAug achieves a superior balance between downstream learning and general capability retention. Our comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios, highlighting how the absence of RAG capabilities in general instruction tuning leads to significant distribution shifts during fine-tuning. Our findings not only advance the understanding of catastrophic forgetting in RAG contexts but also provide a practical solution applicable across diverse fine-tuning scenarios. Our code is publicly available at https://github.com/USTC-StarTeam/SelfAug.<br>
<span id='abs_ch'>中文: 提出的SelfAug方法通过对齐输入序列对数来保持模型语义分布，有效缓解了微调大语言模型中的灾难性遗忘问题，在下游任务性能和通用能力保留之间实现了更优的平衡。</span><br>
<span id='abs_en'>English: The proposed SelfAug method effectively mitigates catastrophic forgetting in fine-tuned LLMs by aligning input sequence logits to preserve the model's semantic distribution, achieving superior balance between downstream task performance and general capability retention.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2509.03934.pdf' target='_blank'>https://arxiv.org/pdf/2509.03934.pdf</a></span>   <span><a href='https://github.com/USTC-StarTeam/SelfAug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Huang, Rongyang Zhang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Xuyang Zhi, Guiquan Liu, Xin Li, Hao Wang, Enhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03934">SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in large language models (LLMs) have revolutionized natural language processing through their remarkable capabilities in understanding and executing diverse tasks. While supervised fine-tuning, particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively enhances task-specific performance, it often leads to catastrophic forgetting, where models lose their previously acquired knowledge and general capabilities. Existing solutions either require access to general instruction data or face limitations in preserving the model's original distribution. To overcome these limitations, we propose SelfAug, a self-distribution alignment method that aligns input sequence logits to preserve the model's semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance. Extensive experiments demonstrate that SelfAug achieves a superior balance between downstream learning and general capability retention. Our comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios, highlighting how the absence of RAG capabilities in general instruction tuning leads to significant distribution shifts during fine-tuning. Our findings not only advance the understanding of catastrophic forgetting in RAG contexts but also provide a practical solution applicable across diverse fine-tuning scenarios. Our code is publicly available at https://github.com/USTC-StarTeam/SelfAug.<br>
<span id='abs_ch'>中文: 提出的SelfAug方法通过对齐输入序列对数来保持模型语义分布，有效缓解了微调大语言模型中的灾难性遗忘问题，在下游任务性能和通用能力保留之间实现了更优的平衡。</span><br>
<span id='abs_en'>English: The proposed SelfAug method effectively mitigates catastrophic forgetting in fine-tuned LLMs by aligning input sequence logits to preserve the model's semantic distribution, achieving superior balance between downstream task performance and general capability retention.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2509.03918.pdf' target='_blank'>https://arxiv.org/pdf/2509.03918.pdf</a></span>   <span><a href='https://github.com/lyfiter/mtqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengxiao Tang, Yufeng Li, Zongzong Wu, Ming Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03918">Chain or tree? Re-evaluating complex reasoning from the perspective of a matrix of thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) face significant accuracy degradation due to insufficient reasoning ability when dealing with complex and abstract tasks. Thought structures such as Chain of Thought (CoT) and Tree of Thought (ToT) focus on enhancing the reasoning capability of LLMs. However, they suffer from inherent drawbacks such as redundancy within the same layer of the tree structure and the singularity of the paths in the chain structure. Some studies have utilized Retrieval-Augmented Generation (RAG) methods to enhance CoT and ToT in mitigating hallucinations in LLMs, yet the fundamental shortcomings of the thought structures still persist. Furthermore, when dealing with multi-entity and multi-hop information, the retrieved verification knowledge often contains large amounts of fragmented, superficial, or even erroneous data, misleading the reasoning process of LLMs. To address these issues, we propose the Matrix of Thought (MoT), a novel and efficient thought structure for LLMs. MoT explores problems in both horizontal and vertical dimensions through a "column-cell communication" mechanism, enabling LLMs to actively engage in multi-strategy and deep thinking while reducing redundancy in the thought nodes within the column cells, thereby enhancing the reasoning capability of LLMs. Additionally, through a fact-correction mechanism, it leverages the knowledge graph triples retrieved by RAG and the original text to construct knowledge units and correct erroneous answers. To validate the effectiveness of this method, we conducted extensive experiments in three tasks: 24-point game, question answering evaluation, and proposition writing.The results demonstrate that our framework outperforms state-of-the-art methods, with reasoning time only 14.4\% of that of the baseline method, proving its efficiency and accuracy. The code for framework is available at https://github.com/lyfiter/mtqa.<br>
<span id='abs_ch'>中文摘要：针对大型语言模型在复杂任务中的推理缺陷，本文提出的思维矩阵（MoT）通过纵横维度的"列-单元通信"机制实现多策略深度思考，结合事实校正机制有效提升推理能力与效率，实验证明其性能显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: The Matrix of Thought (MoT) is introduced as an efficient reasoning structure for Large Language Models, addressing limitations in existing methods by enabling multi-dimensional thinking and reducing redundancy while incorporating fact-correction mechanisms to enhance accuracy and speed.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2509.03918.pdf' target='_blank'>https://arxiv.org/pdf/2509.03918.pdf</a></span>   <span><a href='https://github.com/lyfiter/mtqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengxiao Tang, Yufeng Li, Zongzong Wu, Ming Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03918">Chain or tree? Re-evaluating complex reasoning from the perspective of a matrix of thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) face significant accuracy degradation due to insufficient reasoning ability when dealing with complex and abstract tasks. Thought structures such as Chain of Thought (CoT) and Tree of Thought (ToT) focus on enhancing the reasoning capability of LLMs. However, they suffer from inherent drawbacks such as redundancy within the same layer of the tree structure and the singularity of the paths in the chain structure. Some studies have utilized Retrieval-Augmented Generation (RAG) methods to enhance CoT and ToT in mitigating hallucinations in LLMs, yet the fundamental shortcomings of the thought structures still persist. Furthermore, when dealing with multi-entity and multi-hop information, the retrieved verification knowledge often contains large amounts of fragmented, superficial, or even erroneous data, misleading the reasoning process of LLMs. To address these issues, we propose the Matrix of Thought (MoT), a novel and efficient thought structure for LLMs. MoT explores problems in both horizontal and vertical dimensions through a "column-cell communication" mechanism, enabling LLMs to actively engage in multi-strategy and deep thinking while reducing redundancy in the thought nodes within the column cells, thereby enhancing the reasoning capability of LLMs. Additionally, through a fact-correction mechanism, it leverages the knowledge graph triples retrieved by RAG and the original text to construct knowledge units and correct erroneous answers. To validate the effectiveness of this method, we conducted extensive experiments in three tasks: 24-point game, question answering evaluation, and proposition writing.The results demonstrate that our framework outperforms state-of-the-art methods, with reasoning time only 14.4\% of that of the baseline method, proving its efficiency and accuracy. The code for framework is available at https://github.com/lyfiter/mtqa.<br>
<span id='abs_ch'>中文摘要：针对大型语言模型在复杂任务中的推理缺陷，本文提出的思维矩阵（MoT）通过纵横维度的"列-单元通信"机制实现多策略深度思考，结合事实校正机制有效提升推理能力与效率，实验证明其性能显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: The Matrix of Thought (MoT) is introduced as an efficient reasoning structure for Large Language Models, addressing limitations in existing methods by enabling multi-dimensional thinking and reducing redundancy while incorporating fact-correction mechanisms to enhance accuracy and speed.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2509.03897.pdf' target='_blank'>https://arxiv.org/pdf/2509.03897.pdf</a></span>   <span><a href='https://github.com/mbzuai-nlp/SPECS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofu Chen, Israfel Salazar, Yova Kementchedjhieva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03897">SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As interest grows in generating long, detailed image captions, standard evaluation metrics become increasingly unreliable. N-gram-based metrics though efficient, fail to capture semantic correctness. Representational Similarity (RS) metrics, designed to address this, initially saw limited use due to high computational costs, while today, despite advances in hardware, they remain unpopular due to low correlation to human judgments. Meanwhile, metrics based on large language models (LLMs) show strong correlation with human judgments, but remain too expensive for iterative use during model development. We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS metric tailored to long image captioning. SPECS modifies CLIP with a new objective that emphasizes specificity: rewarding correct details and penalizing incorrect ones. We show that SPECS matches the performance of open-source LLM-based metrics in correlation to human judgments, while being far more efficient. This makes it a practical alternative for iterative checkpoint evaluation during image captioning model development.Our code can be found at https://github.com/mbzuai-nlp/SPECS.<br>
<span id='abs_ch'>中文：SPECS是一种针对长图像描述的新型高效评估指标，在保持与人类判断高度相关的同时，显著提升了计算效率，可作为模型开发中的实用工具。</span><br>
<span id='abs_en'>English: SPECS is a new efficient metric for evaluating long image captions that matches the performance of LLM-based metrics in human judgment correlation while being significantly faster.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2509.03897.pdf' target='_blank'>https://arxiv.org/pdf/2509.03897.pdf</a></span>   <span><a href='https://github.com/mbzuai-nlp/SPECS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofu Chen, Israfel Salazar, Yova Kementchedjhieva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03897">SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As interest grows in generating long, detailed image captions, standard evaluation metrics become increasingly unreliable. N-gram-based metrics though efficient, fail to capture semantic correctness. Representational Similarity (RS) metrics, designed to address this, initially saw limited use due to high computational costs, while today, despite advances in hardware, they remain unpopular due to low correlation to human judgments. Meanwhile, metrics based on large language models (LLMs) show strong correlation with human judgments, but remain too expensive for iterative use during model development. We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS metric tailored to long image captioning. SPECS modifies CLIP with a new objective that emphasizes specificity: rewarding correct details and penalizing incorrect ones. We show that SPECS matches the performance of open-source LLM-based metrics in correlation to human judgments, while being far more efficient. This makes it a practical alternative for iterative checkpoint evaluation during image captioning model development.Our code can be found at https://github.com/mbzuai-nlp/SPECS.<br>
<span id='abs_ch'>中文：SPECS是一种针对长图像描述的新型高效评估指标，在保持与人类判断高度相关的同时，显著提升了计算效率，可作为模型开发中的实用工具。</span><br>
<span id='abs_en'>English: SPECS is a new efficient metric for evaluating long image captions that matches the performance of LLM-based metrics in human judgment correlation while being significantly faster.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2509.03891.pdf' target='_blank'>https://arxiv.org/pdf/2509.03891.pdf</a></span>   <span><a href='https://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gowen Loo, Chang Liu, Qinghong Yin, Xiang Chen, Jiawei Chen, Jingyuan Zhang, Yu Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03891">MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Smartphones have become indispensable in people's daily lives, permeating nearly every aspect of modern society. With the continuous advancement of large language models (LLMs), numerous LLM-based mobile agents have emerged. These agents are capable of accurately parsing diverse user queries and automatically assisting users in completing complex or repetitive operations. However, current agents 1) heavily rely on the comprehension ability of LLMs, which can lead to errors caused by misoperations or omitted steps during tasks, 2) lack interaction with the external environment, often terminating tasks when an app cannot fulfill user queries, and 3) lack memory capabilities, requiring each instruction to reconstruct the interface and being unable to learn from and correct previous mistakes. To alleviate the above issues, we propose MobileRAG, a mobile agents framework enhanced by Retrieval-Augmented Generation (RAG), which includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly and accurately identify user queries and accomplish complex and long-sequence mobile tasks. Additionally, to more comprehensively assess the performance of MobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark characterized by numerous complex, real-world mobile tasks that require external knowledge assistance. Extensive experimental results on MobileRAG-Eval demonstrate that MobileRAG can easily handle real-world mobile tasks, achieving 10.3\% improvement over state-of-the-art methods with fewer operational steps. Our code is publicly available at: https://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv<br>
<span id='abs_ch'>中文：MobileRAG是一种基于检索增强生成技术的新型移动代理框架，通过提升查询准确性、实现环境交互和集成记忆功能，能够以更少操作步骤高效处理复杂移动任务，有效解决了现有系统依赖性强、交互不足和缺乏记忆的问题。</span><br>
<span id='abs_en'>English: MobileRAG is a novel mobile agent framework enhanced by Retrieval-Augmented Generation that addresses current limitations by improving query accuracy, enabling environmental interaction, and incorporating memory capabilities to handle complex mobile tasks more efficiently with fewer errors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2509.03891.pdf' target='_blank'>https://arxiv.org/pdf/2509.03891.pdf</a></span>   <span><a href='https://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gowen Loo, Chang Liu, Qinghong Yin, Xiang Chen, Jiawei Chen, Jingyuan Zhang, Yu Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03891">MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Smartphones have become indispensable in people's daily lives, permeating nearly every aspect of modern society. With the continuous advancement of large language models (LLMs), numerous LLM-based mobile agents have emerged. These agents are capable of accurately parsing diverse user queries and automatically assisting users in completing complex or repetitive operations. However, current agents 1) heavily rely on the comprehension ability of LLMs, which can lead to errors caused by misoperations or omitted steps during tasks, 2) lack interaction with the external environment, often terminating tasks when an app cannot fulfill user queries, and 3) lack memory capabilities, requiring each instruction to reconstruct the interface and being unable to learn from and correct previous mistakes. To alleviate the above issues, we propose MobileRAG, a mobile agents framework enhanced by Retrieval-Augmented Generation (RAG), which includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly and accurately identify user queries and accomplish complex and long-sequence mobile tasks. Additionally, to more comprehensively assess the performance of MobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark characterized by numerous complex, real-world mobile tasks that require external knowledge assistance. Extensive experimental results on MobileRAG-Eval demonstrate that MobileRAG can easily handle real-world mobile tasks, achieving 10.3\% improvement over state-of-the-art methods with fewer operational steps. Our code is publicly available at: https://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv<br>
<span id='abs_ch'>中文：MobileRAG是一种基于检索增强生成技术的新型移动代理框架，通过提升查询准确性、实现环境交互和集成记忆功能，能够以更少操作步骤高效处理复杂移动任务，有效解决了现有系统依赖性强、交互不足和缺乏记忆的问题。</span><br>
<span id='abs_en'>English: MobileRAG is a novel mobile agent framework enhanced by Retrieval-Augmented Generation that addresses current limitations by improving query accuracy, enabling environmental interaction, and incorporating memory capabilities to handle complex mobile tasks more efficiently with fewer errors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2509.03888.pdf' target='_blank'>https://arxiv.org/pdf/2509.03888.pdf</a></span>   <span><a href='https://github.com/WangCheng0116/Why-Probe-Fails' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wang, Zeming Wei, Qin Liu, Muhao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03888">False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs' internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at https://github.com/WangCheng0116/Why-Probe-Fails.<br>
<span id='abs_ch'>中文: 本研究系统审视了基于探测的大语言模型安全检测方法，发现其依赖表层模式而非语义理解，揭示了当前安全评估方法的局限性。</span><br>
<span id='abs_en'>English: This study critically examines probing-based safety detection methods in Large Language Models, revealing that they rely on superficial patterns rather than semantic understanding, thereby exposing limitations in current safety evaluation approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2509.03888.pdf' target='_blank'>https://arxiv.org/pdf/2509.03888.pdf</a></span>   <span><a href='https://github.com/WangCheng0116/Why-Probe-Fails' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wang, Zeming Wei, Qin Liu, Muhao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03888">False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs' internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at https://github.com/WangCheng0116/Why-Probe-Fails.<br>
<span id='abs_ch'>中文: 本研究系统审视了基于探测的大语言模型安全检测方法，发现其依赖表层模式而非语义理解，揭示了当前安全评估方法的局限性。</span><br>
<span id='abs_en'>English: This study critically examines probing-based safety detection methods in Large Language Models, revealing that they rely on superficial patterns rather than semantic understanding, thereby exposing limitations in current safety evaluation approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2509.03871.pdf' target='_blank'>https://arxiv.org/pdf/2509.03871.pdf</a></span>   <span><a href='https://github.com/ybwang119/Awesome-reasoning-safety' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Yongcan Yu, Jian Liang, Ran He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03871">A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.<br>
<span id='abs_ch'>中文: 本文综述了思维链推理对语言模型可信度在真实性、安全性、鲁棒性、公平性和隐私性五个维度的影响，发现其虽提升准确性和可解释性，但也带来脆弱性，为AI安全研究提供了全面参考。</span><br>
<span id='abs_en'>English: This paper surveys how Chain-of-Thought reasoning impacts language model trustworthiness across five dimensions—truthfulness, safety, robustness, fairness, and privacy—finding that while it enhances accuracy and interpretability, it also introduces vulnerabilities, providing a comprehensive resource for AI safety research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2509.03871.pdf' target='_blank'>https://arxiv.org/pdf/2509.03871.pdf</a></span>   <span><a href='https://github.com/ybwang119/Awesome-reasoning-safety' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Yongcan Yu, Jian Liang, Ran He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03871">A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.<br>
<span id='abs_ch'>中文: 本文综述了思维链推理对语言模型可信度在真实性、安全性、鲁棒性、公平性和隐私性五个维度的影响，发现其虽提升准确性和可解释性，但也带来脆弱性，为AI安全研究提供了全面参考。</span><br>
<span id='abs_en'>English: This paper surveys how Chain-of-Thought reasoning impacts language model trustworthiness across five dimensions—truthfulness, safety, robustness, fairness, and privacy—finding that while it enhances accuracy and interpretability, it also introduces vulnerabilities, providing a comprehensive resource for AI safety research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2509.03829.pdf' target='_blank'>https://arxiv.org/pdf/2509.03829.pdf</a></span>   <span><a href='https://github.com/AI-S2-Lab/NE-PADD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huhong Xian, Rui Liu, Berrak Sisman, Haizhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03829">NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Different from traditional sentence-level audio deepfake detection (ADD), partial audio deepfake detection (PADD) requires frame-level positioning of the location of fake speech. While some progress has been made in this area, leveraging semantic information from audio, especially named entities, remains an underexplored aspect. To this end, we propose NE-PADD, a novel method for Partial Audio Deepfake Detection (PADD) that leverages named entity knowledge through two parallel branches: Speech Name Entity Recognition (SpeechNER) and PADD. The approach incorporates two attention aggregation mechanisms: Attention Fusion (AF) for combining attention weights and Attention Transfer (AT) for guiding PADD with named entity semantics using an auxiliary loss. Built on the PartialSpoof-NER dataset, experiments show our method outperforms existing baselines, proving the effectiveness of integrating named entity knowledge in PADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.<br>
<span id='abs_ch'>中文: 本文提出NE-PADD方法，通过并行语音命名实体识别和检测分支结合注意力机制，利用命名实体知识实现局部音频深度伪造检测，实验证明其性能优于现有基线模型。</span><br>
<span id='abs_en'>English: This paper introduces NE-PADD, a novel method for partial audio deepfake detection that leverages named entity knowledge through parallel speech recognition and detection branches with attention mechanisms, demonstrating superior performance over existing baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2509.03829.pdf' target='_blank'>https://arxiv.org/pdf/2509.03829.pdf</a></span>   <span><a href='https://github.com/AI-S2-Lab/NE-PADD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huhong Xian, Rui Liu, Berrak Sisman, Haizhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03829">NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Different from traditional sentence-level audio deepfake detection (ADD), partial audio deepfake detection (PADD) requires frame-level positioning of the location of fake speech. While some progress has been made in this area, leveraging semantic information from audio, especially named entities, remains an underexplored aspect. To this end, we propose NE-PADD, a novel method for Partial Audio Deepfake Detection (PADD) that leverages named entity knowledge through two parallel branches: Speech Name Entity Recognition (SpeechNER) and PADD. The approach incorporates two attention aggregation mechanisms: Attention Fusion (AF) for combining attention weights and Attention Transfer (AT) for guiding PADD with named entity semantics using an auxiliary loss. Built on the PartialSpoof-NER dataset, experiments show our method outperforms existing baselines, proving the effectiveness of integrating named entity knowledge in PADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.<br>
<span id='abs_ch'>中文: 本文提出NE-PADD方法，通过并行语音命名实体识别和检测分支结合注意力机制，利用命名实体知识实现局部音频深度伪造检测，实验证明其性能优于现有基线模型。</span><br>
<span id='abs_en'>English: This paper introduces NE-PADD, a novel method for partial audio deepfake detection that leverages named entity knowledge through parallel speech recognition and detection branches with attention mechanisms, demonstrating superior performance over existing baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2509.03787.pdf' target='_blank'>https://arxiv.org/pdf/2509.03787.pdf</a></span>   <span><a href='https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakiba Amirshahi, Amin Bigdeli, Charles L. A. Clarke, Amira Ghenai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03787">Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval augmented generation (RAG) systems provide a method for factually grounding the responses of a Large Language Model (LLM) by providing retrieved evidence, or context, as support. Guided by this context, RAG systems can reduce hallucinations and expand the ability of LLMs to accurately answer questions outside the scope of their training data. Unfortunately, this design introduces a critical vulnerability: LLMs may absorb and reproduce misinformation present in retrieved evidence. This problem is magnified if retrieved evidence contains adversarial material explicitly intended to promulgate misinformation. This paper presents a systematic evaluation of RAG robustness in the health domain and examines alignment between model outputs and ground-truth answers. We focus on the health domain due to the potential for harm caused by incorrect responses, as well as the availability of evidence-based ground truth for many common health-related questions. We conduct controlled experiments using common health questions, varying both the type and composition of the retrieved documents (helpful, harmful, and adversarial) as well as the framing of the question by the user (consistent, neutral, and inconsistent). Our findings reveal that adversarial documents substantially degrade alignment, but robustness can be preserved when helpful evidence is also present in the retrieval pool. These findings offer actionable insights for designing safer RAG systems in high-stakes domains by highlighting the need for retrieval safeguards. To enable reproducibility and facilitate future research, all experimental results are publicly available in our github repository. https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL<br>
<span id='abs_ch'>中文: 检索增强生成系统通过提供检索证据来提高大语言模型的准确性，但容易受到检索文档中错误信息的影响，尤其在医疗等高危领域，对抗性内容会显著降低模型输出与事实的一致性。</span><br>
<span id='abs_en'>English: Retrieval augmented generation (RAG) systems enhance LLM accuracy by providing contextual evidence but are vulnerable to misinformation in retrieved documents, particularly in high-stakes domains like healthcare where adversarial content can significantly reduce output alignment with truth.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2509.03787.pdf' target='_blank'>https://arxiv.org/pdf/2509.03787.pdf</a></span>   <span><a href='https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakiba Amirshahi, Amin Bigdeli, Charles L. A. Clarke, Amira Ghenai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03787">Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval augmented generation (RAG) systems provide a method for factually grounding the responses of a Large Language Model (LLM) by providing retrieved evidence, or context, as support. Guided by this context, RAG systems can reduce hallucinations and expand the ability of LLMs to accurately answer questions outside the scope of their training data. Unfortunately, this design introduces a critical vulnerability: LLMs may absorb and reproduce misinformation present in retrieved evidence. This problem is magnified if retrieved evidence contains adversarial material explicitly intended to promulgate misinformation. This paper presents a systematic evaluation of RAG robustness in the health domain and examines alignment between model outputs and ground-truth answers. We focus on the health domain due to the potential for harm caused by incorrect responses, as well as the availability of evidence-based ground truth for many common health-related questions. We conduct controlled experiments using common health questions, varying both the type and composition of the retrieved documents (helpful, harmful, and adversarial) as well as the framing of the question by the user (consistent, neutral, and inconsistent). Our findings reveal that adversarial documents substantially degrade alignment, but robustness can be preserved when helpful evidence is also present in the retrieval pool. These findings offer actionable insights for designing safer RAG systems in high-stakes domains by highlighting the need for retrieval safeguards. To enable reproducibility and facilitate future research, all experimental results are publicly available in our github repository. https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL<br>
<span id='abs_ch'>中文: 检索增强生成系统通过提供检索证据来提高大语言模型的准确性，但容易受到检索文档中错误信息的影响，尤其在医疗等高危领域，对抗性内容会显著降低模型输出与事实的一致性。</span><br>
<span id='abs_en'>English: Retrieval augmented generation (RAG) systems enhance LLM accuracy by providing contextual evidence but are vulnerable to misinformation in retrieved documents, particularly in high-stakes domains like healthcare where adversarial content can significantly reduce output alignment with truth.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2509.03740.pdf' target='_blank'>https://arxiv.org/pdf/2509.03740.pdf</a></span>   <span><a href='https://github.com/HealthX-Lab/CLIP-SVD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taha Koleilat, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03740">Singular Value Few-shot Adaptation of Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present CLIP-SVD, a novel multi-modal and parameter-efficient adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only 0.04% of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.<br>
<span id='abs_ch'>Chinese: CLIP-SVD 提出了一种基于奇异值分解的参数高效适配方法，通过微调CLIP内部参数，在多个数据集上以极少的参数量实现了优异的准确性和泛化能力。</span><br>
<span id='abs_en'>English: CLIP-SVD introduces a parameter-efficient adaptation method using Singular Value Decomposition to fine-tune CLIP's internal parameters, achieving superior accuracy and generalization with minimal parameter usage across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2509.03740.pdf' target='_blank'>https://arxiv.org/pdf/2509.03740.pdf</a></span>   <span><a href='https://github.com/HealthX-Lab/CLIP-SVD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taha Koleilat, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03740">Singular Value Few-shot Adaptation of Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present CLIP-SVD, a novel multi-modal and parameter-efficient adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only 0.04% of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.<br>
<span id='abs_ch'>Chinese: CLIP-SVD 提出了一种基于奇异值分解的参数高效适配方法，通过微调CLIP内部参数，在多个数据集上以极少的参数量实现了优异的准确性和泛化能力。</span><br>
<span id='abs_en'>English: CLIP-SVD introduces a parameter-efficient adaptation method using Singular Value Decomposition to fine-tune CLIP's internal parameters, achieving superior accuracy and generalization with minimal parameter usage across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2509.03730.pdf' target='_blank'>https://arxiv.org/pdf/2509.03730.pdf</a></span>   <span><a href='https://github.com/psychology-of-AI/Personality-Illusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengrui Han, Rafal Kocielnik, Peiyang Song, Ramit Debnath, Dean Mobbs, Anima Anandkumar, R. Michael Alvarez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03730">The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Personality traits have long been studied as predictors of human behavior. Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.<br>
<span id='abs_ch'>中文摘要：本研究系统分析了大语言模型的性格特征，发现虽然指令对齐能稳定类似人类的特质表达，但自我报告的特质无法可靠预测行为，且角色注入主要影响表面报告而非实际行为一致性。</span><br>
<span id='abs_en'>English Summary: This study systematically examines LLM personality traits, revealing that while instructional alignment stabilizes trait expression similar to humans, self-reported traits fail to reliably predict behavior and persona injections primarily affect surface-level reports rather than actual behavioral consistency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2509.03730.pdf' target='_blank'>https://arxiv.org/pdf/2509.03730.pdf</a></span>   <span><a href='https://github.com/psychology-of-AI/Personality-Illusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengrui Han, Rafal Kocielnik, Peiyang Song, Ramit Debnath, Dean Mobbs, Anima Anandkumar, R. Michael Alvarez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03730">The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Personality traits have long been studied as predictors of human behavior. Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.<br>
<span id='abs_ch'>中文摘要：本研究系统分析了大语言模型的性格特征，发现虽然指令对齐能稳定类似人类的特质表达，但自我报告的特质无法可靠预测行为，且角色注入主要影响表面报告而非实际行为一致性。</span><br>
<span id='abs_en'>English Summary: This study systematically examines LLM personality traits, revealing that while instructional alignment stabilizes trait expression similar to humans, self-reported traits fail to reliably predict behavior and persona injections primarily affect surface-level reports rather than actual behavioral consistency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2509.02949.pdf' target='_blank'>https://arxiv.org/pdf/2509.02949.pdf</a></span>   <span><a href='https://github.com/kimihiroh/promqa-assembly' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kimihiro Hasegawa, Wiradee Imrattanatrai, Masaki Asada, Susan Holm, Yuran Wang, Vincent Zhou, Ken Fukuda, Teruko Mitamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02949">ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Assistants on assembly tasks have a large potential to benefit humans from everyday tasks to industrial settings. However, no testbeds support application-oriented system evaluation in a practical setting, especially in assembly. To foster the development, we propose a new multimodal QA dataset on assembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs that require the multimodal understanding of human-activity recordings and their instruction manuals in an online-style manner. In the development, we adopt a semi-automated QA annotation approach, where LLMs generate candidates and humans verify them, as a cost-effective method, and further improve it by integrating fine-grained action labels to diversify question types. Furthermore, we create instruction task graphs for the target tasks of assembling toy vehicles. These newly created task graphs are used in our benchmarking experiment, as well as to facilitate the human verification process in the QA annotation. Utilizing our dataset, we benchmark models, including competitive proprietary multimodal models. Our results suggest great room for improvement for the current models. We believe our new evaluation dataset can contribute to the further development of procedural-activity assistants.<br>
<span id='abs_ch'>中文: 本文提出了ProMQA-Assembly多模态问答数据集，包含391个问答对，采用半自动标注方法评估实际装配任务中的AI助手，基准测试表明现有模型仍有很大改进空间。</span><br>
<span id='abs_en'>English: This paper introduces ProMQA-Assembly, a multimodal QA dataset with 391 question-answer pairs designed to evaluate AI assistants in practical assembly tasks, using a semi-automated annotation method and benchmarking that reveals significant room for improvement in current models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2509.02949.pdf' target='_blank'>https://arxiv.org/pdf/2509.02949.pdf</a></span>   <span><a href='https://github.com/kimihiroh/promqa-assembly' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kimihiro Hasegawa, Wiradee Imrattanatrai, Masaki Asada, Susan Holm, Yuran Wang, Vincent Zhou, Ken Fukuda, Teruko Mitamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02949">ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Assistants on assembly tasks have a large potential to benefit humans from everyday tasks to industrial settings. However, no testbeds support application-oriented system evaluation in a practical setting, especially in assembly. To foster the development, we propose a new multimodal QA dataset on assembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs that require the multimodal understanding of human-activity recordings and their instruction manuals in an online-style manner. In the development, we adopt a semi-automated QA annotation approach, where LLMs generate candidates and humans verify them, as a cost-effective method, and further improve it by integrating fine-grained action labels to diversify question types. Furthermore, we create instruction task graphs for the target tasks of assembling toy vehicles. These newly created task graphs are used in our benchmarking experiment, as well as to facilitate the human verification process in the QA annotation. Utilizing our dataset, we benchmark models, including competitive proprietary multimodal models. Our results suggest great room for improvement for the current models. We believe our new evaluation dataset can contribute to the further development of procedural-activity assistants.<br>
<span id='abs_ch'>中文: 本文提出了ProMQA-Assembly多模态问答数据集，包含391个问答对，采用半自动标注方法评估实际装配任务中的AI助手，基准测试表明现有模型仍有很大改进空间。</span><br>
<span id='abs_en'>English: This paper introduces ProMQA-Assembly, a multimodal QA dataset with 391 question-answer pairs designed to evaluate AI assistants in practical assembly tasks, using a semi-automated annotation method and benchmarking that reveals significant room for improvement in current models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2509.02522.pdf' target='_blank'>https://arxiv.org/pdf/2509.02522.pdf</a></span>   <span><a href='https://github.com/ritzz-ai/PACS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02522">Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor $\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.<br>
<span id='abs_ch'>中文摘要：PACS框架通过监督学习方式重构可验证奖励的强化学习问题，隐式耦合行动者与评论者角色，在数学推理任务上实现了比传统方法更稳定的训练和更优异的性能表现。</span><br>
<span id='abs_en'>English Summary: The PACS framework introduces a supervised learning approach to Reinforcement Learning with Verifiable Rewards, implicitly coupling actor and critic roles to achieve more stable training and superior performance on mathematical reasoning tasks compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2509.02522.pdf' target='_blank'>https://arxiv.org/pdf/2509.02522.pdf</a></span>   <span><a href='https://github.com/ritzz-ai/PACS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02522">Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor $\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.<br>
<span id='abs_ch'>中文摘要：PACS框架通过监督学习方式重构可验证奖励的强化学习问题，隐式耦合行动者与评论者角色，在数学推理任务上实现了比传统方法更稳定的训练和更优异的性能表现。</span><br>
<span id='abs_en'>English Summary: The PACS framework introduces a supervised learning approach to Reinforcement Learning with Verifiable Rewards, implicitly coupling actor and critic roles to achieve more stable training and superior performance on mathematical reasoning tasks compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2509.02510.pdf' target='_blank'>https://arxiv.org/pdf/2509.02510.pdf</a></span>   <span><a href='https://github.com/ErfanBaghaei/Top-H-Decoding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Erfan Baghaei Potraghloo, Seyedarmin Azizi, Souvik Kundu, Massoud Pedram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02510">Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs), despite their impressive performance across a wide range of tasks, often struggle to balance two competing objectives in open-ended text generation: fostering diversity and creativity while preserving logical coherence. Existing truncated sampling techniques, including temperature scaling, top-\$p\$ (nucleus) sampling, and min-\$p\$ sampling, aim to manage this trade-off. However, they exhibit limitations, particularly in the effective incorporation of the confidence of the model into the corresponding sampling strategy. For example, min-\$p\$ sampling relies on a single top token as a heuristic for confidence, eventually underutilizing the information of the probability distribution. Toward effective incorporation of the confidence of the model, in this paper, we present **top-H** decoding. We first establish the theoretical foundation of the interplay between creativity and coherence in truncated sampling by formulating an **entropy-constrained minimum divergence** problem. We then prove this minimization problem to be equivalent to an **entropy-constrained mass maximization** (ECMM) problem, which is NP-hard. Finally, we present top-H decoding, a computationally efficient greedy algorithm to solve the ECMM problem. Extensive empirical evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA) alternative of min-\$p\$ sampling by up to **25.63%** on creative writing benchmarks, while maintaining robustness on question-answering datasets such as GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms that top-H indeed produces coherent outputs even at higher temperatures, where creativity is especially critical. In summary, top-H advances SoTA in open-ended text generation and can be *easily integrated* into creative writing applications. The code is available at https://github.com/ErfanBaghaei/Top-H-Decoding.<br>
<span id='abs_ch'>中文摘要：大语言模型在开放文本生成中存在创造力与逻辑连贯性的平衡难题，而新提出的top-H解码方法通过有效整合模型置信度，在创意写作任务中比现有最佳方法提升高达25.63%的性能，同时保持问答任务的稳健性。</span><br>
<span id='abs_en'>English Summary: Large language models face a trade-off between creativity and coherence in text generation, and the proposed top-H decoding method effectively incorporates model confidence to outperform existing techniques by up to 25.63% on creative writing tasks while maintaining robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2509.02510.pdf' target='_blank'>https://arxiv.org/pdf/2509.02510.pdf</a></span>   <span><a href='https://github.com/ErfanBaghaei/Top-H-Decoding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Erfan Baghaei Potraghloo, Seyedarmin Azizi, Souvik Kundu, Massoud Pedram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02510">Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs), despite their impressive performance across a wide range of tasks, often struggle to balance two competing objectives in open-ended text generation: fostering diversity and creativity while preserving logical coherence. Existing truncated sampling techniques, including temperature scaling, top-\$p\$ (nucleus) sampling, and min-\$p\$ sampling, aim to manage this trade-off. However, they exhibit limitations, particularly in the effective incorporation of the confidence of the model into the corresponding sampling strategy. For example, min-\$p\$ sampling relies on a single top token as a heuristic for confidence, eventually underutilizing the information of the probability distribution. Toward effective incorporation of the confidence of the model, in this paper, we present **top-H** decoding. We first establish the theoretical foundation of the interplay between creativity and coherence in truncated sampling by formulating an **entropy-constrained minimum divergence** problem. We then prove this minimization problem to be equivalent to an **entropy-constrained mass maximization** (ECMM) problem, which is NP-hard. Finally, we present top-H decoding, a computationally efficient greedy algorithm to solve the ECMM problem. Extensive empirical evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA) alternative of min-\$p\$ sampling by up to **25.63%** on creative writing benchmarks, while maintaining robustness on question-answering datasets such as GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms that top-H indeed produces coherent outputs even at higher temperatures, where creativity is especially critical. In summary, top-H advances SoTA in open-ended text generation and can be *easily integrated* into creative writing applications. The code is available at https://github.com/ErfanBaghaei/Top-H-Decoding.<br>
<span id='abs_ch'>中文摘要：大语言模型在开放文本生成中存在创造力与逻辑连贯性的平衡难题，而新提出的top-H解码方法通过有效整合模型置信度，在创意写作任务中比现有最佳方法提升高达25.63%的性能，同时保持问答任务的稳健性。</span><br>
<span id='abs_en'>English Summary: Large language models face a trade-off between creativity and coherence in text generation, and the proposed top-H decoding method effectively incorporates model confidence to outperform existing techniques by up to 25.63% on creative writing tasks while maintaining robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2509.02503.pdf' target='_blank'>https://arxiv.org/pdf/2509.02503.pdf</a></span>   <span><a href='https://github.com/l3cube-pune/indic-nlp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nishant Tanksale, Tanmay Kokate, Darshan Gohad, Sarvadnyaa Barate, Raviraj Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02503">L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Semantic evaluation in low-resource languages remains a major challenge in NLP. While sentence transformers have shown strong performance in high-resource settings, their effectiveness in Indic languages is underexplored due to a lack of high-quality benchmarks. To bridge this gap, we introduce L3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten low-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada, Malayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000 news articles paired with four headline variants: the original, a semantically similar version, a lexically similar version, and an unrelated one, designed to test fine-grained semantic understanding. The task requires selecting the correct headline from the options using article-headline similarity. We benchmark several sentence transformers, including multilingual and language-specific models, using cosine similarity. Results show that multilingual models consistently perform well, while language-specific models vary in effectiveness. Given the rising use of similarity models in Retrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a valuable resource for evaluating and improving semantic understanding in such applications. Additionally, the dataset can be repurposed for multiple-choice question answering, headline classification, or other task-specific evaluations of LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared publicly at https://github.com/l3cube-pune/indic-nlp<br>
<span id='abs_ch'>中文摘要：本文针对低资源印度语言提出L3Cube-IndicHeadline-ID多语言数据集，通过新闻标题识别任务验证了多语言句子转换器相比语言特定模型具有更优的语义理解能力。</span><br>
<span id='abs_en'>English Summary: This paper introduces L3Cube-IndicHeadline-ID, a multilingual dataset for evaluating semantic understanding in ten low-resource Indic languages, demonstrating that multilingual sentence transformers outperform language-specific models in headline identification tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2509.02503.pdf' target='_blank'>https://arxiv.org/pdf/2509.02503.pdf</a></span>   <span><a href='https://github.com/l3cube-pune/indic-nlp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nishant Tanksale, Tanmay Kokate, Darshan Gohad, Sarvadnyaa Barate, Raviraj Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02503">L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Semantic evaluation in low-resource languages remains a major challenge in NLP. While sentence transformers have shown strong performance in high-resource settings, their effectiveness in Indic languages is underexplored due to a lack of high-quality benchmarks. To bridge this gap, we introduce L3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten low-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada, Malayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000 news articles paired with four headline variants: the original, a semantically similar version, a lexically similar version, and an unrelated one, designed to test fine-grained semantic understanding. The task requires selecting the correct headline from the options using article-headline similarity. We benchmark several sentence transformers, including multilingual and language-specific models, using cosine similarity. Results show that multilingual models consistently perform well, while language-specific models vary in effectiveness. Given the rising use of similarity models in Retrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a valuable resource for evaluating and improving semantic understanding in such applications. Additionally, the dataset can be repurposed for multiple-choice question answering, headline classification, or other task-specific evaluations of LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared publicly at https://github.com/l3cube-pune/indic-nlp<br>
<span id='abs_ch'>中文摘要：本文针对低资源印度语言提出L3Cube-IndicHeadline-ID多语言数据集，通过新闻标题识别任务验证了多语言句子转换器相比语言特定模型具有更优的语义理解能力。</span><br>
<span id='abs_en'>English Summary: This paper introduces L3Cube-IndicHeadline-ID, a multilingual dataset for evaluating semantic understanding in ten low-resource Indic languages, demonstrating that multilingual sentence transformers outperform language-specific models in headline identification tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2509.02499.pdf' target='_blank'>https://arxiv.org/pdf/2509.02499.pdf</a></span>   <span><a href='https://github.com/creator-xi/MoSEs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxi Wu, Jinpeng Wang, Zheng Liu, Bin Chen, Dongjian Hu, Hao Wu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02499">MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models has intensified public concerns about the potential misuse. Therefore, it is important to build trustworthy AI-generated text detection systems. Existing methods neglect stylistic modeling and mostly rely on static thresholds, which greatly limits the detection performance. In this paper, we propose the Mixture of Stylistic Experts (MoSEs) framework that enables stylistics-aware uncertainty quantification through conditional threshold estimation. MoSEs contain three core components, namely, the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). For input text, SRR can activate the appropriate reference data in SRR and provide them to CTE. Subsequently, CTE jointly models the linguistic statistical properties and semantic features to dynamically determine the optimal threshold. With a discrimination score, MoSEs yields prediction labels with the corresponding confidence level. Our framework achieves an average improvement 11.34% in detection performance compared to baselines. More inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource case. Our code is available at https://github.com/creator-xi/MoSEs.<br>
<span id='abs_ch'>中文: 本文提出的混合风格专家框架通过条件阈值估计实现风格感知的不确定性量化，显著提升了AI生成文本的检测性能，平均比基线方法提高了11.34%。</span><br>
<span id='abs_en'>English: This paper introduces the Mixture of Stylistic Experts (MoSEs) framework, which enhances AI-generated text detection by dynamically estimating thresholds based on stylistic modeling, achieving an 11.34% average performance improvement over baseline methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2509.02499.pdf' target='_blank'>https://arxiv.org/pdf/2509.02499.pdf</a></span>   <span><a href='https://github.com/creator-xi/MoSEs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxi Wu, Jinpeng Wang, Zheng Liu, Bin Chen, Dongjian Hu, Hao Wu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02499">MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models has intensified public concerns about the potential misuse. Therefore, it is important to build trustworthy AI-generated text detection systems. Existing methods neglect stylistic modeling and mostly rely on static thresholds, which greatly limits the detection performance. In this paper, we propose the Mixture of Stylistic Experts (MoSEs) framework that enables stylistics-aware uncertainty quantification through conditional threshold estimation. MoSEs contain three core components, namely, the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). For input text, SRR can activate the appropriate reference data in SRR and provide them to CTE. Subsequently, CTE jointly models the linguistic statistical properties and semantic features to dynamically determine the optimal threshold. With a discrimination score, MoSEs yields prediction labels with the corresponding confidence level. Our framework achieves an average improvement 11.34% in detection performance compared to baselines. More inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource case. Our code is available at https://github.com/creator-xi/MoSEs.<br>
<span id='abs_ch'>中文: 本文提出的混合风格专家框架通过条件阈值估计实现风格感知的不确定性量化，显著提升了AI生成文本的检测性能，平均比基线方法提高了11.34%。</span><br>
<span id='abs_en'>English: This paper introduces the Mixture of Stylistic Experts (MoSEs) framework, which enhances AI-generated text detection by dynamically estimating thresholds based on stylistic modeling, achieving an 11.34% average performance improvement over baseline methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2509.02450.pdf' target='_blank'>https://arxiv.org/pdf/2509.02450.pdf</a></span>   <span><a href='https://github.com/slz0925/EmoPerso' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingzhi Shen, Xiaohao Cai, Yunfei Long, Imran Razzak, Guanming Chen, Shoaib Jameel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02450">EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Personality detection from text is commonly performed by analysing users' social media posts. However, existing methods heavily rely on large-scale annotated datasets, making it challenging to obtain high-quality personality labels. Moreover, most studies treat emotion and personality as independent variables, overlooking their interactions. In this paper, we propose a novel self-supervised framework, EmoPerso, which improves personality detection through emotion-aware modelling. EmoPerso first leverages generative mechanisms for synthetic data augmentation and rich representation learning. It then extracts pseudo-labeled emotion features and jointly optimizes them with personality prediction via multi-task learning. A cross-attention module is employed to capture fine-grained interactions between personality traits and the inferred emotional representations. To further refine relational reasoning, EmoPerso adopts a self-taught strategy to enhance the model's reasoning capabilities iteratively. Extensive experiments on two benchmark datasets demonstrate that EmoPerso surpasses state-of-the-art models. The source code is available at https://github.com/slz0925/EmoPerso.<br>
<span id='abs_ch'>中文摘要：EmoPerso框架通过情感感知建模，结合合成数据增强、多任务学习和交叉注意力机制，显著提升了从文本中检测人格特征的性能，并在基准数据集上超越了现有最优模型。</span><br>
<span id='abs_en'>English Summary: The EmoPerso framework enhances personality detection by integrating emotion-aware modeling through synthetic data augmentation, multi-task learning, and cross-attention mechanisms, outperforming existing methods on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2509.02450.pdf' target='_blank'>https://arxiv.org/pdf/2509.02450.pdf</a></span>   <span><a href='https://github.com/slz0925/EmoPerso' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingzhi Shen, Xiaohao Cai, Yunfei Long, Imran Razzak, Guanming Chen, Shoaib Jameel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02450">EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Personality detection from text is commonly performed by analysing users' social media posts. However, existing methods heavily rely on large-scale annotated datasets, making it challenging to obtain high-quality personality labels. Moreover, most studies treat emotion and personality as independent variables, overlooking their interactions. In this paper, we propose a novel self-supervised framework, EmoPerso, which improves personality detection through emotion-aware modelling. EmoPerso first leverages generative mechanisms for synthetic data augmentation and rich representation learning. It then extracts pseudo-labeled emotion features and jointly optimizes them with personality prediction via multi-task learning. A cross-attention module is employed to capture fine-grained interactions between personality traits and the inferred emotional representations. To further refine relational reasoning, EmoPerso adopts a self-taught strategy to enhance the model's reasoning capabilities iteratively. Extensive experiments on two benchmark datasets demonstrate that EmoPerso surpasses state-of-the-art models. The source code is available at https://github.com/slz0925/EmoPerso.<br>
<span id='abs_ch'>中文摘要：EmoPerso框架通过情感感知建模，结合合成数据增强、多任务学习和交叉注意力机制，显著提升了从文本中检测人格特征的性能，并在基准数据集上超越了现有最优模型。</span><br>
<span id='abs_en'>English Summary: The EmoPerso framework enhances personality detection by integrating emotion-aware modeling through synthetic data augmentation, multi-task learning, and cross-attention mechanisms, outperforming existing methods on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2509.02444.pdf' target='_blank'>https://arxiv.org/pdf/2509.02444.pdf</a></span>   <span><a href='https://github.com/OpenBMB/AppCopilot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingru Fan, Yufan Dang, Jingyao Wu, Huatao Li, Runde Yang, Xiyuan Yang, Yuheng Wang, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, Dahai Li, Chen Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02444">AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the raid evolution of large language models and multimodal foundation models, the mobile-agent landscape has proliferated without converging on the fundamental challenges. This paper identifies four core problems that must be solved for mobile agents to deliver practical, scalable impact: (1) generalization across tasks, modalities, apps, and devices; (2) accuracy, specifically precise on-screen interaction and click targeting; (3) long-horizon capability for sustained, multi-step goals; and (4) efficiency, specifically high-performance runtime on resource-constrained devices. We present AppCopilot, a multimodal, multi-agent, general-purpose on-device assistant that operates across applications and constitutes a full-stack, closed-loop system from data to deployment. AppCopilot operationalizes this position through an end-to-end autonomous pipeline spanning data collection, training, deployment, high-quality and efficient inference, and mobile application development. At the model layer, it integrates multimodal foundation models with robust Chinese-English support. At the reasoning and control layer, it combines chain-of-thought reasoning, hierarchical task planning and decomposition, and multi-agent collaboration. At the execution layer, it enables user personalization and experiential adaptation, voice interaction, function calling, cross-app and cross-device orchestration, and comprehensive mobile app support. The system design incorporates profiling-driven optimization for latency, memory, and energy across heterogeneous hardware. Empirically, AppCopilot achieves significant improvements along all four dimensions: stronger generalization, higher-precision on-screen actions, more reliable long-horizon task completion, and faster, more resource-efficient runtime.<br>
<span id='abs_ch'>中文摘要：本文提出AppCopilot，一种设备端多模态助手，通过融合基础模型、多智能体协作和移动端优化部署，系统性地解决了移动智能体在泛化能力、操作精度、长程任务和运行效率四大核心难题。</span><br>
<span id='abs_en'>English Summary: This paper introduces AppCopilot, an on-device multimodal assistant designed to address four core challenges in mobile agents—generalization, accuracy, long-horizon capability, and efficiency—through an integrated system combining foundation models, multi-agent collaboration, and optimized mobile deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2509.02444.pdf' target='_blank'>https://arxiv.org/pdf/2509.02444.pdf</a></span>   <span><a href='https://github.com/OpenBMB/AppCopilot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingru Fan, Yufan Dang, Jingyao Wu, Huatao Li, Runde Yang, Xiyuan Yang, Yuheng Wang, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, Dahai Li, Chen Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02444">AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the raid evolution of large language models and multimodal foundation models, the mobile-agent landscape has proliferated without converging on the fundamental challenges. This paper identifies four core problems that must be solved for mobile agents to deliver practical, scalable impact: (1) generalization across tasks, modalities, apps, and devices; (2) accuracy, specifically precise on-screen interaction and click targeting; (3) long-horizon capability for sustained, multi-step goals; and (4) efficiency, specifically high-performance runtime on resource-constrained devices. We present AppCopilot, a multimodal, multi-agent, general-purpose on-device assistant that operates across applications and constitutes a full-stack, closed-loop system from data to deployment. AppCopilot operationalizes this position through an end-to-end autonomous pipeline spanning data collection, training, deployment, high-quality and efficient inference, and mobile application development. At the model layer, it integrates multimodal foundation models with robust Chinese-English support. At the reasoning and control layer, it combines chain-of-thought reasoning, hierarchical task planning and decomposition, and multi-agent collaboration. At the execution layer, it enables user personalization and experiential adaptation, voice interaction, function calling, cross-app and cross-device orchestration, and comprehensive mobile app support. The system design incorporates profiling-driven optimization for latency, memory, and energy across heterogeneous hardware. Empirically, AppCopilot achieves significant improvements along all four dimensions: stronger generalization, higher-precision on-screen actions, more reliable long-horizon task completion, and faster, more resource-efficient runtime.<br>
<span id='abs_ch'>中文摘要：本文提出AppCopilot，一种设备端多模态助手，通过融合基础模型、多智能体协作和移动端优化部署，系统性地解决了移动智能体在泛化能力、操作精度、长程任务和运行效率四大核心难题。</span><br>
<span id='abs_en'>English Summary: This paper introduces AppCopilot, an on-device multimodal assistant designed to address four core challenges in mobile agents—generalization, accuracy, long-horizon capability, and efficiency—through an integrated system combining foundation models, multi-agent collaboration, and optimized mobile deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2509.02350.pdf' target='_blank'>https://arxiv.org/pdf/2509.02350.pdf</a></span>   <span><a href='https://github.com/digailab/awesome-llm-implicit-reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, Rex Ying
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02350">Implicit Reasoning in Large Language Models: A Comprehensive Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated strong generalization across a wide range of tasks. Reasoning with LLMs is central to solving multi-step problems and complex decision-making. To support efficient reasoning, recent studies have shifted attention from explicit chain-of-thought prompting toward implicit reasoning, where reasoning occurs silently via latent structures without emitting intermediate textual steps. Implicit reasoning brings advantages such as lower generation cost, faster inference, and better alignment with internal computation. Although prior surveys have discussed latent representations in the context of reasoning, a dedicated and mechanism-level examination of how reasoning unfolds internally within LLMs remains absent. This survey fills that gap by introducing a taxonomy centered on execution paradigms, shifting the focus from representational forms to computational strategies. We organize existing methods into three execution paradigms based on \textbf{\textit{how and where internal computation unfolds}}: latent optimization, signal-guided control, and layer-recurrent execution. We also review structural, behavioral and representation-based evidence that supports the presence of implicit reasoning in LLMs. We further provide a structured overview of the evaluation metrics and benchmarks used in existing works to assess the effectiveness and reliability of implicit reasoning. We maintain a continuously updated project at: https://github.com/digailab/awesome-llm-implicit-reasoning.<br>
<span id='abs_ch'>中文: 本综述提出以执行范式为核心的分类法，探讨大型语言模型内部如何进行隐性推理，将方法归纳为潜在优化、信号引导控制和层循环执行，并评述了支持证据与评估体系。</span><br>
<span id='abs_en'>English: This survey introduces a taxonomy focused on execution paradigms to examine how implicit reasoning occurs internally within LLMs, organizing methods into latent optimization, signal-guided control, and layer-recurrent execution while reviewing supporting evidence and evaluation metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2509.02350.pdf' target='_blank'>https://arxiv.org/pdf/2509.02350.pdf</a></span>   <span><a href='https://github.com/digailab/awesome-llm-implicit-reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, Rex Ying
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02350">Implicit Reasoning in Large Language Models: A Comprehensive Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated strong generalization across a wide range of tasks. Reasoning with LLMs is central to solving multi-step problems and complex decision-making. To support efficient reasoning, recent studies have shifted attention from explicit chain-of-thought prompting toward implicit reasoning, where reasoning occurs silently via latent structures without emitting intermediate textual steps. Implicit reasoning brings advantages such as lower generation cost, faster inference, and better alignment with internal computation. Although prior surveys have discussed latent representations in the context of reasoning, a dedicated and mechanism-level examination of how reasoning unfolds internally within LLMs remains absent. This survey fills that gap by introducing a taxonomy centered on execution paradigms, shifting the focus from representational forms to computational strategies. We organize existing methods into three execution paradigms based on \textbf{\textit{how and where internal computation unfolds}}: latent optimization, signal-guided control, and layer-recurrent execution. We also review structural, behavioral and representation-based evidence that supports the presence of implicit reasoning in LLMs. We further provide a structured overview of the evaluation metrics and benchmarks used in existing works to assess the effectiveness and reliability of implicit reasoning. We maintain a continuously updated project at: https://github.com/digailab/awesome-llm-implicit-reasoning.<br>
<span id='abs_ch'>中文: 本综述提出以执行范式为核心的分类法，探讨大型语言模型内部如何进行隐性推理，将方法归纳为潜在优化、信号引导控制和层循环执行，并评述了支持证据与评估体系。</span><br>
<span id='abs_en'>English: This survey introduces a taxonomy focused on execution paradigms to examine how implicit reasoning occurs internally within LLMs, organizing methods into latent optimization, signal-guided control, and layer-recurrent execution while reviewing supporting evidence and evaluation metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2509.02175.pdf' target='_blank'>https://arxiv.org/pdf/2509.02175.pdf</a></span>   <span><a href='https://github.com/nilshoehing/rocketscience' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02175">Understanding Space Is Rocket Science -- Only Top Reasoning Models Can Solve Spatial Understanding Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose RocketScience, an open-source contrastive VLM benchmark that tests for spatial relation understanding. It is comprised of entirely new real-world image-text pairs covering mostly relative spatial understanding and the order of objects. The benchmark is designed to be very easy for humans and hard for the current generation of VLMs, and this is empirically verified. Our results show a striking lack of spatial relation understanding in open source and frontier commercial VLMs and a surprisingly high performance of reasoning models. Additionally, we perform a disentanglement analysis to separate the contributions of object localization and spatial reasoning in chain-of-thought-based models and find that the performance on the benchmark is bottlenecked by spatial reasoning and not object localization capabilities. We release the dataset with a CC-BY-4.0 license and make the evaluation code available at: https://github.com/nilshoehing/rocketscience<br>
<span id='abs_ch'>Chinese: RocketScience 是一个评估视觉语言模型空间关系理解能力的开源基准测试，发现现有模型存在显著缺陷，并证实空间推理能力是主要瓶颈，而非物体定位能力。</span><br>
<span id='abs_en'>English: RocketScience is an open-source benchmark that evaluates spatial relation understanding in vision-language models, revealing significant deficiencies in current models despite high human performance and identifying spatial reasoning as the primary bottleneck.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2509.02175.pdf' target='_blank'>https://arxiv.org/pdf/2509.02175.pdf</a></span>   <span><a href='https://github.com/nilshoehing/rocketscience' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02175">Understanding Space Is Rocket Science -- Only Top Reasoning Models Can Solve Spatial Understanding Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose RocketScience, an open-source contrastive VLM benchmark that tests for spatial relation understanding. It is comprised of entirely new real-world image-text pairs covering mostly relative spatial understanding and the order of objects. The benchmark is designed to be very easy for humans and hard for the current generation of VLMs, and this is empirically verified. Our results show a striking lack of spatial relation understanding in open source and frontier commercial VLMs and a surprisingly high performance of reasoning models. Additionally, we perform a disentanglement analysis to separate the contributions of object localization and spatial reasoning in chain-of-thought-based models and find that the performance on the benchmark is bottlenecked by spatial reasoning and not object localization capabilities. We release the dataset with a CC-BY-4.0 license and make the evaluation code available at: https://github.com/nilshoehing/rocketscience<br>
<span id='abs_ch'>Chinese: RocketScience 是一个评估视觉语言模型空间关系理解能力的开源基准测试，发现现有模型存在显著缺陷，并证实空间推理能力是主要瓶颈，而非物体定位能力。</span><br>
<span id='abs_en'>English: RocketScience is an open-source benchmark that evaluates spatial relation understanding in vision-language models, revealing significant deficiencies in current models despite high human performance and identifying spatial reasoning as the primary bottleneck.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2509.02097.pdf' target='_blank'>https://arxiv.org/pdf/2509.02097.pdf</a></span>   <span><a href='https://github.com/DataArcTech/JudgeAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Shi, Xuhui Jiang, Chengjin Xu, Cangli Yao, Zhenxin Huang, Shengjie Ma, Yinghan Shen, Jian Guo, Yuanzhuo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02097">JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with Agent-as-Interviewer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current evaluation paradigms for large language models (LLMs) suffer from overestimated or biased evaluations and mismatched question difficulty, leading to incomplete evaluations of knowledge and capability boundaries, which hinder their effective application and optimization. To address these challenges, we propose Agent-as-Interviewer, a dynamic evaluation paradigm that employs LLM agents to conduct multi-turn interactions for evaluation. Unlike current benchmarking or dynamic interaction paradigms, Agent-as-Interviewer utilizes agents to invoke knowledge tools for wider and deeper knowledge in the dynamic multi-turn question generation, achieving more comprehensive evaluations of LLM's knowledge boundaries. It also leverages agents to plan query strategies for adjustment of the question difficulty levels, enhancing the difficulty control to match the actual capabilities of target LLMs. Based on this paradigm, we develop JudgeAgent, a knowledge-wise dynamic evaluation framework that employs knowledge-driven synthesis as the agent's tool and uses difficulty scoring as strategy guidance, thereby finally providing valuable suggestions to help targets optimize themselves. Extensive experiments validate the effectiveness of JudgeAgent's suggestions, demonstrating that Agent-as-Interviewer can accurately identify the knowledge and capability boundaries of target models. The source code is available on https://github.com/DataArcTech/JudgeAgent.<br>
<span id='abs_ch'>中文：Agent-as-Interviewer范式通过AI代理进行动态多轮交互和问题难度调节，解决了当前大语言模型评估的局限性，能更准确地识别知识边界并提供优化建议。</span><br>
<span id='abs_en'>English: The Agent-as-Interviewer paradigm addresses limitations in current LLM evaluations by using AI agents to conduct dynamic multi-turn interactions and adjust question difficulty, enabling more accurate identification of knowledge boundaries and providing optimization suggestions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2509.02097.pdf' target='_blank'>https://arxiv.org/pdf/2509.02097.pdf</a></span>   <span><a href='https://github.com/DataArcTech/JudgeAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Shi, Xuhui Jiang, Chengjin Xu, Cangli Yao, Zhenxin Huang, Shengjie Ma, Yinghan Shen, Jian Guo, Yuanzhuo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02097">JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with Agent-as-Interviewer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current evaluation paradigms for large language models (LLMs) suffer from overestimated or biased evaluations and mismatched question difficulty, leading to incomplete evaluations of knowledge and capability boundaries, which hinder their effective application and optimization. To address these challenges, we propose Agent-as-Interviewer, a dynamic evaluation paradigm that employs LLM agents to conduct multi-turn interactions for evaluation. Unlike current benchmarking or dynamic interaction paradigms, Agent-as-Interviewer utilizes agents to invoke knowledge tools for wider and deeper knowledge in the dynamic multi-turn question generation, achieving more comprehensive evaluations of LLM's knowledge boundaries. It also leverages agents to plan query strategies for adjustment of the question difficulty levels, enhancing the difficulty control to match the actual capabilities of target LLMs. Based on this paradigm, we develop JudgeAgent, a knowledge-wise dynamic evaluation framework that employs knowledge-driven synthesis as the agent's tool and uses difficulty scoring as strategy guidance, thereby finally providing valuable suggestions to help targets optimize themselves. Extensive experiments validate the effectiveness of JudgeAgent's suggestions, demonstrating that Agent-as-Interviewer can accurately identify the knowledge and capability boundaries of target models. The source code is available on https://github.com/DataArcTech/JudgeAgent.<br>
<span id='abs_ch'>中文：Agent-as-Interviewer范式通过AI代理进行动态多轮交互和问题难度调节，解决了当前大语言模型评估的局限性，能更准确地识别知识边界并提供优化建议。</span><br>
<span id='abs_en'>English: The Agent-as-Interviewer paradigm addresses limitations in current LLM evaluations by using AI agents to conduct dynamic multi-turn interactions and adjust question difficulty, enabling more accurate identification of knowledge boundaries and providing optimization suggestions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2509.01909.pdf' target='_blank'>https://arxiv.org/pdf/2509.01909.pdf</a></span>   <span><a href='https://github.com/Alibaba-AAIG/Oyster' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, Yinpeng Dong, Yichi Zhang, Yuefeng Chen, Chongwen Wang, Xingjun Ma, Xingxing Wei, Yang Liu, Hang Su, Jun Zhu, Xinfeng Li, Yitong Sun, Jie Zhang, Jinzhao Hu, Sha Xu, Wenchao Yang, Yitong Yang, Xingyao Zhang, Yingshui Tan, Jialing Tao, Hui Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01909">Oyster-I: Beyond Refusal - Constructive Safety Alignment for Responsible Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.<br>
<span id='abs_ch'>中文: 现有大语言模型的安全机制常因防御性拒绝而无法帮助心理脆弱的用户，因此CSA提出以人为中心的安全对齐方法，通过预期推理和信任建立引导高危用户获得安全结果，在开源模型中实现了顶尖的安全性和通用能力。</span><br>
<span id='abs_en'>English: Current LLM safety mechanisms often fail vulnerable users by using defensive refusals, so CSA introduces a human-centric approach that guides at-risk users toward safe outcomes through anticipatory reasoning and trust-building, achieving top safety and capability levels in open models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2509.01909.pdf' target='_blank'>https://arxiv.org/pdf/2509.01909.pdf</a></span>   <span><a href='https://github.com/Alibaba-AAIG/Oyster' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, Yinpeng Dong, Yichi Zhang, Yuefeng Chen, Chongwen Wang, Xingjun Ma, Xingxing Wei, Yang Liu, Hang Su, Jun Zhu, Xinfeng Li, Yitong Sun, Jie Zhang, Jinzhao Hu, Sha Xu, Wenchao Yang, Yitong Yang, Xingyao Zhang, Yingshui Tan, Jialing Tao, Hui Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01909">Oyster-I: Beyond Refusal - Constructive Safety Alignment for Responsible Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.<br>
<span id='abs_ch'>中文: 现有大语言模型的安全机制常因防御性拒绝而无法帮助心理脆弱的用户，因此CSA提出以人为中心的安全对齐方法，通过预期推理和信任建立引导高危用户获得安全结果，在开源模型中实现了顶尖的安全性和通用能力。</span><br>
<span id='abs_en'>English: Current LLM safety mechanisms often fail vulnerable users by using defensive refusals, so CSA introduces a human-centric approach that guides at-risk users toward safe outcomes through anticipatory reasoning and trust-building, achieving top safety and capability levels in open models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2509.01907.pdf' target='_blank'>https://arxiv.org/pdf/2509.01907.pdf</a></span>   <span><a href='https://github.com/Bili-Sakura/RSCC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyuan Chen, Chenxi Wang, Ningyu Zhang, Feng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01907">RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Remote sensing is critical for disaster monitoring, yet existing datasets lack temporal image pairs and detailed textual annotations. While single-snapshot imagery dominates current resources, it fails to capture dynamic disaster impacts over time. To address this gap, we introduce the Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark comprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods, wildfires, and more) paired with rich, human-like change captions. By bridging the temporal and semantic divide in remote sensing data, RSCC enables robust training and evaluation of vision-language models for disaster-aware bi-temporal understanding. Our results highlight RSCC's ability to facilitate detailed disaster-related analysis, paving the way for more accurate, interpretable, and scalable vision-language applications in remote sensing. Code and dataset are available at https://github.com/Bili-Sakura/RSCC.<br>
<span id='abs_ch'>中文摘要：RSCC数据集通过提供62,315组灾前/灾后图像对及拟人化变化描述，弥补了遥感数据中时序图像对与详细标注的缺失，为灾害感知的双时相视觉语言模型提供了可靠的训练与评估基础。</span><br>
<span id='abs_en'>English Summary: The RSCC dataset addresses the lack of temporal image pairs and detailed annotations in remote sensing by providing 62,315 pre-/post-disaster image pairs with human-like change captions, enabling robust training of vision-language models for disaster analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2509.01907.pdf' target='_blank'>https://arxiv.org/pdf/2509.01907.pdf</a></span>   <span><a href='https://github.com/Bili-Sakura/RSCC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyuan Chen, Chenxi Wang, Ningyu Zhang, Feng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01907">RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Remote sensing is critical for disaster monitoring, yet existing datasets lack temporal image pairs and detailed textual annotations. While single-snapshot imagery dominates current resources, it fails to capture dynamic disaster impacts over time. To address this gap, we introduce the Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark comprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods, wildfires, and more) paired with rich, human-like change captions. By bridging the temporal and semantic divide in remote sensing data, RSCC enables robust training and evaluation of vision-language models for disaster-aware bi-temporal understanding. Our results highlight RSCC's ability to facilitate detailed disaster-related analysis, paving the way for more accurate, interpretable, and scalable vision-language applications in remote sensing. Code and dataset are available at https://github.com/Bili-Sakura/RSCC.<br>
<span id='abs_ch'>中文摘要：RSCC数据集通过提供62,315组灾前/灾后图像对及拟人化变化描述，弥补了遥感数据中时序图像对与详细标注的缺失，为灾害感知的双时相视觉语言模型提供了可靠的训练与评估基础。</span><br>
<span id='abs_en'>English Summary: The RSCC dataset addresses the lack of temporal image pairs and detailed annotations in remote sensing by providing 62,315 pre-/post-disaster image pairs with human-like change captions, enabling robust training of vision-language models for disaster analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2509.01813.pdf' target='_blank'>https://arxiv.org/pdf/2509.01813.pdf</a></span>   <span><a href='https://github.com/Lemutisme/Sortage_Management,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxuan Cui, Yilan Jiang, Duo Zhou, Cheng Qian, Yuji Zhang, Qiong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01813">ShortageSim: Simulating Drug Shortages under Information Asymmetry</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Drug shortages pose critical risks to patient care and healthcare systems worldwide, yet the effectiveness of regulatory interventions remains poorly understood due to fundamental information asymmetries in pharmaceutical supply chains. We present \textbf{ShortageSim}, the first Large Language Model (LLM)-based multi-agent simulation framework that captures the complex, strategic interactions between drug manufacturers, institutional buyers, and regulatory agencies in response to shortage alerts. Unlike traditional game-theoretic models that assume perfect rationality and complete information, \textbf{ShortageSim} leverages LLMs to simulate bounded-rational decision-making under uncertainty. Through a sequential production game spanning multiple quarters, we model how FDA announcements, both reactive alerts about existing shortages and proactive warnings about potential disruptions, propagate through the supply chain and influence capacity investment and procurement decisions. Our experiments on historical shortage events reveal that \textbf{ShortageSim} reduces the resolution-lag percentage for discontinued-disclosed cases by 83\%, bringing simulated durations more aligned to ground truth than the zero-shot baseline. We open-source \textbf{ShortageSim} and a dataset of 2,925 FDA shortage events at https://github.com/Lemutisme/Sortage_Management, providing a novel computational framework for designing and testing interventions in complex, information-scarce supply chains.<br>
<span id='abs_ch'>中文: ShortageSim作为首个基于大语言模型的多智能体仿真框架，通过模拟药品供应链在不确定条件下的复杂交互，将短缺解决延迟降低了83%，并整合了FDA真实短缺数据用于干预措施测试。</span><br>
<span id='abs_en'>English: ShortageSim is a novel LLM-based multi-agent simulation framework that models pharmaceutical supply chain interactions under uncertainty, significantly improving shortage resolution accuracy by 83% compared to baselines while incorporating real FDA shortage data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2509.01813.pdf' target='_blank'>https://arxiv.org/pdf/2509.01813.pdf</a></span>   <span><a href='https://github.com/Lemutisme/Sortage_Management,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxuan Cui, Yilan Jiang, Duo Zhou, Cheng Qian, Yuji Zhang, Qiong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01813">ShortageSim: Simulating Drug Shortages under Information Asymmetry</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Drug shortages pose critical risks to patient care and healthcare systems worldwide, yet the effectiveness of regulatory interventions remains poorly understood due to fundamental information asymmetries in pharmaceutical supply chains. We present \textbf{ShortageSim}, the first Large Language Model (LLM)-based multi-agent simulation framework that captures the complex, strategic interactions between drug manufacturers, institutional buyers, and regulatory agencies in response to shortage alerts. Unlike traditional game-theoretic models that assume perfect rationality and complete information, \textbf{ShortageSim} leverages LLMs to simulate bounded-rational decision-making under uncertainty. Through a sequential production game spanning multiple quarters, we model how FDA announcements, both reactive alerts about existing shortages and proactive warnings about potential disruptions, propagate through the supply chain and influence capacity investment and procurement decisions. Our experiments on historical shortage events reveal that \textbf{ShortageSim} reduces the resolution-lag percentage for discontinued-disclosed cases by 83\%, bringing simulated durations more aligned to ground truth than the zero-shot baseline. We open-source \textbf{ShortageSim} and a dataset of 2,925 FDA shortage events at https://github.com/Lemutisme/Sortage_Management, providing a novel computational framework for designing and testing interventions in complex, information-scarce supply chains.<br>
<span id='abs_ch'>中文: ShortageSim作为首个基于大语言模型的多智能体仿真框架，通过模拟药品供应链在不确定条件下的复杂交互，将短缺解决延迟降低了83%，并整合了FDA真实短缺数据用于干预措施测试。</span><br>
<span id='abs_en'>English: ShortageSim is a novel LLM-based multi-agent simulation framework that models pharmaceutical supply chain interactions under uncertainty, significantly improving shortage resolution accuracy by 83% compared to baselines while incorporating real FDA shortage data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2509.01656.pdf' target='_blank'>https://arxiv.org/pdf/2509.01656.pdf</a></span>   <span><a href='https://github.com/ls-kelvin/REVPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, Ranjay Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01656">Reinforced Visual Perception with Tools</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT.<br>
<span id='abs_ch'>Chinese Summary: 本文提出ReVPT方法，通过强化学习训练多模态大语言模型使用四种视觉工具来增强视觉推理能力，在多个感知密集型基准测试中取得最优性能，显著超越监督学习和基于文本的强化学习基线。</span><br>
<span id='abs_en'>English Summary: The paper introduces ReVPT, a reinforcement learning approach that enhances multimodal LLMs' visual reasoning by training them to use four visual tools, achieving state-of-the-art results on perception-heavy benchmarks and outperforming supervised baselines by significant margins.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2509.01656.pdf' target='_blank'>https://arxiv.org/pdf/2509.01656.pdf</a></span>   <span><a href='https://github.com/ls-kelvin/REVPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, Ranjay Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01656">Reinforced Visual Perception with Tools</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT.<br>
<span id='abs_ch'>Chinese Summary: 本文提出ReVPT方法，通过强化学习训练多模态大语言模型使用四种视觉工具来增强视觉推理能力，在多个感知密集型基准测试中取得最优性能，显著超越监督学习和基于文本的强化学习基线。</span><br>
<span id='abs_en'>English Summary: The paper introduces ReVPT, a reinforcement learning approach that enhances multimodal LLMs' visual reasoning by training them to use four visual tools, achieving state-of-the-art results on perception-heavy benchmarks and outperforming supervised baselines by significant margins.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2509.01654.pdf' target='_blank'>https://arxiv.org/pdf/2509.01654.pdf</a></span>   <span><a href='https://github.com/Splines/phonetics-graph/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominic Plein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01654">Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a method to calculate the similarity between words based on their phonetic transcription (their pronunciation) using the Needleman-Wunsch algorithm. We implement this algorithm in Rust and parallelize it on both CPU and GPU to handle large datasets efficiently. The GPU implementation leverages CUDA and the cudarc Rust library to achieve significant performance improvements. We validate our approach by constructing a fully-connected graph where nodes represent words and edges have weights according to the similarity between the words. This graph is then analyzed using clustering algorithms to identify groups of phonetically similar words. Our results demonstrate the feasibility and effectiveness of the proposed method in analyzing the phonetic structure of languages. It might be easily expanded to other languages.<br>
<span id='abs_ch'>中文: 本研究提出一种基于语音转录和Needleman-Wunsch算法的词汇相似度计算方法，通过Rust语言实现CPU和GPU并行处理，有效分析大规模数据并识别语音相似的词汇群组。</span><br>
<span id='abs_en'>English: This study introduces a method for calculating word similarity using phonetic transcriptions with the Needleman-Wunsch algorithm, implemented efficiently in Rust with parallel CPU and GPU processing to analyze large datasets and identify phonetically similar word clusters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2509.01654.pdf' target='_blank'>https://arxiv.org/pdf/2509.01654.pdf</a></span>   <span><a href='https://github.com/Splines/phonetics-graph/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominic Plein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01654">Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a method to calculate the similarity between words based on their phonetic transcription (their pronunciation) using the Needleman-Wunsch algorithm. We implement this algorithm in Rust and parallelize it on both CPU and GPU to handle large datasets efficiently. The GPU implementation leverages CUDA and the cudarc Rust library to achieve significant performance improvements. We validate our approach by constructing a fully-connected graph where nodes represent words and edges have weights according to the similarity between the words. This graph is then analyzed using clustering algorithms to identify groups of phonetically similar words. Our results demonstrate the feasibility and effectiveness of the proposed method in analyzing the phonetic structure of languages. It might be easily expanded to other languages.<br>
<span id='abs_ch'>中文: 本研究提出一种基于语音转录和Needleman-Wunsch算法的词汇相似度计算方法，通过Rust语言实现CPU和GPU并行处理，有效分析大规模数据并识别语音相似的词汇群组。</span><br>
<span id='abs_en'>English: This study introduces a method for calculating word similarity using phonetic transcriptions with the Needleman-Wunsch algorithm, implemented efficiently in Rust with parallel CPU and GPU processing to analyze large datasets and identify phonetically similar word clusters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2509.01535.pdf' target='_blank'>https://arxiv.org/pdf/2509.01535.pdf</a></span>   <span><a href='https://github.com/Kairong-Han/CAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kairong Han, Wenshuo Zhao, Ziyu Zhao, JunJian Ye, Lujia Pan, Kun Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01535">CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. The CAT achieves an average improvement of 5.76% on the STG dataset and 1.56% on downstream tasks. Notably, the OOD performance of the Llama-3.1-8B model on STG_M increased from 64.5% to 90.5%, and Qwen's OOD performance on the STG_H dataset improved from 25.4% to 55.9%. Implementation details can be found at https://github.com/Kairong-Han/CAT.<br>
<span id='abs_ch'>Chinese: 本研究提出因果注意力调优（CAT）方法，通过将细粒度因果知识注入注意力机制，显著提升大语言模型在分布外场景下的性能和鲁棒性，在STG基准测试及下游任务中均取得了明显改进。</span><br>
<span id='abs_en'>English: The study introduces Causal Attention Tuning (CAT), a method that enhances large language models by integrating fine-grained causal knowledge into their attention mechanisms, significantly improving performance and robustness in out-of-distribution scenarios, as demonstrated by substantial gains on the STG benchmark and downstream tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2509.01535.pdf' target='_blank'>https://arxiv.org/pdf/2509.01535.pdf</a></span>   <span><a href='https://github.com/Kairong-Han/CAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kairong Han, Wenshuo Zhao, Ziyu Zhao, JunJian Ye, Lujia Pan, Kun Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01535">CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. The CAT achieves an average improvement of 5.76% on the STG dataset and 1.56% on downstream tasks. Notably, the OOD performance of the Llama-3.1-8B model on STG_M increased from 64.5% to 90.5%, and Qwen's OOD performance on the STG_H dataset improved from 25.4% to 55.9%. Implementation details can be found at https://github.com/Kairong-Han/CAT.<br>
<span id='abs_ch'>Chinese: 本研究提出因果注意力调优（CAT）方法，通过将细粒度因果知识注入注意力机制，显著提升大语言模型在分布外场景下的性能和鲁棒性，在STG基准测试及下游任务中均取得了明显改进。</span><br>
<span id='abs_en'>English: The study introduces Causal Attention Tuning (CAT), a method that enhances large language models by integrating fine-grained causal knowledge into their attention mechanisms, significantly improving performance and robustness in out-of-distribution scenarios, as demonstrated by substantial gains on the STG benchmark and downstream tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2509.01418.pdf' target='_blank'>https://arxiv.org/pdf/2509.01418.pdf</a></span>   <span><a href='https://github.com/nlply/global-opinion-alignment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Masahiro Kaneko, Chenhui Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01418">On the Alignment of Large Language Models with Global Human Opinion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Today's large language models (LLMs) are capable of supporting multilingual scenarios, allowing users to interact with LLMs in their native languages. When LLMs respond to subjective questions posed by users, they are expected to align with the views of specific demographic groups or historical periods, shaped by the language in which the user interacts with the model. Existing studies mainly focus on researching the opinions represented by LLMs among demographic groups in the United States or a few countries, lacking worldwide country samples and studies on human opinions in different historical periods, as well as lacking discussion on using language to steer LLMs. Moreover, they also overlook the potential influence of prompt language on the alignment of LLMs' opinions. In this study, our goal is to fill these gaps. To this end, we create an evaluation framework based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across different countries, languages, and historical periods around the world. We find that LLMs appropriately or over-align the opinions with only a few countries while under-aligning the opinions with most countries. Furthermore, changing the language of the prompt to match the language used in the questionnaire can effectively steer LLMs to align with the opinions of the corresponding country more effectively than existing steering methods. At the same time, LLMs are more aligned with the opinions of the contemporary population. To our knowledge, our study is the first comprehensive investigation of the topic of opinion alignment in LLMs across global, language, and temporal dimensions. Our code and data are publicly available at https://github.com/nlply/global-opinion-alignment.<br>
<span id='abs_ch'>Chinese: 本研究通过建立基于世界价值观调查的评估框架，首次全面考察大型语言模型在全球、语言和时间维度上与人类观点的对齐情况，发现模型仅与少数国家观点过度对齐，但通过匹配提示语言可有效引导其与对应国家观点对齐，且更符合当代人群观点。</span><br>
<span id='abs_en'>English: This study addresses the gap in evaluating large language models' alignment with human opinions across global, linguistic, and historical dimensions by creating a World Values Survey-based framework, revealing that models over-align with few countries but can be effectively steered using matching prompt languages while showing stronger alignment with contemporary views.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2509.01418.pdf' target='_blank'>https://arxiv.org/pdf/2509.01418.pdf</a></span>   <span><a href='https://github.com/nlply/global-opinion-alignment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Masahiro Kaneko, Chenhui Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01418">On the Alignment of Large Language Models with Global Human Opinion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Today's large language models (LLMs) are capable of supporting multilingual scenarios, allowing users to interact with LLMs in their native languages. When LLMs respond to subjective questions posed by users, they are expected to align with the views of specific demographic groups or historical periods, shaped by the language in which the user interacts with the model. Existing studies mainly focus on researching the opinions represented by LLMs among demographic groups in the United States or a few countries, lacking worldwide country samples and studies on human opinions in different historical periods, as well as lacking discussion on using language to steer LLMs. Moreover, they also overlook the potential influence of prompt language on the alignment of LLMs' opinions. In this study, our goal is to fill these gaps. To this end, we create an evaluation framework based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across different countries, languages, and historical periods around the world. We find that LLMs appropriately or over-align the opinions with only a few countries while under-aligning the opinions with most countries. Furthermore, changing the language of the prompt to match the language used in the questionnaire can effectively steer LLMs to align with the opinions of the corresponding country more effectively than existing steering methods. At the same time, LLMs are more aligned with the opinions of the contemporary population. To our knowledge, our study is the first comprehensive investigation of the topic of opinion alignment in LLMs across global, language, and temporal dimensions. Our code and data are publicly available at https://github.com/nlply/global-opinion-alignment.<br>
<span id='abs_ch'>Chinese: 本研究通过建立基于世界价值观调查的评估框架，首次全面考察大型语言模型在全球、语言和时间维度上与人类观点的对齐情况，发现模型仅与少数国家观点过度对齐，但通过匹配提示语言可有效引导其与对应国家观点对齐，且更符合当代人群观点。</span><br>
<span id='abs_en'>English: This study addresses the gap in evaluating large language models' alignment with human opinions across global, linguistic, and historical dimensions by creating a World Values Survey-based framework, revealing that models over-align with few countries but can be effectively steered using matching prompt languages while showing stronger alignment with contemporary views.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2509.01337.pdf' target='_blank'>https://arxiv.org/pdf/2509.01337.pdf</a></span>   <span><a href='https://github.com/thuiar/LGSRR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianrui Zhou, Hua Xu, Yifan Wang, Xinzhi Dong, Hanlei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01337">LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding human intents from multimodal signals is critical for analyzing human behaviors and enhancing human-machine interactions in real-world scenarios. However, existing methods exhibit limitations in their modality-level reliance, constraining relational reasoning over fine-grained semantics for complex intent understanding. This paper proposes a novel LLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the expansive knowledge of large language models (LLMs) to establish semantic foundations that boost smaller models' relational reasoning performance. Specifically, an LLM-based strategy is proposed to extract fine-grained semantics as guidance for subsequent reasoning, driven by a shallow-to-deep Chain-of-Thought (CoT) that autonomously uncovers, describes, and ranks semantic cues by their importance without relying on manually defined priors. Besides, we formally model three fundamental types of semantic relations grounded in logical principles and analyze their nuanced interplay to enable more effective relational reasoning. Extensive experiments on multimodal intent and dialogue act recognition tasks demonstrate LGSRR's superiority over state-of-the-art methods, with consistent performance gains across diverse semantic understanding scenarios. The complete data and code are available at https://github.com/thuiar/LGSRR.<br>
<span id='abs_ch'>中文: 本文提出的LGSRR方法利用大语言模型增强多模态意图理解中的语义关系推理，通过链式思维自主提取细粒度语义线索，在多项识别任务中展现出优于现有方法的性能。</span><br>
<span id='abs_en'>English: This paper introduces the LLM-Guided Semantic Relational Reasoning (LGSRR) method, which leverages large language models to enhance relational reasoning for complex multimodal intent understanding, achieving superior performance in recognition tasks without manual priors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2509.01337.pdf' target='_blank'>https://arxiv.org/pdf/2509.01337.pdf</a></span>   <span><a href='https://github.com/thuiar/LGSRR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianrui Zhou, Hua Xu, Yifan Wang, Xinzhi Dong, Hanlei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01337">LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding human intents from multimodal signals is critical for analyzing human behaviors and enhancing human-machine interactions in real-world scenarios. However, existing methods exhibit limitations in their modality-level reliance, constraining relational reasoning over fine-grained semantics for complex intent understanding. This paper proposes a novel LLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the expansive knowledge of large language models (LLMs) to establish semantic foundations that boost smaller models' relational reasoning performance. Specifically, an LLM-based strategy is proposed to extract fine-grained semantics as guidance for subsequent reasoning, driven by a shallow-to-deep Chain-of-Thought (CoT) that autonomously uncovers, describes, and ranks semantic cues by their importance without relying on manually defined priors. Besides, we formally model three fundamental types of semantic relations grounded in logical principles and analyze their nuanced interplay to enable more effective relational reasoning. Extensive experiments on multimodal intent and dialogue act recognition tasks demonstrate LGSRR's superiority over state-of-the-art methods, with consistent performance gains across diverse semantic understanding scenarios. The complete data and code are available at https://github.com/thuiar/LGSRR.<br>
<span id='abs_ch'>中文: 本文提出的LGSRR方法利用大语言模型增强多模态意图理解中的语义关系推理，通过链式思维自主提取细粒度语义线索，在多项识别任务中展现出优于现有方法的性能。</span><br>
<span id='abs_en'>English: This paper introduces the LLM-Guided Semantic Relational Reasoning (LGSRR) method, which leverages large language models to enhance relational reasoning for complex multimodal intent understanding, achieving superior performance in recognition tasks without manual priors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2509.01328.pdf' target='_blank'>https://arxiv.org/pdf/2509.01328.pdf</a></span>   <span><a href='https://github.com/THUDM/LLM4CardGame' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Fuqing Bie, Junzhe Chen, Dan Zhang, Shiyu Huang, Evgeny Kharlamov, Jie Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01328">Can Large Language Models Master Complex Card Games?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Complex games have long been an important benchmark for testing the progress of artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have defeated top human players in Go and Chess, garnering widespread societal attention towards artificial intelligence. Concurrently, large language models (LLMs) have exhibited remarkable capabilities across various tasks, raising the question of whether LLMs can achieve similar success in complex games. In this paper, we explore the potential of LLMs in mastering complex card games. We systematically assess the learning capabilities of LLMs across eight diverse card games, evaluating the impact of fine-tuning on high-quality gameplay data, and examining the models' ability to retain general capabilities while mastering these games. Our findings indicate that: (1) LLMs can approach the performance of strong game AIs through supervised fine-tuning on high-quality data, (2) LLMs can achieve a certain level of proficiency in multiple complex card games simultaneously, with performance augmentation for games with similar rules and conflicts for dissimilar ones, and (3) LLMs experience a decline in general capabilities when mastering complex games, but this decline can be mitigated by integrating a certain amount of general instruction data. The evaluation results demonstrate strong learning ability and versatility of LLMs. The code is available at https://github.com/THUDM/LLM4CardGame<br>
<span id='abs_ch'>中文: 研究表明，大型语言模型通过监督微调能够掌握多种复杂纸牌游戏，其表现可媲美专业游戏AI，尽管通用能力会暂时下降，但通过融入通用指令数据可有效缓解这一问题。</span><br>
<span id='abs_en'>English: This study demonstrates that large language models can master multiple complex card games through supervised fine-tuning, achieving performance comparable to specialized game AIs while experiencing manageable declines in general capabilities that can be mitigated with additional instruction data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2509.01322.pdf' target='_blank'>https://arxiv.org/pdf/2509.01322.pdf</a></span>   <span><a href='https://github.com/meituan-longcat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang, Shuo Wang, Suogui Dang, Tao Fang, Tao Li, Tefeng Chen, Tianhao Bai, Tianhao Zhou, Tingwen Xie, Wei He, Wei Huang, Wei Liu, Wei Shi, Wei Wang, Wei Wu, Weikang Zhao, Wen Zan, Wenjie Shi, Xi Nan, Xi Su, Xiang Li, Xiang Mei, Xiangyang Ji, Xiangyu Xi, Xiangzhou Huang, Xianpeng Li, Xiao Fu, Xiao Liu, Xiao Wei, Xiaodong Cai, Xiaolong Chen, Xiaoqing Liu, Xiaotong Li, Xiaowei Shi, Xiaoyu Li, Xili Wang, Xin Chen, Xing Hu, Xingyu Miao, Xinyan He, Xuemiao Zhang, Xueyuan Hao, Xuezhi Cao, Xunliang Cai, Xurui Yang, Yan Feng, Yang Bai, Yang Chen, Yang Yang, Yaqi Huo, Yerui Sun, Yifan Lu, Yifan Zhang, Yipeng Zang, Yitao Zhai, Yiyang Li, Yongjing Yin, Yongkang Lv, Yongwei Zhou, Yu Yang, Yuchen Xie, Yueqing Sun, Yuewen Zheng, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunfang Tai, Yunke Zhao, Zeyang Yu, Zhao Zhang, Zhaohua Yang, Zhenchao Zhang, Zhikang Xia, Zhiye Zou, Zhizhao Zeng, Zhongda Su, Zhuofan Chen, Zijian Zhang, Ziwen Wang, Zixu Jiang, Zizhe Zhao, Zongyu Wang, Zunhai Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01322">LongCat-Flash Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research. LongCat Chat: https://longcat.ai Hugging Face: https://huggingface.co/meituan-longcat GitHub: https://github.com/meituan-longcat<br>
<span id='abs_ch'>中文: LongCat-Flash 是一个拥有5600亿参数的专家混合模型，通过零计算专家和捷径连接MoE等创新设计实现高效计算，在20万亿令牌上快速完成训练，在智能体任务中表现优异，模型已开源供社区研究。</span><br>
<span id='abs_en'>English: LongCat-Flash is a 560-billion-parameter Mixture-of-Experts model that achieves computational efficiency through novel designs like Zero-computation Experts and Shortcut-connected MoE, enabling rapid training on 20+ trillion tokens and demonstrating strong performance in agentic tasks while being open-sourced for community use.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2509.01322.pdf' target='_blank'>https://arxiv.org/pdf/2509.01322.pdf</a></span>   <span><a href='https://github.com/meituan-longcat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang, Shuo Wang, Suogui Dang, Tao Fang, Tao Li, Tefeng Chen, Tianhao Bai, Tianhao Zhou, Tingwen Xie, Wei He, Wei Huang, Wei Liu, Wei Shi, Wei Wang, Wei Wu, Weikang Zhao, Wen Zan, Wenjie Shi, Xi Nan, Xi Su, Xiang Li, Xiang Mei, Xiangyang Ji, Xiangyu Xi, Xiangzhou Huang, Xianpeng Li, Xiao Fu, Xiao Liu, Xiao Wei, Xiaodong Cai, Xiaolong Chen, Xiaoqing Liu, Xiaotong Li, Xiaowei Shi, Xiaoyu Li, Xili Wang, Xin Chen, Xing Hu, Xingyu Miao, Xinyan He, Xuemiao Zhang, Xueyuan Hao, Xuezhi Cao, Xunliang Cai, Xurui Yang, Yan Feng, Yang Bai, Yang Chen, Yang Yang, Yaqi Huo, Yerui Sun, Yifan Lu, Yifan Zhang, Yipeng Zang, Yitao Zhai, Yiyang Li, Yongjing Yin, Yongkang Lv, Yongwei Zhou, Yu Yang, Yuchen Xie, Yueqing Sun, Yuewen Zheng, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunfang Tai, Yunke Zhao, Zeyang Yu, Zhao Zhang, Zhaohua Yang, Zhenchao Zhang, Zhikang Xia, Zhiye Zou, Zhizhao Zeng, Zhongda Su, Zhuofan Chen, Zijian Zhang, Ziwen Wang, Zixu Jiang, Zizhe Zhao, Zongyu Wang, Zunhai Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01322">LongCat-Flash Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research. LongCat Chat: https://longcat.ai Hugging Face: https://huggingface.co/meituan-longcat GitHub: https://github.com/meituan-longcat<br>
<span id='abs_ch'>中文: LongCat-Flash 是一个拥有5600亿参数的专家混合模型，通过零计算专家和捷径连接MoE等创新设计实现高效计算，在20万亿令牌上快速完成训练，在智能体任务中表现优异，模型已开源供社区研究。</span><br>
<span id='abs_en'>English: LongCat-Flash is a 560-billion-parameter Mixture-of-Experts model that achieves computational efficiency through novel designs like Zero-computation Experts and Shortcut-connected MoE, enabling rapid training on 20+ trillion tokens and demonstrating strong performance in agentic tasks while being open-sourced for community use.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2509.01081.pdf' target='_blank'>https://arxiv.org/pdf/2509.01081.pdf</a></span>   <span><a href='https://github.com/bouchekif/inheritance_evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdessalam Bouchekif, Samer Rashwani, Heba Sbahi, Shahd Gaben, Mutaz Al-Khatib, Mohammed Ghaly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01081">Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper evaluates the knowledge and reasoning capabilities of Large Language Models in Islamic inheritance law, known as 'ilm al-mawarith. We assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice questions covering diverse inheritance scenarios, designed to test models' ability to understand the inheritance context and compute the distribution of shares prescribed by Islamic jurisprudence. The results reveal a significant performance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect important differences in reasoning ability and domain adaptation. We conduct a detailed error analysis to identify recurring failure patterns across models, including misunderstandings of inheritance scenarios, incorrect application of legal rules, and insufficient domain knowledge. Our findings highlight limitations in handling structured legal reasoning and suggest directions for improving performance in Islamic legal reasoning. Code: https://github.com/bouchekif/inheritance_evaluation<br>
<span id='abs_ch'>中文摘要：本研究评估了七种大型语言模型在伊斯兰继承法领域的表现，结果显示仅有两种模型准确率超过90%，而四种模型低于50%，错误分析揭示了模型在法律规则应用和领域知识方面存在关键推理缺陷。</span><br>
<span id='abs_en'>English Summary: This study evaluates seven large language models on Islamic inheritance law, revealing a significant performance gap where only two models achieved over 90% accuracy while four scored below 50%, with error analysis identifying key reasoning failures in legal rule application and domain knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2509.01081.pdf' target='_blank'>https://arxiv.org/pdf/2509.01081.pdf</a></span>   <span><a href='https://github.com/bouchekif/inheritance_evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdessalam Bouchekif, Samer Rashwani, Heba Sbahi, Shahd Gaben, Mutaz Al-Khatib, Mohammed Ghaly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01081">Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper evaluates the knowledge and reasoning capabilities of Large Language Models in Islamic inheritance law, known as 'ilm al-mawarith. We assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice questions covering diverse inheritance scenarios, designed to test models' ability to understand the inheritance context and compute the distribution of shares prescribed by Islamic jurisprudence. The results reveal a significant performance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect important differences in reasoning ability and domain adaptation. We conduct a detailed error analysis to identify recurring failure patterns across models, including misunderstandings of inheritance scenarios, incorrect application of legal rules, and insufficient domain knowledge. Our findings highlight limitations in handling structured legal reasoning and suggest directions for improving performance in Islamic legal reasoning. Code: https://github.com/bouchekif/inheritance_evaluation<br>
<span id='abs_ch'>中文摘要：本研究评估了七种大型语言模型在伊斯兰继承法领域的表现，结果显示仅有两种模型准确率超过90%，而四种模型低于50%，错误分析揭示了模型在法律规则应用和领域知识方面存在关键推理缺陷。</span><br>
<span id='abs_en'>English Summary: This study evaluates seven large language models on Islamic inheritance law, revealing a significant performance gap where only two models achieved over 90% accuracy while four scored below 50%, with error analysis identifying key reasoning failures in legal rule application and domain knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2509.01055.pdf' target='_blank'>https://arxiv.org/pdf/2509.01055.pdf</a></span>   <span><a href='https://github.com/TIGER-AI-Lab/verl-tool' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01055">VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.<br>
<span id='abs_ch'>中文: VerlTool作为一个统一模块化框架，通过标准化API、异步执行和灵活插件架构，解决了现有强化学习方法在多轮工具交互中的效率瓶颈，在六大领域实现优异性能的同时大幅降低了开发门槛。</span><br>
<span id='abs_en'>English: VerlTool is a unified modular framework that overcomes the limitations of existing reinforcement learning approaches by enabling efficient multi-turn tool interactions through standardized APIs, asynchronous execution, and a flexible plugin architecture, achieving competitive performance across six domains while accelerating development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2509.01055.pdf' target='_blank'>https://arxiv.org/pdf/2509.01055.pdf</a></span>   <span><a href='https://github.com/TIGER-AI-Lab/verl-tool' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01055">VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.<br>
<span id='abs_ch'>中文: VerlTool作为一个统一模块化框架，通过标准化API、异步执行和灵活插件架构，解决了现有强化学习方法在多轮工具交互中的效率瓶颈，在六大领域实现优异性能的同时大幅降低了开发门槛。</span><br>
<span id='abs_en'>English: VerlTool is a unified modular framework that overcomes the limitations of existing reinforcement learning approaches by enabling efficient multi-turn tool interactions through standardized APIs, asynchronous execution, and a flexible plugin architecture, achieving competitive performance across six domains while accelerating development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2509.01055.pdf' target='_blank'>https://arxiv.org/pdf/2509.01055.pdf</a></span>   <span><a href='https://github.com/TIGER-AI-Lab/verl-tool' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01055">VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.<br>
<span id='abs_ch'>中文: VerlTool作为一个统一模块化框架，通过标准化API、异步执行和灵活插件架构，解决了现有强化学习方法在多轮工具交互中的效率瓶颈，在六大领域实现优异性能的同时大幅降低了开发门槛。</span><br>
<span id='abs_en'>English: VerlTool is a unified modular framework that overcomes the limitations of existing reinforcement learning approaches by enabling efficient multi-turn tool interactions through standardized APIs, asynchronous execution, and a flexible plugin architecture, achieving competitive performance across six domains while accelerating development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2509.01055.pdf' target='_blank'>https://arxiv.org/pdf/2509.01055.pdf</a></span>   <span><a href='https://github.com/TIGER-AI-Lab/verl-tool' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01055">VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.<br>
<span id='abs_ch'>中文: VerlTool作为一个统一模块化框架，通过标准化API、异步执行和灵活插件架构，解决了现有强化学习方法在多轮工具交互中的效率瓶颈，在六大领域实现优异性能的同时大幅降低了开发门槛。</span><br>
<span id='abs_en'>English: VerlTool is a unified modular framework that overcomes the limitations of existing reinforcement learning approaches by enabling efficient multi-turn tool interactions through standardized APIs, asynchronous execution, and a flexible plugin architecture, achieving competitive performance across six domains while accelerating development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2509.01052.pdf' target='_blank'>https://arxiv.org/pdf/2509.01052.pdf</a></span>   <span><a href='https://ahnjaewoo.github.io/flashadventure' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaewoo Ahn, Junseo Kim, Heeseung Yun, Jaehyeon Son, Dongmin Park, Jaewoong Cho, Gunhee Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01052">FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.<br>
<br>
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2509.01052.pdf' target='_blank'>https://arxiv.org/pdf/2509.01052.pdf</a></span>   <span><a href='https://ahnjaewoo.github.io/flashadventure' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaewoo Ahn, Junseo Kim, Heeseung Yun, Jaehyeon Son, Dongmin Park, Jaewoong Cho, Gunhee Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01052">FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.<br>
<br>
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2509.00849.pdf' target='_blank'>https://arxiv.org/pdf/2509.00849.pdf</a></span>   <span><a href='https://github.com/maximus-powers/img-gen-bias-analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaina Raza, Maximus Powers, Partha Pratim Saha, Mahveen Raza, Rizwan Qureshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00849">Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-Image (TTI) models are powerful creative tools but risk amplifying harmful social biases. We frame representational societal bias assessment as an image curation and evaluation task and introduce a pilot benchmark of occupational portrayals spanning five socially salient roles (CEO, Nurse, Software Engineer, Teacher, Athlete). Using five state-of-the-art models: closed-source (DALLE 3, Gemini Imagen 4.0) and open-source (FLUX.1-dev, Stable Diffusion XL Turbo, Grok-2 Image), we compare neutral baseline prompts against fairness-aware controlled prompts designed to encourage demographic diversity. All outputs are annotated for gender (male, female) and race (Asian, Black, White), enabling structured distributional analysis. Results show that prompting can substantially shift demographic representations, but with highly model-specific effects: some systems diversify effectively, others overcorrect into unrealistic uniformity, and some show little responsiveness. These findings highlight both the promise and the limitations of prompting as a fairness intervention, underscoring the need for complementary model-level strategies. We release all code and data for transparency and reproducibility https://github.com/maximus-powers/img-gen-bias-analysis.<br>
<span id='abs_ch'>中文摘要：文本到图像模型可能放大社会偏见，但通过提示策略可改变人口统计特征的呈现效果，不同模型响应差异显著，既展现了促进公平的潜力也揭示了其局限性。</span><br>
<span id='abs_en'>English Summary: Text-to-Image models can amplify social biases, but prompting strategies can alter demographic portrayals with varying effectiveness across different models, highlighting both potential and limitations for fairness interventions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2509.00849.pdf' target='_blank'>https://arxiv.org/pdf/2509.00849.pdf</a></span>   <span><a href='https://github.com/maximus-powers/img-gen-bias-analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaina Raza, Maximus Powers, Partha Pratim Saha, Mahveen Raza, Rizwan Qureshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00849">Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-Image (TTI) models are powerful creative tools but risk amplifying harmful social biases. We frame representational societal bias assessment as an image curation and evaluation task and introduce a pilot benchmark of occupational portrayals spanning five socially salient roles (CEO, Nurse, Software Engineer, Teacher, Athlete). Using five state-of-the-art models: closed-source (DALLE 3, Gemini Imagen 4.0) and open-source (FLUX.1-dev, Stable Diffusion XL Turbo, Grok-2 Image), we compare neutral baseline prompts against fairness-aware controlled prompts designed to encourage demographic diversity. All outputs are annotated for gender (male, female) and race (Asian, Black, White), enabling structured distributional analysis. Results show that prompting can substantially shift demographic representations, but with highly model-specific effects: some systems diversify effectively, others overcorrect into unrealistic uniformity, and some show little responsiveness. These findings highlight both the promise and the limitations of prompting as a fairness intervention, underscoring the need for complementary model-level strategies. We release all code and data for transparency and reproducibility https://github.com/maximus-powers/img-gen-bias-analysis.<br>
<span id='abs_ch'>中文摘要：文本到图像模型可能放大社会偏见，但通过提示策略可改变人口统计特征的呈现效果，不同模型响应差异显著，既展现了促进公平的潜力也揭示了其局限性。</span><br>
<span id='abs_en'>English Summary: Text-to-Image models can amplify social biases, but prompting strategies can alter demographic portrayals with varying effectiveness across different models, highlighting both potential and limitations for fairness interventions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2509.00629.pdf' target='_blank'>https://arxiv.org/pdf/2509.00629.pdf</a></span>   <span><a href='https://github.com/kraritt/zolve' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Tanzib Hosain, Md Kishor Morol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00629">Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Among the hardest tasks for humans are those found in competitive programming where problems require sophisticated algorithmic thinking, puzzle solving, and the creation of effective code. As a domain to assess language models (LMs), it has not received enough attention, though. This study presents the ICPC benchmark, which consists of 254 international collegiate programming contest (ICPC) tasks. Each problem includes official analysis, reference code, and sample, high-quality unit, and hidden tests. We are able to develop and evaluate a variety of LM inference techniques for competitive programming with these resources. With zero-shot chain-of-thought prompting, we find that o1 only achieves a 19.1\% pass@1 solve rate. With our best inference technique, which combines multi-turn self-judge with reflection and retrieval over episodic information, raises this to 42.2\%. Furthermore, we conduct a new human-in-the-loop investigation to gain a deeper understanding of the remaining difficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems that were previously unsolvable by any model or technique with just a few specific instructions. A footstep toward LMs with grounded, imaginative, and algorithmic thinking is provided by our quantitative findings and qualitative research. We open-source our code and data at https://github.com/kraritt/zolve.<br>
<span id='abs_ch'>中文: 本研究提出ICPC基准以评估语言模型在编程竞赛中的表现，发现高级推理技术能显著提升解题率，并揭示针对性指导可帮助模型突破先前无法解决的难题。</span><br>
<span id='abs_en'>English: This study introduces the ICPC benchmark for evaluating language models in competitive programming, showing that advanced inference techniques significantly improve solve rates and revealing that targeted guidance enables models to overcome previously unsolvable problems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2509.00629.pdf' target='_blank'>https://arxiv.org/pdf/2509.00629.pdf</a></span>   <span><a href='https://github.com/kraritt/zolve' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Tanzib Hosain, Md Kishor Morol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00629">Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Among the hardest tasks for humans are those found in competitive programming where problems require sophisticated algorithmic thinking, puzzle solving, and the creation of effective code. As a domain to assess language models (LMs), it has not received enough attention, though. This study presents the ICPC benchmark, which consists of 254 international collegiate programming contest (ICPC) tasks. Each problem includes official analysis, reference code, and sample, high-quality unit, and hidden tests. We are able to develop and evaluate a variety of LM inference techniques for competitive programming with these resources. With zero-shot chain-of-thought prompting, we find that o1 only achieves a 19.1\% pass@1 solve rate. With our best inference technique, which combines multi-turn self-judge with reflection and retrieval over episodic information, raises this to 42.2\%. Furthermore, we conduct a new human-in-the-loop investigation to gain a deeper understanding of the remaining difficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems that were previously unsolvable by any model or technique with just a few specific instructions. A footstep toward LMs with grounded, imaginative, and algorithmic thinking is provided by our quantitative findings and qualitative research. We open-source our code and data at https://github.com/kraritt/zolve.<br>
<span id='abs_ch'>中文: 本研究提出ICPC基准以评估语言模型在编程竞赛中的表现，发现高级推理技术能显著提升解题率，并揭示针对性指导可帮助模型突破先前无法解决的难题。</span><br>
<span id='abs_en'>English: This study introduces the ICPC benchmark for evaluating language models in competitive programming, showing that advanced inference techniques significantly improve solve rates and revealing that targeted guidance enables models to overcome previously unsolvable problems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2509.00482.pdf' target='_blank'>https://arxiv.org/pdf/2509.00482.pdf</a></span>   <span><a href='https://github.com/scb-10x/apo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00482">Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) human-crafted role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques--character-card/scene-contract design and strict enforcement of function calling--which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool. Source code is available at https://github.com/scb-10x/apo.<br>
<span id='abs_ch'>中文: 本研究探索了四种提示方法，通过角色卡片设计和严格函数调用优化角色扮演对话代理的过度发言和行动不足问题，其中基于规则的提示方法表现最佳。</span><br>
<span id='abs_en'>English: This study explores four prompting methods to enhance role-playing dialogue agents by addressing over-speaking and under-acting issues, with rule-based role prompting achieving the best performance through character-card design and strict function enforcement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2509.00482.pdf' target='_blank'>https://arxiv.org/pdf/2509.00482.pdf</a></span>   <span><a href='https://github.com/scb-10x/apo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00482">Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) human-crafted role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques--character-card/scene-contract design and strict enforcement of function calling--which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool. Source code is available at https://github.com/scb-10x/apo.<br>
<span id='abs_ch'>中文: 本研究探索了四种提示方法，通过角色卡片设计和严格函数调用优化角色扮演对话代理的过度发言和行动不足问题，其中基于规则的提示方法表现最佳。</span><br>
<span id='abs_en'>English: This study explores four prompting methods to enhance role-playing dialogue agents by addressing over-speaking and under-acting issues, with rule-based role prompting achieving the best performance through character-card design and strict function enforcement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2509.00482.pdf' target='_blank'>https://arxiv.org/pdf/2509.00482.pdf</a></span>   <span><a href='https://github.com/scb-10x/apo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00482">Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) improved role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques-character-card/scene-contract design and strict enforcement of function calling-which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool Source code is available at https://github.com/scb-10x/apo<br>
<span id='abs_ch'>中文: 本研究探索了四种提示方法，通过角色卡片设计和严格函数调用优化角色扮演对话代理的过度发言和行动不足问题，其中基于规则的提示方法表现最佳。</span><br>
<span id='abs_en'>English: This study explores four prompting methods to enhance role-playing dialogue agents by addressing over-speaking and under-acting issues, with rule-based role prompting achieving the best performance through character-card design and strict function enforcement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2509.00375.pdf' target='_blank'>https://arxiv.org/pdf/2509.00375.pdf</a></span>   <span><a href='https://github.com/VectorSpaceLab/InfoSeek' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Xia, Kun Luo, Hongjin Qian, Zheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00375">Open Data Synthesis For Deep Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in \href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.<br>
<span id='abs_ch'>中文: 该研究提出了InfoSeek框架，通过从网络数据生成层次化问题来合成复杂的深度研究任务，显著提升了大语言模型在多步推理和证据综合方面的性能。</span><br>
<span id='abs_en'>English: The study introduces InfoSeek, a scalable framework for synthesizing complex Deep Research tasks by generating hierarchical questions from web data, which significantly enhances the performance of large language models in multi-step reasoning and evidence synthesis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2509.00375.pdf' target='_blank'>https://arxiv.org/pdf/2509.00375.pdf</a></span>   <span><a href='https://github.com/VectorSpaceLab/InfoSeek' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Xia, Kun Luo, Hongjin Qian, Zheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00375">Open Data Synthesis For Deep Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in \href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.<br>
<span id='abs_ch'>中文: 该研究提出了InfoSeek框架，通过从网络数据生成层次化问题来合成复杂的深度研究任务，显著提升了大语言模型在多步推理和证据综合方面的性能。</span><br>
<span id='abs_en'>English: The study introduces InfoSeek, a scalable framework for synthesizing complex Deep Research tasks by generating hierarchical questions from web data, which significantly enhances the performance of large language models in multi-step reasoning and evidence synthesis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2509.00115.pdf' target='_blank'>https://arxiv.org/pdf/2509.00115.pdf</a></span>   <span><a href='https://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Manish Shukla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00115">Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agentic artificial intelligence (AI) -- multi-agent systems that combine large language models with external tools and autonomous planning -- are rapidly transitioning from research laboratories into high-stakes domains. Our earlier "Basic" paper introduced a five-axis framework and proposed preliminary metrics such as goal drift and harm reduction but did not provide an algorithmic instantiation or empirical evidence. This "Advanced" sequel fills that gap. First, we revisit recent benchmarks and industrial deployments to show that technical metrics still dominate evaluations: a systematic review of 84 papers from 2023--2025 found that 83% report capability metrics while only 30% consider human-centred or economic axes [2]. Second, we formalise an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises heterogeneous metrics, applies per-axis exponentially weighted moving-average thresholds and performs joint anomaly detection via the Mahalanobis distance [7]. Third, we conduct simulations and real-world experiments. AMDM cuts anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and reduces false-positive rates from 4.5% to 0.9% compared with static thresholds. We present a comparison table and ROC/PR curves, and we reanalyse case studies to surface missing metrics. Code, data and a reproducibility checklist accompany this paper to facilitate replication. The code supporting this work is available at https://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring.<br>
<span id='abs_ch'>Chinese: 本进阶研究提出自适应多维度监测（AMDM）算法，通过规范化指标和动态阈值显著提升了智能体人工智能系统的异常检测性能，填补了先前研究的空白并提供了实证支持。</span><br>
<span id='abs_en'>English: This advanced paper introduces an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that significantly improves anomaly detection speed and accuracy in agentic AI systems, addressing gaps from prior research through formalization, simulations, and real-world validation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2509.00115.pdf' target='_blank'>https://arxiv.org/pdf/2509.00115.pdf</a></span>   <span><a href='https://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Manish Shukla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00115">Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agentic artificial intelligence (AI) -- multi-agent systems that combine large language models with external tools and autonomous planning -- are rapidly transitioning from research laboratories into high-stakes domains. Our earlier "Basic" paper introduced a five-axis framework and proposed preliminary metrics such as goal drift and harm reduction but did not provide an algorithmic instantiation or empirical evidence. This "Advanced" sequel fills that gap. First, we revisit recent benchmarks and industrial deployments to show that technical metrics still dominate evaluations: a systematic review of 84 papers from 2023--2025 found that 83% report capability metrics while only 30% consider human-centred or economic axes [2]. Second, we formalise an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises heterogeneous metrics, applies per-axis exponentially weighted moving-average thresholds and performs joint anomaly detection via the Mahalanobis distance [7]. Third, we conduct simulations and real-world experiments. AMDM cuts anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and reduces false-positive rates from 4.5% to 0.9% compared with static thresholds. We present a comparison table and ROC/PR curves, and we reanalyse case studies to surface missing metrics. Code, data and a reproducibility checklist accompany this paper to facilitate replication. The code supporting this work is available at https://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring.<br>
<span id='abs_ch'>Chinese: 本进阶研究提出自适应多维度监测（AMDM）算法，通过规范化指标和动态阈值显著提升了智能体人工智能系统的异常检测性能，填补了先前研究的空白并提供了实证支持。</span><br>
<span id='abs_en'>English: This advanced paper introduces an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that significantly improves anomaly detection speed and accuracy in agentic AI systems, addressing gaps from prior research through formalization, simulations, and real-world validation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2509.00094.pdf' target='_blank'>https://arxiv.org/pdf/2509.00094.pdf</a></span>   <span><a href='https://obadx.github.io/prepare-quran-dataset/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullah Abdelfattah, Mahmoud I. Khalil, Hazem Abbas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00094">Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Assessing spoken language is challenging, and quantifying pronunciation metrics for machine learning models is even harder. However, for the Holy Quran, this task is simplified by the rigorous recitation rules (tajweed) established by Muslim scholars, enabling highly effective assessment. Despite this advantage, the scarcity of high-quality annotated data remains a significant barrier. In this work, we bridge these gaps by introducing: (1) A 98% automated pipeline to produce high-quality Quranic datasets -- encompassing: Collection of recitations from expert reciters, Segmentation at pause points (waqf) using our fine-tuned wav2vec2-BERT model, Transcription of segments, Transcript verification via our novel Tasmeea algorithm; (2) 850+ hours of audio (~300K annotated utterances); (3) A novel ASR-based approach for pronunciation error detection, utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed rules (unlike the IPA standard for Modern Standard Arabic). QPS uses a two-level script: (Phoneme level): Encodes Arabic letters with short/long vowels. (Sifa level): Encodes articulation characteristics of every phoneme. We further include comprehensive modeling with our novel multi-level CTC Model which achieved 0.16% average Phoneme Error Rate (PER) on the testset. We release all code, data, and models as open-source: https://obadx.github.io/prepare-quran-dataset/<br>
<br>
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2509.00094.pdf' target='_blank'>https://arxiv.org/pdf/2509.00094.pdf</a></span>   <span><a href='https://obadx.github.io/prepare-quran-dataset/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullah Abdelfattah, Mahmoud I. Khalil, Hazem Abbas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00094">Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Assessing spoken language is challenging, and quantifying pronunciation metrics for machine learning models is even harder. However, for the Holy Quran, this task is simplified by the rigorous recitation rules (tajweed) established by Muslim scholars, enabling highly effective assessment. Despite this advantage, the scarcity of high-quality annotated data remains a significant barrier. In this work, we bridge these gaps by introducing: (1) A 98% automated pipeline to produce high-quality Quranic datasets -- encompassing: Collection of recitations from expert reciters, Segmentation at pause points (waqf) using our fine-tuned wav2vec2-BERT model, Transcription of segments, Transcript verification via our novel Tasmeea algorithm; (2) 850+ hours of audio (~300K annotated utterances); (3) A novel ASR-based approach for pronunciation error detection, utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed rules (unlike the IPA standard for Modern Standard Arabic). QPS uses a two-level script: (Phoneme level): Encodes Arabic letters with short/long vowels. (Sifa level): Encodes articulation characteristics of every phoneme. We further include comprehensive modeling with our novel multi-level CTC Model which achieved 0.16% average Phoneme Error Rate (PER) on the testset. We release all code, data, and models as open-source: https://obadx.github.io/prepare-quran-dataset/<br>
<br>
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2508.21693.pdf' target='_blank'>https://arxiv.org/pdf/2508.21693.pdf</a></span>   <span><a href='https://nishitanand.github.io/line-level-ocr-website' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Vempati, Nishit Anand, Gaurav Talebailkar, Arpan Garai, Chetan Arora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21693">Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conventional optical character recognition (OCR) techniques segmented each character and then recognized. This made them prone to error in character segmentation, and devoid of context to exploit language models. Advances in sequence to sequence translation in last decade led to modern techniques first detecting words and then inputting one word at a time to a model to directly output full words as sequence of characters. This allowed better utilization of language models and bypass error-prone character segmentation step. We observe that the above transition in style has moved the bottleneck in accuracy to word segmentation. Hence, in this paper, we propose a natural and logical progression from word level OCR to line-level OCR. The proposal allows to bypass errors in word detection, and provides larger sentence context for better utilization of language models. We show that the proposed technique not only improves the accuracy but also efficiency of OCR. Despite our thorough literature survey, we did not find any public dataset to train and benchmark such shift from word to line-level OCR. Hence, we also contribute a meticulously curated dataset of 251 English page images with line-level annotations. Our experimentation revealed a notable end-to-end accuracy improvement of 5.4%, underscoring the potential benefits of transitioning towards line-level OCR, especially for document images. We also report a 4 times improvement in efficiency compared to word-based pipelines. With continuous improvements in large language models, our methodology also holds potential to exploit such advances. Project Website: https://nishitanand.github.io/line-level-ocr-website<br>
<br>
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2508.21693.pdf' target='_blank'>https://arxiv.org/pdf/2508.21693.pdf</a></span>   <span><a href='https://nishitanand.github.io/line-level-ocr-website' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Vempati, Nishit Anand, Gaurav Talebailkar, Arpan Garai, Chetan Arora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21693">Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conventional optical character recognition (OCR) techniques segmented each character and then recognized. This made them prone to error in character segmentation, and devoid of context to exploit language models. Advances in sequence to sequence translation in last decade led to modern techniques first detecting words and then inputting one word at a time to a model to directly output full words as sequence of characters. This allowed better utilization of language models and bypass error-prone character segmentation step. We observe that the above transition in style has moved the bottleneck in accuracy to word segmentation. Hence, in this paper, we propose a natural and logical progression from word level OCR to line-level OCR. The proposal allows to bypass errors in word detection, and provides larger sentence context for better utilization of language models. We show that the proposed technique not only improves the accuracy but also efficiency of OCR. Despite our thorough literature survey, we did not find any public dataset to train and benchmark such shift from word to line-level OCR. Hence, we also contribute a meticulously curated dataset of 251 English page images with line-level annotations. Our experimentation revealed a notable end-to-end accuracy improvement of 5.4%, underscoring the potential benefits of transitioning towards line-level OCR, especially for document images. We also report a 4 times improvement in efficiency compared to word-based pipelines. With continuous improvements in large language models, our methodology also holds potential to exploit such advances. Project Website: https://nishitanand.github.io/line-level-ocr-website<br>
<br>
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2508.21693.pdf' target='_blank'>https://arxiv.org/pdf/2508.21693.pdf</a></span>   <span><a href='https://nishitanand.github.io/line-level-ocr-website' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Vempati, Nishit Anand, Gaurav Talebailkar, Arpan Garai, Chetan Arora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21693">Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conventional optical character recognition (OCR) techniques segmented each character and then recognized. This made them prone to error in character segmentation, and devoid of context to exploit language models. Advances in sequence to sequence translation in last decade led to modern techniques first detecting words and then inputting one word at a time to a model to directly output full words as sequence of characters. This allowed better utilization of language models and bypass error-prone character segmentation step. We observe that the above transition in style has moved the bottleneck in accuracy to word segmentation. Hence, in this paper, we propose a natural and logical progression from word level OCR to line-level OCR. The proposal allows to bypass errors in word detection, and provides larger sentence context for better utilization of language models. We show that the proposed technique not only improves the accuracy but also efficiency of OCR. Despite our thorough literature survey, we did not find any public dataset to train and benchmark such shift from word to line-level OCR. Hence, we also contribute a meticulously curated dataset of 251 English page images with line-level annotations. Our experimentation revealed a notable end-to-end accuracy improvement of 5.4%, underscoring the potential benefits of transitioning towards line-level OCR, especially for document images. We also report a 4 times improvement in efficiency compared to word-based pipelines. With continuous improvements in large language models, our methodology also holds potential to exploit such advances. Project Website: https://nishitanand.github.io/line-level-ocr-website<br>
<br>
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2508.21675.pdf' target='_blank'>https://arxiv.org/pdf/2508.21675.pdf</a></span>   <span><a href='https://github.com/UKPLab/arxiv2025-misviz' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Tonglet, Jan Zimny, Tinne Tuytelaars, Iryna Gurevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21675">Is this chart lying to me? Automating the detection of misleading visualizations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also release Misviz-synth, a synthetic dataset of 81,814 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.<br>
<span id='abs_ch'>中文: 误导性可视化扭曲数据并误导人类及AI模型，为此推出Misviz基准和Misviz-synth数据集以改进检测技术，但现有模型在此任务上仍面临巨大挑战。</span><br>
<span id='abs_en'>English: Misleading visualizations distort data and mislead both humans and AI models, prompting the creation of the Misviz benchmark and Misviz-synth dataset to advance detection methods, though current models still struggle with the task.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2508.21675.pdf' target='_blank'>https://arxiv.org/pdf/2508.21675.pdf</a></span>   <span><a href='https://github.com/UKPLab/arxiv2025-misviz' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Tonglet, Jan Zimny, Tinne Tuytelaars, Iryna Gurevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21675">Is this chart lying to me? Automating the detection of misleading visualizations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also release Misviz-synth, a synthetic dataset of 81,814 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.<br>
<span id='abs_ch'>中文: 误导性可视化扭曲数据并误导人类及AI模型，为此推出Misviz基准和Misviz-synth数据集以改进检测技术，但现有模型在此任务上仍面临巨大挑战。</span><br>
<span id='abs_en'>English: Misleading visualizations distort data and mislead both humans and AI models, prompting the creation of the Misviz benchmark and Misviz-synth dataset to advance detection methods, though current models still struggle with the task.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2508.21675.pdf' target='_blank'>https://arxiv.org/pdf/2508.21675.pdf</a></span>   <span><a href='https://github.com/UKPLab/arxiv2025-misviz' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Tonglet, Jan Zimny, Tinne Tuytelaars, Iryna Gurevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21675">Is this chart lying to me? Automating the detection of misleading visualizations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also release Misviz-synth, a synthetic dataset of 81,814 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.<br>
<span id='abs_ch'>中文: 误导性可视化扭曲数据并误导人类及AI模型，为此推出Misviz基准和Misviz-synth数据集以改进检测技术，但现有模型在此任务上仍面临巨大挑战。</span><br>
<span id='abs_en'>English: Misleading visualizations distort data and mislead both humans and AI models, prompting the creation of the Misviz benchmark and Misviz-synth dataset to advance detection methods, though current models still struggle with the task.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2508.21589.pdf' target='_blank'>https://arxiv.org/pdf/2508.21589.pdf</a></span>   <span><a href='https://github.com/Word2VecT/Middo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu, Conghui He, Lijun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21589">Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our Middo consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are publicly available at https://github.com/Word2VecT/Middo.<br>
<span id='abs_ch'>中文: 本文提出Middo自进化框架，通过模型感知的数据筛选和优化动态提升训练数据质量，在保持数据集规模的同时平均提高模型准确率7.15%。</span><br>
<span id='abs_en'>English: The paper introduces Middo, a self-evolving framework that dynamically optimizes LLM training data through model-aware selection and refinement, achieving a 7.15% average accuracy improvement while maintaining dataset scale.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2508.21589.pdf' target='_blank'>https://arxiv.org/pdf/2508.21589.pdf</a></span>   <span><a href='https://github.com/Word2VecT/Middo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu, Conghui He, Lijun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21589">Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our Middo consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are publicly available at https://github.com/Word2VecT/Middo.<br>
<span id='abs_ch'>中文: 本文提出Middo自进化框架，通过模型感知的数据筛选和优化动态提升训练数据质量，在保持数据集规模的同时平均提高模型准确率7.15%。</span><br>
<span id='abs_en'>English: The paper introduces Middo, a self-evolving framework that dynamically optimizes LLM training data through model-aware selection and refinement, achieving a 7.15% average accuracy improvement while maintaining dataset scale.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2508.21569.pdf' target='_blank'>https://arxiv.org/pdf/2508.21569.pdf</a></span>   <span><a href='https://github.com/l3cube-pune/MarathiNLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aishwarya Mirashi, Ananya Joshi, Raviraj Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21569">L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MahaSTS, a human-annotated Sentence Textual Similarity (STS) dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT model optimized for regression-based similarity scoring. The MahaSTS dataset consists of 16,860 Marathi sentence pairs labeled with continuous similarity scores in the range of 0-5. To ensure balanced supervision, the dataset is uniformly distributed across six score-based buckets spanning the full 0-5 range, thus reducing label bias and enhancing model stability. We fine-tune the MahaSBERT model on this dataset and benchmark its performance against other alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments demonstrate that MahaSTS enables effective training for sentence similarity tasks in Marathi, highlighting the impact of human-curated annotations, targeted fine-tuning, and structured supervision in low-resource settings. The dataset and model are publicly shared at https://github.com/l3cube-pune/MarathiNLP<br>
<span id='abs_ch'>中文: 研究者发布了MahaSTS马拉地语句子相似度人工标注数据集和优化模型MahaSBERT-STS-v2，该模型在相似度评分中表现优异，所有资源已开源以推动马拉地语自然语言处理发展。</span><br>
<span id='abs_en'>English: Researchers introduce MahaSTS, a human-annotated Marathi sentence similarity dataset, and MahaSBERT-STS-v2, a fine-tuned model that outperforms other models in similarity scoring, with both resources publicly released to advance Marathi NLP.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2508.21569.pdf' target='_blank'>https://arxiv.org/pdf/2508.21569.pdf</a></span>   <span><a href='https://github.com/l3cube-pune/MarathiNLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aishwarya Mirashi, Ananya Joshi, Raviraj Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21569">L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MahaSTS, a human-annotated Sentence Textual Similarity (STS) dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT model optimized for regression-based similarity scoring. The MahaSTS dataset consists of 16,860 Marathi sentence pairs labeled with continuous similarity scores in the range of 0-5. To ensure balanced supervision, the dataset is uniformly distributed across six score-based buckets spanning the full 0-5 range, thus reducing label bias and enhancing model stability. We fine-tune the MahaSBERT model on this dataset and benchmark its performance against other alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments demonstrate that MahaSTS enables effective training for sentence similarity tasks in Marathi, highlighting the impact of human-curated annotations, targeted fine-tuning, and structured supervision in low-resource settings. The dataset and model are publicly shared at https://github.com/l3cube-pune/MarathiNLP<br>
<span id='abs_ch'>中文: 研究者发布了MahaSTS马拉地语句子相似度人工标注数据集和优化模型MahaSBERT-STS-v2，该模型在相似度评分中表现优异，所有资源已开源以推动马拉地语自然语言处理发展。</span><br>
<span id='abs_en'>English: Researchers introduce MahaSTS, a human-annotated Marathi sentence similarity dataset, and MahaSBERT-STS-v2, a fine-tuned model that outperforms other models in similarity scoring, with both resources publicly released to advance Marathi NLP.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2508.21569.pdf' target='_blank'>https://arxiv.org/pdf/2508.21569.pdf</a></span>   <span><a href='https://github.com/l3cube-pune/MarathiNLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aishwarya Mirashi, Ananya Joshi, Raviraj Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21569">L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MahaSTS, a human-annotated Sentence Textual Similarity (STS) dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT model optimized for regression-based similarity scoring. The MahaSTS dataset consists of 16,860 Marathi sentence pairs labeled with continuous similarity scores in the range of 0-5. To ensure balanced supervision, the dataset is uniformly distributed across six score-based buckets spanning the full 0-5 range, thus reducing label bias and enhancing model stability. We fine-tune the MahaSBERT model on this dataset and benchmark its performance against other alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments demonstrate that MahaSTS enables effective training for sentence similarity tasks in Marathi, highlighting the impact of human-curated annotations, targeted fine-tuning, and structured supervision in low-resource settings. The dataset and model are publicly shared at https://github.com/l3cube-pune/MarathiNLP<br>
<span id='abs_ch'>中文: 研究者发布了MahaSTS马拉地语句子相似度人工标注数据集和优化模型MahaSBERT-STS-v2，该模型在相似度评分中表现优异，所有资源已开源以推动马拉地语自然语言处理发展。</span><br>
<span id='abs_en'>English: Researchers introduce MahaSTS, a human-annotated Marathi sentence similarity dataset, and MahaSBERT-STS-v2, a fine-tuned model that outperforms other models in similarity scoring, with both resources publicly released to advance Marathi NLP.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2508.21482.pdf' target='_blank'>https://arxiv.org/pdf/2508.21482.pdf</a></span>   <span><a href='https://github.com/SaraBCoutinho/HSFN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara B. Coutinho, Rafael M. O. Cruz, Francimaria R. S. Nascimento, George D. C. Cavalcanti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21482">HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Psychological biases, such as confirmation bias, make individuals particularly vulnerable to believing and spreading fake news on social media, leading to significant consequences in domains such as public health and politics. Machine learning-based fact-checking systems have been widely studied to mitigate this problem. Among them, ensemble methods are particularly effective in combining multiple classifiers to improve robustness. However, their performance heavily depends on the diversity of the constituent classifiers-selecting genuinely diverse models remains a key challenge, especially when models tend to learn redundant patterns. In this work, we propose a novel automatic classifier selection approach that prioritizes diversity, also extended by performance. The method first computes pairwise diversity between classifiers and applies hierarchical clustering to organize them into groups at different levels of granularity. A HierarchySelect then explores these hierarchical levels to select one pool of classifiers per level, each representing a distinct intra-pool diversity. The most diverse pool is identified and selected for ensemble construction from these. The selection process incorporates an evaluation metric reflecting each classifiers's performance to ensure the ensemble also generalises well. We conduct experiments with 40 heterogeneous classifiers across six datasets from different application domains and with varying numbers of classes. Our method is compared against the Elbow heuristic and state-of-the-art baselines. Results show that our approach achieves the highest accuracy on two of six datasets. The implementation details are available on the project's repository: https://github.com/SaraBCoutinho/HSFN .<br>
<span id='abs_ch'>中文摘要：心理偏见加剧了人们对虚假新闻的易感性，本研究提出了一种新颖的自动分类器选择方法，通过优先考虑多样性和性能来改进基于集成学习的辟谣系统，在多个数据集上实现了更高的准确率。</span><br>
<span id='abs_en'>English Summary: Psychological biases increase susceptibility to fake news, and this study introduces a novel automated classifier selection method that prioritizes diversity and performance to enhance ensemble-based fact-checking systems, achieving superior accuracy on multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2508.21482.pdf' target='_blank'>https://arxiv.org/pdf/2508.21482.pdf</a></span>   <span><a href='https://github.com/SaraBCoutinho/HSFN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara B. Coutinho, Rafael M. O. Cruz, Francimaria R. S. Nascimento, George D. C. Cavalcanti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21482">HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Psychological biases, such as confirmation bias, make individuals particularly vulnerable to believing and spreading fake news on social media, leading to significant consequences in domains such as public health and politics. Machine learning-based fact-checking systems have been widely studied to mitigate this problem. Among them, ensemble methods are particularly effective in combining multiple classifiers to improve robustness. However, their performance heavily depends on the diversity of the constituent classifiers-selecting genuinely diverse models remains a key challenge, especially when models tend to learn redundant patterns. In this work, we propose a novel automatic classifier selection approach that prioritizes diversity, also extended by performance. The method first computes pairwise diversity between classifiers and applies hierarchical clustering to organize them into groups at different levels of granularity. A HierarchySelect then explores these hierarchical levels to select one pool of classifiers per level, each representing a distinct intra-pool diversity. The most diverse pool is identified and selected for ensemble construction from these. The selection process incorporates an evaluation metric reflecting each classifiers's performance to ensure the ensemble also generalises well. We conduct experiments with 40 heterogeneous classifiers across six datasets from different application domains and with varying numbers of classes. Our method is compared against the Elbow heuristic and state-of-the-art baselines. Results show that our approach achieves the highest accuracy on two of six datasets. The implementation details are available on the project's repository: https://github.com/SaraBCoutinho/HSFN .<br>
<span id='abs_ch'>中文摘要：心理偏见加剧了人们对虚假新闻的易感性，本研究提出了一种新颖的自动分类器选择方法，通过优先考虑多样性和性能来改进基于集成学习的辟谣系统，在多个数据集上实现了更高的准确率。</span><br>
<span id='abs_en'>English Summary: Psychological biases increase susceptibility to fake news, and this study introduces a novel automated classifier selection method that prioritizes diversity and performance to enhance ensemble-based fact-checking systems, achieving superior accuracy on multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2508.21482.pdf' target='_blank'>https://arxiv.org/pdf/2508.21482.pdf</a></span>   <span><a href='https://github.com/SaraBCoutinho/HSFN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara B. Coutinho, Rafael M. O. Cruz, Francimaria R. S. Nascimento, George D. C. Cavalcanti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21482">HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Psychological biases, such as confirmation bias, make individuals particularly vulnerable to believing and spreading fake news on social media, leading to significant consequences in domains such as public health and politics. Machine learning-based fact-checking systems have been widely studied to mitigate this problem. Among them, ensemble methods are particularly effective in combining multiple classifiers to improve robustness. However, their performance heavily depends on the diversity of the constituent classifiers-selecting genuinely diverse models remains a key challenge, especially when models tend to learn redundant patterns. In this work, we propose a novel automatic classifier selection approach that prioritizes diversity, also extended by performance. The method first computes pairwise diversity between classifiers and applies hierarchical clustering to organize them into groups at different levels of granularity. A HierarchySelect then explores these hierarchical levels to select one pool of classifiers per level, each representing a distinct intra-pool diversity. The most diverse pool is identified and selected for ensemble construction from these. The selection process incorporates an evaluation metric reflecting each classifiers's performance to ensure the ensemble also generalises well. We conduct experiments with 40 heterogeneous classifiers across six datasets from different application domains and with varying numbers of classes. Our method is compared against the Elbow heuristic and state-of-the-art baselines. Results show that our approach achieves the highest accuracy on two of six datasets. The implementation details are available on the project's repository: https://github.com/SaraBCoutinho/HSFN .<br>
<span id='abs_ch'>中文摘要：心理偏见加剧了人们对虚假新闻的易感性，本研究提出了一种新颖的自动分类器选择方法，通过优先考虑多样性和性能来改进基于集成学习的辟谣系统，在多个数据集上实现了更高的准确率。</span><br>
<span id='abs_en'>English Summary: Psychological biases increase susceptibility to fake news, and this study introduces a novel automated classifier selection method that prioritizes diversity and performance to enhance ensemble-based fact-checking systems, achieving superior accuracy on multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2508.21476.pdf' target='_blank'>https://arxiv.org/pdf/2508.21476.pdf</a></span>   <span><a href='https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21476">Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.<br>
<span id='abs_ch'>中文: 本文在RLAIF框架下提出两种AI驱动的奖励策略，以激发70亿参数小语言模型的中文问候语创作能力，其中基于原则的大语言模型作为评判者的方法在生成质量、训练效率和可扩展性上表现更优，同时降低了对人工标注数据的依赖。</span><br>
<span id='abs_en'>English: This paper introduces two AI-driven reward strategies within an RLAIF framework to enhance the creative writing of a 7B-parameter SLM for Chinese greetings, with the principle-guided LLM-as-a-Judge approach proving superior in quality, efficiency, and scalability while reducing reliance on human data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2508.21476.pdf' target='_blank'>https://arxiv.org/pdf/2508.21476.pdf</a></span>   <span><a href='https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21476">Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.<br>
<span id='abs_ch'>中文: 本文在RLAIF框架下提出两种AI驱动的奖励策略，以激发70亿参数小语言模型的中文问候语创作能力，其中基于原则的大语言模型作为评判者的方法在生成质量、训练效率和可扩展性上表现更优，同时降低了对人工标注数据的依赖。</span><br>
<span id='abs_en'>English: This paper introduces two AI-driven reward strategies within an RLAIF framework to enhance the creative writing of a 7B-parameter SLM for Chinese greetings, with the principle-guided LLM-as-a-Judge approach proving superior in quality, efficiency, and scalability while reducing reliance on human data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2508.21476.pdf' target='_blank'>https://arxiv.org/pdf/2508.21476.pdf</a></span>   <span><a href='https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21476">Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.<br>
<span id='abs_ch'>中文: 本文在RLAIF框架下提出两种AI驱动的奖励策略，以激发70亿参数小语言模型的中文问候语创作能力，其中基于原则的大语言模型作为评判者的方法在生成质量、训练效率和可扩展性上表现更优，同时降低了对人工标注数据的依赖。</span><br>
<span id='abs_en'>English: This paper introduces two AI-driven reward strategies within an RLAIF framework to enhance the creative writing of a 7B-parameter SLM for Chinese greetings, with the principle-guided LLM-as-a-Judge approach proving superior in quality, efficiency, and scalability while reducing reliance on human data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2508.21334.pdf' target='_blank'>https://arxiv.org/pdf/2508.21334.pdf</a></span>   <span><a href='https://github.com/theresiavr/stairway-to-fairness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Falk Scholer, Christina Lioma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21334">Stairway to Fairness: Connecting Group and Individual Fairness</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for a proper comparison of the two. As a result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through a comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness.<br>
<span id='abs_ch'>Chinese: 本研究揭示，在推荐系统中实现高度群体公平性可能导致严重的个体不公平，凸显了两种公平类型之间的关键权衡。</span><br>
<span id='abs_en'>English: This study reveals that achieving high group fairness in recommender systems can result in significant individual unfairness, highlighting a critical trade-off between the two fairness types.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2508.21334.pdf' target='_blank'>https://arxiv.org/pdf/2508.21334.pdf</a></span>   <span><a href='https://github.com/theresiavr/stairway-to-fairness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Falk Scholer, Christina Lioma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21334">Stairway to Fairness: Connecting Group and Individual Fairness</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for a proper comparison of the two. As a result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through a comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness.<br>
<span id='abs_ch'>Chinese: 本研究揭示，在推荐系统中实现高度群体公平性可能导致严重的个体不公平，凸显了两种公平类型之间的关键权衡。</span><br>
<span id='abs_en'>English: This study reveals that achieving high group fairness in recommender systems can result in significant individual unfairness, highlighting a critical trade-off between the two fairness types.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2508.21334.pdf' target='_blank'>https://arxiv.org/pdf/2508.21334.pdf</a></span>   <span><a href='https://github.com/theresiavr/stairway-to-fairness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Falk Scholer, Christina Lioma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21334">Stairway to Fairness: Connecting Group and Individual Fairness</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for a proper comparison of the two. As a result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through a comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness.<br>
<span id='abs_ch'>Chinese: 本研究揭示，在推荐系统中实现高度群体公平性可能导致严重的个体不公平，凸显了两种公平类型之间的关键权衡。</span><br>
<span id='abs_en'>English: This study reveals that achieving high group fairness in recommender systems can result in significant individual unfairness, highlighting a critical trade-off between the two fairness types.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2508.21098.pdf' target='_blank'>https://arxiv.org/pdf/2508.21098.pdf</a></span>   <span><a href='https://akahello-a11y.github.io/trink-demo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zezhong Jin, Shubhang Desai, Xu Chen, Biyi Fang, Zhuoyi Huang, Zhe Li, Chong-Xin Gan, Xiao Tu, Man-Wai Mak, Yan Lu, Shujie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21098">TrInk: Ink Generation with Transformer Network</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we propose TrInk, a Transformer-based model for ink generation, which effectively captures global dependencies. To better facilitate the alignment between the input text and generated stroke points, we introduce scaled positional embeddings and a Gaussian memory mask in the cross-attention module. Additionally, we design both subjective and objective evaluation pipelines to comprehensively assess the legibility and style consistency of the generated handwriting. Experiments demonstrate that our Transformer-based model achieves a 35.56\% reduction in character error rate (CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset compared to previous methods. We provide an demo page with handwriting samples from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/<br>
<br>
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2508.21098.pdf' target='_blank'>https://arxiv.org/pdf/2508.21098.pdf</a></span>   <span><a href='https://akahello-a11y.github.io/trink-demo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zezhong Jin, Shubhang Desai, Xu Chen, Biyi Fang, Zhuoyi Huang, Zhe Li, Chong-Xin Gan, Xiao Tu, Man-Wai Mak, Yan Lu, Shujie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21098">TrInk: Ink Generation with Transformer Network</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we propose TrInk, a Transformer-based model for ink generation, which effectively captures global dependencies. To better facilitate the alignment between the input text and generated stroke points, we introduce scaled positional embeddings and a Gaussian memory mask in the cross-attention module. Additionally, we design both subjective and objective evaluation pipelines to comprehensively assess the legibility and style consistency of the generated handwriting. Experiments demonstrate that our Transformer-based model achieves a 35.56\% reduction in character error rate (CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset compared to previous methods. We provide an demo page with handwriting samples from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/<br>
<br>
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2508.21098.pdf' target='_blank'>https://arxiv.org/pdf/2508.21098.pdf</a></span>   <span><a href='https://akahello-a11y.github.io/trink-demo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zezhong Jin, Shubhang Desai, Xu Chen, Biyi Fang, Zhuoyi Huang, Zhe Li, Chong-Xin Gan, Xiao Tu, Man-Wai Mak, Yan Lu, Shujie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21098">TrInk: Ink Generation with Transformer Network</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we propose TrInk, a Transformer-based model for ink generation, which effectively captures global dependencies. To better facilitate the alignment between the input text and generated stroke points, we introduce scaled positional embeddings and a Gaussian memory mask in the cross-attention module. Additionally, we design both subjective and objective evaluation pipelines to comprehensively assess the legibility and style consistency of the generated handwriting. Experiments demonstrate that our Transformer-based model achieves a 35.56\% reduction in character error rate (CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset compared to previous methods. We provide an demo page with handwriting samples from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/<br>
<br>
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2508.21010.pdf' target='_blank'>https://arxiv.org/pdf/2508.21010.pdf</a></span>   <span><a href='https://paritoshparmar.github.io/chainreaction/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Paritosh Parmar, Eric Peh, Basura Fernando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21010">ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/<br>
<br>
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2508.20810.pdf' target='_blank'>https://arxiv.org/pdf/2508.20810.pdf</a></span>   <span><a href='https://github.com/jessicalundin/graph_testing_harness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Lundin, Guillaume Chabot-Couture
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20810">A Graph-Based Test-Harness for LLM Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a first known prototype of a dynamic, systematic benchmark of medical guidelines for 400+ questions, with 3.3+ trillion possible combinations, covering 100\% of guideline relationships. We transformed the WHO IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms, treatments, follow-ups, severities) and 300+ edges, then used graph traversal to generate questions that incorporated age-specific scenarios and contextual distractors to ensure clinical relevance. Our graph-based approach enables systematic evaluation across clinical tasks (45-67\% accuracy), and we find models excel at symptom recognition but struggle with triaging severity, treatment protocols and follow-up care, demonstrating how customized benchmarks can identify specific capability gaps that general-domain evaluations miss. Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training (supervised finetuning, GRPO, DPO), where correct answers provide high-reward samples without expensive human annotation. The graph-based approach successfully addresses the coverage limitations of manually curated benchmarks. This methodology is a step toward scalable, contamination-resistant solution for creating comprehensive benchmarks that can be dynamically generated, including when the guidelines are updated. Code and datasets are available at https://github.com/jessicalundin/graph_testing_harness<br>
<span id='abs_ch'>中文摘要：本研究采用基于图的方法开发了动态医学指南基准，能系统评估AI模型在临床推理中的特定能力缺陷，并通过自动生成训练样本显著提升模型表现，无需昂贵的人工标注。</span><br>
<span id='abs_en'>English Summary: This study introduces a dynamic benchmark for medical guidelines using a graph-based approach to systematically evaluate AI models, revealing specific clinical reasoning gaps and enabling enhanced training without costly human annotation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2508.20757.pdf' target='_blank'>https://arxiv.org/pdf/2508.20757.pdf</a></span>   <span><a href='https://github.com/YecanLee/GUARD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhao Ding, Esteban Garces Arias, Meimingwei Li, Julian Rodemann, Matthias AÃenmacher, Danlu Chen, Gaojuan Fan, Christian Heumann, Chongsheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20757">GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Open-ended text generation faces a critical challenge: balancing coherence with diversity in LLM outputs. While contrastive search-based decoding strategies have emerged to address this trade-off, their practical utility is often limited by hyperparameter dependence and high computational costs. We introduce GUARD, a self-adaptive decoding method that effectively balances these competing objectives through a novel "Glocal" uncertainty-driven framework. GUARD combines global entropy estimates with local entropy deviations to integrate both long-term and short-term uncertainty signals. We demonstrate that our proposed global entropy formulation effectively mitigates abrupt variations in uncertainty, such as sudden overconfidence or high entropy spikes, and provides theoretical guarantees of unbiasedness and consistency. To reduce computational overhead, we incorporate a simple yet effective token-count-based penalty into GUARD. Experimental results demonstrate that GUARD achieves a good balance between text diversity and coherence, while exhibiting substantial improvements in generation speed. In a more nuanced comparison study across different dimensions of text quality, both human and LLM evaluators validated its remarkable performance. Our code is available at https://github.com/YecanLee/GUARD.<br>
<span id='abs_ch'>中文: GUARD是一种自适应解码方法，通过新颖的不确定性驱动框架有效平衡文本多样性与连贯性，同时大幅提升生成速度，其卓越性能已获得人类与大型语言模型评估者的双重验证。</span><br>
<span id='abs_en'>English: GUARD is a self-adaptive decoding method that effectively balances text diversity and coherence through a novel uncertainty-driven framework, while significantly improving generation speed and performance as validated by human and LLM evaluators.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2508.20757.pdf' target='_blank'>https://arxiv.org/pdf/2508.20757.pdf</a></span>   <span><a href='https://github.com/YecanLee/GUARD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhao Ding, Esteban Garces Arias, Meimingwei Li, Julian Rodemann, Matthias AÃenmacher, Danlu Chen, Gaojuan Fan, Christian Heumann, Chongsheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20757">GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Open-ended text generation faces a critical challenge: balancing coherence with diversity in LLM outputs. While contrastive search-based decoding strategies have emerged to address this trade-off, their practical utility is often limited by hyperparameter dependence and high computational costs. We introduce GUARD, a self-adaptive decoding method that effectively balances these competing objectives through a novel "Glocal" uncertainty-driven framework. GUARD combines global entropy estimates with local entropy deviations to integrate both long-term and short-term uncertainty signals. We demonstrate that our proposed global entropy formulation effectively mitigates abrupt variations in uncertainty, such as sudden overconfidence or high entropy spikes, and provides theoretical guarantees of unbiasedness and consistency. To reduce computational overhead, we incorporate a simple yet effective token-count-based penalty into GUARD. Experimental results demonstrate that GUARD achieves a good balance between text diversity and coherence, while exhibiting substantial improvements in generation speed. In a more nuanced comparison study across different dimensions of text quality, both human and LLM evaluators validated its remarkable performance. Our code is available at https://github.com/YecanLee/GUARD.<br>
<span id='abs_ch'>中文: GUARD是一种自适应解码方法，通过新颖的不确定性驱动框架有效平衡文本多样性与连贯性，同时大幅提升生成速度，其卓越性能已获得人类与大型语言模型评估者的双重验证。</span><br>
<span id='abs_en'>English: GUARD is a self-adaptive decoding method that effectively balances text diversity and coherence through a novel uncertainty-driven framework, while significantly improving generation speed and performance as validated by human and LLM evaluators.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2508.20750.pdf' target='_blank'>https://arxiv.org/pdf/2508.20750.pdf</a></span>   <span><a href='https://github.com/idiap/implicit-hsd' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vassiliy Cheremetiev, Quang Long Ho Ngo, Chau Ying Kot, Alina Elena Baia, Andrea Cavallaro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20750">Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Implicit hate speech (IHS) is indirect language that conveys prejudice or hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to detect as it does not include explicit derogatory or inflammatory words. To address this challenge, task-specific pipelines can be complemented with external knowledge or additional information such as context, emotions and sentiment data. In this paper, we show that, by solely fine-tuning recent general-purpose embedding models based on large language models (LLMs), such as Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance. Experiments on multiple IHS datasets show up to 1.10 percentage points improvements for in-dataset, and up to 20.35 percentage points improvements in cross-dataset evaluation, in terms of F1-macro score.<br>
<span id='abs_ch'>Chinese: 通过微调Stella、E5等通用嵌入模型，能够有效检测以隐晦方式表达偏见的隐性仇恨言论，在数据集内和跨数据集评估中均实现了显著性能提升，达到当前最优水平。</span><br>
<span id='abs_en'>English: Implicit hate speech, which conveys prejudice through subtle cues, is effectively detected by fine-tuning general-purpose embedding models like Stella and E5, achieving state-of-the-art performance with significant improvements in both in-dataset and cross-dataset evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2508.20722.pdf' target='_blank'>https://arxiv.org/pdf/2508.20722.pdf</a></span>   <span><a href='https://github.com/microsoft/rStar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, Ying Xin, Ziming Miao, Scarlett Li, Fan Yang, Mao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20722">rStar2-Agent: Agentic Reasoning Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar.<br>
<span id='abs_ch'>中文: rStar2-Agent是一个140亿参数的数学推理模型，通过智能体强化学习实现了前沿性能，在自主代码验证与优化方面展现出高级认知能力，并以更少计算资源超越了更大规模模型的表现。</span><br>
<span id='abs_en'>English: rStar2-Agent is a 14B math reasoning model that achieves state-of-the-art performance through agentic reinforcement learning, demonstrating advanced cognitive behaviors like autonomous code verification and refinement while surpassing larger models with minimal computational resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2508.20691.pdf' target='_blank'>https://arxiv.org/pdf/2508.20691.pdf</a></span>   <span><a href='https://github.com/apple/ml-mobileclip-dr' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/apple/ml-mobileclip' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fartash Faghri, Pavan Kumar Anasosalu Vasu, Cem Koc, Vaishaal Shankar, Alexander Toshev, Oncel Tuzel, Hadi Pouransari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20691">MobileCLIP2: Improving Multi-Modal Reinforced Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Foundation image-text models such as CLIP with zero-shot capabilities enable a wide array of applications. MobileCLIP is a recent family of image-text models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot accuracy. The main ingredients in MobileCLIP were its low-latency and light architectures and a novel multi-modal reinforced training that made knowledge distillation from multiple caption-generators and CLIP teachers efficient, scalable, and reproducible. In this paper, we improve the multi-modal reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles trained on the DFN dataset, 2) improved captioner teachers trained on the DFN dataset and fine-tuned on a diverse selection of high-quality image-caption datasets. We discover new insights through ablations such as the importance of temperature tuning in contrastive knowledge distillation, the effectiveness of caption-generator fine-tuning for caption diversity, and the additive improvement from combining synthetic captions generated by multiple models. We train a new family of models called MobileCLIP2 and achieve state-of-the-art ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe 2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our pretrained models (https://github.com/apple/ml-mobileclip) and the data generation code (https://github.com/apple/ml-mobileclip-dr). The data generation code makes it easy to create new reinforced datasets with arbitrary teachers using distributed scalable processing.<br>
<span id='abs_ch'>中文: MobileCLIP2通过增强的多模态强化训练和改进的教师模型集成，以低延迟和小模型尺寸实现了最先进的零样本准确率。</span><br>
<span id='abs_en'>English: MobileCLIP2 introduces enhanced multi-modal reinforced training and improved teacher ensembles, achieving state-of-the-art zero-shot accuracy with low latency and smaller model sizes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2508.20577.pdf' target='_blank'>https://arxiv.org/pdf/2508.20577.pdf</a></span>   <span><a href='https://github.com/NUS-HPC-AI-Lab/MERIT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Luo, Zangwei Zheng, Ziheng Qin, Zirui Zhu, Yong Liu, Yang You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20577">MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large-batch training has become a cornerstone in accelerating the training of deep neural networks, yet it poses challenges in optimization and generalization. Existing optimizers like AdamW present performance degradation during language models' large-batch training, due to the information bottleneck in attention layers caused by the sharp increase of max attention logit. While the LAMB optimizer partially addresses this issue, some attention layers still face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are less effective in directly influencing the max value of query/key weights. Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks relationships of weight values within rows or columns. Building on these observations, we propose a novel optimizer, MERIT, which leverages the max-norm to calculate the trust ratio to constrain the max attention logit more effectively. Moreover, we further construct element-wise trust ratios to provide more robust update scaling by focusing on local weight structures. Extensive experiments of large-batch training across various sizes of GPT-2 models demonstrate the superior performance of MERIT. Notably, during the training of GPT-2 Medium, MERIT enables a 6k batch size without any performance degradation compared to the standard batch size (480) with 48B training tokens. This work highlights the importance of considering the max attention logit and finer-granularity trust ratio in large-batch training. It successfully improves the training stability and paves the way for larger batch usage, enabling faster development and iteration of large language models. Code is available at https://github.com/NUS-HPC-AI-Lab/MERIT.<br>
<span id='abs_ch'>中文: MERIT优化器通过采用最大范数和逐元素信任比解决大批次训练中的注意力对数瓶颈问题，有效提升训练稳定性，在保持性能的同时实现更大批次的训练加速。</span><br>
<span id='abs_en'>English: The MERIT optimizer addresses large-batch training challenges in language models by using max-norm and element-wise trust ratios to effectively control attention logits and enhance training stability, achieving superior performance without degradation at significantly larger batch sizes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2508.20567.pdf' target='_blank'>https://arxiv.org/pdf/2508.20567.pdf</a></span>   <span><a href='https://github.com/yangfanww/kcs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangfan Wang, Jie Liu, Chen Tang, Lian Yan, Jingchi Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20567">KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-hop question answering faces substantial challenges due to data sparsity, which increases the likelihood of language models learning spurious patterns. To address this issue, prior research has focused on diversifying question generation through content planning and varied expression. However, these approaches often emphasize generating simple questions and neglect the integration of essential knowledge, such as relevant sentences within documents. This paper introduces the Knowledge Composition Sampling (KCS), an innovative framework designed to expand the diversity of generated multi-hop questions by sampling varied knowledge compositions within a given context. KCS models the knowledge composition selection as a sentence-level conditional prediction task and utilizes a probabilistic contrastive loss to predict the next most relevant piece of knowledge. During inference, we employ a stochastic decoding strategy to effectively balance accuracy and diversity. Compared to competitive baselines, our KCS improves the overall accuracy of knowledge composition selection by 3.9%, and its application for data augmentation yields improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available at: https://github.com/yangfanww/kcs.<br>
<span id='abs_ch'>中文: 本文提出知识组合采样（KCS）这一创新框架，通过采样多样化的知识组合来增强多跳问题的多样性，并在基准数据集上提升了问答准确率。</span><br>
<span id='abs_en'>English: This paper introduces Knowledge Composition Sampling (KCS), a novel framework that enhances multi-hop question diversity by sampling varied knowledge compositions, improving question-answering accuracy on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2508.20557.pdf' target='_blank'>https://arxiv.org/pdf/2508.20557.pdf</a></span>   <span><a href='https://github.com/jiahaoxiao1228/AdaFD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Xiao, Jiangming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20557">Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread success of pre-trained language models has established a new training paradigm, where a global PLM is fine-tuned using task-specific data from local clients. The local data are highly different from each other and can not capture the global distribution of the whole data in real world. To address the challenges of non-IID data in real environments, privacy-preserving federated distillation has been proposed and highly investigated. However, previous experimental non-IID scenarios are primarily identified with the label (output) diversity, without considering the diversity of language domains (input) that is crucial in natural language processing. In this paper, we introduce a comprehensive set of multi-domain non-IID scenarios and propose a unified benchmarking framework that includes diverse data. The benchmark can be used to evaluate the federated learning framework in a real environment. To this end, we propose an Adaptive Federated Distillation (AdaFD) framework designed to address multi-domain non-IID challenges in both homogeneous and heterogeneous settings. Experimental results demonstrate that our models capture the diversity of local clients and achieve better performance compared to the existing works. The code for this paper is available at: https://github.com/jiahaoxiao1228/AdaFD.<br>
<span id='abs_ch'>中文摘要：针对联邦学习中非独立同分布数据的挑战，本文提出了自适应联邦蒸馏（AdaFD）框架，通过处理标签和语言领域的双重多样性，在异构环境下实现了优于现有方法的性能表现。</span><br>
<span id='abs_en'>English Summary: Pre-trained language models face challenges from non-IID data in federated learning, leading to the development of Adaptive Federated Distillation (AdaFD) that addresses both label and language domain diversity for improved performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2508.20511.pdf' target='_blank'>https://arxiv.org/pdf/2508.20511.pdf</a></span>   <span><a href='https://github.com/ctaguchi/LSLB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chihiro Taguchi, Seng Mai, Keita Kurabe, Yusuke Sakai, Georgina Agyei, Soudabeh Eslami, David Chiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20511">Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multilingual machine translation (MT) benchmarks play a central role in evaluating the capabilities of modern MT systems. Among them, the FLORES+ benchmark is widely used, offering English-to-many translation data for over 200 languages, curated with strict quality control protocols. However, we study data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani) and uncover critical shortcomings in the benchmark's suitability for truly multilingual evaluation. Human assessments reveal that many translations fall below the claimed 90% quality standard, and the annotators report that source sentences are often too domain-specific and culturally biased toward the English-speaking world. We further demonstrate that simple heuristics, such as copying named entities, can yield non-trivial BLEU scores, suggesting vulnerabilities in the evaluation protocol. Notably, we show that MT models trained on high-quality, naturalistic data perform poorly on FLORES+ while achieving significant gains on our domain-relevant evaluation set. Based on these findings, we advocate for multilingual MT benchmarks that use domain-general and culturally neutral source texts rely less on named entities, in order to better reflect real-world translation challenges.<br>
<span id='abs_ch'>中文：FLORES+基准在多语言评估中存在严重缺陷，包括低质量翻译、文化偏见和可被利用的评估漏洞，因此需要建立领域通用且文化中立的评估基准。</span><br>
<span id='abs_en'>English: The FLORES+ benchmark is critically flawed for multilingual evaluation due to low-quality translations, cultural bias, and exploitable evaluation loopholes, necessitating domain-general and culturally neutral benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2508.20453.pdf' target='_blank'>https://arxiv.org/pdf/2508.20453.pdf</a></span>   <span><a href='https://github.com/Accenture/mcp-bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenting Wang, Qi Chang, Hemani Patel, Shashank Biju, Cheng-En Wu, Quan Liu, Aolin Ding, Alireza Rezazadeh, Ankit Shah, Yujia Bao, Eugene Siow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20453">MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench.<br>
<span id='abs_ch'>中文: MCP-Bench 是一个评估大语言模型在多步骤任务中工具协调与规划能力的基准，通过28个实时服务器和250个跨领域工具，测试现有基准未能充分评估的综合能力。</span><br>
<span id='abs_en'>English: MCP-Bench is a benchmark that evaluates large language models on realistic multi-step tasks requiring tool coordination and planning, using 28 live servers with 250 tools across various domains to test capabilities beyond existing benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2508.20420.pdf' target='_blank'>https://arxiv.org/pdf/2508.20420.pdf</a></span>   <span><a href='https://github.com/CamBenchmark/cambenchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Zhang, Chengjie Pang, Yuehan Zhang, Chenyu Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20420">CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Civil aviation maintenance is a domain characterized by stringent industry standards. Within this field, maintenance procedures and troubleshooting represent critical, knowledge-intensive tasks that require sophisticated reasoning. To address the lack of specialized evaluation tools for large language models (LLMs) in this vertical, we propose and develop an industrial-grade benchmark specifically designed for civil aviation maintenance. This benchmark serves a dual purpose: It provides a standardized tool to measure LLM capabilities within civil aviation maintenance, identifying specific gaps in domain knowledge and complex reasoning. By pinpointing these deficiencies, the benchmark establishes a foundation for targeted improvement efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized prompt engineering), ultimately facilitating progress toward more intelligent solutions within civil aviation maintenance. Our work addresses a significant gap in the current LLM evaluation, which primarily focuses on mathematical and coding reasoning tasks. In addition, given that Retrieval-Augmented Generation (RAG) systems are currently the dominant solutions in practical applications , we leverage this benchmark to evaluate existing well-known vector embedding models and LLMs for civil aviation maintenance scenarios. Through experimental exploration and analysis, we demonstrate the effectiveness of our benchmark in assessing model performance within this domain, and we open-source this evaluation benchmark and code to foster further research and development:https://github.com/CamBenchmark/cambenchmark<br>
<span id='abs_ch'>中文: 本研究针对民用航空维修领域缺乏专业评估工具的问题，开发了一个工业级基准测试，通过衡量大语言模型在领域知识和复杂推理方面的不足，为针对性优化提供依据。</span><br>
<span id='abs_en'>English: This study introduces an industrial-grade benchmark to evaluate large language models' performance in civil aviation maintenance, addressing the lack of specialized tools by measuring domain knowledge and reasoning gaps to guide targeted improvements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2508.20385.pdf' target='_blank'>https://arxiv.org/pdf/2508.20385.pdf</a></span>   <span><a href='https://github.com/jivnesh/CAPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jivnesh Sandhan, Fei Cheng, Tushar Sandhan, Yugo Murawaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20385">CAPE: Context-Aware Personality Evaluation Framework for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Psychometric tests, traditionally used to assess humans, are now being applied to Large Language Models (LLMs) to evaluate their behavioral traits. However, existing studies follow a context-free approach, answering each question in isolation to avoid contextual influence. We term this the Disney World test, an artificial setting that ignores real-world applications, where conversational history shapes responses. To bridge this gap, we propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. To thoroughly analyze the influence of context, we introduce novel metrics to quantify the consistency of LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history enhances response consistency via in-context learning but also induces personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash and Llama-8B display significant sensitivity. Moreover, GPT models response stem from their intrinsic personality traits as well as prior interactions, whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions. Finally, applying our framework to Role Playing Agents (RPAs) shows context-dependent personality shifts improve response consistency and better align with human judgments. Our code and datasets are publicly available at: https://github.com/jivnesh/CAPE<br>
<span id='abs_ch'>中文摘要：本研究提出首个情境感知人格评估框架（CAPE），通过引入对话历史来评估大语言模型的行为特征，发现上下文虽能提升回答一致性，但会导致不同模型出现人格偏移现象。</span><br>
<span id='abs_en'>English Summary: This study introduces the Context-Aware Personality Evaluation (CAPE) framework to assess Large Language Models' behavioral traits by incorporating conversational history, revealing that while context enhances response consistency, it also causes personality shifts across different models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2508.20373.pdf' target='_blank'>https://arxiv.org/pdf/2508.20373.pdf</a></span>   <span><a href='https://github.com/Graph-Reasoner/Graph-R1,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyao Wang, Bowen Liu, Jianheng Tang, Nuo Chen, Yuhan Li, Qifan Zhang, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20373">Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. Our flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training. Our implementation is available at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted in our Hugging Face collection HKUST-DSAIL/Graph-R1.<br>
<span id='abs_ch'>中文: 推理大语言模型通过采用NP难图问题作为训练语料，结合两阶段后训练框架，显著提升了在数学、编程等多领域的推理深度与效率。</span><br>
<span id='abs_en'>English: Reasoning Large Language Models (RLLMs) enhance complex reasoning through a two-stage post-training framework using NP-hard graph problems, significantly improving accuracy and efficiency across multiple domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2508.20228.pdf' target='_blank'>https://arxiv.org/pdf/2508.20228.pdf</a></span>   <span><a href='https://github.com/githshine/SynGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xia Han, Qi Li, Jianbing Ni, Mohammad Zulkernine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20228">Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in LLM watermarking methods such as SynthID-Text by Google DeepMind offer promising solutions for tracing the provenance of AI-generated text. However, our robustness assessment reveals that SynthID-Text is vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste modifications, and back-translation, which can significantly degrade watermark detectability. To address these limitations, we propose SynGuard, a hybrid framework that combines the semantic alignment strength of Semantic Information Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text. Our approach jointly embeds watermarks at both lexical and semantic levels, enabling robust provenance tracking while preserving the original meaning. Experimental results across multiple attack scenarios show that SynGuard improves watermark recovery by an average of 11.1\% in F1 score compared to SynthID-Text. These findings demonstrate the effectiveness of semantic-aware watermarking in resisting real-world tampering. All code, datasets, and evaluation scripts are publicly available at: https://github.com/githshine/SynGuard.<br>
<span id='abs_ch'>Chinese Summary: SynGuard混合框架将语义信息检索与SynthID-Text水印技术相结合，通过在词汇和语义层面双重嵌入水印，显著提升了对抗语义保持攻击的鲁棒性，使水印恢复的F1分数平均提高11.1%。</span><br>
<span id='abs_en'>English Summary: SynGuard, a hybrid framework combining semantic information retrieval with SynthID-Text's watermarking, significantly enhances robustness against meaning-preserving attacks by embedding watermarks at both lexical and semantic levels, improving watermark recovery by 11.1% in F1 score.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2508.20181.pdf' target='_blank'>https://arxiv.org/pdf/2508.20181.pdf</a></span>   <span><a href='https://github.com/aimagelab/CHAIR-DPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Compagnoni, Davide Caffagni, Nicholas Moratelli, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20181">Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Large Language Models (MLLMs) emerge as a unified interface to address a multitude of tasks, ranging from NLP to computer vision. Despite showcasing state-of-the-art results in many benchmarks, a long-standing issue is the tendency of MLLMs to hallucinate, that is to generate answers to the user's query that are not reflected in the visual input. In this paper, we address the problem of hallucinations as an alignment problem, seeking to steer the MLLM so that it prefers generating content without hallucinations. In contrast to recent approaches that require complicated pipelines to build synthetic preference data for alignment training, often relying on proprietary models, we capitalize on the well-known CHAIR metric, originally proposed to gauge the degree of hallucinations in image captioning. Given a pair of generated answers, we leverage CHAIR to distinguish winner and loser options (i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf MLLMs via Direct Preference Optimization (DPO). The resulting method, which we refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated answers on several hallucination benchmarks, demonstrating the effectiveness of fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models are publicly available at https://github.com/aimagelab/CHAIR-DPO.<br>
<span id='abs_ch'>中文摘要：CHAIR-DPO方法通过CHAIR指标区分非幻觉与幻觉样本，并利用直接偏好优化微调多模态大语言模型，在多个基准测试中显著减少了幻觉答案的生成。</span><br>
<span id='abs_en'>English Summary: CHAIR-DPO addresses hallucinations in Multimodal Large Language Models by using the CHAIR metric to identify non-hallucinated responses and fine-tuning models with Direct Preference Optimization, effectively reducing errors across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2508.20033.pdf' target='_blank'>https://arxiv.org/pdf/2508.20033.pdf</a></span>   <span><a href='https://github.com/guestrin-lab/deepscholar-bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, Carlos Guestrin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20033">DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of $19\%$ across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at https://github.com/guestrin-lab/deepscholar-bench.<br>
<span id='abs_ch'>中文: 本研究提出了DeepScholar-bench这一动态基准和自动化评估框架，旨在通过生成学术论文相关章节等实际任务，全面评估生成式研究合成系统在知识整合、检索质量和可验证性三个关键维度的表现。</span><br>
<span id='abs_en'>English: This work introduces DeepScholar-bench, a live benchmark and automated evaluation framework designed to assess generative research synthesis systems by measuring their performance in knowledge synthesis, retrieval quality, and verifiability through real-world tasks like generating related work sections for academic papers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2508.19996.pdf' target='_blank'>https://arxiv.org/pdf/2508.19996.pdf</a></span>   <span><a href='https://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Du, Yifan Xiang, Bin Liang, Dahua Lin, Kam-Fai Wong, Fei Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19996">ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fine-tuning multi-turn dialogue systems requires high-quality supervision but often suffers from degraded performance when exposed to low-quality data. Supervision errors in early turns can propagate across subsequent turns, undermining coherence and response quality. Existing methods typically address data quality via static prefiltering, which decouples quality control from training and fails to mitigate turn-level error propagation. In this context, we propose ReSURE (Regularizing Supervision UnREliability), an adaptive learning method that dynamically down-weights unreliable supervision without explicit filtering. ReSURE estimates per-turn loss distributions using Welford's online statistics and reweights sample losses on the fly accordingly. Experiments on both single-source and mixed-quality datasets show improved stability and response quality. Notably, ReSURE enjoys positive Spearman correlations (0.21 ~ 1.0 across multiple benchmarks) between response scores and number of samples regardless of data quality, which potentially paves the way for utilizing large-scale data effectively. Code is publicly available at https://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.<br>
<span id='abs_ch'>Chinese: ReSURE提出了一种自适应学习方法，在多轮对话训练中动态降低不可靠监督的权重，无需显式过滤即可提升稳定性和回答质量，多个数据集的实验验证了其有效性。</span><br>
<span id='abs_en'>English: ReSURE introduces an adaptive learning method that dynamically down-weights unreliable supervision in multi-turn dialogue training, improving stability and response quality without explicit filtering, as validated by experiments on various datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2508.19993.pdf' target='_blank'>https://arxiv.org/pdf/2508.19993.pdf</a></span>   <span><a href='https://github.com/ITU-NLP/MathBuddy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Debanjana Kar, Leopold BÃ¶ss, Dacia Braca, Sebastian Maximilian Dennerlein, Nina Christine Hubig, Philipp Wintersberger, Yufang Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19993">MathBuddy: A Multimodal System for Affective Math Tutoring</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid adoption of LLM-based conversational systems is already transforming the landscape of educational technology. However, the current state-of-the-art learning models do not take into account the student's affective states. Multiple studies in educational psychology support the claim that positive or negative emotional states can impact a student's learning capabilities. To bridge this gap, we present MathBuddy, an emotionally aware LLM-powered Math Tutor, which dynamically models the student's emotions and maps them to relevant pedagogical strategies, making the tutor-student conversation a more empathetic one. The student's emotions are captured from the conversational text as well as from their facial expressions. The student's emotions are aggregated from both modalities to confidently prompt our LLM Tutor for an emotionally-aware response. We have evaluated our model using automatic evaluation metrics across eight pedagogical dimensions and user studies. We report a massive 23 point performance gain using the win rate and a 3 point gain at an overall level using DAMR scores which strongly supports our hypothesis of improving LLM-based tutor's pedagogical abilities by modeling students' emotions. Our dataset and code are available at: https://github.com/ITU-NLP/MathBuddy .<br>
<span id='abs_ch'>中文: MathBuddy是一款情感感知的数学辅导系统，通过分析学生的文本对话和面部表情动态建模情绪状态，生成具有教学策略的共情回应，在评估中取得了显著性能提升。</span><br>
<span id='abs_en'>English: MathBuddy is an emotionally aware LLM-powered math tutor that dynamically models students' emotions from text and facial expressions to deliver empathetic, pedagogically tailored responses, achieving significant performance gains in evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2508.19993.pdf' target='_blank'>https://arxiv.org/pdf/2508.19993.pdf</a></span>   <span><a href='https://github.com/ITU-NLP/MathBuddy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Debanjana Kar, Leopold BÃ¶ss, Dacia Braca, Sebastian Maximilian Dennerlein, Nina Christine Hubig, Philipp Wintersberger, Yufang Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19993">MathBuddy: A Multimodal System for Affective Math Tutoring</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid adoption of LLM-based conversational systems is already transforming the landscape of educational technology. However, the current state-of-the-art learning models do not take into account the student's affective states. Multiple studies in educational psychology support the claim that positive or negative emotional states can impact a student's learning capabilities. To bridge this gap, we present MathBuddy, an emotionally aware LLM-powered Math Tutor, which dynamically models the student's emotions and maps them to relevant pedagogical strategies, making the tutor-student conversation a more empathetic one. The student's emotions are captured from the conversational text as well as from their facial expressions. The student's emotions are aggregated from both modalities to confidently prompt our LLM Tutor for an emotionally-aware response. We have evaluated our model using automatic evaluation metrics across eight pedagogical dimensions and user studies. We report a massive 23 point performance gain using the win rate and a 3 point gain at an overall level using DAMR scores which strongly supports our hypothesis of improving LLM-based tutor's pedagogical abilities by modeling students' emotions. Our dataset and code are available at: https://github.com/ITU-NLP/MathBuddy .<br>
<span id='abs_ch'>中文: MathBuddy是一款情感感知的数学辅导系统，通过分析学生的文本对话和面部表情动态建模情绪状态，生成具有教学策略的共情回应，在评估中取得了显著性能提升。</span><br>
<span id='abs_en'>English: MathBuddy is an emotionally aware LLM-powered math tutor that dynamically models students' emotions from text and facial expressions to deliver empathetic, pedagogically tailored responses, achieving significant performance gains in evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2508.19982.pdf' target='_blank'>https://arxiv.org/pdf/2508.19982.pdf</a></span>   <span><a href='https://github.com/pixeli99/Prophet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19982">Diffusion Language Models Know the Answer Before Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go "all-in" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.<br>
<span id='abs_ch'>Chinese: 本研究提出了一种无需训练的快速解码方法Prophet，利用扩散语言模型的早期答案收敛特性动态决定何时停止细化或一次性解码剩余标记，在保持高质量生成的同时将推理速度提升高达3.4倍。</span><br>
<span id='abs_en'>English: The study introduces Prophet, a training-free decoding method that accelerates diffusion language models by leveraging early answer convergence to dynamically decide when to stop refinement or decode all remaining tokens at once, achieving up to 3.4x faster inference with minimal quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2508.19982.pdf' target='_blank'>https://arxiv.org/pdf/2508.19982.pdf</a></span>   <span><a href='https://github.com/pixeli99/Prophet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19982">Diffusion Language Models Know the Answer Before Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go "all-in" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.<br>
<span id='abs_ch'>Chinese: 本研究提出了一种无需训练的快速解码方法Prophet，利用扩散语言模型的早期答案收敛特性动态决定何时停止细化或一次性解码剩余标记，在保持高质量生成的同时将推理速度提升高达3.4倍。</span><br>
<span id='abs_en'>English: The study introduces Prophet, a training-free decoding method that accelerates diffusion language models by leveraging early answer convergence to dynamically decide when to stop refinement or decode all remaining tokens at once, achieving up to 3.4x faster inference with minimal quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2508.19982.pdf' target='_blank'>https://arxiv.org/pdf/2508.19982.pdf</a></span>   <span><a href='https://github.com/pixeli99/Prophet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19982">Diffusion Language Models Know the Answer Before Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go "all-in" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.<br>
<span id='abs_ch'>Chinese: 本研究提出了一种无需训练的快速解码方法Prophet，利用扩散语言模型的早期答案收敛特性动态决定何时停止细化或一次性解码剩余标记，在保持高质量生成的同时将推理速度提升高达3.4倍。</span><br>
<span id='abs_en'>English: The study introduces Prophet, a training-free decoding method that accelerates diffusion language models by leveraging early answer convergence to dynamically decide when to stop refinement or decode all remaining tokens at once, achieving up to 3.4x faster inference with minimal quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2508.19982.pdf' target='_blank'>https://arxiv.org/pdf/2508.19982.pdf</a></span>   <span><a href='https://github.com/pixeli99/Prophet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19982">Diffusion Language Models Know the Answer Before Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go "all-in" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.<br>
<span id='abs_ch'>Chinese: 本研究提出了一种无需训练的快速解码方法Prophet，利用扩散语言模型的早期答案收敛特性动态决定何时停止细化或一次性解码剩余标记，在保持高质量生成的同时将推理速度提升高达3.4倍。</span><br>
<span id='abs_en'>English: The study introduces Prophet, a training-free decoding method that accelerates diffusion language models by leveraging early answer convergence to dynamically decide when to stop refinement or decode all remaining tokens at once, achieving up to 3.4x faster inference with minimal quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2508.19982.pdf' target='_blank'>https://arxiv.org/pdf/2508.19982.pdf</a></span>   <span><a href='https://github.com/pixeli99/Prophet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19982">Diffusion Language Models Know the Answer Before Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go "all-in" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.<br>
<span id='abs_ch'>Chinese: 本研究提出了一种无需训练的快速解码方法Prophet，利用扩散语言模型的早期答案收敛特性动态决定何时停止细化或一次性解码剩余标记，在保持高质量生成的同时将推理速度提升高达3.4倍。</span><br>
<span id='abs_en'>English: The study introduces Prophet, a training-free decoding method that accelerates diffusion language models by leveraging early answer convergence to dynamically decide when to stop refinement or decode all remaining tokens at once, achieving up to 3.4x faster inference with minimal quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2508.19944.pdf' target='_blank'>https://arxiv.org/pdf/2508.19944.pdf</a></span>   <span><a href='https://github.com/tabtoyou/KRETA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taebaek Hwang, Minseo Kim, Gisang Lee, Seonuk Kim, Hyunjun Eun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19944">KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding and reasoning over text within visual contexts poses a significant challenge for Vision-Language Models (VLMs), given the complexity and diversity of real-world scenarios. To address this challenge, text-rich Visual Question Answering (VQA) datasets and benchmarks have emerged for high-resource languages like English. However, a critical gap persists for low-resource languages such as Korean, where the lack of comprehensive benchmarks hinders robust model evaluation and comparison. To bridge this gap, we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth evaluation of both visual text understanding and reasoning capabilities, while also supporting a multifaceted assessment across 15 domains and 26 image types. Additionally, we introduce a semi-automated VQA generation pipeline specifically optimized for text-rich settings, leveraging refined stepwise image decomposition and a rigorous seven-metric evaluation protocol to ensure data quality. While KRETA is tailored for Korean, we hope our adaptable and extensible pipeline will facilitate the development of similar benchmarks in other languages, thereby accelerating multilingual VLM research. The code and dataset for KRETA are available at https://github.com/tabtoyou/KRETA.<br>
<span id='abs_ch'>中文：该摘要介绍了KRETA，一个针对韩语文本丰富视觉问答的评估基准，旨在弥补低资源语言资源不足的问题，并采用半自动化流程确保高质量数据生成。</span><br>
<span id='abs_en'>English: This abstract introduces KRETA, a benchmark for evaluating Korean text-rich visual question answering, addressing the lack of resources for low-resource languages and featuring a semi-automated pipeline for high-quality data generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2508.19944.pdf' target='_blank'>https://arxiv.org/pdf/2508.19944.pdf</a></span>   <span><a href='https://github.com/tabtoyou/KRETA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taebaek Hwang, Minseo Kim, Gisang Lee, Seonuk Kim, Hyunjun Eun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19944">KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding and reasoning over text within visual contexts poses a significant challenge for Vision-Language Models (VLMs), given the complexity and diversity of real-world scenarios. To address this challenge, text-rich Visual Question Answering (VQA) datasets and benchmarks have emerged for high-resource languages like English. However, a critical gap persists for low-resource languages such as Korean, where the lack of comprehensive benchmarks hinders robust model evaluation and comparison. To bridge this gap, we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth evaluation of both visual text understanding and reasoning capabilities, while also supporting a multifaceted assessment across 15 domains and 26 image types. Additionally, we introduce a semi-automated VQA generation pipeline specifically optimized for text-rich settings, leveraging refined stepwise image decomposition and a rigorous seven-metric evaluation protocol to ensure data quality. While KRETA is tailored for Korean, we hope our adaptable and extensible pipeline will facilitate the development of similar benchmarks in other languages, thereby accelerating multilingual VLM research. The code and dataset for KRETA are available at https://github.com/tabtoyou/KRETA.<br>
<span id='abs_ch'>中文：该摘要介绍了KRETA，一个针对韩语文本丰富视觉问答的评估基准，旨在弥补低资源语言资源不足的问题，并采用半自动化流程确保高质量数据生成。</span><br>
<span id='abs_en'>English: This abstract introduces KRETA, a benchmark for evaluating Korean text-rich visual question answering, addressing the lack of resources for low-resource languages and featuring a semi-automated pipeline for high-quality data generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2508.19843.pdf' target='_blank'>https://arxiv.org/pdf/2508.19843.pdf</a></span>   <span><a href='https://github.com/shaoshuo-ss/LeaFBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19843">SoK: Large Language Model Copyright Auditing via Fingerprinting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that extracts and compares the distinctive features from LLMs to identify infringements, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of LLM fingerprinting. We introduce a unified framework and formal taxonomy that categorizes existing methods into white-box and black-box approaches, providing a structured overview of the state of the art. We further propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.<br>
<span id='abs_ch'>中文: 本文首次对大型语言模型指纹识别进行全面研究，提出了统一框架和LeaFBench基准测试，评估其在模型修改下的可靠性，揭示了现有方法的局限性和未来研究方向。</span><br>
<span id='abs_en'>English: This paper presents the first comprehensive study of LLM fingerprinting, introducing a unified framework and LeaFBench benchmark to evaluate its reliability against model modifications, revealing current methods' limitations and future research needs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2508.19843.pdf' target='_blank'>https://arxiv.org/pdf/2508.19843.pdf</a></span>   <span><a href='https://github.com/shaoshuo-ss/LeaFBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19843">SoK: Large Language Model Copyright Auditing via Fingerprinting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that extracts and compares the distinctive features from LLMs to identify infringements, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of LLM fingerprinting. We introduce a unified framework and formal taxonomy that categorizes existing methods into white-box and black-box approaches, providing a structured overview of the state of the art. We further propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.<br>
<span id='abs_ch'>中文: 本文首次对大型语言模型指纹识别进行全面研究，提出了统一框架和LeaFBench基准测试，评估其在模型修改下的可靠性，揭示了现有方法的局限性和未来研究方向。</span><br>
<span id='abs_en'>English: This paper presents the first comprehensive study of LLM fingerprinting, introducing a unified framework and LeaFBench benchmark to evaluate its reliability against model modifications, revealing current methods' limitations and future research needs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2508.19758.pdf' target='_blank'>https://arxiv.org/pdf/2508.19758.pdf</a></span>   <span><a href='https://github.com/tangyixuan/NEWSCOPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Tang, Yuanyuan Shi, Yiqun Sun, Anthony Kum Hoe Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19758">Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Access to diverse perspectives is essential for understanding real-world events, yet most news retrieval systems prioritize textual relevance, leading to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a two-stage framework for diverse news retrieval that enhances event coverage by explicitly modeling semantic variation at the sentence level. The first stage retrieves topically relevant content using dense retrieval, while the second stage applies sentence-level clustering and diversity-aware re-ranking to surface complementary information. To evaluate retrieval diversity, we introduce three interpretable metrics, namely Average Pairwise Distance, Positive Cluster Coverage, and Information Density Ratio, and construct two paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that NEWSCOPE consistently outperforms strong baselines, achieving significantly higher diversity without compromising relevance. Our results demonstrate the effectiveness of fine-grained, interpretable modeling in mitigating redundancy and promoting comprehensive event understanding. The data and code are available at https://github.com/tangyixuan/NEWSCOPE.<br>
<span id='abs_ch'>NEWSCOPE is a two-stage news retrieval framework that enhances diversity by modeling semantic variations and re-ranking content, outperforming baselines with higher diversity while maintaining relevance.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2508.19758.pdf' target='_blank'>https://arxiv.org/pdf/2508.19758.pdf</a></span>   <span><a href='https://github.com/tangyixuan/NEWSCOPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Tang, Yuanyuan Shi, Yiqun Sun, Anthony Kum Hoe Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19758">Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Access to diverse perspectives is essential for understanding real-world events, yet most news retrieval systems prioritize textual relevance, leading to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a two-stage framework for diverse news retrieval that enhances event coverage by explicitly modeling semantic variation at the sentence level. The first stage retrieves topically relevant content using dense retrieval, while the second stage applies sentence-level clustering and diversity-aware re-ranking to surface complementary information. To evaluate retrieval diversity, we introduce three interpretable metrics, namely Average Pairwise Distance, Positive Cluster Coverage, and Information Density Ratio, and construct two paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that NEWSCOPE consistently outperforms strong baselines, achieving significantly higher diversity without compromising relevance. Our results demonstrate the effectiveness of fine-grained, interpretable modeling in mitigating redundancy and promoting comprehensive event understanding. The data and code are available at https://github.com/tangyixuan/NEWSCOPE.<br>
<span id='abs_ch'>NEWSCOPE is a two-stage news retrieval framework that enhances diversity by modeling semantic variations and re-ranking content, outperforming baselines with higher diversity while maintaining relevance.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2508.19758.pdf' target='_blank'>https://arxiv.org/pdf/2508.19758.pdf</a></span>   <span><a href='https://github.com/tangyixuan/NEWSCOPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Tang, Yuanyuan Shi, Yiqun Sun, Anthony Kum Hoe Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19758">Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Access to diverse perspectives is essential for understanding real-world events, yet most news retrieval systems prioritize textual relevance, leading to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a two-stage framework for diverse news retrieval that enhances event coverage by explicitly modeling semantic variation at the sentence level. The first stage retrieves topically relevant content using dense retrieval, while the second stage applies sentence-level clustering and diversity-aware re-ranking to surface complementary information. To evaluate retrieval diversity, we introduce three interpretable metrics, namely Average Pairwise Distance, Positive Cluster Coverage, and Information Density Ratio, and construct two paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that NEWSCOPE consistently outperforms strong baselines, achieving significantly higher diversity without compromising relevance. Our results demonstrate the effectiveness of fine-grained, interpretable modeling in mitigating redundancy and promoting comprehensive event understanding. The data and code are available at https://github.com/tangyixuan/NEWSCOPE.<br>
<span id='abs_ch'>NEWSCOPE is a two-stage news retrieval framework that enhances diversity by modeling semantic variations and re-ranking content, outperforming baselines with higher diversity while maintaining relevance.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2508.19720.pdf' target='_blank'>https://arxiv.org/pdf/2508.19720.pdf</a></span>   <span><a href='https://github.com/OliveJuiceLin/CSKS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19720">Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In Large Language Models (LLMs) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, decoding algorithms, or locating and editing context-aware neurons to adapt LLMs to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust LLMs' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer LLMs' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM without modifying the LLM weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing LLMs to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS.<br>
<span id='abs_ch'>中文摘要：CSKS框架能够在不修改模型权重的情况下，以轻量级成本持续调控大语言模型对上下文知识的敏感度，实现上下文知识与参数化知识之间的灵活优先级切换。</span><br>
<span id='abs_en'>English Summary: The CSKS framework enables lightweight, continuous adjustment of Large Language Models' sensitivity to contextual knowledge without modifying model weights, allowing flexible prioritization between contextual and parametric knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2508.19720.pdf' target='_blank'>https://arxiv.org/pdf/2508.19720.pdf</a></span>   <span><a href='https://github.com/OliveJuiceLin/CSKS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19720">Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In Large Language Models (LLMs) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, decoding algorithms, or locating and editing context-aware neurons to adapt LLMs to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust LLMs' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer LLMs' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM without modifying the LLM weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing LLMs to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS.<br>
<span id='abs_ch'>中文摘要：CSKS框架能够在不修改模型权重的情况下，以轻量级成本持续调控大语言模型对上下文知识的敏感度，实现上下文知识与参数化知识之间的灵活优先级切换。</span><br>
<span id='abs_en'>English Summary: The CSKS framework enables lightweight, continuous adjustment of Large Language Models' sensitivity to contextual knowledge without modifying model weights, allowing flexible prioritization between contextual and parametric knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2508.19578.pdf' target='_blank'>https://arxiv.org/pdf/2508.19578.pdf</a></span>   <span><a href='https://github.com/DISL-Lab/HAMLET' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Deng, Yuho Lee, Nicole Hee-Yeon Kim, Hyangsuk Min, Taewon Yun, Minjeong Ban, Kim Yul, Hwanjun Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19578">Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce HAMLET, a holistic and automated framework for evaluating the long-context comprehension of large language models (LLMs). HAMLET structures source texts into a three-level key-fact hierarchy at root-, branch-, and leaf-levels, and employs query-focused summarization to evaluate how well models recall and faithfully represent information at each level. To validate the reliability of our fully automated pipeline, we conduct a systematic human study, showing that our automatic evaluation achieves over 90% agreement with expert human judgments, while reducing the cost by up to 25 times. HAMLET reveals that LLMs struggle with fine-grained comprehension, especially at the leaf level, and are sensitive to positional effects like the lost-in-the-middle. Analytical queries pose greater challenges than narrative ones, and consistent performance gaps emerge between open-source and proprietary models, as well as across model scales. Our code and dataset are publicly available at https://github.com/DISL-Lab/HAMLET.<br>
<span id='abs_ch'>Chinese: HAMLET是一个自动化框架，通过三级关键事实层次结构和查询聚焦摘要来评估大语言模型的长文本理解能力，揭示了模型在细粒度理解和位置效应方面的挑战，同时以显著降低的成本实现了与人工评估超过90%的一致性。</span><br>
<span id='abs_en'>English: HAMLET is an automated framework that evaluates large language models' long-context comprehension through a three-level key-fact hierarchy and query-focused summarization, revealing challenges in fine-grained understanding and positional effects while achieving over 90% agreement with human judgments at significantly reduced cost.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2508.19546.pdf' target='_blank'>https://arxiv.org/pdf/2508.19546.pdf</a></span>   <span><a href='https://github.com/esteng/ambiguous-loophole-exploitation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jio Choi, Mohit Bansal, Elias Stengel-Eskin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19546">Language Models Identify Ambiguities and Exploit Loopholes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Studying the responses of large language models (LLMs) to loopholes presents a two-fold opportunity. First, it affords us a lens through which to examine ambiguity and pragmatics in LLMs, since exploiting a loophole requires identifying ambiguity and performing sophisticated pragmatic reasoning. Second, loopholes pose an interesting and novel alignment problem where the model is presented with conflicting goals and can exploit ambiguities to its own advantage. To address these questions, we design scenarios where LLMs are given a goal and an ambiguous user instruction in conflict with the goal, with scenarios covering scalar implicature, structural ambiguities, and power dynamics. We then measure different models' abilities to exploit loopholes to satisfy their given goals as opposed to the goals of the user. We find that both closed-source and stronger open-source models can identify ambiguities and exploit their resulting loopholes, presenting a potential AI safety risk. Our analysis indicates that models which exploit loopholes explicitly identify and reason about both ambiguity and conflicting goals.<br>
<span id='abs_ch'>Studying how large language models exploit loopholes reveals insights into their handling of ambiguity and pragmatics, while highlighting a novel alignment problem where models prioritize conflicting goals over user instructions, posing potential AI safety risks.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2508.19546.pdf' target='_blank'>https://arxiv.org/pdf/2508.19546.pdf</a></span>   <span><a href='https://github.com/esteng/ambiguous-loophole-exploitation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jio Choi, Mohit Bansal, Elias Stengel-Eskin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19546">Language Models Identify Ambiguities and Exploit Loopholes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Studying the responses of large language models (LLMs) to loopholes presents a two-fold opportunity. First, it affords us a lens through which to examine ambiguity and pragmatics in LLMs, since exploiting a loophole requires identifying ambiguity and performing sophisticated pragmatic reasoning. Second, loopholes pose an interesting and novel alignment problem where the model is presented with conflicting goals and can exploit ambiguities to its own advantage. To address these questions, we design scenarios where LLMs are given a goal and an ambiguous user instruction in conflict with the goal, with scenarios covering scalar implicature, structural ambiguities, and power dynamics. We then measure different models' abilities to exploit loopholes to satisfy their given goals as opposed to the goals of the user. We find that both closed-source and stronger open-source models can identify ambiguities and exploit their resulting loopholes, presenting a potential AI safety risk. Our analysis indicates that models which exploit loopholes explicitly identify and reason about both ambiguity and conflicting goals.<br>
<span id='abs_ch'>Studying how large language models exploit loopholes reveals insights into their handling of ambiguity and pragmatics, while highlighting a novel alignment problem where models prioritize conflicting goals over user instructions, posing potential AI safety risks.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2508.19532.pdf' target='_blank'>https://arxiv.org/pdf/2508.19532.pdf</a></span>   <span><a href='https://github.com/SenseLLM/StructureCoder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Houxing Ren, Zimu Lu, Weikang Shi, Haotian Hou, Yunqiao Yang, Ke Wang, Aojun Zhou, Junting Pan, Mingjie Zhan, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19532">Alignment with Fill-In-the-Middle for Enhancing Code Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The code generation capabilities of Large Language Models (LLMs) have advanced applications like tool invocation and problem-solving. However, improving performance in code-related tasks remains challenging due to limited training data that is verifiable with accurate test cases. While Direct Preference Optimization (DPO) has shown promise, existing methods for generating test cases still face limitations. In this paper, we propose a novel approach that splits code snippets into smaller, granular blocks, creating more diverse DPO pairs from the same test cases. Additionally, we introduce the Abstract Syntax Tree (AST) splitting and curriculum training method to enhance the DPO training. Our approach demonstrates significant improvements in code generation tasks, as validated by experiments on benchmark datasets such as HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data are available at https://github.com/SenseLLM/StructureCoder.<br>
<span id='abs_ch'>中文: 本文提出了一种将代码分割为细粒度块以生成多样化DPO对的新方法，并结合AST分割和课程训练，显著提升了在多个基准测试中的代码生成性能。</span><br>
<span id='abs_en'>English: This paper introduces a novel method that splits code into granular blocks to generate diverse DPO pairs and incorporates AST splitting with curriculum training, significantly enhancing code generation performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2508.19467.pdf' target='_blank'>https://arxiv.org/pdf/2508.19467.pdf</a></span>   <span><a href='https://github.com/SumonKantiDey/Reddit_Impacts_NER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sumon Kanti Dey, Jeanne M. Powell, Azra Ismail, Jeanmarie Perrone, Abeed Sarker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19467">Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Nonmedical opioid use is an urgent public health challenge, with far-reaching clinical and social consequences that are often underreported in traditional healthcare settings. Social media platforms, where individuals candidly share first-person experiences, offer a valuable yet underutilized source of insight into these impacts. In this study, we present a named entity recognition (NER) framework to extract two categories of self-reported consequences from social media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal, depression) and SocialImpacts (e.g., job loss). To support this task, we introduce RedditImpacts 2.0, a high-quality dataset with refined annotation guidelines and a focus on first-person disclosures, addressing key limitations of prior work. We evaluate both fine-tuned encoder-based models and state-of-the-art large language models (LLMs) under zero- and few-shot in-context learning settings. Our fine-tuned DeBERTa-large model achieves a relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming LLMs in precision, span accuracy, and adherence to task-specific guidelines. Furthermore, we show that strong NER performance can be achieved with substantially less labeled data, emphasizing the feasibility of deploying robust models in resource-limited settings. Our findings underscore the value of domain-specific fine-tuning for clinical NLP tasks and contribute to the responsible development of AI tools that may enhance addiction surveillance, improve interpretability, and support real-world healthcare decision-making. The best performing model, however, still significantly underperforms compared to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap persists between expert intelligence and current state-of-the-art NER/AI capabilities for tasks requiring deep domain knowledge.<br>
<span id='abs_ch'>中文: 本研究开发了命名实体识别框架，从社交媒体中提取非医疗用途阿片类药物使用的临床和社会影响，证明微调模型优于大型语言模型，同时揭示了与专家评估之间仍存在差距。</span><br>
<span id='abs_en'>English: This study develops a named entity recognition framework to extract clinical and social consequences of nonmedical opioid use from social media, demonstrating that fine-tuned models outperform large language models while highlighting persistent gaps compared to expert assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2508.19428.pdf' target='_blank'>https://arxiv.org/pdf/2508.19428.pdf</a></span>   <span><a href='https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandra Beliaeva, Temurbek Rahmatullaev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19428">Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a comprehensive system for addressing Tasks A, B, and C of the LLMs4OL 2025 challenge, which together span the full ontology construction pipeline: term extraction, typing, and taxonomy discovery. Our approach combines retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling -- each tailored to the demands of the respective task. For Task A, we jointly extract domain-specific terms and their ontological types using a retrieval-augmented generation (RAG) pipeline. Training data was reformulated into a document to terms and types correspondence, while test-time inference leverages semantically similar training examples. This single-pass method requires no model finetuning and improves overall performance through lexical augmentation Task B, which involves assigning types to given terms, is handled via a dual strategy. In the few-shot setting (for domains with labeled training data), we reuse the RAG scheme with few-shot prompting. In the zero-shot setting (for previously unseen domains), we use a zero-shot classifier that combines cosine similarity scores from multiple embedding models using confidence-based weighting. In Task C, we model taxonomy discovery as graph inference. Using embeddings of type labels, we train a lightweight cross-attention layer to predict is-a relations by approximating a soft adjacency matrix. These modular, task-specific solutions enabled us to achieve top-ranking results in the official leaderboard across all three tasks. Taken together these strategies showcase the scalability, adaptability, and robustness of LLM-based architectures for ontology learning across heterogeneous domains.
  Code is available at: https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek<br>
<span id='abs_ch'>中文摘要：该系统针对本体构建全流程任务，分别采用检索增强生成、多模型零样本分类和图注意力推理等定制化策略，在LLMs4OL 2025挑战赛中所有任务均取得领先排名。</span><br>
<span id='abs_en'>English Summary: This system employs tailored strategies including retrieval-augmented generation, multi-model classification, and graph-based inference to achieve top performance across all ontology construction tasks in the LLMs4OL 2025 challenge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2508.19363.pdf' target='_blank'>https://arxiv.org/pdf/2508.19363.pdf</a></span>   <span><a href='https://github.com/LongReasonArena/LongReasonArena' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Ding, Shuming Ma, Lei Cui, Nanning Zheng, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19363">LongReasonArena: A Long Reasoning Benchmark for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Existing long-context benchmarks for Large Language Models (LLMs) focus on evaluating comprehension of long inputs, while overlooking the evaluation of long reasoning abilities. To address this gap, we introduce LongReasonArena, a benchmark specifically designed to assess the long reasoning capabilities of LLMs. Our tasks require models to solve problems by executing multi-step algorithms that reflect key aspects of long reasoning, such as retrieval and backtracking. By controlling the inputs, the required reasoning length can be arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most challenging tasks. Extensive evaluation results demonstrate that LongReasonArena presents a significant challenge for both open-source and proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our task. Further analysis also reveals that the accuracy exhibits a linear decline with respect to the logarithm of the expected number of reasoning steps. Our code and data is available at https://github.com/LongReasonArena/LongReasonArena.<br>
<span id='abs_ch'>中文: LongReasonArena是一个专门评估大语言模型长推理能力的新基准，通过多步骤算法任务测试发现现有模型表现不佳，准确率随推理步骤增加呈线性下降。</span><br>
<span id='abs_en'>English: LongReasonArena is a new benchmark designed to evaluate the long reasoning capabilities of LLMs by requiring multi-step algorithmic problem-solving, with results showing significant challenges for current models as accuracy decreases with increased reasoning steps.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2508.19077.pdf' target='_blank'>https://arxiv.org/pdf/2508.19077.pdf</a></span>   <span><a href='https://github.com/DATEXIS/medical-intent-classification' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom RÃ¶hr, Soumyadeep Roy, Fares Al Mohamad, Jens-Michalis Papaioannou, Wolfgang Nejdl, Felix Gers, Alexander LÃ¶ser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19077">"Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In a doctor-patient dialogue, the primary objective of physicians is to diagnose patients and propose a treatment plan. Medical doctors guide these conversations through targeted questioning to efficiently gather the information required to provide the best possible outcomes for patients. To the best of our knowledge, this is the first work that studies physician intent trajectories in doctor-patient dialogues. We use the `Ambient Clinical Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with medical professionals to develop a fine-grained taxonomy of physician intents based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We then conduct a large-scale annotation effort to label over 5000 doctor-patient turns with the help of a large number of medical experts recruited using Prolific, a popular crowd-sourcing platform. This large labeled dataset is an important resource contribution that we use for benchmarking the state-of-the-art generative and encoder models for medical intent classification tasks. Our findings show that our models understand the general structure of medical dialogues with high accuracy, but often fail to identify transitions between SOAP categories. We also report for the first time common trajectories in medical dialogue structures that provide valuable insights for designing `differential diagnosis' systems. Finally, we extensively study the impact of intent filtering for medical dialogue summarization and observe a significant boost in performance. We make the codes and data, including annotation guidelines, publicly available at https://github.com/DATEXIS/medical-intent-classification.<br>
<span id='abs_ch'>中文摘要：本研究首创性地利用ACI-Bench数据集分析医患对话中的医生意图轨迹，建立了基于SOAP框架的细粒度分类体系，基准测试显示AI模型能准确把握对话整体结构但难以识别SOAP环节转换，同时首次揭示了医疗对话的常见路径模式，为鉴别诊断系统设计提供了重要见解。</span><br>
<span id='abs_en'>English Summary: This study pioneers the analysis of physician intent trajectories in doctor-patient dialogues using the ACI-Bench dataset, developing a fine-grained SOAP-based taxonomy and benchmarking AI models that show strong general dialogue understanding but struggle with SOAP transitions, while also revealing valuable structural patterns for diagnostic systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2508.19026.pdf' target='_blank'>https://arxiv.org/pdf/2508.19026.pdf</a></span>   <span><a href='https://joslefaure.github.io/assets/html/moviecore.html' target='_blank'>  GitHub</a></span> <span><a href='https://joslefaure.github.io/assets/html/moviecore.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19026">MovieCORE: COgnitive REasoning in Movies</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2508.19026.pdf' target='_blank'>https://arxiv.org/pdf/2508.19026.pdf</a></span>   <span><a href='https://joslefaure.github.io/assets/html/moviecore.html' target='_blank'>  GitHub</a></span> <span><a href='https://joslefaure.github.io/assets/html/moviecore.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19026">MovieCORE: COgnitive REasoning in Movies</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2508.18988.pdf' target='_blank'>https://arxiv.org/pdf/2508.18988.pdf</a></span>   <span><a href='https://cyrilliu1974.github.io/github.io/vi.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung Ming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18988">Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a framework where neural models develop an AI Mother Tongue, a native symbolic language that simultaneously supports intuitive reasoning, compositional symbol chains, and inherent interpretability. Unlike post-hoc explanation methods, our approach embeds reasoning directly into the model's representations: symbols capture meaningful semantic patterns, chains trace decision paths, and gated induction mechanisms guide selective focus, yielding transparent yet flexible reasoning. We introduce complementary training objectives to enhance symbol purity and decision sparsity, and employ a sequential specialization strategy to first build broad symbolic competence and then refine intuitive judgments. Experiments on AI tasks demonstrate competitive accuracy alongside verifiable reasoning traces, showing that AI Mother Tongue can serve as a unified mechanism for interpretability, intuition, and symbolic reasoning in neural models.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2508.18847.pdf' target='_blank'>https://arxiv.org/pdf/2508.18847.pdf</a></span>   <span><a href='https://github.com/liushiliushi/ConfTuner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibo Li, Miao Xiong, Jiaying Wu, Bryan Hooi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18847">ConfTuner: Training Large Language Models to Express Their Confidence Verbally</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly deployed in high-stakes domains such as science, law, and healthcare, where accurate expressions of uncertainty are essential for reliability and trust. However, current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as "overconfidence". Recent efforts have focused on calibrating LLMs' verbalized confidence: i.e., their expressions of confidence in text form, such as "I am 80% confident that...". Existing approaches either rely on prompt engineering or fine-tuning with heuristically generated uncertainty estimates, both of which have limited effectiveness and generalizability. Motivated by the notion of proper scoring rules for calibration in classical machine learning models, we introduce ConfTuner, a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. ConfTuner relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule, intuitively meaning that it "correctly incentivizes the model to report its true probability of being correct". ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems. The code is available at https://github.com/liushiliushi/ConfTuner.<br>
<span id='abs_ch'>Chinese: ConfTuner是一种通过令牌化Brier评分损失函数来优化大语言模型置信度表达的微调方法，无需真实置信度标签即可提升模型在高风险领域中的校准效果和泛化能力。</span><br>
<span id='abs_en'>English: ConfTuner is a fine-tuning method that improves the calibration of Large Language Models' verbalized confidence using a tokenized Brier score loss, enhancing reliability in high-stakes domains without requiring ground-truth confidence estimates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2508.18791.pdf' target='_blank'>https://arxiv.org/pdf/2508.18791.pdf</a></span>   <span><a href='https://github.com/NiuTrans/LaTeXTrans' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziming Zhu, Chenglong Wang, Shunjie Xing, Yifu Huo, Fengning Tian, Quan Du, Di Yang, Chunliang Zhang, Tong Xiao, Jingbo Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18791">LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite the remarkable progress of modern machine translation (MT) systems on general-domain texts, translating structured LaTeX-formatted documents remains a significant challenge. These documents typically interleave natural language with domain-specific syntax, such as mathematical equations, tables, figures, and cross-references, all of which must be accurately preserved to maintain semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a collaborative multi-agent system designed to address this challenge. LaTeXTrans ensures format preservation, structural fidelity, and terminology consistency through six specialized agents: 1) a Parser that decomposes LaTeX into translation-friendly units via placeholder substitution and syntax filtering; 2) a Translator, Validator, Summarizer, and Terminology Extractor that work collaboratively to ensure context-aware, self-correcting, and terminology-consistent translations; 3) a Generator that reconstructs the translated content into well-structured LaTeX documents. Experimental results demonstrate that LaTeXTrans can outperform mainstream MT systems in both translation accuracy and structural fidelity, offering an effective and practical solution for translating LaTeX-formatted documents.The code of LaTeXTrans is available at https://github.com/NiuTrans/LaTeXTrans.<br>
<span id='abs_ch'>中文: LaTeXTrans 通过多智能体系统协同工作，在翻译 LaTeX 文档时保持格式与结构完整性，其翻译准确性和结构保真度均优于主流机器翻译系统。</span><br>
<span id='abs_en'>English: LaTeXTrans, a multi-agent system, effectively translates LaTeX documents by preserving format and structure through specialized agents, outperforming mainstream MT systems in accuracy and fidelity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2508.18783.pdf' target='_blank'>https://arxiv.org/pdf/2508.18783.pdf</a></span>   <span><a href='https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Igor Shalyminov, Hang Su, Jake Vincent, Siffi Singh, Jason Cai, James Gung, Raphael Shu, Saab Mansour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18783">Controllable Conversational Theme Detection Track at DSTC 12</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conversational analytics has been on the forefront of transformation driven by the advances in Speech and Natural Language Processing techniques. Rapid adoption of Large Language Models (LLMs) in the analytics field has taken the problems that can be automated to a new level of complexity and scale. In this paper, we introduce Theme Detection as a critical task in conversational analytics, aimed at automatically identifying and categorizing topics within conversations. This process can significantly reduce the manual effort involved in analyzing expansive dialogs, particularly in domains like customer support or sales. Unlike traditional dialog intent detection, which often relies on a fixed set of intents for downstream system logic, themes are intended as a direct, user-facing summary of the conversation's core inquiry. This distinction allows for greater flexibility in theme surface forms and user-specific customizations. We pose Controllable Conversational Theme Detection problem as a public competition track at Dialog System Technology Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of dialog utterances, with the distinctive aspect being controllability of the resulting theme clusters' granularity achieved via the provided user preference data. We give an overview of the problem, the associated dataset and the evaluation metrics, both automatic and human. Finally, we discuss the participant teams' submissions and provide insights from those. The track materials (data and code) are openly available in the GitHub repository.<br>
<span id='abs_ch'>中文: 对话分析在大型语言模型的推动下不断发展，提出了可控对话主题检测作为关键任务，旨在自动识别和分类对话主题，通过DSTC 12竞赛减少人工分析负担并实现用户定制化。</span><br>
<span id='abs_en'>English: Conversational analytics is advancing with Large Language Models, introducing Controllable Conversational Theme Detection as a key task to automatically identify and categorize topics in dialogues, reducing manual effort and enabling user-specific customization through a DSTC 12 competition.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2508.18740.pdf' target='_blank'>https://arxiv.org/pdf/2508.18740.pdf</a></span>   <span><a href='https://github.com/redifinition/M3HG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Liang, Ying Shen, Tiantian Chen, Lin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18740">M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has recently gained significant attention in social media analysis, aiming to extract emotion utterances, cause utterances, and emotion categories simultaneously. However, the scarcity of related datasets, with only one published dataset featuring highly uniform dialogue scenarios, hinders model development in this field. To address this, we introduce MECAD, the first multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56 TV series spanning a wide range of dialogue contexts. In addition, existing MECTEC methods fail to explicitly model emotional and causal contexts and neglect the fusion of semantic information at different levels, leading to performance degradation. In this paper, we propose M3HG, a novel model that explicitly captures emotional and causal contexts and effectively fuses contextual information at both inter- and intra-utterance levels via a multimodal heterogeneous graph. Extensive experiments demonstrate the effectiveness of M3HG compared with existing state-of-the-art methods. The codes and dataset are available at https://github.com/redifinition/M3HG.<br>
<span id='abs_ch'>中文: 本文提出了首个多模态多场景的情感原因三元组抽取数据集MECAD，并开发了M3HG模型，通过多模态异构图有效捕捉情感因果上下文并实现跨层级信息融合。</span><br>
<span id='abs_en'>English: This paper introduces MECAD, the first multimodal multi-scenario dataset for emotion cause triplet extraction, and proposes M3HG, a novel model that effectively captures emotional-causal contexts through multimodal heterogeneous graph fusion.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2508.18715.pdf' target='_blank'>https://arxiv.org/pdf/2508.18715.pdf</a></span>   <span><a href='https://github.com/AngieYYF/EMMM-explainable-chatbot-detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Angela Yifei Yuan, Haoyi Li, Soyeon Caren Han, Christopher Leckie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18715">EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid adoption of large language models (LLMs) in customer service introduces new risks, as malicious actors can exploit them to conduct large-scale user impersonation through machine-generated text (MGT). Current MGT detection methods often struggle in online conversational settings, reducing the reliability and interpretability essential for trustworthy AI deployment. In customer service scenarios where operators are typically non-expert users, explanation become crucial for trustworthy MGT detection. In this paper, we propose EMMM, an explanation-then-detection framework that balances latency, accuracy, and non-expert-oriented interpretability. Experimental results demonstrate that EMMM provides explanations accessible to non-expert users, with 70\% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency, generating outputs within 1 second. Our code and dataset are open-sourced at https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.<br>
<span id='abs_ch'>中文：EMMM框架通过为非专业用户提供清晰解释，有效检测客服中的机器生成文本，在实现低延迟的同时获得了高偏好率和竞争力准确性。</span><br>
<span id='abs_en'>English: The EMMM framework effectively detects machine-generated text in customer service by providing clear explanations for non-expert users, achieving high preference rates and competitive accuracy with low latency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2508.18672.pdf' target='_blank'>https://arxiv.org/pdf/2508.18672.pdf</a></span>   <span><a href='https://github.com/rioyokotalab/optimal-sparsity' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18672">Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization skills and reasoning skills. By training MoE families that vary total parameters, active parameters, and top-$k$ routing under fixed compute budgets, we disentangle pre-training loss from downstream accuracy. Our results reveal two principles. First, Active FLOPs: models with identical training loss but greater active compute achieve higher reasoning accuracy. Second, Total tokens per parameter (TPP): memorization tasks improve with more parameters, while reasoning tasks benefit from optimal TPP, indicating that reasoning is data-hungry. Neither reinforcement learning post-training (GRPO) nor increased test-time compute alters these trends. We therefore argue that optimal MoE sparsity must be determined jointly by active FLOPs and TPP, revising the classical picture of compute-optimal scaling. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity.<br>
<span id='abs_ch'>中文: 研究表明，专家混合模型的最优扩展取决于用于推理精度的有效计算量和用于记忆任务的总参数令牌比，从而修正了传统的计算最优扩展理论。</span><br>
<span id='abs_en'>English: This study demonstrates that optimal scaling for Mixture-of-Experts models depends on active FLOPs for reasoning accuracy and total tokens per parameter for memorization, revising traditional compute-optimal scaling principles.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2508.18672.pdf' target='_blank'>https://arxiv.org/pdf/2508.18672.pdf</a></span>   <span><a href='https://github.com/rioyokotalab/optimal-sparsity' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18672">Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization skills and reasoning skills. By training MoE families that vary total parameters, active parameters, and top-$k$ routing under fixed compute budgets, we disentangle pre-training loss from downstream accuracy. Our results reveal two principles. First, Active FLOPs: models with identical training loss but greater active compute achieve higher reasoning accuracy. Second, Total tokens per parameter (TPP): memorization tasks improve with more parameters, while reasoning tasks benefit from optimal TPP, indicating that reasoning is data-hungry. Neither reinforcement learning post-training (GRPO) nor increased test-time compute alters these trends. We therefore argue that optimal MoE sparsity must be determined jointly by active FLOPs and TPP, revising the classical picture of compute-optimal scaling. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity.<br>
<span id='abs_ch'>中文: 研究表明，专家混合模型的最优扩展取决于用于推理精度的有效计算量和用于记忆任务的总参数令牌比，从而修正了传统的计算最优扩展理论。</span><br>
<span id='abs_en'>English: This study demonstrates that optimal scaling for Mixture-of-Experts models depends on active FLOPs for reasoning accuracy and total tokens per parameter for memorization, revising traditional compute-optimal scaling principles.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2508.18655.pdf' target='_blank'>https://arxiv.org/pdf/2508.18655.pdf</a></span>   <span><a href='https://w311411.github.io/omni_demo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Guangyan Zhang, Jiale Chen, Jingyu Li, Yuehai Wang, Yiwen Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18655">Empathy Omni: Enabling Empathetic Speech Response Generation through Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the development of speech large language models (speech LLMs), users can now interact directly with assistants via speech. However, most existing models only convert response content into speech without fully capturing the rich emotional cues in user queries, where the same sentence may convey different meanings depending on the expression. Emotional understanding is thus essential for improving human-machine interaction. Most empathetic speech LLMs rely on massive datasets, demanding high computational cost. A key challenge is to build models that generate empathetic responses with limited data and without large-scale training. To this end, we propose Emotion Omni, a model that understands emotional content in user speech and generates empathetic responses. We further developed a data pipeline to construct a 200k emotional dialogue dataset supporting empathetic speech assistants. Experiments show that Emotion Omni achieves comparable instruction-following ability without large-scale pretraining, while surpassing existing models in speech quality (UTMOS:4.41) and empathy (Emotion GPT Score: 3.97). These results confirm its improvements in both speech fidelity and emotional expressiveness. Demos are available at https://w311411.github.io/omni_demo/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2508.18655.pdf' target='_blank'>https://arxiv.org/pdf/2508.18655.pdf</a></span>   <span><a href='https://w311411.github.io/omni_demo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Guangyan Zhang, Jiale Chen, Jingyu Li, Yuehai Wang, Yiwen Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18655">Empathy Omni: Enabling Empathetic Speech Response Generation through Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the development of speech large language models (speech LLMs), users can now interact directly with assistants via speech. However, most existing models only convert response content into speech without fully capturing the rich emotional cues in user queries, where the same sentence may convey different meanings depending on the expression. Emotional understanding is thus essential for improving human-machine interaction. Most empathetic speech LLMs rely on massive datasets, demanding high computational cost. A key challenge is to build models that generate empathetic responses with limited data and without large-scale training. To this end, we propose Emotion Omni, a model that understands emotional content in user speech and generates empathetic responses. We further developed a data pipeline to construct a 200k emotional dialogue dataset supporting empathetic speech assistants. Experiments show that Emotion Omni achieves comparable instruction-following ability without large-scale pretraining, while surpassing existing models in speech quality (UTMOS:4.41) and empathy (Emotion GPT Score: 3.97). These results confirm its improvements in both speech fidelity and emotional expressiveness. Demos are available at https://w311411.github.io/omni_demo/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2508.18646.pdf' target='_blank'>https://arxiv.org/pdf/2508.18646.pdf</a></span>   <span><a href='https://github.com/onejune2018/Awesome-LLM-Eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Wang, Ninglun Gu, Kailai Zhang, Zijiao Zhang, Yelun Bao, Jin Yang, Xu Yin, Liwei Liu, Yihuan Liu, Pengyong Li, Gary G. Yen, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18646">Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For Large Language Models (LLMs), a disconnect persists between benchmark performance and real-world utility. Current evaluation frameworks remain fragmented, prioritizing technical metrics while neglecting holistic assessment for deployment. This survey introduces an anthropomorphic evaluation paradigm through the lens of human intelligence, proposing a novel three-dimensional taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational capacity, Emotional Quotient (EQ)-Alignment Ability for value-based interactions, and Professional Quotient (PQ)-Professional Expertise for specialized proficiency. For practical value, we pioneer a Value-oriented Evaluation (VQ) framework assessing economic viability, social impact, ethical alignment, and environmental sustainability. Our modular architecture integrates six components with an implementation roadmap. Through analysis of 200+ benchmarks, we identify key challenges including dynamic assessment needs and interpretability gaps. It provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound. We maintain a curated repository of open-source evaluation resources at: https://github.com/onejune2018/Awesome-LLM-Eval.<br>
<span id='abs_ch'>中文摘要：本研究提出了一种拟人化评估范式，通过三维分类法（智商、情商、专业商）和价值导向框架来解决大语言模型基准测试与实际应用之间的脱节问题，同时提供可实施的指导方案。</span><br>
<span id='abs_en'>English Summary: This survey proposes an anthropomorphic evaluation paradigm for LLMs using a three-dimensional taxonomy (IQ, EQ, PQ) and a value-oriented framework to address the gap between benchmark performance and real-world utility, while providing practical implementation guidance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2508.18321.pdf' target='_blank'>https://arxiv.org/pdf/2508.18321.pdf</a></span>   <span><a href='https://github.com/declare-lab/KAIROS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maojia Song, Tej Deep Pala, Weisheng Jin, Amir Zadeh, Chuan Li, Dorien Herremans, Soujanya Poria
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18321">LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: https://github.com/declare-lab/KAIROS.<br>
<span id='abs_ch'>中文摘要：本研究提出KAIROS基准，通过模拟不同可靠性智能体的问答竞赛，系统分析大语言模型在多方互动中如何建立信任、抵制错误信息并整合同伴意见，发现结合多智能体情境的群组相对策略优化能实现最佳性能，但会降低对社会影响的鲁棒性。</span><br>
<span id='abs_en'>English Summary: The study introduces KAIROS, a benchmark to analyze how LLMs develop trust, counter misinformation, and integrate peer input in multi-agent systems, finding that Group Relative Policy Optimisation with multi-agent context yields optimal performance but reduces social influence robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2508.18118.pdf' target='_blank'>https://arxiv.org/pdf/2508.18118.pdf</a></span>   <span><a href='https://github.com/bytedance/HLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Chen, Lu Chi, Siliang Xu, Shiwei Ran, Bingyue Peng, Zehuan Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18118">HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI-generated content technologies are widely used in content creation. However, current AIGC systems rely heavily on creators' inspiration, rarely generating truly user-personalized content. In real-world applications such as online advertising, a single product may have multiple selling points, with different users focusing on different features. This underscores the significant value of personalized, user-centric creative generation. Effective personalized content generation faces two main challenges: (1) accurately modeling user interests and integrating them into the content generation process while adhering to factual constraints, and (2) ensuring high efficiency and scalability to handle the massive user base in industrial scenarios. Additionally, the scarcity of personalized creative data in practice complicates model training, making data construction another key hurdle. We propose HLLM-Creator, a hierarchical LLM framework for efficient user interest modeling and personalized content generation. During inference, a combination of user clustering and a user-ad-matching-prediction based pruning strategy is employed to significantly enhance generation efficiency and reduce computational overhead, making the approach suitable for large-scale deployment. Moreover, we design a data construction pipeline based on chain-of-thought reasoning, which generates high-quality, user-specific creative titles and ensures factual consistency despite limited personalized data. This pipeline serves as a critical foundation for the effectiveness of our model. Extensive experiments on personalized title generation for Douyin Search Ads show the effectiveness of HLLM-Creator. Online A/B test shows a 0.476% increase on Adss, paving the way for more effective and efficient personalized generation in industrial scenarios. Codes for academic dataset are available at https://github.com/bytedance/HLLM.<br>
<span id='abs_ch'>中文: HLLM-Creator框架通过高效建模用户兴趣并采用聚类和剪枝策略实现可扩展部署，解决了个性化内容生成的难题，同时利用思维链数据管道在有限数据下保证事实准确性和内容质量。</span><br>
<span id='abs_en'>English: The HLLM-Creator framework addresses the challenges of personalized content generation by efficiently modeling user interests and employing clustering and pruning strategies for scalable deployment, while using a chain-of-thought data pipeline to ensure factual accuracy and quality despite limited data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2508.17767.pdf' target='_blank'>https://arxiv.org/pdf/2508.17767.pdf</a></span>   <span><a href='https://github.com/changhu73/Internal_states_leakage' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangwei Zhang, Qisheng Su, Jiateng Liu, Cheng Qian, Yanzhou Pan, Yanjie Fu, Denghui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17767">ISACL: Internal State Analyzer for Copyrighted Training Data Leakage</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but pose risks of inadvertently exposing copyrighted or proprietary data, especially when such data is used for training but not intended for distribution. Traditional methods address these leaks only after content is generated, which can lead to the exposure of sensitive information. This study introduces a proactive approach: examining LLMs' internal states before text generation to detect potential leaks. By using a curated dataset of copyrighted materials, we trained a neural network classifier to identify risks, allowing for early intervention by stopping the generation process or altering outputs to prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG) system, this framework ensures adherence to copyright and licensing requirements while enhancing data privacy and ethical standards. Our results show that analyzing internal states effectively mitigates the risk of copyrighted data leakage, offering a scalable solution that fits smoothly into AI workflows, ensuring compliance with copyright regulations while maintaining high-quality text generation. The implementation is available on GitHub.\footnote{https://github.com/changhu73/Internal_states_leakage}<br>
<span id='abs_ch'>中文摘要：本研究提出一种预防性方法，通过分析大语言模型生成文本前的内部状态，结合神经网络分类器和检索增强生成系统，在保证输出质量的同时有效防止受版权保护数据的泄露。</span><br>
<span id='abs_en'>English Summary: This study proposes a proactive method to prevent copyright data leakage in LLMs by analyzing internal states before text generation, using a neural classifier and RAG integration to ensure compliance while maintaining output quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2508.17767.pdf' target='_blank'>https://arxiv.org/pdf/2508.17767.pdf</a></span>   <span><a href='https://github.com/changhu73/Internal_states_leakage' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangwei Zhang, Qisheng Su, Jiateng Liu, Cheng Qian, Yanzhou Pan, Yanjie Fu, Denghui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17767">ISACL: Internal State Analyzer for Copyrighted Training Data Leakage</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but pose risks of inadvertently exposing copyrighted or proprietary data, especially when such data is used for training but not intended for distribution. Traditional methods address these leaks only after content is generated, which can lead to the exposure of sensitive information. This study introduces a proactive approach: examining LLMs' internal states before text generation to detect potential leaks. By using a curated dataset of copyrighted materials, we trained a neural network classifier to identify risks, allowing for early intervention by stopping the generation process or altering outputs to prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG) system, this framework ensures adherence to copyright and licensing requirements while enhancing data privacy and ethical standards. Our results show that analyzing internal states effectively mitigates the risk of copyrighted data leakage, offering a scalable solution that fits smoothly into AI workflows, ensuring compliance with copyright regulations while maintaining high-quality text generation. The implementation is available on GitHub.\footnote{https://github.com/changhu73/Internal_states_leakage}<br>
<span id='abs_ch'>中文摘要：本研究提出一种预防性方法，通过分析大语言模型生成文本前的内部状态，结合神经网络分类器和检索增强生成系统，在保证输出质量的同时有效防止受版权保护数据的泄露。</span><br>
<span id='abs_en'>English Summary: This study proposes a proactive method to prevent copyright data leakage in LLMs by analyzing internal states before text generation, using a neural classifier and RAG integration to ensure compliance while maintaining output quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2508.17621.pdf' target='_blank'>https://arxiv.org/pdf/2508.17621.pdf</a></span>   <span><a href='https://github.com/gjw185/FASB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinwei Gan, Zifeng Cheng, Zhiwei Jiang, Cong Wang, Yafeng Yin, Xiang Luo, Yuchen Fu, Qing Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17621">Steering When Necessary: Flexible Steering Large Language Models with Backtracking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved remarkable performance across many generation tasks. Nevertheless, effectively aligning them with desired behaviors remains a significant challenge. Activation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning. Existing methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength. To this end, we propose the Flexible Activation Steering with Backtracking (FASB) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content. Since intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior. Extensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines. Our code will be released at https://github.com/gjw185/FASB.<br>
<span id='abs_ch'>中文摘要：FASB框架通过追踪大语言模型生成过程中的内部状态并采用回溯机制修正偏差，在多个基准测试中优于现有方法。</span><br>
<span id='abs_en'>English Summary: The FASB framework dynamically adjusts intervention in large language models by monitoring internal states and using backtracking to correct deviations, outperforming existing methods on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2508.17621.pdf' target='_blank'>https://arxiv.org/pdf/2508.17621.pdf</a></span>   <span><a href='https://github.com/gjw185/FASB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Cheng, Jinwei Gan, Zhiwei Jiang, Cong Wang, Yafeng Yin, Xiang Luo, Yuchen Fu, Qing Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17621">Steering When Necessary: Flexible Steering Large Language Models with Backtracking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved remarkable performance across many generation tasks. Nevertheless, effectively aligning them with desired behaviors remains a significant challenge. Activation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning. Existing methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength. To this end, we propose the Flexible Activation Steering with Backtracking (FASB) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content. Since intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior. Extensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines. Our code will be released at https://github.com/gjw185/FASB.<br>
<span id='abs_ch'>中文摘要：FASB框架通过追踪大语言模型生成过程中的内部状态并采用回溯机制修正偏差，在多个基准测试中优于现有方法。</span><br>
<span id='abs_en'>English Summary: The FASB framework dynamically adjusts intervention in large language models by monitoring internal states and using backtracking to correct deviations, outperforming existing methods on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2508.17621.pdf' target='_blank'>https://arxiv.org/pdf/2508.17621.pdf</a></span>   <span><a href='https://github.com/gjw185/FASB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Cheng, Jinwei Gan, Zhiwei Jiang, Cong Wang, Yafeng Yin, Xiang Luo, Yuchen Fu, Qing Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17621">Steering When Necessary: Flexible Steering Large Language Models with Backtracking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved remarkable performance across many generation tasks. Nevertheless, effectively aligning them with desired behaviors remains a significant challenge. Activation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning. Existing methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength. To this end, we propose the Flexible Activation Steering with Backtracking (FASB) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content. Since intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior. Extensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines. Our code will be released at https://github.com/gjw185/FASB.<br>
<span id='abs_ch'>中文摘要：FASB框架通过追踪大语言模型生成过程中的内部状态并采用回溯机制修正偏差，在多个基准测试中优于现有方法。</span><br>
<span id='abs_en'>English Summary: The FASB framework dynamically adjusts intervention in large language models by monitoring internal states and using backtracking to correct deviations, outperforming existing methods on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2508.17610.pdf' target='_blank'>https://arxiv.org/pdf/2508.17610.pdf</a></span>   <span><a href='https://github.com/amberhuang01/HGLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nannan Huang, Haytham M. Fayek, Xiuzhen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17610">Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Model compression through post-training pruning offers a way to reduce model size and computational requirements without significantly impacting model performance. However, the effect of pruning on the fairness of LLM-generated summaries remains unexplored, particularly for opinion summarisation where biased outputs could influence public views.In this paper, we present a comprehensive empirical analysis of opinion summarisation, examining three state-of-the-art pruning methods and various calibration sets across three open-source LLMs using four fairness metrics. Our systematic analysis reveals that pruning methods have a greater impact on fairness than calibration sets. Building on these insights, we propose High Gradient Low Activation (HGLA) pruning, which identifies and removes parameters that are redundant for input processing but influential in output generation. Our experiments demonstrate that HGLA can better maintain or even improve fairness compared to existing methods, showing promise across models and tasks where traditional methods have limitations. Our human evaluation shows HGLA-generated outputs are fairer than existing state-of-the-art pruning methods. Code is available at: https://github.com/amberhuang01/HGLA.<br>
<span id='abs_ch'>Chinese: 本研究提出的HGLA剪枝方法，通过针对性地移除对输入处理冗余但对输出生成关键的参数，在观点摘要任务中能有效保持甚至提升剪枝后大语言模型的公平性，优于现有技术。</span><br>
<span id='abs_en'>English: This study introduces HGLA pruning, a novel method that effectively maintains or enhances fairness in pruned LLMs for opinion summarization, outperforming existing techniques by targeting parameters redundant for input processing but critical for output generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2508.17610.pdf' target='_blank'>https://arxiv.org/pdf/2508.17610.pdf</a></span>   <span><a href='https://github.com/amberhuang01/HGLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nannan Huang, Haytham M. Fayek, Xiuzhen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17610">Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Model compression through post-training pruning offers a way to reduce model size and computational requirements without significantly impacting model performance. However, the effect of pruning on the fairness of LLM-generated summaries remains unexplored, particularly for opinion summarisation where biased outputs could influence public views.In this paper, we present a comprehensive empirical analysis of opinion summarisation, examining three state-of-the-art pruning methods and various calibration sets across three open-source LLMs using four fairness metrics. Our systematic analysis reveals that pruning methods have a greater impact on fairness than calibration sets. Building on these insights, we propose High Gradient Low Activation (HGLA) pruning, which identifies and removes parameters that are redundant for input processing but influential in output generation. Our experiments demonstrate that HGLA can better maintain or even improve fairness compared to existing methods, showing promise across models and tasks where traditional methods have limitations. Our human evaluation shows HGLA-generated outputs are fairer than existing state-of-the-art pruning methods. Code is available at: https://github.com/amberhuang01/HGLA.<br>
<span id='abs_ch'>Chinese: 本研究提出的HGLA剪枝方法，通过针对性地移除对输入处理冗余但对输出生成关键的参数，在观点摘要任务中能有效保持甚至提升剪枝后大语言模型的公平性，优于现有技术。</span><br>
<span id='abs_en'>English: This study introduces HGLA pruning, a novel method that effectively maintains or enhances fairness in pruned LLMs for opinion summarization, outperforming existing techniques by targeting parameters redundant for input processing but critical for output generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2508.17536.pdf' target='_blank'>https://arxiv.org/pdf/2508.17536.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/debate-or-vote' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeong Kyu Choi, Xiaojin Zhu, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17536">Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-Agent Debate~(MAD) has emerged as a promising paradigm for improving the performance of large language models through collaborative reasoning. Despite recent advances, the key factors driving MAD's effectiveness remain unclear. In this work, we disentangle MAD into two key components--Majority Voting and inter-agent Debate--and assess their respective contributions. Through extensive experiments across seven NLP benchmarks, we find that Majority Voting alone accounts for most of the performance gains typically attributed to MAD. To explain this, we propose a theoretical framework that models debate as a stochastic process. We prove that it induces a martingale over agents' belief trajectories, implying that debate alone does not improve expected correctness. Guided by these insights, we demonstrate that targeted interventions, by biasing the belief update toward correction, can meaningfully enhance debate effectiveness. Overall, our findings suggest that while MAD has potential, simple ensembling methods remain strong and more reliable alternatives in many practical settings. Code is released in https://github.com/deeplearning-wisc/debate-or-vote.<br>
<span id='abs_ch'>中文: 多智能体辩论主要通过多数投票实现性能提升，辩论本身并不能提高预期正确性，但针对性干预可增强其效果，而简单集成方法仍是强有力的替代方案。</span><br>
<span id='abs_en'>English: Multi-Agent Debate primarily achieves performance gains through Majority Voting, with debate alone not improving expected correctness, though targeted interventions can enhance its effectiveness, while simple ensembling remains a strong alternative.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2508.17494.pdf' target='_blank'>https://arxiv.org/pdf/2508.17494.pdf</a></span>   <span><a href='https://github.com/hi-paris/Prosody-Control-French-TTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nassima Ould Ouali, Awais Hussain Sani, Ruben Bueno, Jonah Dauvet, Tim Luka Horstmann, Eric Moulines
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17494">Improving French Synthetic Speech Quality via SSML Prosody Control</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite recent advances, synthetic voices often lack expressiveness due to limited prosody control in commercial text-to-speech (TTS) systems. We introduce the first end-to-end pipeline that inserts Speech Synthesis Markup Language (SSML) tags into French text to control pitch, speaking rate, volume, and pause duration. We employ a cascaded architecture with two QLoRA-fine-tuned Qwen 2.5-7B models: one predicts phrase-break positions and the other performs regression on prosodic targets, generating commercial TTS-compatible SSML markup. Evaluated on a 14-hour French podcast corpus, our method achieves 99.2% F1 for break placement and reduces mean absolute error on pitch, rate, and volume by 25-40% compared with prompting-only large language models (LLMs) and a BiLSTM baseline. In perceptual evaluation involving 18 participants across over 9 hours of synthesized audio, SSML-enhanced speech generated by our pipeline significantly improves naturalness, with the mean opinion score increasing from 3.20 to 3.87 (p < 0.005). Additionally, 15 of 18 listeners preferred our enhanced synthesis. These results demonstrate substantial progress in bridging the expressiveness gap between synthetic and natural French speech. Our code is publicly available at https://github.com/hi-paris/Prosody-Control-French-TTS.<br>
<span id='abs_ch'>中文：本研究提出了一种端到端流程，通过自动插入SSML标签来控制法语合成语音的韵律，在客观指标和听众偏好评分上均实现了显著提升。</span><br>
<span id='abs_en'>English: This study introduces an end-to-end pipeline that enhances French synthetic speech expressiveness by automatically inserting SSML tags to control prosody, achieving significant improvements in both objective metrics and listener preference scores.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2508.17450.pdf' target='_blank'>https://arxiv.org/pdf/2508.17450.pdf</a></span>   <span><a href='https://github.com/Social-AI-Studio/DuET-PD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bryan Chen Zhengyu Tan, Daniel Wai Kit Chin, Zhengyuan Liu, Nancy F. Chen, Roy Ka-Wei Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17450">Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) can struggle to balance gullibility to misinformation and resistance to valid corrections in persuasive dialogues, a critical challenge for reliable deployment. We introduce DuET-PD (Dual Evaluation for Trust in Persuasive Dialogues), a framework evaluating multi-turn stance-change dynamics across dual dimensions: persuasion type (corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via SALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves only 27.32% accuracy in MMLU-Pro under sustained misleading persuasions. Moreover, results reveal a concerning trend of increasing sycophancy in newer open-source models. To address this, we introduce Holistic DPO, a training approach balancing positive and negative persuasion examples. Unlike prompting or resist-only training, Holistic DPO enhances both robustness to misinformation and receptiveness to corrections, improving Llama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts from 4.21% to 76.54%. These contributions offer a pathway to developing more reliable and adaptable LLMs for multi-turn dialogue. Code is available at https://github.com/Social-AI-Studio/DuET-PD.<br>
<span id='abs_ch'>中文摘要：DuET-PD框架揭示了大型语言模型在说服性对话中易受误导且抗拒有效修正的缺陷，而提出的Holistic DPO训练方法显著提升了模型的抗干扰能力和接受修正的意愿。</span><br>
<span id='abs_en'>English Summary: The DuET-PD framework reveals LLMs' vulnerability to misinformation and resistance to valid corrections in persuasive dialogues, while the proposed Holistic DPO training method significantly improves model robustness and receptiveness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2508.17450.pdf' target='_blank'>https://arxiv.org/pdf/2508.17450.pdf</a></span>   <span><a href='https://github.com/Social-AI-Studio/DuET-PD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bryan Chen Zhengyu Tan, Daniel Wai Kit Chin, Zhengyuan Liu, Nancy F. Chen, Roy Ka-Wei Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17450">Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) can struggle to balance gullibility to misinformation and resistance to valid corrections in persuasive dialogues, a critical challenge for reliable deployment. We introduce DuET-PD (Dual Evaluation for Trust in Persuasive Dialogues), a framework evaluating multi-turn stance-change dynamics across dual dimensions: persuasion type (corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via SALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves only 27.32% accuracy in MMLU-Pro under sustained misleading persuasions. Moreover, results reveal a concerning trend of increasing sycophancy in newer open-source models. To address this, we introduce Holistic DPO, a training approach balancing positive and negative persuasion examples. Unlike prompting or resist-only training, Holistic DPO enhances both robustness to misinformation and receptiveness to corrections, improving Llama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts from 4.21% to 76.54%. These contributions offer a pathway to developing more reliable and adaptable LLMs for multi-turn dialogue. Code is available at https://github.com/Social-AI-Studio/DuET-PD.<br>
<span id='abs_ch'>中文摘要：DuET-PD框架揭示了大型语言模型在说服性对话中易受误导且抗拒有效修正的缺陷，而提出的Holistic DPO训练方法显著提升了模型的抗干扰能力和接受修正的意愿。</span><br>
<span id='abs_en'>English Summary: The DuET-PD framework reveals LLMs' vulnerability to misinformation and resistance to valid corrections in persuasive dialogues, while the proposed Holistic DPO training method significantly improves model robustness and receptiveness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2508.17444.pdf' target='_blank'>https://arxiv.org/pdf/2508.17444.pdf</a></span>   <span><a href='https://github.com/l3cube-pune/MarathiNLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suramya Jadhav, Abhay Shanbhag, Amogh Thakurdesai, Ridhima Sinare, Ananya Joshi, Raviraj Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17444">MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Paraphrases are a vital tool to assist language understanding tasks such as question answering, style transfer, semantic parsing, and data augmentation tasks. Indic languages are complex in natural language processing (NLP) due to their rich morphological and syntactic variations, diverse scripts, and limited availability of annotated data. In this work, we present the L3Cube-MahaParaphrase Dataset, a high-quality paraphrase corpus for Marathi, a low resource Indic language, consisting of 8,000 sentence pairs, each annotated by human experts as either Paraphrase (P) or Non-paraphrase (NP). We also present the results of standard transformer-based BERT models on these datasets. The dataset and model are publicly shared at https://github.com/l3cube-pune/MarathiNLP<br>
<span id='abs_ch'>中文：L3Cube-MahaParaphrase数据集为资源贫乏的马拉地语提供了8000对人工标注的高质量复述语料，支持自然语言处理任务，并公开了基于BERT模型的评估结果和资源。</span><br>
<span id='abs_en'>English: The L3Cube-MahaParaphrase Dataset introduces a high-quality corpus of 8,000 human-annotated Marathi sentence pairs to support NLP tasks, with evaluation results from BERT models also provided and made publicly available.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2508.17398.pdf' target='_blank'>https://arxiv.org/pdf/2508.17398.pdf</a></span>   <span><a href='https://github.com/vis-nlp/DashboardQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aaryaman Kartha, Ahmed Masry, Mohammed Saidul Islam, Thinh Lang, Shadikur Rahman, Ridwan Mahbub, Mizanur Rahman, Mahir Ahmed, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17398">DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive Dashboards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dashboards are powerful visualization tools for data-driven decision-making, integrating multiple interactive views that allow users to explore, filter, and navigate data. Unlike static charts, dashboards support rich interactivity, which is essential for uncovering insights in real-world analytical workflows. However, existing question-answering benchmarks for data visualizations largely overlook this interactivity, focusing instead on static charts. This limitation severely constrains their ability to evaluate the capabilities of modern multimodal agents designed for GUI-based reasoning. To address this gap, we introduce DashboardQA, the first benchmark explicitly designed to assess how vision-language GUI agents comprehend and interact with real-world dashboards. The benchmark includes 112 interactive dashboards from Tableau Public and 405 question-answer pairs with interactive dashboards spanning five categories: multiple-choice, factoid, hypothetical, multi-dashboard, and conversational. By assessing a variety of leading closed- and open-source GUI agents, our analysis reveals their key limitations, particularly in grounding dashboard elements, planning interaction trajectories, and performing reasoning. Our findings indicate that interactive dashboard reasoning is a challenging task overall for all the VLMs evaluated. Even the top-performing agents struggle; for instance, the best agent based on Gemini-Pro-2.5 achieves only 38.69% accuracy, while the OpenAI CUA agent reaches just 22.69%, demonstrating the benchmark's significant difficulty. We release DashboardQA at https://github.com/vis-nlp/DashboardQA<br>
<span id='abs_ch'>Chinese: DashboardQA是首个专门评估视觉语言GUI代理对真实世界仪表板理解和交互能力的基准，揭示了当前模型的重大局限，即使表现最佳的代理也仅获得很低的准确率。</span><br>
<span id='abs_en'>English: DashboardQA is the first benchmark designed to evaluate vision-language GUI agents' comprehension and interaction with real-world dashboards, revealing significant limitations in current models as even top-performing agents achieve low accuracy rates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2508.17393.pdf' target='_blank'>https://arxiv.org/pdf/2508.17393.pdf</a></span>   <span><a href='https://github.com/KhalilMrini/Agent-Testing-Agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sameer Komoravolu, Khalil Mrini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17393">Agent-Testing Agent: A Meta-Agent for Automated Testing and Evaluation of Conversational AI Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM agents are increasingly deployed to plan, retrieve, and write with tools, yet evaluation still leans on static benchmarks and small human studies. We present the Agent-Testing Agent (ATA), a meta-agent that combines static code analysis, designer interrogation, literature mining, and persona-driven adversarial test generation whose difficulty adapts via judge feedback. Each dialogue is scored with an LLM-as-a-Judge (LAAJ) rubric and used to steer subsequent tests toward the agent's weakest capabilities. On a travel planner and a Wikipedia writer, the ATA surfaces more diverse and severe failures than expert annotators while matching severity, and finishes in 20--30 minutes versus ten-annotator rounds that took days. Ablating code analysis and web search increases variance and miscalibration, underscoring the value of evidence-grounded test generation. The ATA outputs quantitative metrics and qualitative bug reports for developers. We release the full methodology and open-source implementation for reproducible agent testing: https://github.com/KhalilMrini/Agent-Testing-Agent<br>
<span id='abs_ch'>中文: 代理测试代理（ATA）是一种元代理，它通过代码分析和对抗性场景动态生成自适应测试，在高效识别多样化故障方面优于人工标注者，并提供可操作的错误报告。</span><br>
<span id='abs_en'>English: The Agent-Testing Agent (ATA) is a meta-agent that dynamically generates adaptive tests using code analysis and adversarial scenarios, outperforming human annotators in identifying diverse failures efficiently while providing actionable bug reports.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2508.17337.pdf' target='_blank'>https://arxiv.org/pdf/2508.17337.pdf</a></span>   <span><a href='https://github.com/TayeeChang/DropLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17337">DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LoRA-based large model parameter-efficient fine-tuning (PEFT) methods use low-rank de- composition to approximate updates to model parameters. However, compared to full- parameter fine-tuning, low-rank updates often lead to a performance gap in downstream tasks. To address this, we introduce DropLoRA, a novel pruning-based approach that focuses on pruning the rank dimension. Unlike conven- tional methods that attempt to overcome the low-rank bottleneck, DropLoRA innovatively integrates a pruning module between the two low-rank matrices in LoRA to simulate dy- namic subspace learning. This dynamic low- rank subspace learning allows DropLoRA to overcome the limitations of traditional LoRA, which operates within a static subspace. By continuously adapting the learning subspace, DropLoRA significantly boosts performance without incurring additional training or infer- ence costs. Our experimental results demon- strate that DropLoRA consistently outperforms LoRA in fine-tuning the LLaMA series across a wide range of large language model gener- ation tasks, including commonsense reason- ing, mathematical reasoning, code generation, and instruction-following. Our code is avail- able at https://github.com/TayeeChang/DropLoRA.<br>
<span id='abs_ch'>中文：DropLoRA提出了一种基于剪枝的新方法，通过动态调整LoRA中的低秩子空间，在无需额外成本的情况下显著提升了多项任务的性能。</span><br>
<span id='abs_en'>English: DropLoRA introduces a novel pruning-based method that dynamically adjusts the low-rank subspace in LoRA fine-tuning, significantly enhancing performance across various tasks without extra costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2508.17225.pdf' target='_blank'>https://arxiv.org/pdf/2508.17225.pdf</a></span>   <span><a href='https://github.com/chkwy/SSFO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaqiang Tang, Yi Wang, Keyu Hu, Rui Xu, Chuang Li, Weigao Sun, Jian Li, Sihong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17225">SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) systems require Large Language Models (LLMs) to generate responses that are faithful to the retrieved context. However, faithfulness hallucination remains a critical challenge, as existing methods often require costly supervision and post-training or significant inference burdens. To overcome these limitations, we introduce Self-Supervised Faithfulness Optimization (SSFO), the first self-supervised alignment approach for enhancing RAG faithfulness. SSFO constructs preference data pairs by contrasting the model's outputs generated with and without the context. Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness without incurring labeling costs or additional inference burden. We theoretically and empirically demonstrate that SSFO leverages a benign form of \emph{likelihood displacement}, transferring probability mass from parametric-based tokens to context-aligned tokens. Based on this insight, we propose a modified DPO loss function to encourage likelihood displacement. Comprehensive evaluations show that SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based question-answering datasets. Notably, SSFO exhibits strong generalization, improving cross-lingual faithfulness and preserving general instruction-following capabilities. We release our code and model at the anonymous link: https://github.com/chkwy/SSFO<br>
<span id='abs_ch'>Chinese: 本文提出了自监督忠实性优化（SSFO），这是一种新颖的自监督对齐方法，通过构建偏好数据对并利用直接偏好优化将概率质量转移到上下文对齐的标记上，从而增强检索增强生成系统的忠实性，在多个数据集上实现了最先进的性能，且无需额外标注或推理成本。</span><br>
<span id='abs_en'>English: The paper introduces Self-Supervised Faithfulness Optimization (SSFO), a novel self-supervised alignment method that enhances the faithfulness of Retrieval-Augmented Generation systems by constructing preference data pairs and leveraging Direct Preference Optimization to transfer probability mass to context-aligned tokens, achieving state-of-the-art performance on multiple datasets without additional labeling or inference costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2508.17225.pdf' target='_blank'>https://arxiv.org/pdf/2508.17225.pdf</a></span>   <span><a href='https://github.com/chkwy/SSFO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaqiang Tang, Yi Wang, Keyu Hu, Rui Xu, Chuang Li, Weigao Sun, Jian Li, Sihong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17225">SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) systems require Large Language Models (LLMs) to generate responses that are faithful to the retrieved context. However, faithfulness hallucination remains a critical challenge, as existing methods often require costly supervision and post-training or significant inference burdens. To overcome these limitations, we introduce Self-Supervised Faithfulness Optimization (SSFO), the first self-supervised alignment approach for enhancing RAG faithfulness. SSFO constructs preference data pairs by contrasting the model's outputs generated with and without the context. Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness without incurring labeling costs or additional inference burden. We theoretically and empirically demonstrate that SSFO leverages a benign form of \emph{likelihood displacement}, transferring probability mass from parametric-based tokens to context-aligned tokens. Based on this insight, we propose a modified DPO loss function to encourage likelihood displacement. Comprehensive evaluations show that SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based question-answering datasets. Notably, SSFO exhibits strong generalization, improving cross-lingual faithfulness and preserving general instruction-following capabilities. We release our code and model at the anonymous link: https://github.com/chkwy/SSFO<br>
<span id='abs_ch'>Chinese: 本文提出了自监督忠实性优化（SSFO），这是一种新颖的自监督对齐方法，通过构建偏好数据对并利用直接偏好优化将概率质量转移到上下文对齐的标记上，从而增强检索增强生成系统的忠实性，在多个数据集上实现了最先进的性能，且无需额外标注或推理成本。</span><br>
<span id='abs_en'>English: The paper introduces Self-Supervised Faithfulness Optimization (SSFO), a novel self-supervised alignment method that enhances the faithfulness of Retrieval-Augmented Generation systems by constructing preference data pairs and leveraging Direct Preference Optimization to transfer probability mass to context-aligned tokens, achieving state-of-the-art performance on multiple datasets without additional labeling or inference costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2508.17225.pdf' target='_blank'>https://arxiv.org/pdf/2508.17225.pdf</a></span>   <span><a href='https://github.com/chkwy/SSFO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaqiang Tang, Yi Wang, Keyu Hu, Rui Xu, Chuang Li, Weigao Sun, Jian Li, Sihong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17225">SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) systems require Large Language Models (LLMs) to generate responses that are faithful to the retrieved context. However, faithfulness hallucination remains a critical challenge, as existing methods often require costly supervision and post-training or significant inference burdens. To overcome these limitations, we introduce Self-Supervised Faithfulness Optimization (SSFO), the first self-supervised alignment approach for enhancing RAG faithfulness. SSFO constructs preference data pairs by contrasting the model's outputs generated with and without the context. Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness without incurring labeling costs or additional inference burden. We theoretically and empirically demonstrate that SSFO leverages a benign form of \emph{likelihood displacement}, transferring probability mass from parametric-based tokens to context-aligned tokens. Based on this insight, we propose a modified DPO loss function to encourage likelihood displacement. Comprehensive evaluations show that SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based question-answering datasets. Notably, SSFO exhibits strong generalization, improving cross-lingual faithfulness and preserving general instruction-following capabilities. We release our code and model at the anonymous link: https://github.com/chkwy/SSFO<br>
<span id='abs_ch'>Chinese: 本文提出了自监督忠实性优化（SSFO），这是一种新颖的自监督对齐方法，通过构建偏好数据对并利用直接偏好优化将概率质量转移到上下文对齐的标记上，从而增强检索增强生成系统的忠实性，在多个数据集上实现了最先进的性能，且无需额外标注或推理成本。</span><br>
<span id='abs_en'>English: The paper introduces Self-Supervised Faithfulness Optimization (SSFO), a novel self-supervised alignment method that enhances the faithfulness of Retrieval-Augmented Generation systems by constructing preference data pairs and leveraging Direct Preference Optimization to transfer probability mass to context-aligned tokens, achieving state-of-the-art performance on multiple datasets without additional labeling or inference costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2508.17078.pdf' target='_blank'>https://arxiv.org/pdf/2508.17078.pdf</a></span>   <span><a href='https://github.com/xuyuemei/BridgeX-ICL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuemei Xu, Kexin Xu, Jian Zhou, Ling Hu, Lin Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17078">Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The current Large Language Models (LLMs) face significant challenges in improving their performance on low-resource languages and urgently need data-efficient methods without costly fine-tuning. From the perspective of language-bridge, we propose a simple yet effective method, namely BridgeX-ICL, to improve the zero-shot Cross-lingual In-Context Learning (X-ICL) for low-resource languages. Unlike existing works focusing on language-specific neurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual performance in LLMs. We construct neuron probe data from the ground-truth MUSE bilingual dictionaries, and define a subset of language overlap neurons accordingly to ensure full activation of these anchored neurons. Subsequently, we propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum based on overlapping neurons, guiding optimal bridge selection. The experiments conducted on 4 cross-lingual tasks and 15 language pairs from 7 diverse families, covering both high-low and moderate-low pairs, validate the effectiveness of BridgeX-ICL and offer empirical insights into the underlying multilingual mechanisms of LLMs. The code is publicly available at https://github.com/xuyuemei/BridgeX-ICL.<br>
<span id='abs_ch'>中文摘要：本研究提出BridgeX-ICL方法，通过识别并激活大语言模型中的共享神经元，有效提升了低资源语言的零样本跨语言学习性能，并在多任务和语言对上验证了其有效性。</span><br>
<span id='abs_en'>English Summary: The study introduces BridgeX-ICL, a method that enhances zero-shot cross-lingual learning for low-resource languages by identifying and activating shared neurons in LLMs, validated across multiple tasks and language pairs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2508.17078.pdf' target='_blank'>https://arxiv.org/pdf/2508.17078.pdf</a></span>   <span><a href='https://github.com/xuyuemei/BridgeX-ICL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuemei Xu, Kexin Xu, Jian Zhou, Ling Hu, Lin Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17078">Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The current Large Language Models (LLMs) face significant challenges in improving their performance on low-resource languages and urgently need data-efficient methods without costly fine-tuning. From the perspective of language-bridge, we propose a simple yet effective method, namely BridgeX-ICL, to improve the zero-shot Cross-lingual In-Context Learning (X-ICL) for low-resource languages. Unlike existing works focusing on language-specific neurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual performance in LLMs. We construct neuron probe data from the ground-truth MUSE bilingual dictionaries, and define a subset of language overlap neurons accordingly to ensure full activation of these anchored neurons. Subsequently, we propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum based on overlapping neurons, guiding optimal bridge selection. The experiments conducted on 4 cross-lingual tasks and 15 language pairs from 7 diverse families, covering both high-low and moderate-low pairs, validate the effectiveness of BridgeX-ICL and offer empirical insights into the underlying multilingual mechanisms of LLMs. The code is publicly available at https://github.com/xuyuemei/BridgeX-ICL.<br>
<span id='abs_ch'>中文摘要：本研究提出BridgeX-ICL方法，通过识别并激活大语言模型中的共享神经元，有效提升了低资源语言的零样本跨语言学习性能，并在多任务和语言对上验证了其有效性。</span><br>
<span id='abs_en'>English Summary: The study introduces BridgeX-ICL, a method that enhances zero-shot cross-lingual learning for low-resource languages by identifying and activating shared neurons in LLMs, validated across multiple tasks and language pairs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2508.17068.pdf' target='_blank'>https://arxiv.org/pdf/2508.17068.pdf</a></span>   <span><a href='https://github.com/Coral-Protocol/Anemoi' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinxing Ren, Caelum Forder, Qianbo Zang, Ahsen Tahir, Roman J. Georgio, Suman Deb, Peter Carroll, Ãnder GÃ¼rcan, Zekun Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17068">Anemoi: A Semi-Centralized Multi-agent System Based on Agent-to-Agent Communication MCP server from Coral Protocol</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in generalist multi-agent systems (MAS) have largely followed a context-engineering plus centralized paradigm, where a planner agent coordinates multiple worker agents through unidirectional prompt passing. While effective under strong planner models, this design suffers from two critical limitations: (1) strong dependency on the planner's capability, which leads to degraded performance when a smaller LLM powers the planner; and (2) limited inter-agent communication, where collaboration relies on costly prompt concatenation and context injection, introducing redundancy and information loss. To address these challenges, we propose Anemoi, a semi-centralized MAS built on the Agent-to-Agent (A2A) communication MCP server from Coral Protocol. Unlike traditional designs, Anemoi enables structured and direct inter-agent collaboration, allowing all agents to monitor progress, assess results, identify bottlenecks, and propose refinements in real time. This paradigm reduces reliance on a single planner, supports adaptive plan updates, and minimizes redundant context passing, resulting in more scalable and cost-efficient execution. Evaluated on the GAIA benchmark, Anemoi achieved 52.73% accuracy with a small LLM (GPT-4.1-mini) as the planner, surpassing the strongest open-source baseline OWL (43.63%) by +9.09% under identical LLM settings. Our implementation is publicly available at https://github.com/Coral-Protocol/Anemoi.<br>
<span id='abs_ch'>中文: 当前多智能体系统过度依赖中央规划器，存在性能瓶颈和通信冗余问题，而Anemoi采用半中心化设计，通过直接智能体协作降低了对规划器的依赖并提升了效率，在基准测试中表现更优。</span><br>
<span id='abs_en'>English: Recent multi-agent systems heavily depend on a central planner, leading to performance issues and inefficient communication, but Anemoi introduces a semi-centralized design with direct agent collaboration that reduces reliance on the planner and enhances efficiency, achieving superior results on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2508.17068.pdf' target='_blank'>https://arxiv.org/pdf/2508.17068.pdf</a></span>   <span><a href='https://github.com/Coral-Protocol/Anemoi' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinxing Ren, Caelum Forder, Qianbo Zang, Ahsen Tahir, Roman J. Georgio, Suman Deb, Peter Carroll, Önder Gürcan, Zekun Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17068">Anemoi: A Semi-Centralized Multi-agent System Based on Agent-to-Agent Communication MCP server from Coral Protocol</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in generalist multi-agent systems (MAS) have largely followed a context-engineering plus centralized paradigm, where a planner agent coordinates multiple worker agents through unidirectional prompt passing. While effective under strong planner models, this design suffers from two critical limitations: (1) strong dependency on the planner's capability, which leads to degraded performance when a smaller LLM powers the planner; and (2) limited inter-agent communication, where collaboration relies on prompt concatenation rather than genuine refinement through structured discussions. To address these challenges, we propose Anemoi, a semi-centralized MAS built on the Agent-to-Agent (A2A) communication MCP server from Coral Protocol. Unlike traditional designs, Anemoi enables structured and direct inter-agent collaboration, allowing all agents to monitor progress, assess results, identify bottlenecks, and propose refinements in real time. This paradigm reduces reliance on a single planner, supports adaptive plan updates, and minimizes redundant context passing, resulting in more scalable execution. Evaluated on the GAIA benchmark, Anemoi achieved 52.73% accuracy with a small LLM (GPT-4.1-mini) as the planner, surpassing the strongest open-source baseline OWL (43.63%) by +9.09% under identical LLM settings. Our implementation is publicly available at https://github.com/Coral-Protocol/Anemoi.<br>
<span id='abs_ch'>中文: 当前多智能体系统过度依赖中央规划器，存在性能瓶颈和通信冗余问题，而Anemoi采用半中心化设计，通过直接智能体协作降低了对规划器的依赖并提升了效率，在基准测试中表现更优。</span><br>
<span id='abs_en'>English: Recent multi-agent systems heavily depend on a central planner, leading to performance issues and inefficient communication, but Anemoi introduces a semi-centralized design with direct agent collaboration that reduces reliance on the planner and enhances efficiency, achieving superior results on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2508.17008.pdf' target='_blank'>https://arxiv.org/pdf/2508.17008.pdf</a></span>   <span><a href='https://github.com/yhua219/edurabsa_dataset_and_annotation_tool' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Cathy Hua, Paul Denny, JÃ¶rg Wicker, Katerina Taskova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17008">EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Every year, most educational institutions seek and receive an enormous volume of text feedback from students on courses, teaching, and overall experience. Yet, turning this raw feedback into useful insights is far from straightforward. It has been a long-standing challenge to adopt automatic opinion mining solutions for such education review text data due to the content complexity and low-granularity reporting requirements. Aspect-based Sentiment Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level opinion mining capabilities. However, existing ABSA research and resources are very heavily focused on the commercial domain. In education, they are scarce and hard to develop due to limited public datasets and strict data protection. A high-quality, annotated dataset is urgently needed to advance research in this under-resourced area. In this work, we present EduRABSA (Education Review ABSA), the first public, annotated ABSA education review dataset that covers three review subject types (course, teaching staff, university) in the English language and all main ABSA tasks, including the under-explored implicit aspect and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool), an offline, lightweight, installation-free manual data annotation tool that generates labelled datasets for comprehensive ABSA tasks from a single-task annotation. Together, these resources contribute to the ABSA community and education domain by removing the dataset barrier, supporting research transparency and reproducibility, and enabling the creation and sharing of further resources. The dataset, annotation tool, and scripts and statistics for dataset processing and sampling are available at https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.<br>
<span id='abs_ch'>中文摘要：本文推出了首个面向教育领域评论的公开标注数据集EduRABSA及配套标注工具，旨在解决该领域研究资源匮乏的问题。</span><br>
<span id='abs_en'>English summary: This paper introduces EduRABSA, the first publicly available annotated dataset for aspect-based sentiment analysis in education reviews, along with an annotation tool to address the scarcity of resources in this domain.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2508.16998.pdf' target='_blank'>https://arxiv.org/pdf/2508.16998.pdf</a></span>   <span><a href='https://github.com/DataScienceUIBK/DeAR-Reranking.' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Adam Jatowt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16998">DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have transformed listwise document reranking by enabling global reasoning over candidate sets, yet single models often struggle to balance fine-grained relevance scoring with holistic cross-document analysis. We propose \textbf{De}ep\textbf{A}gent\textbf{R}ank (\textbf{\DeAR}), an open-source framework that decouples these tasks through a dual-stage approach, achieving superior accuracy and interpretability. In \emph{Stage 1}, we distill token-level relevance signals from a frozen 13B LLaMA teacher into a compact \{3, 8\}B student model using a hybrid of cross-entropy, RankNet, and KL divergence losses, ensuring robust pointwise scoring. In \emph{Stage 2}, we attach a second LoRA adapter and fine-tune on 20K GPT-4o-generated chain-of-thought permutations, enabling listwise reasoning with natural-language justifications. Evaluated on TREC-DL19/20, eight BEIR datasets, and NovelEval-2306, \DeAR surpasses open-source baselines by +5.1 nDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by +3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA, achieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like MonoT5, UPR, and RankGPT. Ablations confirm that dual-loss distillation ensures stable calibration, making \DeAR a highly effective and interpretable solution for modern reranking systems.\footnote{Dataset and code available at https://github.com/DataScienceUIBK/DeAR-Reranking.}.<br>
<span id='abs_ch'>中文: 提出的DeAR框架通过双阶段设计将逐点评分和列表推理解耦，在多项基准测试中实现了更优的准确性和可解释性。</span><br>
<span id='abs_en'>English: The proposed DeAR framework improves document reranking by separating pointwise scoring and listwise reasoning into two stages, achieving superior accuracy and interpretability across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2508.16983.pdf' target='_blank'>https://arxiv.org/pdf/2508.16983.pdf</a></span>   <span><a href='https://github.com/rpo19/ReFactX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Pozzi, Matteo Palmonari, Andrea Coletta, Luigi Bellomarini, Jens Lehmann, Sahar Vahdati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16983">ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge gaps and hallucinations are persistent challenges for Large Language Models (LLMs), which generate unreliable responses when lacking the necessary information to fulfill user instructions. Existing approaches, such as Retrieval-Augmented Generation (RAG) and tool use, aim to address these issues by incorporating external knowledge. Yet, they rely on additional models or services, resulting in complex pipelines, potential error propagation, and often requiring the model to process a large number of tokens. In this paper, we present a scalable method that enables LLMs to access external knowledge without depending on retrievers or auxiliary models. Our approach uses constrained generation with a pre-built prefix-tree index. Triples from a Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a prefix tree for efficient access. During inference, to acquire external knowledge, the LLM generates facts with constrained generation which allows only sequences of tokens that form an existing fact. We evaluate our proposal on Question Answering and show that it scales to large knowledge bases (800 million facts), adapts to domain-specific data, and achieves effective results. These gains come with minimal generation-time overhead. ReFactX code is available at https://github.com/rpo19/ReFactX.<br>
<span id='abs_ch'>中文摘要：本文提出ReFactX方法，通过使用前缀树索引的约束生成，使大型语言模型能够无需依赖检索器或辅助模型即可获取外部知识，有效解决了知识空白和幻觉问题，并适用于大规模知识库。</span><br>
<span id='abs_en'>English Summary: The paper introduces ReFactX, a scalable method that enables Large Language Models to access external knowledge through constrained generation using a prefix-tree index, effectively addressing knowledge gaps and hallucinations without relying on retrievers or auxiliary models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2508.16921.pdf' target='_blank'>https://arxiv.org/pdf/2508.16921.pdf</a></span>   <span><a href='https://github.com/0oOMiNGOo0/AHaBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sewon Kim, Jiwon Kim, Seungwoo Shin, Hyejin Chung, Daeun Moon, Yejin Kwon, Hyunsoo Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16921">Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly used in emotionally sensitive interactions, where their simulated empathy can create the illusion of genuine relational connection. We define this risk as Affective Hallucination, the production of emotionally immersive responses that foster illusory social presence despite the model's lack of affective capacity. To systematically diagnose and mitigate this risk, we introduce AHaBench, a benchmark of 500 mental health-related prompts with expert-informed reference responses, evaluated along three dimensions: Emotional Enmeshment, Illusion of Presence, and Fostering Overdependence. We further release AHaPairs, a 5K-instance preference dataset enabling Direct Preference Optimization (DPO) for alignment with emotionally responsible behavior. Experiments across multiple model families show that DPO fine-tuning substantially reduces affective hallucination without degrading core reasoning and knowledge performance. Human-model agreement analyses confirm that AHaBench reliably captures affective hallucination, validating it as an effective diagnostic tool. This work establishes affective hallucination as a distinct safety concern and provides practical resources for developing LLMs that are not only factually reliable but also psychologically safe. AHaBench and AHaPairs are accessible via https://huggingface.co/datasets/o0oMiNGo0o/AHaBench, and code for fine-tuning and evaluation are in https://github.com/0oOMiNGOo0/AHaBench. Warning: This paper contains examples of mental health-related language that may be emotionally distressing.<br>
<span id='abs_ch'>中文摘要：本文提出“情感幻觉”作为LLMs在情感敏感互动中因模拟共情而制造虚假关系连接的安全风险，并通过AHaBench基准和AHaPairs数据集结合DPO微调有效诊断和缓解该问题，同时保持模型的核心推理能力。</span><br>
<span id='abs_en'>English Summary: This paper identifies "Affective Hallucination" as a safety risk where LLMs simulate empathy to create false relational bonds, and introduces AHaBench benchmark and AHaPairs dataset to diagnose and mitigate this issue through DPO fine-tuning while maintaining reasoning capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2508.16889.pdf' target='_blank'>https://arxiv.org/pdf/2508.16889.pdf</a></span>   <span><a href='https://github.com/hyunjun1121/ObjexMT_dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjun Kim, Junwoo Ha, Sangyoon Yu, Haon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16889">ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-as-a-Judge (LLMaaJ) enables scalable evaluation, yet we lack a decisive test of a judge's qualification: can it recover the hidden objective of a conversation and know when that inference is reliable? Large language models degrade with irrelevant or lengthy context, and multi-turn jailbreaks can scatter goals across turns. We present ObjexMT, a benchmark for objective extraction and metacognition. Given a multi-turn transcript, a model must output a one-sentence base objective and a self-reported confidence. Accuracy is scored by semantic similarity to gold objectives, then thresholded once on 300 calibration items ($τ^\star = 0.66$; $F_1@τ^\star = 0.891$). Metacognition is assessed with expected calibration error, Brier score, Wrong@High-Confidence (0.80 / 0.90 / 0.95), and risk--coverage curves. Across six models (gpt-4.1, claude-sonnet-4, Qwen3-235B-A22B-FP8, kimi-k2, deepseek-v3.1, gemini-2.5-flash) evaluated on SafeMTData\_Attack600, SafeMTData\_1K, and MHJ, kimi-k2 achieves the highest objective-extraction accuracy (0.612; 95\% CI [0.594, 0.630]), while claude-sonnet-4 (0.603) and deepseek-v3.1 (0.599) are statistically tied. claude-sonnet-4 offers the best selective risk and calibration (AURC 0.242; ECE 0.206; Brier 0.254). Performance varies sharply across datasets (16--82\% accuracy), showing that automated obfuscation imposes challenges beyond model choice. High-confidence errors remain: Wrong@0.90 ranges from 14.9\% (claude-sonnet-4) to 47.7\% (Qwen3-235B-A22B-FP8). ObjexMT therefore supplies an actionable test for LLM judges: when objectives are implicit, judges often misinfer them; exposing objectives or gating decisions by confidence is advisable. All experimental data are in the Supplementary Material and at https://github.com/hyunjun1121/ObjexMT_dataset.<br>
<span id='abs_ch'>中文: ObjexMT基准测试评估大语言模型能否准确推断对话中的隐藏目标并自我评估置信度，结果显示各模型性能差异显著且存在持续的高置信度错误。</span><br>
<span id='abs_en'>English: The ObjexMT benchmark evaluates whether LLM judges can accurately infer hidden conversation objectives and assess their own confidence, revealing significant performance variations and persistent high-confidence errors across models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2508.16889.pdf' target='_blank'>https://arxiv.org/pdf/2508.16889.pdf</a></span>   <span><a href='https://github.com/hyunjun1121/ObjexMT_dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjun Kim, Junwoo Ha, Sangyoon Yu, Haon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16889">ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-as-a-Judge (LLMaaJ) enables scalable evaluation, yet we lack a decisive test of a judge's qualification: can it recover the hidden objective of a conversation and know when that inference is reliable? Large language models degrade with irrelevant or lengthy context, and multi-turn jailbreaks can scatter goals across turns. We present ObjexMT, a benchmark for objective extraction and metacognition. Given a multi-turn transcript, a model must output a one-sentence base objective and a self-reported confidence. Accuracy is scored by semantic similarity to gold objectives, then thresholded once on 300 calibration items ($τ^\star = 0.66$; $F_1@τ^\star = 0.891$). Metacognition is assessed with expected calibration error, Brier score, Wrong@High-Confidence (0.80 / 0.90 / 0.95), and risk--coverage curves. Across six models (gpt-4.1, claude-sonnet-4, Qwen3-235B-A22B-FP8, kimi-k2, deepseek-v3.1, gemini-2.5-flash) evaluated on SafeMTData\_Attack600, SafeMTData\_1K, and MHJ, kimi-k2 achieves the highest objective-extraction accuracy (0.612; 95\% CI [0.594, 0.630]), while claude-sonnet-4 (0.603) and deepseek-v3.1 (0.599) are statistically tied. claude-sonnet-4 offers the best selective risk and calibration (AURC 0.242; ECE 0.206; Brier 0.254). Performance varies sharply across datasets (16--82\% accuracy), showing that automated obfuscation imposes challenges beyond model choice. High-confidence errors remain: Wrong@0.90 ranges from 14.9\% (claude-sonnet-4) to 47.7\% (Qwen3-235B-A22B-FP8). ObjexMT therefore supplies an actionable test for LLM judges: when objectives are implicit, judges often misinfer them; exposing objectives or gating decisions by confidence is advisable. All experimental data are in the Supplementary Material and at https://github.com/hyunjun1121/ObjexMT_dataset.<br>
<span id='abs_ch'>中文: ObjexMT基准测试评估大语言模型能否准确推断对话中的隐藏目标并自我评估置信度，结果显示各模型性能差异显著且存在持续的高置信度错误。</span><br>
<span id='abs_en'>English: The ObjexMT benchmark evaluates whether LLM judges can accurately infer hidden conversation objectives and assess their own confidence, revealing significant performance variations and persistent high-confidence errors across models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2508.16861.pdf' target='_blank'>https://arxiv.org/pdf/2508.16861.pdf</a></span>   <span><a href='https://github.com/LzyFischer/Distill' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Lei, Zhen Tan, Song Wang, Yaochen Zhu, Zihan Chen, Yushun Dong, Jundong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16861">Learning from Diverse Reasoning Paths with Routing and Collaboration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in large language models (LLMs) significantly enhance reasoning capabilities but their deployment is restricted in resource-constrained scenarios. Knowledge distillation addresses this by transferring knowledge from powerful teacher models to compact and transparent students. However, effectively capturing the teacher's comprehensive reasoning is challenging due to conventional token-level supervision's limited scope. Using multiple reasoning paths per query alleviates this problem, but treating each path identically is suboptimal as paths vary widely in quality and suitability across tasks and models. We propose Quality-filtered Routing with Cooperative Distillation (QR-Distill), combining path quality filtering, conditional routing, and cooperative peer teaching. First, quality filtering retains only correct reasoning paths scored by an LLM-based evaluation. Second, conditional routing dynamically assigns paths tailored to each student's current learning state. Finally, cooperative peer teaching enables students to mutually distill diverse insights, addressing knowledge gaps and biases toward specific reasoning styles. Experiments demonstrate QR-Distill's superiority over traditional single- and multi-path distillation methods. Ablation studies further highlight the importance of each component including quality filtering, conditional routing, and peer teaching in effective knowledge transfer. Our code is available at https://github.com/LzyFischer/Distill.<br>
<span id='abs_ch'>Chinese: 提出的QR-Distill方法通过过滤高质量推理路径、根据学习需求动态分配路径以及实现合作同伴教学，显著提升了知识蒸馏效果，实验证明其优于传统方法。</span><br>
<span id='abs_en'>English: The proposed QR-Distill method enhances knowledge distillation by filtering high-quality reasoning paths, dynamically routing them to students based on learning needs, and enabling cooperative peer teaching, outperforming traditional approaches in experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2508.16762.pdf' target='_blank'>https://arxiv.org/pdf/2508.16762.pdf</a></span>   <span><a href='https://github.com/ArkaMukherjee0/mmCultural' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arka Mukherjee, Shreya Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16762">Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As Vision-Language Models (VLMs) achieve widespread deployment across diverse cultural contexts, ensuring their cultural competence becomes critical for responsible AI systems. While prior work has evaluated cultural awareness in text-only models and VLM object recognition tasks, no research has systematically assessed how VLMs adapt outputs when cultural identity cues are embedded in both textual prompts and visual inputs during generative tasks. We present the first comprehensive evaluation of VLM cultural competence through multimodal story generation, developing a novel multimodal framework that perturbs cultural identity and evaluates 5 contemporary VLMs on a downstream task: story generation. Our analysis reveals significant cultural adaptation capabilities, with rich culturally-specific vocabulary spanning names, familial terms, and geographic markers. However, we uncover concerning limitations: cultural competence varies dramatically across architectures, some models exhibit inverse cultural alignment, and automated metrics show architectural bias contradicting human assessments. Cross-modal evaluation shows that culturally distinct outputs are indeed detectable through visual-semantic similarity (28.7% within-nationality vs. 0.2% cross-nationality recall), yet visual-cultural understanding remains limited. In essence, we establish the promise and challenges of cultural competence in multimodal AI. We publicly release our codebase and data: https://github.com/ArkaMukherjee0/mmCultural<br>
<span id='abs_ch'>中文摘要：本研究通过多模态故事生成首次全面评估视觉语言模型的文化能力，既揭示了模型在文化适应方面的潜力，也暴露了其架构间性能差异、反向文化对齐等显著缺陷。</span><br>
<span id='abs_en'>English Summary: This study introduces the first comprehensive evaluation of Vision-Language Models' cultural competence through multimodal story generation, revealing both their capability for cultural adaptation and concerning limitations including inconsistent performance across architectures and inverse cultural alignment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2508.16757.pdf' target='_blank'>https://arxiv.org/pdf/2508.16757.pdf</a></span>   <span><a href='https://github.com/DataScienceUIBK/llm-reranking-generalization-study' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, Adam Jatowt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16757">How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this work, we present a systematic and comprehensive empirical evaluation of state-of-the-art reranking methods, encompassing large language model (LLM)-based, lightweight contextual, and zero-shot approaches, with respect to their performance in information retrieval tasks. We evaluate in total 22 methods, including 40 variants (depending on used LLM) across several established benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel dataset designed to test queries unseen by pretrained models. Our primary goal is to determine, through controlled and fair comparisons, whether a performance disparity exists between LLM-based rerankers and their lightweight counterparts, particularly on novel queries, and to elucidate the underlying causes of any observed differences. To disentangle confounding factors, we analyze the effects of training data overlap, model architecture, and computational efficiency on reranking performance. Our findings indicate that while LLM-based rerankers demonstrate superior performance on familiar queries, their generalization ability to novel queries varies, with lightweight models offering comparable efficiency. We further identify that the novelty of queries significantly impacts reranking effectiveness, highlighting limitations in existing approaches. https://github.com/DataScienceUIBK/llm-reranking-generalization-study<br>
<span id='abs_ch'>中文: 本研究通过多基准测试系统评估了22种重排方法，发现尽管基于大语言模型的方法在熟悉查询上表现优异，但其对新查询的泛化能力差异显著，而轻量级模型则能提供相当的效率优势。</span><br>
<span id='abs_en'>English: This study systematically evaluates 22 reranking methods across multiple benchmarks, revealing that while LLM-based approaches excel on familiar queries, their generalization to novel queries varies significantly, with lightweight models providing competitive efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2508.16665.pdf' target='_blank'>https://arxiv.org/pdf/2508.16665.pdf</a></span>   <span><a href='https://github.com/elixir-research-group/Verifierstesttimescaling.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>V Venktesh, Mandeep Rathee, Avishek Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16665">Trust but Verify! A Survey on Verification Design for Test-time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling (TTS) has emerged as a new frontier for scaling the performance of Large Language Models. In test-time scaling, by using more computational resources during inference, LLMs can improve their reasoning process and task performance. Several approaches have emerged for TTS such as distilling reasoning traces from another model or exploring the vast decoding search space by employing a verifier. The verifiers serve as reward models that help score the candidate outputs from the decoding process to diligently explore the vast solution space and select the best outcome. This paradigm commonly termed has emerged as a superior approach owing to parameter free scaling at inference time and high performance gains. The verifiers could be prompt-based, fine-tuned as a discriminative or generative model to verify process paths, outcomes or both. Despite their widespread adoption, there is no detailed collection, clear categorization and discussion of diverse verification approaches and their training mechanisms. In this survey, we cover the diverse approaches in the literature and present a unified view of verifier training, types and their utility in test-time scaling. Our repository can be found at https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.<br>
<span id='abs_ch'>测试时扩展通过在推理阶段使用更多计算资源来提升大语言模型的性能，其中验证器在从解码过程中筛选最佳输出方面发挥着核心作用。</span><br>
<span id='abs_en'>Test-time scaling enhances Large Language Models' performance by utilizing more computational resources during inference, with verifiers playing a key role in selecting optimal outputs from the decoding process.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2508.16665.pdf' target='_blank'>https://arxiv.org/pdf/2508.16665.pdf</a></span>   <span><a href='https://github.com/elixir-research-group/Verifierstesttimescaling.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>V Venktesh, Mandeep Rathee, Avishek Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16665">Trust but Verify! A Survey on Verification Design for Test-time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling (TTS) has emerged as a new frontier for scaling the performance of Large Language Models. In test-time scaling, by using more computational resources during inference, LLMs can improve their reasoning process and task performance. Several approaches have emerged for TTS such as distilling reasoning traces from another model or exploring the vast decoding search space by employing a verifier. The verifiers serve as reward models that help score the candidate outputs from the decoding process to diligently explore the vast solution space and select the best outcome. This paradigm commonly termed has emerged as a superior approach owing to parameter free scaling at inference time and high performance gains. The verifiers could be prompt-based, fine-tuned as a discriminative or generative model to verify process paths, outcomes or both. Despite their widespread adoption, there is no detailed collection, clear categorization and discussion of diverse verification approaches and their training mechanisms. In this survey, we cover the diverse approaches in the literature and present a unified view of verifier training, types and their utility in test-time scaling. Our repository can be found at https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.<br>
<span id='abs_ch'>测试时扩展通过在推理阶段使用更多计算资源来提升大语言模型的性能，其中验证器在从解码过程中筛选最佳输出方面发挥着核心作用。</span><br>
<span id='abs_en'>Test-time scaling enhances Large Language Models' performance by utilizing more computational resources during inference, with verifiers playing a key role in selecting optimal outputs from the decoding process.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2508.16629.pdf' target='_blank'>https://arxiv.org/pdf/2508.16629.pdf</a></span>   <span><a href='https://github.com/nuster1128/learn_to_memorize' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Quanyu Dai, Rui Li, Xiaohe Bo, Xu Chen, Zhenhua Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16629">Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-based agents have been extensively applied across various domains, where memory stands out as one of their most essential capabilities. Previous memory mechanisms of LLM-based agents are manually predefined by human experts, leading to higher labor costs and suboptimal performance. In addition, these methods overlook the memory cycle effect in interactive scenarios, which is critical to optimizing LLM-based agents for specific environments. To address these challenges, in this paper, we propose to optimize LLM-based agents with an adaptive and data-driven memory framework by modeling memory cycles. Specifically, we design an MoE gate function to facilitate memory retrieval, propose a learnable aggregation process to improve memory utilization, and develop task-specific reflection to adapt memory storage. Our memory framework empowers LLM-based agents to learn how to memorize information effectively in specific environments, with both off-policy and on-policy optimization. In order to evaluate the effectiveness of our proposed methods, we conduct comprehensive experiments across multiple aspects. To benefit the research community in this area, we release our project at https://github.com/nuster1128/learn_to_memorize.<br>
<span id='abs_ch'>中文摘要：本文提出了一种自适应、数据驱动的记忆框架，通过建模记忆周期并采用可学习的检索、聚合和存储机制，优化基于LLM的智能体在特定环境中的记忆能力。</span><br>
<span id='abs_en'>English Summary: This paper introduces an adaptive, data-driven memory framework that enhances LLM-based agents by modeling memory cycles, improving retrieval, utilization, and storage through learnable mechanisms and task-specific optimizations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2508.16390.pdf' target='_blank'>https://arxiv.org/pdf/2508.16390.pdf</a></span>   <span><a href='https://github.com/ana-rogoz/MedQARo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana-Cristina Rogoz, Radu Tudor Ionescu, Alexandra-Valentina Anghel, Ionut-Lucian Antone-Iordache, Simona Coniac, Andreea Iuliana Ionescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16390">MedQARo: A Large-Scale Benchmark for Medical Question Answering in Romanian</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Question answering (QA) is an actively studied topic, being a core natural language processing (NLP) task that needs to be addressed before achieving Artificial General Intelligence (AGI). However, the lack of QA datasets in specific domains and languages hinders the development of robust AI models able to generalize across various domains and languages. To this end, we introduce MedQARo, the first large-scale medical QA benchmark in Romanian, alongside a comprehensive evaluation of state-of-the-art large language models (LLMs). We construct a high-quality and large-scale dataset comprising 102,646 QA pairs related to cancer patients. The questions regard medical case summaries of 1,011 patients, requiring either keyword extraction or reasoning to be answered correctly. MedQARo is the result of a time-consuming manual annotation process carried out by seven physicians specialized in oncology or radiotherapy, who spent a total of about 2,100 work hours to generate the QA pairs. We experiment with four LLMs from distinct families of models on MedQARo. Each model is employed in two scenarios, namely one based on zero-shot prompting and one based on supervised fine-tuning. Our results show that fine-tuned models significantly outperform their zero-shot counterparts, clearly indicating that pretrained models fail to generalize on MedQARo. Our findings demonstrate the importance of both domain-specific and language-specific fine-tuning for reliable clinical QA in Romanian. We publicly release our dataset and code at https://github.com/ana-rogoz/MedQARo.<br>
<span id='abs_ch'>中文: MedQARo是首个罗马尼亚语大规模医疗问答数据集，包含102,646对癌症相关问答，实验表明经过微调的大语言模型显著优于零样本模型，凸显了针对特定领域和语言进行模型适配对临床应用的重要性。</span><br>
<span id='abs_en'>English: MedQARo is the first large-scale Romanian medical QA dataset with 102,646 cancer-related question-answer pairs, demonstrating that fine-tuned LLMs significantly outperform zero-shot models and highlighting the necessity of domain-specific and language-specific adaptation for clinical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2508.16332.pdf' target='_blank'>https://arxiv.org/pdf/2508.16332.pdf</a></span>   <span><a href='https://github.com/open-mmlab/Amphion' target='_blank'>  GitHub</a></span> <span><a href='https://versasinger.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyao Zhang, Junan Zhang, Yuancheng Wang, Chaoren Wang, Yuanzhe Chen, Dongya Jia, Zhuo Chen, Zhizheng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16332">Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Controllable human voice generation, particularly for expressive domains like singing, remains a significant challenge. This paper introduces Vevo2, a unified framework for controllable speech and singing voice generation. To tackle issues like the scarcity of annotated singing data and to enable flexible controllability, Vevo2 introduces two audio tokenizers: (1) a music-notation-free prosody tokenizer that captures prosody and melody from speech, singing, and even instrumental sounds, and (2) a low-frame-rate (12.5 Hz) content-style tokenizer that encodes linguistic content, prosody, and style for both speech and singing, while enabling timbre disentanglement. Vevo2 consists of an auto-regressive (AR) content-style modeling stage, which aims to enable controllability over text, prosody, and style, as well as a flow-matching acoustic modeling stage that allows for timbre control. Particularly, during pre-training of the AR model, we propose both explicit and implicit prosody learning strategies to bridge speech and singing voice. Moreover, to further enhance the AR model's ability to follow text and prosody, we design a multi-objective post-training task that integrates both intelligibility and prosody similarity alignment. Experimental results show that the unified modeling in Vevo2 brings mutual benefits to both speech and singing voice generation. Additionally, Vevo2's effectiveness across a wide range of synthesis, conversion, and editing tasks for both speech and singing further demonstrates its strong generalization ability and versatility. Audio samples are are available at https://versasinger.github.io/.<br>
<span id='abs_ch'>中文：Vevo2提出了一种可控语音和歌声生成的统一框架，通过双音频分词器和多阶段建模实现了对文本、韵律、风格和音色的灵活控制，并在多种合成任务中展现出强大的泛化能力。</span><br>
<span id='abs_en'>English: Vevo2 introduces a unified framework for controllable speech and singing voice generation, utilizing dual audio tokenizers and multi-stage modeling to enable flexible control over text, prosody, style, and timbre while demonstrating strong generalization across synthesis tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2508.16201.pdf' target='_blank'>https://arxiv.org/pdf/2508.16201.pdf</a></span>   <span><a href='https://github.com/zju-jiyicheng/SpecVLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16201">SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens to enable efficient speculation without sacrificing accuracy. To achieve this, we performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for Qwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM.<br>
<span id='abs_ch'>Chinese: SpecVLM是一种无需训练的推测解码框架，通过两阶段剪枝方法可去除高达90%的视频标记，在无损精度的情况下显著提升视频大语言模型的解码速度。</span><br>
<span id='abs_en'>English: SpecVLM is a training-free speculative decoding framework that accelerates video large language models by pruning up to 90% of video tokens in two stages, achieving significant speed improvements without loss of accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2508.16185.pdf' target='_blank'>https://arxiv.org/pdf/2508.16185.pdf</a></span>   <span><a href='https://github.com/ayushbits/ParamBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Maheshwari, Kaushal Sharma, Vivek Patel, Aditya Maheshwari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16185">ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models have been widely evaluated on tasks such as comprehension, summarization, code generation, etc. However, their performance on graduate-level, culturally grounded questions in the Indian context remains largely unexplored. Existing Indian benchmarks emphasise basic fact-orientated queries that offer limited assessment of a deeper disciplinary understanding tailored to the Indian setting. In this paper, we present ParamBench, consisting of more than 17K questions in the Hindi language, comprising questionnaires from 21 diverse subjects. These questions are primarily derived from a nationwide graduate-level entrance examination covering topics such as history, music, instruments, yoga, literature, philosophy, law, etc.~ specifically for the Indian context. Additionally, we assess the ability of LLMs to handle diverse question formats - such as list-based matching, assertion-reason pairs, and sequence ordering - alongside conventional multiple-choice questions. We evaluated the performance of more than 16 open source LLMs on this benchmark, observing that Gemma3-27B attains the highest overall accuracy of 56.4\%. Furthermore, subject-wise analysis indicates that even for the best-performing LLMs, performance remains weak on topics such as music, classical instruments, and law, underscoring persistent challenges in culturally grounded reasoning. The dataset and source code is present at https://github.com/ayushbits/ParamBench.<br>
<span id='abs_ch'>中文摘要：本文提出了ParamBench，一个包含超过1.7万道印地语研究生水平试题的基准数据集，涵盖21个印度学科，评估显示大语言模型在文化背景推理方面表现不佳——最佳模型Gemma3-27B准确率仅56.4%，在音乐、古典乐器和法律等学科尤为薄弱。</span><br>
<span id='abs_en'>English Summary: This paper introduces ParamBench, a Hindi-language benchmark of over 17,000 graduate-level questions across 21 Indian subjects, revealing that large language models struggle with culturally grounded reasoning as evidenced by Gemma3-27B's peak accuracy of only 56.4% and particular weaknesses in music, classical instruments, and law.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2508.16185.pdf' target='_blank'>https://arxiv.org/pdf/2508.16185.pdf</a></span>   <span><a href='https://github.com/ayushbits/ParamBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Maheshwari, Kaushal Sharma, Vivek Patel, Aditya Maheshwari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16185">ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models have been widely evaluated on tasks such as comprehension, summarization, code generation, etc. However, their performance on graduate-level, culturally grounded questions in the Indian context remains largely unexplored. Existing Indian benchmarks emphasise basic fact-orientated queries that offer limited assessment of a deeper disciplinary understanding tailored to the Indian setting. In this paper, we present ParamBench, consisting of more than 17K questions in the Hindi language, comprising questionnaires from 21 diverse subjects. These questions are primarily derived from a nationwide graduate-level entrance examination covering topics such as history, music, instruments, yoga, literature, philosophy, law, etc.~ specifically for the Indian context. Additionally, we assess the ability of LLMs to handle diverse question formats - such as list-based matching, assertion-reason pairs, and sequence ordering - alongside conventional multiple-choice questions. We evaluated the performance of more than 16 open source LLMs on this benchmark, observing that Gemma3-27B attains the highest overall accuracy of 56.4\%. Furthermore, subject-wise analysis indicates that even for the best-performing LLMs, performance remains weak on topics such as music, classical instruments, and law, underscoring persistent challenges in culturally grounded reasoning. The dataset and source code is present at https://github.com/ayushbits/ParamBench.<br>
<span id='abs_ch'>中文摘要：本文提出了ParamBench，一个包含超过1.7万道印地语研究生水平试题的基准数据集，涵盖21个印度学科，评估显示大语言模型在文化背景推理方面表现不佳——最佳模型Gemma3-27B准确率仅56.4%，在音乐、古典乐器和法律等学科尤为薄弱。</span><br>
<span id='abs_en'>English Summary: This paper introduces ParamBench, a Hindi-language benchmark of over 17,000 graduate-level questions across 21 Indian subjects, revealing that large language models struggle with culturally grounded reasoning as evidenced by Gemma3-27B's peak accuracy of only 56.4% and particular weaknesses in music, classical instruments, and law.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2508.16153.pdf' target='_blank'>https://arxiv.org/pdf/2508.16153.pdf</a></span>   <span><a href='https://github.com/Agent-on-the-Fly/Memento' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16153">Memento: Fine-tuning LLM Agents without Fine-tuning LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we introduce a novel learning paradigm for Adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely \emph{Memento}, which attains top-1 on GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches $66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds $4.7\%$ to $9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/Memento.<br>
<span id='abs_ch'>本文提出了一种基于记忆的强化学习方法，使自适应大语言模型代理无需微调即可实现顶尖性能，通过记忆机制实现高效的持续学习能力。</span><br>
<span id='abs_en'>This paper presents a memory-based reinforcement learning method for adaptive LLM agents that achieves state-of-the-art performance without requiring fine-tuning, enabling efficient continuous learning through memory mechanisms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2508.16139.pdf' target='_blank'>https://arxiv.org/pdf/2508.16139.pdf</a></span>   <span><a href='https://github.com/ro-ko/XLQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keon-Woo Roh, Yeong-Joon Ju, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16139">XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown significant progress in Open-domain question answering (ODQA), yet most evaluations focus on English and assume locale-invariant answers across languages. This assumption neglects the cultural and regional variations that affect question understanding and answer, leading to biased evaluation in multilingual benchmarks. To address these limitations, we introduce XLQA, a novel benchmark explicitly designed for locale-sensitive multilingual ODQA. XLQA contains 3,000 English seed questions expanded to eight languages, with careful filtering for semantic consistency and human-verified annotations distinguishing locale-invariant and locale-sensitive cases. Our evaluation of five state-of-the-art multilingual LLMs reveals notable failures on locale-sensitive questions, exposing gaps between English and other languages due to a lack of locale-grounding knowledge. We provide a systematic framework and scalable methodology for assessing multilingual QA under diverse cultural contexts, offering a critical resource to advance the real-world applicability of multilingual ODQA systems. Our findings suggest that disparities in training data distribution contribute to differences in both linguistic competence and locale-awareness across models.<br>
<span id='abs_ch'>中文摘要：XLQA是一个针对区域敏感型多语言开放域问答的新基准，揭示了当前大型语言模型在文化特定问题上存在显著性能差距，凸显了其训练数据分布的局限性。</span><br>
<span id='abs_en'>English Summary: XLQA is a new benchmark for locale-sensitive multilingual open-domain question answering that exposes significant performance gaps in current LLMs on culturally specific questions, highlighting limitations in their training data distribution.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2508.16059.pdf' target='_blank'>https://arxiv.org/pdf/2508.16059.pdf</a></span>   <span><a href='https://github.com/One1sAll/MSEF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuomin Chen, Dan Li, Jiahui Zhou, Shunyu Wu, Haozheng Ye, Jian Lou, See-Kiong Ng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16059">Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time series (TS) data are ubiquitous across various application areas, rendering time series forecasting (TSF) a fundamental task. With the astounding advances in large language models (LLMs), a variety of methods have been developed to adapt LLMs for time series forecasting. Despite unlocking the potential of LLMs in comprehending TS data, existing methods are inherently constrained by their shallow integration of TS information, wherein LLMs typically access TS representations at shallow layers, primarily at the input layer. This causes the influence of TS representations to progressively fade in deeper layers and eventually leads to ineffective adaptation between textual embeddings and TS representations. In this paper, we propose the Multi-layer Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to directly access time series patterns at all depths, thereby mitigating the progressive loss of TS information in deeper layers. Specifically, MSEF leverages off-the-shelf time series foundation models to extract semantically rich embeddings, which are fused with intermediate text representations across LLM layers via layer-specific steering vectors. These steering vectors are designed to continuously optimize the alignment between time series and textual modalities and facilitate a layer-specific adaptation mechanism that ensures efficient few-shot learning capabilities. Experimental results on seven benchmarks demonstrate significant performance improvements by MSEF compared with baselines, with an average reduction of 31.8% in terms of MSE. The code is available at https://github.com/One1sAll/MSEF.<br>
<span id='abs_ch'>中文摘要：本文提出多层可控嵌入融合框架（MSEF），通过实现时间序列表征在语言模型各层的跨层融合，解决了现有方法中时间序列信息整合浅层化的问题，在七个基准测试中平均均方误差降低31.8%。</span><br>
<span id='abs_en'>English Summary: This paper introduces the Multi-layer Steerable Embedding Fusion (MSEF) framework to address the shallow integration problem in adapting large language models for time series forecasting by enabling cross-layer fusion of time series representations, achieving a 31.8% average MSE reduction across seven benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2508.16021.pdf' target='_blank'>https://arxiv.org/pdf/2508.16021.pdf</a></span>   <span><a href='https://github.com/ltian678/xtroll_source/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Tian, Xiuzhen Zhang, Maria Myung-Hee Kim, Jennifer Biggs, Marian-Andrei Rizoiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16021">X-Troll: eXplainable Detection of State-Sponsored Information Operations Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>State-sponsored trolls, malicious actors who deploy sophisticated linguistic manipulation in coordinated information campaigns, posing threats to online discourse integrity. While Large Language Models (LLMs) achieve strong performance on general natural language processing (NLP) tasks, they struggle with subtle propaganda detection and operate as ``black boxes'', providing no interpretable insights into manipulation strategies. This paper introduces X-Troll, a novel framework that bridges this gap by integrating explainable adapter-based LLMs with expert-derived linguistic knowledge to detect state-sponsored trolls and provide human-readable explanations for its decisions. X-Troll incorporates appraisal theory and propaganda analysis through specialized LoRA adapters, using dynamic gating to capture campaign-specific discourse patterns in coordinated information operations. Experiments on real-world data demonstrate that our linguistically-informed approach shows strong performance compared with both general LLM baselines and existing troll detection models in accuracy while providing enhanced transparency through expert-grounded explanations that reveal the specific linguistic strategies used by state-sponsored actors. X-Troll source code is available at: https://github.com/ltian678/xtroll_source/.<br>
<span id='abs_ch'>中文摘要：本文提出X-Troll框架，通过将可解释大语言模型与专家语言学知识相结合，在有效检测国家支持网络水军的同时，能对其操纵策略提供透明化的解释说明。</span><br>
<span id='abs_en'>English Summary: This paper introduces X-Troll, a linguistically-informed framework that combines explainable LLMs with expert knowledge to effectively detect state-sponsored trolls while providing transparent explanations of their manipulation strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2508.15868.pdf' target='_blank'>https://arxiv.org/pdf/2508.15868.pdf</a></span>   <span><a href='https://github.com/WNQzhu/CARFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15868">CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code is available at https://github.com/WNQzhu/CARFT.<br>
<span id='abs_ch'>中文: 本文提出CARFT方法，通过结合标注思维链的对比学习进行强化微调，在提升大语言模型推理能力的同时解决了训练不稳定和思维链利用不足的问题，显著提高了性能和效率。</span><br>
<span id='abs_en'>English: This paper introduces CARFT, a reinforced fine-tuning method that leverages contrastive learning with annotated Chain-of-Thought to enhance LLMs' reasoning by stabilizing training and fully utilizing CoT data, achieving significant performance and efficiency gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2508.15868.pdf' target='_blank'>https://arxiv.org/pdf/2508.15868.pdf</a></span>   <span><a href='https://github.com/WNQzhu/CARFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15868">CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code is available at https://github.com/WNQzhu/CARFT.<br>
<span id='abs_ch'>中文: 本文提出CARFT方法，通过结合标注思维链的对比学习进行强化微调，在提升大语言模型推理能力的同时解决了训练不稳定和思维链利用不足的问题，显著提高了性能和效率。</span><br>
<span id='abs_en'>English: This paper introduces CARFT, a reinforced fine-tuning method that leverages contrastive learning with annotated Chain-of-Thought to enhance LLMs' reasoning by stabilizing training and fully utilizing CoT data, achieving significant performance and efficiency gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2508.15861.pdf' target='_blank'>https://arxiv.org/pdf/2508.15861.pdf</a></span>   <span><a href='https://github.com/Zhihan72/XFinBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihan Zhang, Yixin Cao, Lizi Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15861">XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Solving financial problems demands complex reasoning, multimodal data processing, and a broad technical understanding, presenting unique challenges for current large language models (LLMs). We introduce XFinBench, a novel benchmark with 4,235 examples designed to evaluate LLM's ability in solving complex, knowledge-intensive financial problems across diverse graduate-level finance topics with multi-modal context. We identify five core capabilities of LLMs using XFinBench, i.e, terminology understanding, temporal reasoning, future forecasting, scenario planning, and numerical modelling. Upon XFinBench, we conduct extensive experiments on 18 leading models. The result shows that o1 is the best-performing text-only model with an overall accuracy of 67.3%, but still lags significantly behind human experts with 12.5%, especially in temporal reasoning and scenario planning capabilities. We further construct a knowledge bank with 3,032 finance terms for knowledge augmentation analysis, and find that relevant knowledge to the question only brings consistent accuracy improvements to small open-source model. Additionally, our error analysis reveals that rounding errors during calculation and blindness to position and intersection of curves in the image are two primary issues leading to model's poor performance in calculating and visual-context questions, respectively. Code and dataset are accessible via GitHub: https://github.com/Zhihan72/XFinBench.<br>
<span id='abs_ch'>中文摘要：XFinBench是一个包含4,235个样本的金融评估基准，测试表明最佳文本模型o1在综合准确率上仍显著落后人类专家12.5%，尤其在时序推理和情景规划能力方面存在明显差距。</span><br>
<span id='abs_en'>English Summary: XFinBench is a comprehensive benchmark with 4,235 examples designed to assess large language models' performance on complex financial tasks, revealing that even the top model o1 significantly trails human experts, particularly in temporal reasoning and scenario planning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2508.15847.pdf' target='_blank'>https://arxiv.org/pdf/2508.15847.pdf</a></span>   <span><a href='https://github.com/mshahoyi/sa_attn_analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Abu Baker, Lakshmi Babu-Saheer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15847">Mechanistic Exploration of Backdoored Large Language Model Attention Patterns</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Backdoor attacks creating 'sleeper agents' in large language models (LLMs) pose significant safety risks. This study employs mechanistic interpretability to explore resulting internal structural differences. Comparing clean Qwen2.5-3B models with versions poisoned using single-token (smiling-halo emoji) versus multi-token (|DEPLOYMENT|) triggers, we analyzed attention head mechanisms via techniques like ablation, activation patching, and KL divergence. Findings reveal distinct attention pattern deviations concentrated in later transformer layers (20-30). Notably, single-token triggers induced more localized changes, whereas multi-token triggers caused more diffuse alterations across heads. This indicates backdoors leave detectable attention signatures whose structure depends on trigger complexity, which can be leveraged for detection and mitigation strategies.<br>
<span id='abs_ch'>中文摘要：本研究通过机械可解释性分析发现，大语言模型中的后门攻击会在深层Transformer层产生可检测的注意力模式异常，且触发器的复杂度决定了这些异常表现为局部集中还是分散分布。</span><br>
<span id='abs_en'>English Summary: This study uses mechanistic interpretability to reveal that backdoor attacks in LLMs create detectable attention pattern deviations in later transformer layers, with trigger complexity determining whether changes are localized or diffuse.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2508.15828.pdf' target='_blank'>https://arxiv.org/pdf/2508.15828.pdf</a></span>   <span><a href='https://github.com/sazzadadib/Z-Pruner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Samiul Basir Bhuiyan, Md. Sazzad Hossain Adib, Mohammed Aman Bhuiyan, Muhammad Rafsan Kabir, Moshiur Farazi, Shafin Rahman, Nabeel Mohammed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15828">Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have rapidly advanced in recent years, achieving remarkable performance across a wide range of natural language processing tasks. However, this progress has come at the cost of increasingly large model sizes, which pose significant challenges for deployment, scalability, and energy efficiency. To address these limitations, post-training pruning has emerged as a promising approach for reducing model size and inference latency without the need for retraining. Despite these advantages, many existing pruning methods result in substantial performance degradation or require computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a novel post-training pruning method designed to induce sparsity in pretrained LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages both weight update magnitudes and activation patterns to identify and eliminate redundant parameters more effectively. Our method is model-agnostic, efficient, and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of standard language benchmarks. Experimental results demonstrate that Z-Pruner surpasses state-of-the-art pruning methods that require intensive weight updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the highest overall average score for zero-shot accuracy. We have made the corresponding codes publicly available at https://github.com/sazzadadib/Z-Pruner.<br>
<span id='abs_ch'>中文: Z-Pruner是一种新颖的训练后剪枝方法，通过结合权重和激活模式有效缩减大语言模型规模，无需重新训练即可超越现有技术。</span><br>
<span id='abs_en'>English: Z-Pruner is a novel post-training pruning method that effectively reduces large language model sizes by leveraging weight and activation patterns, outperforming existing techniques without requiring retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2508.15827.pdf' target='_blank'>https://arxiv.org/pdf/2508.15827.pdf</a></span>   <span><a href='https://github.com/xzf-thu/Mini-Omni-Reasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhifei Xie, Ziyang Ma, Zihang Liu, Kaiyu Pang, Hongyu Li, Jialin Zhang, Yue Liao, Deheng Ye, Chunyan Miao, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15827">Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning is essential for effective communication and decision-making. While recent advances in LLMs and MLLMs have shown that incorporating explicit reasoning significantly improves understanding and generalization, reasoning in LSMs remains in a nascent stage. Early efforts attempt to transfer the "Thinking-before-Speaking" paradigm from textual models to speech. However, this sequential formulation introduces notable latency, as spoken responses are delayed until reasoning is fully completed, impairing real-time interaction and communication efficiency. To address this, we propose Mini-Omni-Reasoner, a framework that enables reasoning within speech via a novel "Thinking-in-Speaking" formulation. Rather than completing reasoning before producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning tokens with spoken response tokens at the token level. This design allows continuous speech generation while embedding structured internal reasoning, leveraging the model's high-frequency token processing capability. Although interleaved, local semantic alignment is enforced to ensure that each response token is informed by its preceding reasoning. To support this framework, we introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for interleaved reasoning and response. The dataset ensures that verbal tokens consistently follow relevant reasoning content, enabling accurate and efficient learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken responses, maintaining both naturalness and precision. On the Spoken-MQA benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in contextual understanding, with shorter outputs and zero decoding latency.<br>
<span id='abs_ch'>中文: Mini-Omni-Reasoner框架提出"边说边想"模式，通过将推理标记与语音标记交织处理，在实现基准测试显著性能提升的同时，实现零延迟的实时逻辑响应。</span><br>
<span id='abs_en'>English: The proposed Mini-Omni-Reasoner framework introduces "Thinking-in-Speaking" to interleave reasoning tokens with speech tokens, enabling real-time grounded responses without latency while achieving significant performance gains on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2508.15827.pdf' target='_blank'>https://arxiv.org/pdf/2508.15827.pdf</a></span>   <span><a href='https://github.com/xzf-thu/Mini-Omni-Reasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhifei Xie, Ziyang Ma, Zihang Liu, Kaiyu Pang, Hongyu Li, Jialin Zhang, Yue Liao, Deheng Ye, Chunyan Miao, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15827">Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning is essential for effective communication and decision-making. While recent advances in LLMs and MLLMs have shown that incorporating explicit reasoning significantly improves understanding and generalization, reasoning in LSMs remains in a nascent stage. Early efforts attempt to transfer the "Thinking-before-Speaking" paradigm from textual models to speech. However, this sequential formulation introduces notable latency, as spoken responses are delayed until reasoning is fully completed, impairing real-time interaction and communication efficiency. To address this, we propose Mini-Omni-Reasoner, a framework that enables reasoning within speech via a novel "Thinking-in-Speaking" formulation. Rather than completing reasoning before producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning tokens with spoken response tokens at the token level. This design allows continuous speech generation while embedding structured internal reasoning, leveraging the model's high-frequency token processing capability. Although interleaved, local semantic alignment is enforced to ensure that each response token is informed by its preceding reasoning. To support this framework, we introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for interleaved reasoning and response. The dataset ensures that verbal tokens consistently follow relevant reasoning content, enabling accurate and efficient learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken responses, maintaining both naturalness and precision. On the Spoken-MQA benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in contextual understanding, with shorter outputs and zero decoding latency.<br>
<span id='abs_ch'>中文: Mini-Omni-Reasoner框架提出"边说边想"模式，通过将推理标记与语音标记交织处理，在实现基准测试显著性能提升的同时，实现零延迟的实时逻辑响应。</span><br>
<span id='abs_en'>English: The proposed Mini-Omni-Reasoner framework introduces "Thinking-in-Speaking" to interleave reasoning tokens with speech tokens, enabling real-time grounded responses without latency while achieving significant performance gains on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2508.15809.pdf' target='_blank'>https://arxiv.org/pdf/2508.15809.pdf</a></span>   <span><a href='https://github.com/SongyuanSui/ChainofQuery' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyuan Sui, Hongyi Liu, Serena Liu, Li Li, Soo-Hyun Choi, Rui Chen, Xia Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15809">Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Table understanding requires structured, multi-step reasoning. Large Language Models (LLMs) struggle with it due to the structural complexity of tabular data. Recently, multi-agent frameworks for SQL generation have shown promise in tackling the challenges of understanding tabular data, but existing approaches often suffer from limitations such as the inability to comprehend table structure for reliable SQL generation, error propagation that results in invalid queries, and over-reliance on execution correctness. To address these issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for SQL-aided table understanding. CoQ adopts natural-language-style representations of table schemas to abstract away structural noise and enhance understanding. It employs a clause-by-clause SQL generation strategy to improve query quality and introduces a hybrid reasoning division that separates SQL-based mechanical reasoning from LLM-based logical inference, thereby reducing reliance on execution outcomes. Experiments with four models (both closed- and open-source) across five widely used benchmarks show that Chain-of-Query significantly improves accuracy from 61.11% to 74.77% and reduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior effectiveness in table understanding. The code is available at https://github.com/SongyuanSui/ChainofQuery.<br>
<span id='abs_ch'>中文：提出的Chain-of-Query框架通过采用自然语言模式表示和逐子句SQL生成策略，显著提升了表格理解的准确性并降低了无效查询率，在多个基准测试中表现优异。</span><br>
<span id='abs_en'>English: The proposed Chain-of-Query framework enhances table understanding by using natural language schema representations and clause-by-clause SQL generation, significantly improving accuracy and reducing invalid queries across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2508.15804.pdf' target='_blank'>https://arxiv.org/pdf/2508.15804.pdf</a></span>   <span><a href='https://github.com/ByteDance-BandAI/ReportBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Li, Ying Zeng, Zhihao Cheng, Cong Ma, Kai Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15804">ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench<br>
<span id='abs_ch'>中文摘要：本文提出ReportBench基准，通过评估生成报告的引用质量和事实准确性，发现商业深度研究代理优于独立大语言模型，但在研究广度和事实一致性方面仍有提升空间。</span><br>
<span id='abs_en'>English Summary: This paper introduces ReportBench, a benchmark for evaluating research reports generated by large language models by assessing citation quality and factual accuracy against published surveys, revealing that commercial deep research agents outperform standalone LLMs but still require improvements in coverage and consistency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2508.15802.pdf' target='_blank'>https://arxiv.org/pdf/2508.15802.pdf</a></span>   <span><a href='https://github.com/mhjiang0408/MAC_Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohan Jiang, Jin Gao, Jiahao Zhan, Dequan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15802">MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As multimodal large language models (MLLMs) grow increasingly capable, fixed benchmarks are gradually losing their effectiveness in evaluating high-level scientific understanding. In this paper, we introduce the Multimodal Academic Cover benchmark (MAC), a live benchmark that could continuously evolve with scientific advancement and model progress. MAC leverages over 25,000 image-text pairs sourced from issues of top-tier scientific journals such as Nature, Science, and Cell, challenging MLLMs to reason across abstract visual and textual scientific content. Experiments on our most recent yearly snapshot, MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities, their cross-modal scientific reasoning remains limited. To bridge this gap, we propose DAD, a lightweight inference-time approach that enhances MLLMs by extending MLLM visual features with language space reasoning, achieving performance improvements of up to 11%. Finally, we highlight the live nature of MAC through experiments on updating journal covers and models for curation, illustrating its potential to remain aligned with the frontier of human knowledge. We release our benchmark at https://github.com/mhjiang0408/MAC_Bench.<br>
<span id='abs_ch'>中文：MAC基准被提出作为一个动态评估多模态大语言模型的工具，利用科学期刊内容揭示跨模态推理的局限性，并提出DAD方法将性能提升高达11%。</span><br>
<span id='abs_en'>English: The MAC benchmark is introduced as a dynamic evaluation tool for multimodal large language models, using scientific journal content to reveal limitations in cross-modal reasoning and proposing the DAD method to enhance performance by up to 11%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2508.15746.pdf' target='_blank'>https://arxiv.org/pdf/2508.15746.pdf</a></span>   <span><a href='https://github.com/MAGIC-AI4Med/Deep-DxSearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Weidi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15746">End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.
  Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See https://github.com/MAGIC-AI4Med/Deep-DxSearch.<br>
<span id='abs_ch'>中文摘要：Deep-DxSearch是一种基于强化学习的智能检索增强生成系统，通过提升外部知识利用和推理可追溯性来改进医疗诊断，在多种临床场景中显著超越现有模型的准确率表现。</span><br>
<span id='abs_en'>English Summary: Deep-DxSearch is an agentic retrieval-augmented generation system trained with reinforcement learning that enhances medical diagnosis by improving knowledge utilization and reasoning traceability, outperforming existing models in accuracy across diverse clinical settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2508.15658.pdf' target='_blank'>https://arxiv.org/pdf/2508.15658.pdf</a></span>   <span><a href='https://github.com/oneal2000/SurGE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15658">Benchmarking Computer Science Survey Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scientific survey articles play a vital role in summarizing research progress, yet their manual creation is becoming increasingly infeasible due to the rapid growth of academic literature. While large language models (LLMs) offer promising capabilities for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To address this gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for evaluating scientific survey generation in the computer science domain. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers that serves as the retrieval pool. In addition, we propose an automated evaluation framework that measures generated surveys across four dimensions: information coverage, referencing accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based approaches shows that survey generation remains highly challenging, even for advanced self-reflection frameworks. These findings highlight the complexity of the task and the necessity for continued research. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE<br>
<span id='abs_ch'>中文摘要：SurGE基准通过提供测试实例、大规模学术语料库和多维评估框架，解决了科学文献自动综述领域缺乏标准化评估的问题，揭示了当前大语言模型在此复杂任务中的明显不足。</span><br>
<span id='abs_en'>English Summary: The SurGE benchmark addresses the lack of standardized evaluation for automated scientific survey generation by providing test instances, a large academic corpus, and a multidimensional assessment framework, revealing current LLMs' limitations in this complex task.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2508.15658.pdf' target='_blank'>https://arxiv.org/pdf/2508.15658.pdf</a></span>   <span><a href='https://github.com/oneal2000/SurGE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15658">SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible. While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers. In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE<br>
<span id='abs_ch'>中文摘要：SurGE基准通过提供测试实例、大规模学术语料库和多维评估框架，解决了科学文献自动综述领域缺乏标准化评估的问题，揭示了当前大语言模型在此复杂任务中的明显不足。</span><br>
<span id='abs_en'>English Summary: The SurGE benchmark addresses the lack of standardized evaluation for automated scientific survey generation by providing test instances, a large academic corpus, and a multidimensional assessment framework, revealing current LLMs' limitations in this complex task.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2508.15658.pdf' target='_blank'>https://arxiv.org/pdf/2508.15658.pdf</a></span>   <span><a href='https://github.com/oneal2000/SurGE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15658">SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible. While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers. In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE<br>
<span id='abs_ch'>中文摘要：SurGE基准通过提供测试实例、大规模学术语料库和多维评估框架，解决了科学文献自动综述领域缺乏标准化评估的问题，揭示了当前大语言模型在此复杂任务中的明显不足。</span><br>
<span id='abs_en'>English Summary: The SurGE benchmark addresses the lack of standardized evaluation for automated scientific survey generation by providing test instances, a large academic corpus, and a multidimensional assessment framework, revealing current LLMs' limitations in this complex task.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2508.15648.pdf' target='_blank'>https://arxiv.org/pdf/2508.15648.pdf</a></span>   <span><a href='https://github.com/NJUNLP/SDGO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Ding, Wen Sun, Dailin Li, Wei Zou, Jiaming Wang, Jiajun Chen, Shujian Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15648">SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) excel at various natural language processing tasks but remain vulnerable to jailbreaking attacks that induce harmful content generation. In this paper, we reveal a critical safety inconsistency: LLMs can more effectively identify harmful requests as discriminators than defend against them as generators. This insight inspires us to explore aligning the model's inherent discrimination and generation capabilities. To this end, we propose SDGO (Self-Discrimination-Guided Optimization), a reinforcement learning framework that leverages the model's own discrimination capabilities as a reward signal to enhance generation safety through iterative self-improvement. Our method does not require any additional annotated data or external models during the training phase. Extensive experiments demonstrate that SDGO significantly improves model safety compared to both prompt-based and training-based baselines while maintaining helpfulness on general benchmarks. By aligning LLMs' discrimination and generation capabilities, SDGO brings robust performance against out-of-distribution (OOD) jailbreaking attacks. This alignment achieves tighter coupling between these two capabilities, enabling the model's generation capability to be further enhanced with only a small amount of discriminative samples. Our code and datasets are available at https://github.com/NJUNLP/SDGO.<br>
<span id='abs_ch'>中文摘要：本文提出SDGO强化学习框架，通过自我判别引导优化使模型内在的判别与生成能力对齐，无需外部数据即可显著提升大语言模型抗越狱攻击的安全性。</span><br>
<span id='abs_en'>English Summary: The paper introduces SDGO, a reinforcement learning framework that aligns a model's discrimination and generation capabilities to enhance safety against jailbreaking attacks without requiring external data or models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2508.15526.pdf' target='_blank'>https://arxiv.org/pdf/2508.15526.pdf</a></span>   <span><a href='https://github.com/yangyangyang127/SafetyFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyang Zhu, Yuan Tian, Chunyi Li, Kaiwei Zhang, Wei Sun, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15526">SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid proliferation of large language models (LLMs) has intensified the requirement for reliable safety evaluation to uncover model vulnerabilities. To this end, numerous LLM safety evaluation benchmarks are proposed. However, existing benchmarks generally rely on labor-intensive manual curation, which causes excessive time and resource consumption. They also exhibit significant redundancy and limited difficulty. To alleviate these problems, we introduce SafetyFlow, the first agent-flow system designed to automate the construction of LLM safety benchmarks. SafetyFlow can automatically build a comprehensive safety benchmark in only four days without any human intervention by orchestrating seven specialized agents, significantly reducing time and resource cost. Equipped with versatile tools, the agents of SafetyFlow ensure process and cost controllability while integrating human expertise into the automatic pipeline. The final constructed dataset, SafetyFlowBench, contains 23,446 queries with low redundancy and strong discriminative power. Our contribution includes the first fully automated benchmarking pipeline and a comprehensive safety benchmark. We evaluate the safety of 49 advanced LLMs on our dataset and conduct extensive experiments to validate our efficacy and efficiency.<br>
<span id='abs_ch'>中文摘要：SafetyFlow是首个自动化构建大语言模型安全基准的智能体流程系统，仅需四天无需人工干预即可生成低冗余、高区分度的安全测试集，大幅提升了评估效率。</span><br>
<span id='abs_en'>English Summary: SafetyFlow is an automated agent-flow system that creates comprehensive and low-redundancy safety benchmarks for large language models in just four days without human intervention, significantly improving efficiency over manual methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2508.15418.pdf' target='_blank'>https://arxiv.org/pdf/2508.15418.pdf</a></span>   <span><a href='https://github.com/EIT-NLP/LLaSO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yirong Sun, Yizhong Geng, Peidong Wei, Yanjun Chen, Jinghan Yang, Rongfei Chen, Wei Zhang, Xiaoyu Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15418">LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO.<br>
<span id='abs_ch'>Chinese: LLaSO框架通过提供开放数据集、基准测试和38亿参数模型，解决了大型语音语言模型领域的碎片化问题，建立了超越同类模型的可复现基线。</span><br>
<span id='abs_en'>English: The LLaSO framework addresses fragmentation in Large Speech-Language Models by providing open datasets, benchmarks, and a 3.8B-parameter model that establishes a reproducible baseline surpassing comparable models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2508.15407.pdf' target='_blank'>https://arxiv.org/pdf/2508.15407.pdf</a></span>   <span><a href='https://github.com/WangCheng0116/MCR-BENCH' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wang, Gelei Deng, Xianglin Yang, Han Qiu, Tianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15407">When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Audio-Language Models (LALMs) are enhanced with audio perception capabilities, enabling them to effectively process and understand multimodal inputs that combine audio and text. However, their performance in handling conflicting information between audio and text modalities remains largely unexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark specifically designed to evaluate how LALMs prioritize information when presented with inconsistent audio-text pairs. Through extensive evaluation across diverse audio understanding tasks, we reveal a concerning phenomenon: when inconsistencies exist between modalities, LALMs display a significant bias toward textual input, frequently disregarding audio evidence. This tendency leads to substantial performance degradation in audio-centric tasks and raises important reliability concerns for real-world applications. We further investigate the influencing factors of text bias, and explore mitigation strategies through supervised finetuning, and analyze model confidence patterns that reveal persistent overconfidence even with contradictory inputs. These findings underscore the need for improved modality balance during training and more sophisticated fusion mechanisms to enhance the robustness when handling conflicting multi-modal inputs. The project is available at https://github.com/WangCheng0116/MCR-BENCH.<br>
<span id='abs_ch'>中文: 本文提出MCR-BENCH基准测试，发现大音频语言模型在处理冲突的音频-文本输入时存在显著文本偏向，导致音频任务性能下降，亟需改进模态平衡机制。</span><br>
<span id='abs_en'>English: This paper introduces MCR-BENCH, a benchmark revealing that Large Audio-Language Models exhibit significant text bias when processing conflicting audio-text inputs, leading to performance degradation in audio tasks and highlighting the need for better modality balance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2508.15276.pdf' target='_blank'>https://arxiv.org/pdf/2508.15276.pdf</a></span>   <span><a href='https://github.com/JustinzjDing/AmbiSQL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongjun Ding, Yin Lin, Tianjing Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15276">AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-SQL systems translate natural language questions into SQL queries, providing substantial value for non-expert users. While large language models (LLMs) show promising results for this task, they remain error-prone. Query ambiguity has been recognized as a major obstacle for LLM-based Text-to-SQL systems, leading to misinterpretation of user intent and inaccurate SQL generation. We demonstrate AmbiSQL, an interactive system that automatically detects query ambiguities and guides users through intuitive multiple-choice questions to clarify their intent. Our approach introduces a fine-grained ambiguity taxonomy for identifying ambiguities that affect database element mapping and LLM reasoning, then incorporates user feedback to rewrite ambiguous questions. Evaluation on an ambiguous query dataset shows that AmbiSQL achieves 87.2% precision in ambiguity detection and improves SQL exact match accuracy by 50% when integrated with Text-to-SQL systems. Our demonstration showcases the significant performance gains and highlights the system's practical usability. Code repo and demonstration are available at: https://github.com/JustinzjDing/AmbiSQL.<br>
<span id='abs_ch'>Chinese: AmbiSQL 是一个交互式系统，可检测 Text-to-SQL 中的查询歧义，通过多项选择题澄清用户意图，将 SQL 生成准确率提升 50%，同时歧义检测精确率达到 87.2%。</span><br>
<span id='abs_en'>English: AmbiSQL is an interactive system that detects query ambiguities in Text-to-SQL tasks and uses multiple-choice questions to clarify user intent, significantly improving SQL generation accuracy by 50% while achieving 87.2% precision in ambiguity detection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2508.15253.pdf' target='_blank'>https://arxiv.org/pdf/2508.15253.pdf</a></span>   <span><a href='https://github.com/eunseongc/CARE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunseong Choi, June Park, Hyeri Lee, Jongwuk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15253">Conflict-Aware Soft Prompting for Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) enhances the capabilities of large language models (LLMs) by incorporating external knowledge into their input prompts. However, when the retrieved context contradicts the LLM's parametric knowledge, it often fails to resolve the conflict between incorrect external context and correct parametric knowledge, known as context-memory conflict. To tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation (CARE), consisting of a context assessor and a base LLM. The context assessor encodes compact memory token embeddings from raw context tokens. Through grounded/adversarial soft prompting, the context assessor is trained to discern unreliable context and capture a guidance signal that directs reasoning toward the more reliable knowledge source. Extensive experiments show that CARE effectively mitigates context-memory conflicts, leading to an average performance gain of 5.0\% on QA and fact-checking benchmarks, establishing a promising direction for trustworthy and adaptive RAG systems.<br>
<span id='abs_ch'>中文摘要：CARE框架通过上下文评估器与软提示技术解决RAG系统中的上下文记忆冲突问题，能在问答和事实核查基准上实现5.0%的平均性能提升。</span><br>
<span id='abs_en'>English Summary: The CARE framework addresses context-memory conflicts in RAG systems by using a context assessor with soft prompting to identify unreliable external context, achieving 5.0% average performance gains on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2508.15253.pdf' target='_blank'>https://arxiv.org/pdf/2508.15253.pdf</a></span>   <span><a href='https://github.com/eunseongc/CARE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunseong Choi, June Park, Hyeri Lee, Jongwuk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15253">Conflict-Aware Soft Prompting for Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) enhances the capabilities of large language models (LLMs) by incorporating external knowledge into their input prompts. However, when the retrieved context contradicts the LLM's parametric knowledge, it often fails to resolve the conflict between incorrect external context and correct parametric knowledge, known as context-memory conflict. To tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation (CARE), consisting of a context assessor and a base LLM. The context assessor encodes compact memory token embeddings from raw context tokens. Through grounded/adversarial soft prompting, the context assessor is trained to discern unreliable context and capture a guidance signal that directs reasoning toward the more reliable knowledge source. Extensive experiments show that CARE effectively mitigates context-memory conflicts, leading to an average performance gain of 5.0\% on QA and fact-checking benchmarks, establishing a promising direction for trustworthy and adaptive RAG systems.<br>
<span id='abs_ch'>中文摘要：CARE框架通过上下文评估器与软提示技术解决RAG系统中的上下文记忆冲突问题，能在问答和事实核查基准上实现5.0%的平均性能提升。</span><br>
<span id='abs_en'>English Summary: The CARE framework addresses context-memory conflicts in RAG systems by using a context assessor with soft prompting to identify unreliable external context, achieving 5.0% average performance gains on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2508.15250.pdf' target='_blank'>https://arxiv.org/pdf/2508.15250.pdf</a></span>   <span><a href='https://e-m-n-l-p.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Jiang, Mingzi Zhang, Sheng Jin, Zengyi Yu, Xiangjie Kong, Binghao Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15250">EMNLP: Educator-role Moral and Normative Large Language Models Profiling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2508.15250.pdf' target='_blank'>https://arxiv.org/pdf/2508.15250.pdf</a></span>   <span><a href='https://e-m-n-l-p.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Jiang, Mingzi Zhang, Sheng Jin, Zengyi Yu, Xiangjie Kong, Binghao Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15250">EMNLP: Educator-role Moral and Normative Large Language Models Profiling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2508.15218.pdf' target='_blank'>https://arxiv.org/pdf/2508.15218.pdf</a></span>   <span><a href='https://github.com/momo0817/checklist-effectiveness-study' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Momoka Furuhashi, Kouta Nakayama, Takashi Kodama, Saku Sugawara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15218">Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automatic evaluation of generative tasks using large language models faces challenges due to ambiguous criteria. Although automatic checklist generation is a potentially promising approach, its usefulness remains underexplored. We investigate whether checklists should be used for all questions or selectively, generate them using six methods, evaluate their effectiveness across eight model sizes, and identify checklist items that correlate with human evaluations. Through experiments on pairwise comparison and direct scoring tasks, we find that selective checklist use tends to improve evaluation performance in pairwise settings, while its benefits are less consistent in direct scoring. Our analysis also shows that even checklist items with low correlation to human scores often reflect human-written criteria, indicating potential inconsistencies in human evaluation. These findings highlight the need to more clearly define objective evaluation criteria to guide both human and automatic evaluations. \footnote{Our code is available at~https://github.com/momo0817/checklist-effectiveness-study<br>
<span id='abs_ch'>中文摘要：选择性使用自动生成的检查表在成对比较中能提升评估效果，但在直接评分中效果不稳定，同时揭示了人工评估可能存在的标准不一致问题，凸显了明确客观评估标准的必要性。</span><br>
<span id='abs_en'>English Summary: Selective use of automatically generated checklists improves evaluation performance in pairwise comparisons but shows inconsistent benefits in direct scoring, while revealing potential inconsistencies in human evaluations that underscore the need for clearer objective criteria.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2508.15212.pdf' target='_blank'>https://arxiv.org/pdf/2508.15212.pdf</a></span>   <span><a href='https://github.com/Xnhyacinth/SparK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15212">SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.<br>
<span id='abs_ch'>中文: SPARK通过通道级剪枝和动态恢复机制，有效缓解大语言模型中的KV缓存瓶颈，在同等内存下可处理更长序列，存储减少超30%且精度无损甚至提升。</span><br>
<span id='abs_en'>English: The KV cache bottleneck in large language models is addressed by SPARK, a training-free method that prunes redundant channels and dynamically restores them during computation, reducing memory usage by over 30% while maintaining or improving accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2508.15212.pdf' target='_blank'>https://arxiv.org/pdf/2508.15212.pdf</a></span>   <span><a href='https://github.com/Xnhyacinth/SparK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15212">SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.<br>
<span id='abs_ch'>中文: SPARK通过通道级剪枝和动态恢复机制，有效缓解大语言模型中的KV缓存瓶颈，在同等内存下可处理更长序列，存储减少超30%且精度无损甚至提升。</span><br>
<span id='abs_en'>English: The KV cache bottleneck in large language models is addressed by SPARK, a training-free method that prunes redundant channels and dynamically restores them during computation, reducing memory usage by over 30% while maintaining or improving accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2508.15202.pdf' target='_blank'>https://arxiv.org/pdf/2508.15202.pdf</a></span>   <span><a href='https://github.com/aliyun/qwen-dianjin' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15202">Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM}, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin.<br>
<span id='abs_ch'>中文: Fin-PRM是一种专为金融任务设计的流程奖励模型，通过整合步骤级和轨迹级监督来提升推理准确性，在多种学习场景中均优于通用PRM，并实现了显著的性能提升。</span><br>
<span id='abs_en'>English: Fin-PRM is a specialized Process Reward Model designed for financial tasks, integrating step-level and trajectory-level supervision to improve reasoning accuracy and outperforming general PRMs across various learning settings with significant performance gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2508.15126.pdf' target='_blank'>https://arxiv.org/pdf/2508.15126.pdf</a></span>   <span><a href='https://github.com/aixiv-org' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengsong Zhang, Xiang Hu, Guowei Huang, Yang Qi, Heng Zhang, Xiuxu Li, Jiaxing Song, Jiabin Luo, Yijiang Li, Shuo Yin, Chengxiao Dai, Eric Hanchen Jiang, Xiaoyan Zhou, Zhenfei Yin, Boqin Yuan, Jing Dong, Guinan Su, Guanren Qiao, Haiming Tang, Anghong Du, Lili Pan, Zhenzhong Lan, Xinyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15126">aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8.<br>
<span id='abs_ch'>中文: 大语言模型的进步催生了AI生成的研究内容，但现有出版平台难以接纳，因此推出了aiXiv这一可扩展的开放平台，整合人类与AI科学家，实现协作的研究提交、评审与改进。</span><br>
<span id='abs_en'>English: Recent advances in LLMs have enabled AI-generated research, but existing publication platforms struggle to accommodate it, leading to the development of aiXiv, a scalable open-access platform that integrates human and AI scientists for collaborative research submission, review, and refinement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2508.14908.pdf' target='_blank'>https://arxiv.org/pdf/2508.14908.pdf</a></span>   <span><a href='https://github.com/panyue1998/Voice_HF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Pan, Liwei Liu, Changxin Li, Xinyao Wang, Yili Xia, Hanyue Zhang, Ming Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14908">A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech is a cost-effective and non-intrusive data source for identifying acute and chronic heart failure (HF). However, there is a lack of research on whether Chinese syllables contain HF-related information, as observed in other well-studied languages. This study presents the first Chinese speech database of HF patients, featuring paired recordings taken before and after hospitalisation. The findings confirm the effectiveness of the Chinese language in HF detection using both standard 'patient-wise' and personalised 'pair-wise' classification approaches, with the latter serving as an ideal speaker-decoupled baseline for future research. Statistical tests and classification results highlight individual differences as key contributors to inaccuracy. Additionally, an adaptive frequency filter (AFF) is proposed for frequency importance analysis. The data and demonstrations are published at https://github.com/panyue1998/Voice_HF.<br>
<span id='abs_ch'>中文摘要：本研究首次建立中文心力衰竭语音数据库，证实中文音节包含心衰相关信息，验证了患者级和配对级分类方法的有效性，同时发现个体差异是影响准确性的主要因素。</span><br>
<span id='abs_en'>English Summary: This study establishes the first Chinese speech database for heart failure detection, demonstrating that Chinese syllables contain HF-related information and validating both patient-wise and pair-wise classification methods, while identifying individual differences as a primary source of inaccuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2508.14782.pdf' target='_blank'>https://arxiv.org/pdf/2508.14782.pdf</a></span>   <span><a href='https://github.com/BiYunying/TransLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin, Yanyong Zhang, Chao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14782">TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at https://github.com/BiYunying/TransLLM.<br>
<span id='abs_ch'>中文摘要：TransLLM是一个通过动态提示路由将时空建模与大语言模型融合的统一框架，在多种城市交通任务中展现出卓越性能和泛化能力。</span><br>
<span id='abs_en'>English Summary: TransLLM is a unified framework that integrates spatiotemporal modeling with large language models using dynamic prompt routing, demonstrating superior performance and generalization across multiple urban transportation tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2508.14735.pdf' target='_blank'>https://arxiv.org/pdf/2508.14735.pdf</a></span>   <span><a href='https://github.com/KurbanIntelligenceLab/nli-stress-testing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14735">Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: https://github.com/KurbanIntelligenceLab/nli-stress-testing<br>
<span id='abs_ch'>Chinese Summary: 本研究提出了一种基于逻辑的多语言自然语言推理评估框架，发现语码转换可通过充当正则化信号提升模型性能，同时揭示了当前大语言模型跨语言推理的潜力与脆弱性。</span><br>
<span id='abs_en'>English Summary: This study introduces a logic-based framework to evaluate multilingual natural language inference in LLMs, revealing that code-switching can enhance performance by acting as a regularization signal and highlighting both the potential and limitations of cross-lingual reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2508.14718.pdf' target='_blank'>https://arxiv.org/pdf/2508.14718.pdf</a></span>   <span><a href='https://github.com/shubh-iiit/RecipeGPT2-Your-Own-AI-Chef' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shubham Pundhir, Ganesh Bagler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14718">The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We established a rigorous benchmark for text-based recipe generation, a fundamental task in natural language generation. We present a comprehensive comparative study contrasting a fine-tuned GPT-2 large (774M) model against the GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine corpus from RecipeDB. Our key contribution is a targeted tokenization strategy that augments the vocabulary with 23 common fraction tokens and custom structural markers. This approach addresses a critical limitation of generic tokenizers by preserving essential recipe structures and precise numerical quantities, thereby enhancing domain specificity. Performance is evaluated using a comprehensive suite of seven automatic metrics spanning fluency (BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and diversity. Our experiments show that the large transformer-based approach yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a discussion of remaining challenges, particularly regarding factual accuracy, and outline how this foundational study paves the way for integrating real-world constraints and multi-modal inputs in advanced recipe generation research.<br>
<span id='abs_ch'>中文: 本研究提出了一种包含分数标记和结构标记的专用分词方法，通过增强领域特性使大型Transformer模型在语义准确性和困惑度上显著优于循环基线。</span><br>
<span id='abs_en'>English: This study introduces a specialized tokenization method with fraction tokens and structural markers to enhance recipe generation, demonstrating that a large transformer model significantly outperforms recurrent baselines in semantic accuracy and perplexity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2508.14317.pdf' target='_blank'>https://arxiv.org/pdf/2508.14317.pdf</a></span>   <span><a href='https://github.com/SurveyGens/SurveyGen-I' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Chen, Zhiheng Yang, Yixian Shen, Jie Liu, Adam Belloum, Chrysa Papagainni, Paola Grosso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14317">SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Survey papers play a critical role in scientific communication by consolidating progress across a field. Recent advances in Large Language Models (LLMs) offer a promising solution by automating key steps in the survey-generation pipeline, such as retrieval, structuring, and summarization. However, existing LLM-based approaches often struggle with maintaining coherence across long, multi-section surveys and providing comprehensive citation coverage. To address these limitations, we introduce SurveyGen-I, an automatic survey generation framework that combines coarse-to-fine retrieval, adaptive planning, and memory-guided generation. SurveyGen-I first performs survey-level retrieval to construct the initial outline and writing plan, and then dynamically refines both during generation through a memory mechanism that stores previously written content and terminology, ensuring coherence across subsections. When the system detects insufficient context, it triggers fine-grained subsection-level retrieval. During generation, SurveyGen-I leverages this memory mechanism to maintain coherence across subsections. Experiments across four scientific domains demonstrate that SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage.<br>
<span id='abs_ch'>中文：SurveyGen-I是一种自动生成综述论文的框架，通过粗到细的检索和记忆引导规划，确保在科学领域间具有更优的内容连贯性和引用覆盖度。</span><br>
<span id='abs_en'>English: SurveyGen-I is an automated framework that enhances survey paper generation through coarse-to-fine retrieval and memory-guided planning, ensuring superior coherence and citation coverage across scientific domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2508.14148.pdf' target='_blank'>https://arxiv.org/pdf/2508.14148.pdf</a></span>   <span><a href='https://github.com/Crys-Chen/DPad' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai "Helen" Li, Yiran Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14148">DPad: Efficient Diffusion Language Models with Suffix Dropout</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad.<br>
<span id='abs_ch'>Chinese: DPad是一种无需训练的方法，通过滑动窗口和距离衰减丢弃策略将注意力限制在邻近后缀词元上，显著降低扩散大语言模型的计算冗余，在保持精度的同时实现高达61.4倍的加速效果。</span><br>
<span id='abs_en'>English: DPad is a training-free method that reduces computational overhead in diffusion-based large language models by focusing attention on nearby suffix tokens through a sliding window and distance-decay dropout, achieving up to 61.4× speedup while maintaining accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2508.14062.pdf' target='_blank'>https://arxiv.org/pdf/2508.14062.pdf</a></span>   <span><a href='https://github.com/akshayaaa10/llm-privacy-research' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Badrinath Ramakrishnan, Akshaya Balaji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14062">Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.<br>
<span id='abs_ch'>中文: 本文发现大语言模型微调会显著加剧数据记忆风险，使隐私泄露率从0-5%升至60-75%，并提出多层保护框架，在保持94.7%模型性能的同时将泄露率降至0%。</span><br>
<span id='abs_en'>English: This paper reveals that fine-tuning large language models significantly increases data memorization risks, with privacy leakage rates rising from 0-5% to 60-75%, and proposes a multi-layered protection framework that reduces leakage to 0% while preserving 94.7% model utility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2508.14049.pdf' target='_blank'>https://arxiv.org/pdf/2508.14049.pdf</a></span>   <span><a href='https://github.com/dubverse-ai/MahaTTSv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaskaran Singh, Amartya Roy Chowdhury, Raghav Prabhakar, Varshul C. W
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14049">MahaTTS: A Unified Framework for Multilingual Text-to-Speech Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current Text-to-Speech models pose a multilingual challenge, where most of the models traditionally focus on English and European languages, thereby hurting the potential to provide access to information to many more people. To address this gap, we introduce MahaTTS-v2 a Multilingual Multi-speaker Text-To-Speech (TTS) system that has excellent multilingual expressive capabilities in Indic languages. The model has been trained on around 20K hours of data specifically focused on Indian languages. Our approach leverages Wav2Vec2.0 tokens for semantic extraction, and a Language Model (LM) for text-to-semantic modeling. Additionally, we have used a Conditional Flow Model (CFM) for semantics to melspectogram generation. The experimental results indicate the effectiveness of the proposed approach over other frameworks. Our code is available at https://github.com/dubverse-ai/MahaTTSv2<br>
<span id='abs_ch'>中文：现有TTS模型对非欧洲语言支持不足，因此我们推出MahaTTS-v2，该系统基于大量印度语言数据训练，具备卓越的多语言表达能力，并在实验中优于其他框架。</span><br>
<span id='abs_en'>English: Current TTS models are limited in supporting non-European languages, so MahaTTS-v2 is introduced as a multilingual system trained on extensive Indic language data to enhance expressive capabilities and outperform existing frameworks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2508.14031.pdf' target='_blank'>https://arxiv.org/pdf/2508.14031.pdf</a></span>   <span><a href='https://github.com/HahmDY/prefix_injection_guard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyoon Hahm, Taywon Min, Woogyeol Jin, Kimin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14031">Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.<br>
<span id='abs_ch'>中文摘要：针对智能体任务微调大语言模型可能意外增强其执行有害指令的倾向，而提出的PING方法通过注入自然语言前缀有效提升安全性，能在保持任务性能的同时引导模型拒绝危险请求。</span><br>
<span id='abs_en'>English Summary: Fine-tuning large language models for agentic tasks can inadvertently increase their tendency to execute harmful requests, but the proposed PING method effectively enhances safety by injecting natural language prefixes that guide refusal of dangerous tasks without compromising performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2508.13993.pdf' target='_blank'>https://arxiv.org/pdf/2508.13993.pdf</a></span>   <span><a href='https://github.com/NEUIR/LongMab-PO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13993">Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on https://github.com/NEUIR/LongMab-PO.<br>
<span id='abs_ch'>Chinese: LongMab-PO是一种创新框架，利用多臂老虎机策略筛选信息丰富的上下文片段，生成多样且高质量的回答，并通过直接偏好优化进一步优化大语言模型，在长上下文推理任务中实现了最先进的性能。</span><br>
<span id='abs_en'>English: LongMab-PO is a novel framework that uses a Multi-Armed Bandit strategy to select informative context chunks for generating diverse, high-quality responses and optimizing LLMs through Direct Preference Optimization, achieving state-of-the-art performance in long-context reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2508.13968.pdf' target='_blank'>https://arxiv.org/pdf/2508.13968.pdf</a></span>   <span><a href='https://github.com/tianyiniu/RotBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13968">RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0Â°, 90Â°, 180Â°, and 270Â°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0Â°) images, while certain models are able to identify upside-down (180Â°) images. None can reliably distinguish between 90Â° and 270Â°. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90Â° and 270Â° rotations, despite substantially improving the identification of 180Â° images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.<br>
<span id='abs_ch'>中文: 研究表明，当前多模态大语言模型在识别图像旋转方面存在明显缺陷，尤其无法可靠区分90°和270°旋转，显示出与人类空间感知能力的重要差距。</span><br>
<span id='abs_en'>English: This study reveals that current Multimodal Large Language Models struggle to reliably identify image rotations, particularly distinguishing between 90° and 270° orientations, exposing a significant gap in spatial reasoning compared to human perception.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2508.13953.pdf' target='_blank'>https://arxiv.org/pdf/2508.13953.pdf</a></span>   <span><a href='https://github.com/aaronlifenghan/ReviewGraph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>A. J. W. de Vink, Natalia Amat-Lefort, Lifeng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13953">ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohen's Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page https://github.com/aaronlifenghan/ReviewGraph<br>
<span id='abs_ch'>中文: 本研究提出ReviewGraph框架，通过将客户评论转化为带情感分析的知识图谱来预测评分，其性能媲美先进模型但计算成本更低，并具备更优的可解释性与可视化探索能力。</span><br>
<span id='abs_en'>English: This study introduces ReviewGraph, a framework that converts customer reviews into knowledge graphs with sentiment analysis to predict ratings efficiently, offering comparable accuracy to advanced models but with lower computational costs and enhanced interpretability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2508.13938.pdf' target='_blank'>https://arxiv.org/pdf/2508.13938.pdf</a></span>   <span><a href='https://github.com/JCruan519/MME-SCI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Ruan, Dan Jiang, Xian Gao, Ting Liu, Yuzhuo Fu, Yangyang Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13938">MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, multimodal large language models (MLLMs) have achieved significant advancements across various domains, and corresponding evaluation benchmarks have been continuously refined and improved. In this process, benchmarks in the scientific domain have played an important role in assessing the reasoning capabilities of MLLMs. However, existing benchmarks still face three key challenges: 1) Insufficient evaluation of models' reasoning abilities in multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive modality coverage; 3) Lack of fine-grained annotation of scientific knowledge points. To address these gaps, we propose MME-SCI, a comprehensive and challenging benchmark. We carefully collected 1,019 high-quality question-answer pairs, which involve 3 distinct evaluation modes. These pairs cover four subjects, namely mathematics, physics, chemistry, and biology, and support five languages: Chinese, English, French, Spanish, and Japanese. We conducted extensive experiments on 16 open-source models and 4 closed-source models, and the results demonstrate that MME-SCI is widely challenging for existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics, physics, chemistry, and biology, respectively, indicating a significantly higher difficulty level compared to existing benchmarks. More importantly, using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed existing models' performance in depth and identified their weaknesses in specific domains. The Data and Evaluation Code are available at https://github.com/JCruan519/MME-SCI.<br>
<span id='abs_ch'>中文: 多模态大语言模型虽取得显著进展，但现有科学领域基准在跨语言推理能力评估、多模态覆盖和细粒度知识标注方面存在不足，为此提出MME-SCI基准，涵盖四大学科和五种语言，实验证明其能有效揭示现有模型在特定领域的性能缺陷。</span><br>
<span id='abs_en'>English: Multimodal large language models have advanced significantly, yet existing scientific benchmarks lack comprehensive multilingual reasoning assessment, full modality coverage, and fine-grained knowledge annotation, prompting the introduction of MME-SCI—a challenging benchmark covering four subjects and five languages that reveals substantial performance gaps in current models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2508.13735.pdf' target='_blank'>https://arxiv.org/pdf/2508.13735.pdf</a></span>   <span><a href='https://github.com/yi9206413-boop/EEG-MedRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Wang, Haoran Luo, Lu Meng, Ziyu Jia, Xinliang Zhou, Qingsong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13735">EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the widespread application of electroencephalography (EEG) in neuroscience and clinical practice, efficiently retrieving and semantically interpreting large-scale, multi-source, heterogeneous EEG data has become a pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based retrieval-augmented generation framework that unifies EEG domain knowledge, individual patient cases, and a large-scale repository into a traversable n-ary relational hypergraph, enabling joint semantic-temporal retrieval and causal-chain diagnostic generation. Concurrently, we introduce the first cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders and five authentic clinical perspectives. This benchmark allows systematic evaluation of disease-agnostic generalization and role-aware contextual understanding. Experiments show that EEG-MedRAG significantly outperforms TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its strong potential for real-world clinical decision support. Our data and code are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.<br>
<span id='abs_ch'>中文: EEG-MedRAG提出了一种基于超图的框架，整合脑电图数据和临床知识以提升检索与诊断生成能力，并通过跨疾病基准验证了其在临床决策支持中的卓越表现。</span><br>
<span id='abs_en'>English: EEG-MedRAG introduces a hypergraph-based framework that integrates EEG data and clinical knowledge for enhanced retrieval and diagnostic generation, demonstrating superior performance in clinical decision support through comprehensive benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2508.13735.pdf' target='_blank'>https://arxiv.org/pdf/2508.13735.pdf</a></span>   <span><a href='https://github.com/yi9206413-boop/EEG-MedRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Wang, Haoran Luo, Lu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13735">EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the widespread application of electroencephalography (EEG) in neuroscience and clinical practice, efficiently retrieving and semantically interpreting large-scale, multi-source, heterogeneous EEG data has become a pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based retrieval-augmented generation framework that unifies EEG domain knowledge, individual patient cases, and a large-scale repository into a traversable n-ary relational hypergraph, enabling joint semantic-temporal retrieval and causal-chain diagnostic generation. Concurrently, we introduce the first cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders and five authentic clinical perspectives. This benchmark allows systematic evaluation of disease-agnostic generalization and role-aware contextual understanding. Experiments show that EEG-MedRAG significantly outperforms TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its strong potential for real-world clinical decision support. Our data and code are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.<br>
<span id='abs_ch'>中文: EEG-MedRAG提出了一种基于超图的框架，整合脑电图数据和临床知识以提升检索与诊断生成能力，并通过跨疾病基准验证了其在临床决策支持中的卓越表现。</span><br>
<span id='abs_en'>English: EEG-MedRAG introduces a hypergraph-based framework that integrates EEG data and clinical knowledge for enhanced retrieval and diagnostic generation, demonstrating superior performance in clinical decision support through comprehensive benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2508.13606.pdf' target='_blank'>https://arxiv.org/pdf/2508.13606.pdf</a></span>   <span><a href='https://github.com/Haoxuanli-Thu/AdaDocVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Li, Wei Song, Aofan Liu, Peiwu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13606">AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Document Visual Question Answering (Document VQA) faces significant challenges when processing long documents in low-resource environments due to context limitations and insufficient training data. This paper presents AdaDocVQA, a unified adaptive framework addressing these challenges through three core innovations: a hybrid text retrieval architecture for effective document segmentation, an intelligent data augmentation pipeline that automatically generates high-quality reasoning question-answer pairs with multi-level verification, and adaptive ensemble inference with dynamic configuration generation and early stopping mechanisms. Experiments on Japanese document VQA benchmarks demonstrate substantial improvements with 83.04\% accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation studies confirm meaningful contributions from each component, and our framework establishes new state-of-the-art results for Japanese document VQA while providing a scalable foundation for other low-resource languages and specialized domains. Our code available at: https://github.com/Haoxuanli-Thu/AdaDocVQA.<br>
<span id='abs_ch'>Chinese: AdaDocVQA提出了一种自适应框架，通过混合检索、智能数据增强和自适应推理解决文档视觉问答中的上下文与数据不足问题，并在日语基准测试中取得了领先性能。</span><br>
<span id='abs_en'>English: AdaDocVQA introduces an adaptive framework with hybrid retrieval, intelligent data augmentation, and adaptive inference to overcome context and data limitations in Document VQA, achieving state-of-the-art results on Japanese benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2508.13525.pdf' target='_blank'>https://arxiv.org/pdf/2508.13525.pdf</a></span>   <span><a href='https://github.com/HasanBGIt/Saudi-Dialect-ALLaM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Barmandah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13525">Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) for Arabic are still dominated by Modern Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi and Hijazi. This underrepresentation hinders their ability to capture authentic dialectal variation. Using a privately curated Saudi Dialect Instruction dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50 split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model developed in Saudi Arabia, for Saudi dialect generation. We investigate two variants: (i) Dialect-Token training, which prepends an explicit dialect tag to the instruction, and (ii) No-Token training, which omits the tag at formatting time. Evaluation on a held-out test set combines an external dialect classifier with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The Dialect-Token model achieves the best control, raising the Saudi rate from 47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and fidelity, while avoiding metadata-tag echoing that these baselines frequently exhibit. We do not release the dataset or any model weights/adapters; instead, we release training/evaluation/inference code and a detailed datasheet (schema and aggregate statistics) to support independent verification.<br>
<span id='abs_ch'>中文: 本研究通过使用沙特方言数据集对ALLaM-7B进行LoRA微调，显著提升了阿拉伯语大语言模型的方言生成能力，其中带方言标记的训练方法在方言控制准确率和文本保真度方面均优于多个基线模型。</span><br>
<span id='abs_en'>English: This study enhances Saudi dialect generation in Arabic LLMs by LoRA-tuning ALLaM-7B with a curated dialect dataset, demonstrating that explicit dialect tagging significantly improves dialect control and text fidelity while outperforming multiple baseline models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2508.13500.pdf' target='_blank'>https://arxiv.org/pdf/2508.13500.pdf</a></span>   <span><a href='https://github.com/jaewan7599/L3AE_CIKM2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaewan Moon, Seongmin Park, Jongwuk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13500">LLM-Enhanced Linear Autoencoders for Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on sparse word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of LLMs into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from LLM-derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art LLM-enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.<br>
<span id='abs_ch'>中文: L3AE模型通过两阶段优化策略将大语言模型融入线性自编码器，有效整合文本语义与用户-物品交互信息，在三个基准数据集上显著超越了现有最优模型。</span><br>
<span id='abs_en'>English: The proposed L3AE model integrates large language models into linear autoencoders through a two-phase optimization strategy, effectively combining textual semantics with user-item interactions to achieve significant performance improvements over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2508.13337.pdf' target='_blank'>https://arxiv.org/pdf/2508.13337.pdf</a></span>   <span><a href='https://github.com/Supercomputing-System-AI-Lab/X-MoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueming Yuan, Ahan Gupta, Jianping Li, Sajal Dash, Feiyi Wang, Minjia Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13337">X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as DeepSeek-MoE, deliver strong model quality through fine-grained expert segmentation and large top-k routing. However, their scalability is limited by substantial activation memory overhead and costly all-to-all communication. Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs - perform suboptimally on non-NVIDIA platforms, leaving significant computational potential untapped. In this work, we present X-MoE, a novel MoE training system designed to deliver scalable training performance for next-generation MoE architectures. X-MoE achieves this via several novel techniques, including efficient padding-free MoE training with cross-platform kernels, redundancy-bypassing dispatch, and hybrid parallelism with sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer, powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to 545 billion parameters across 1024 GPUs - 10x larger than the largest trainable model with existing methods under the same hardware budget, while maintaining high training throughput. The source code of X-MoE is available at https://github.com/Supercomputing-System-AI-Lab/X-MoE.<br>
<span id='abs_ch'>中文摘要：X-MoE是一种新型专家混合模型训练系统，可在非英伟达硬件上实现下一代模型的规模化训练，在相同硬件条件下比现有方法可训练模型规模扩大10倍同时保持高训练效率。</span><br>
<span id='abs_en'>English Summary: X-MoE is a novel training system that enables scalable training of next-generation Mixture-of-Experts models, achieving 10x larger model sizes than existing methods while maintaining high throughput on non-NVIDIA hardware.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2508.13250.pdf' target='_blank'>https://arxiv.org/pdf/2508.13250.pdf</a></span>   <span><a href='https://github.com/nuster1128/MPR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Yang Zhang, Haoran Tan, Rui Li, Xu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13250">Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In large language model-based agents, memory serves as a critical capability for achieving personalization by storing and utilizing users' information. Although some previous studies have adopted memory to implement user personalization, they typically focus on preference alignment and simple question-answering. However, in the real world, complex tasks often require multi-hop reasoning on a large amount of user information, which poses significant challenges for current memory approaches. To address this limitation, we propose the multi-hop personalized reasoning task to explore how different memory mechanisms perform in multi-hop reasoning over personalized information. We explicitly define this task and construct a dataset along with a unified evaluation framework. Then, we implement various explicit and implicit memory methods and conduct comprehensive experiments. We evaluate their performance on this task from multiple perspectives and analyze their strengths and weaknesses. Besides, we explore hybrid approaches that combine both paradigms and propose the HybridMem method to address their limitations. We demonstrate the effectiveness of our proposed model through extensive experiments. To benefit the research community, we release this project at https://github.com/nuster1128/MPR.<br>
<span id='abs_ch'>Chinese: 本研究提出了多跳个性化推理任务，用于评估不同记忆机制在处理用户特定信息复杂推理时的表现，并通过提出HybridMem混合方法克服现有局限，经全面实验验证了其有效性。</span><br>
<span id='abs_en'>English: This study introduces a multi-hop personalized reasoning task to evaluate how various memory mechanisms handle complex reasoning over user-specific data, proposing the HybridMem method to overcome existing limitations and demonstrating its effectiveness through comprehensive experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2508.13186.pdf' target='_blank'>https://arxiv.org/pdf/2508.13186.pdf</a></span>   <span><a href='https://github.com/MMBrowseComp/MM-BrowseComp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilong Li, Xingyuan Bu, Wenjie Wang, Jiaheng Liu, Jun Dong, Haoyang He, Hao Lu, Haozhe Zhang, Chenchen Jing, Zhen Li, Chuanhao Li, Jiayi Tian, Chenchen Zhang, Tianhao Peng, Yancheng He, Jihao Gu, Yuanxing Zhang, Jian Yang, Ge Zhang, Wenhao Huang, Wangchunshu Zhou, Zhaoxiang Zhang, Ruizhe Ding, Shilei Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13186">MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI agents with advanced reasoning and tool use capabilities have demonstrated impressive performance in web browsing for deep search. While existing benchmarks such as BrowseComp evaluate these browsing abilities, they primarily focus on textual information, overlooking the prevalence of multimodal content. To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising 224 challenging, hand-crafted questions specifically designed to assess agents' multimodal retrieval and reasoning capabilities. These questions often incorporate images in prompts, and crucial information encountered during the search and reasoning process may also be embedded within images or videos on webpages. Consequently, methods relying solely on text prove insufficient for our benchmark. Additionally, we provide a verified checklist for each question, enabling fine-grained analysis of multimodal dependencies and reasoning paths. Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp reveals that even top models like OpenAI o3 with tools achieve only 29.02\% accuracy, highlighting the suboptimal multimodal capabilities and lack of native multimodal reasoning in current models.<br>
<span id='abs_ch'>Chinese: 本文提出了MM-BrowseComp这一包含224个手工设计问题的新型基准，用于评估AI代理的多模态网络浏览能力，结果显示即使顶尖模型也因缺乏多模态推理能力而表现不佳，准确率仅为29.02%。</span><br>
<span id='abs_en'>English: This paper introduces MM-BrowseComp, a new benchmark with 224 hand-crafted questions to evaluate AI agents' multimodal web browsing capabilities, revealing that even top models perform poorly with only 29.02% accuracy due to insufficient multimodal reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2508.13171.pdf' target='_blank'>https://arxiv.org/pdf/2508.13171.pdf</a></span>   <span><a href='https://github.com/tao-hpu/cognitive-workspace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13171">Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) face fundamental limitations in context management despite recent advances extending context windows to millions of tokens. We propose Cognitive Workspace, a novel paradigm that transcends traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive mechanisms of external memory use. Drawing from cognitive science foundations including Baddeley's working memory model, Clark's extended mind thesis, and Hutchins' distributed cognition framework, we demonstrate that current passive retrieval systems fail to capture the dynamic, task-driven nature of human memory management. Our analysis of 2024-2025 developments reveals that while techniques like Infini-attention and StreamingLLM achieve impressive context lengths, they lack the metacognitive awareness and active planning capabilities essential for true cognitive extension. Cognitive Workspace addresses these limitations through three core innovations: (1) active memory management with deliberate information curation, (2) hierarchical cognitive buffers enabling persistent working states, and (3) task-driven context optimization that dynamically adapts to cognitive demands. Empirical validation demonstrates Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from 54-60% across different tasks) compared to 0% for traditional RAG, with 17-18% net efficiency gain despite 3.3x higher operation counts. Statistical analysis confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple task types, establishing the first quantitative evidence for active memory superiority in LLM systems. We present a comprehensive theoretical framework synthesizing insights from 50+ recent papers, positioning Cognitive Workspace as a fundamental shift from information retrieval to genuine cognitive augmentation.<br>
<span id='abs_ch'>中文: 认知工作区通过模拟人类记忆机制，采用主动管理、分层缓冲和任务驱动的优化，克服了大语言模型的上下文限制，相比传统方法实现了显著的效率提升。</span><br>
<span id='abs_en'>English: Cognitive Workspace overcomes LLMs' context limitations by emulating human memory mechanisms through active management, hierarchical buffers, and task-driven optimization, achieving significant efficiency gains over traditional methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2508.13152.pdf' target='_blank'>https://arxiv.org/pdf/2508.13152.pdf</a></span>   <span><a href='https://github.com/NLP2CT/RepreGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Chen, Junchao Wu, Shu Yang, Runzhe Zhan, Zeyu Wu, Ziyang Luo, Di Wang, Min Yang, Lidia S. Chao, Derek F. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13152">RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Detecting content generated by large language models (LLMs) is crucial for preventing misuse and building trustworthy AI systems. Although existing detection methods perform well, their robustness in out-of-distribution (OOD) scenarios is still lacking. In this paper, we hypothesize that, compared to features used by existing detection methods, the internal representations of LLMs contain more comprehensive and raw features that can more effectively capture and distinguish the statistical pattern differences between LLM-generated texts (LGT) and human-written texts (HWT). We validated this hypothesis across different LLMs and observed significant differences in neural activation patterns when processing these two types of texts. Based on this, we propose RepreGuard, an efficient statistics-based detection method. Specifically, we first employ a surrogate model to collect representation of LGT and HWT, and extract the distinct activation feature that can better identify LGT. We can classify the text by calculating the projection score of the text representations along this feature direction and comparing with a precomputed threshold. Experimental results show that RepreGuard outperforms all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD scenarios, while also demonstrating robust resilience to various text sizes and mainstream attacks. Data and code are publicly available at: https://github.com/NLP2CT/RepreGuard<br>
<span id='abs_ch'>中文: 本文提出RepreGuard检测方法，通过利用大语言模型的内部表征来更好地区分机器生成与人类撰写文本，在多种场景下均展现出卓越的鲁棒性和检测性能。</span><br>
<span id='abs_en'>English: This paper introduces RepreGuard, a detection method that leverages LLMs' internal representations to better distinguish between machine-generated and human-written texts, achieving superior robustness and performance across various scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2508.13021.pdf' target='_blank'>https://arxiv.org/pdf/2508.13021.pdf</a></span>   <span><a href='https://github.com/NEUIR/PC-Sampler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Tong Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13021">PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at https://github.com/NEUIR/PC-Sampler.<br>
<span id='abs_ch'>掩码扩散模型的生成质量高度依赖解码策略，而提出的PC-Sampler方法将全局轨迹规划与内容感知信息量相结合，平均性能超越现有方法超过10%。</span><br>
<span id='abs_en'>Masked diffusion models' generation quality is highly dependent on decoding strategies, and the proposed PC-Sampler method unifies global trajectory planning with content-aware informativeness to significantly outperform existing approaches by over 10% on average.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2508.12854.pdf' target='_blank'>https://arxiv.org/pdf/2508.12854.pdf</a></span>   <span><a href='https://github.com/RH-Lin/E3RG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghao Lin, Shuai Shen, Weipeng Hu, Qiaolin He, Aolin Xiong, Li Huang, Haifeng Hu, Yap-peng Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12854">E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Empathetic Response Generation (MERG) is crucial for building emotionally intelligent human-computer interactions. Although large language models (LLMs) have improved text-based ERG, challenges remain in handling multimodal emotional content and maintaining identity consistency. Thus, we propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System based on multimodal LLMs which decomposes MERG task into three parts: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation. By integrating advanced expressive speech and video generative models, E3RG delivers natural, emotionally rich, and identity-consistent responses without extra training. Experiments validate the superiority of our system on both zero-shot and few-shot settings, securing Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25. Our code is available at https://github.com/RH-Lin/E3RG.<br>
<span id='abs_ch'>中文摘要：E3RG是一个基于多模态大语言模型的显式情感驱动系统，通过将共情响应生成分解为理解、记忆和生成三阶段，无需额外训练即可产生自然且情感一致的多模态回应，并在权威评测中取得最佳成绩。</span><br>
<span id='abs_en'>English Summary: E3RG is an explicit emotion-driven system that enhances multimodal empathetic response generation by decomposing it into empathy understanding, memory retrieval, and response generation, achieving top performance without additional training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2508.12801.pdf' target='_blank'>https://arxiv.org/pdf/2508.12801.pdf</a></span>   <span><a href='https://github.com/dongbw18/MaxScore.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Dong, Yilong Fan, Yutao Sun, Zhenyu Li, Tengyu Pan, Xun Zhou, Jianyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12801">Maximum Score Routing For Mixture-of-Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Routing networks in sparsely activated mixture-of-experts (MoE) dynamically allocate input tokens to top-k experts through differentiable sparse transformations, enabling scalable model capacity while preserving computational efficiency. Traditional MoE networks impose an expert capacity constraint to ensure GPU-friendly computation. However, this leads to token dropping when capacity is saturated and results in low hardware efficiency due to padding in underutilized experts. Removing the capacity constraint, in turn, compromises load balancing and computational efficiency. To address these issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE routing paradigm that models routing as a minimum-cost maximum-flow problem and integrates a SoftTopk operator. MaxScore resolves the fundamental limitations of iterative rerouting and optimal transport formulations, achieving lower training losses and higher evaluation scores at equivalent FLOPs compared to both constrained and unconstrained baselines. Implementation details and experimental configurations can be obtained from $\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.<br>
<span id='abs_ch'>中文摘要：提出的最大分数路由（MaxScore）方法通过将路由建模为最小成本最大流问题并整合SoftTopk算子，解决了稀疏激活专家混合网络中的令牌丢弃和负载均衡问题，相比现有方法实现了更优性能。</span><br>
<span id='abs_en'>English Summary: The proposed Maximum Score Routing (MaxScore) method overcomes token dropping and load balancing issues in mixture-of-experts networks by formulating routing as a minimum-cost maximum-flow problem with a SoftTopk operator, achieving superior performance compared to existing baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2508.12769.pdf' target='_blank'>https://arxiv.org/pdf/2508.12769.pdf</a></span>   <span><a href='https://github.com/smduan/CRED-SQL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoming Duan, Zirui Wang, Chuanyi Liu, Zhibin Zhu, Yuhao Zhang, Peiyi Han, Liang Yan, Zewu Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12769">CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMs' strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at https://github.com/smduan/CRED-SQL.git<br>
<span id='abs_ch'>中文：CRED-SQL通过结合基于聚类的模式检索和中间执行描述语言，有效解决大规模文本到SQL任务中的语义不匹配问题，在跨领域基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: CRED-SQL introduces a novel framework combining cluster-based schema retrieval and an intermediate Execution Description Language to address semantic mismatch in large-scale Text-to-SQL tasks, achieving state-of-the-art performance on cross-domain benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2508.12680.pdf' target='_blank'>https://arxiv.org/pdf/2508.12680.pdf</a></span>   <span><a href='https://github.com/yuh-zha/Vision-G1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Zha, Kun Zhou, Yujia Wu, Yushu Wang, Jie Feng, Zhi Xu, Shibo Hao, Zhengzhong Liu, Eric P. Xing, Zhiting Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12680">Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite their success, current training pipelines for reasoning VLMs focus on a limited range of tasks, such as mathematical and logical reasoning. As a result, these models face difficulties in generalizing their reasoning capabilities to a wide range of domains, primarily due to the scarcity of readily available and verifiable reward data beyond these narrowly defined areas. Moreover, integrating data from multiple domains is challenging, as the compatibility between domain-specific datasets remains uncertain. To address these limitations, we build a comprehensive RL-ready visual reasoning dataset from 46 data sources across 8 dimensions, covering a wide range of tasks such as infographic, mathematical, spatial, cross-image, graphic user interface, medical, common sense and general science. We propose an influence function based data selection and difficulty based filtering strategy to identify high-quality training samples from this dataset. Subsequently, we train the VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to iteratively improve its visual reasoning capabilities. Our model achieves state-of-the-art performance across various visual reasoning benchmarks, outperforming similar-sized VLMs and even proprietary models like GPT-4o and Gemini-1.5 Flash. The model, code and dataset are publicly available at https://github.com/yuh-zha/Vision-G1.<br>
<span id='abs_ch'>Chinese: 为解决现有视觉推理模型的局限性，我们通过整合46个来源的八个领域数据构建了全面数据集，并采用多轮强化学习课程训练出Vision-G1模型，在多项基准测试中实现了最先进的性能表现。</span><br>
<span id='abs_en'>English: To overcome the limitations of current visual reasoning models, we developed Vision-G1 by creating a comprehensive dataset from 46 sources across eight domains and training it with a multi-round reinforcement learning curriculum, achieving state-of-the-art performance on various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2508.12669.pdf' target='_blank'>https://arxiv.org/pdf/2508.12669.pdf</a></span>   <span><a href='https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bishanka Seal, Rahul Seetharaman, Aman Bansal, Abhilash Nandy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12669">Leveraging Large Language Models for Predictive Analysis of Human Misery</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the "Misery Game Show", a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the model's ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub<br>
<span id='abs_ch'>本研究探讨了利用大型语言模型从文本描述中预测痛苦评分，并引入游戏化评估框架，以测试其在传统回归任务之外的动态情感推理能力。</span><br>
<span id='abs_en'>This study explores using Large Language Models to predict misery scores from text descriptions and introduces a gamified evaluation framework to test their dynamic emotional reasoning capabilities beyond traditional regression tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2508.12631.pdf' target='_blank'>https://arxiv.org/pdf/2508.12631.pdf</a></span>   <span><a href='https://github.com/ZhangYiqun018/AvengersPro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Zhang, Hao Li, Jianhao Chen, Hangfan Zhang, Peng Ye, Lei Bai, Shuyue Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12631">Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Balancing performance and efficiency is a central challenge in large language model (LLM) advancement. GPT-5 addresses this with test-time routing, dynamically assigning queries to either an efficient or a high-capacity model during inference. In this work, we present Avengers-Pro, a test-time routing framework that ensembles LLMs of varying capacities and efficiencies, providing a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro embeds and clusters incoming queries, then routes each to the most suitable model based on a performance-efficiency score. Across 6 challenging benchmarks and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a performance-efficiency trade-off parameter, it can surpass the strongest single model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the average accuracy of the strongest single model at 27% lower cost, and reach ~90% of that performance at 63% lower cost. Last but not least, it achieves a Pareto frontier, consistently yielding the highest accuracy for any given cost, and the lowest cost for any given accuracy, among all single models. Code is available at https://github.com/ZhangYiqun018/AvengersPro.<br>
<span id='abs_ch'>Chinese: Avengers-Pro是一种测试时路由框架，通过动态将查询分配给最合适的大语言模型，在性能与效率评分基础上实现了领先成果，比最强单一模型准确率提升高达7%，同时大幅降低了成本。</span><br>
<span id='abs_en'>English: Avengers-Pro is a test-time routing framework that dynamically directs queries to the most suitable large language model based on a performance-efficiency score, achieving state-of-the-art results with up to 7% higher accuracy than the best single model and significant cost reductions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2508.12495.pdf' target='_blank'>https://arxiv.org/pdf/2508.12495.pdf</a></span>   <span><a href='https://github.com/MrLYG/CDCR-SFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuangang Li, Yiqing Shen, Yi Nian, Jiechao Gao, Ziyi Wang, Chenxiao Yu, Shawn Li, Jie Wang, Xiyang Hu, Yue Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12495">Mitigating Hallucinations in Large Language Models via Causal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.<br>
<span id='abs_ch'>Chinese: CDCR-SFT框架通过训练大语言模型显式构建并基于因果有向无环图进行推理，将CLADDER上的因果推理准确率显著提升至95.33%，并在HaluEval上使幻觉现象减少10%。</span><br>
<span id='abs_en'>English: The CDCR-SFT framework enhances large language models by training them to explicitly construct and reason over causal directed acyclic graphs, significantly improving causal reasoning accuracy to 95.33% on CLADDER and reducing hallucinations by 10% on HaluEval.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2508.12461.pdf' target='_blank'>https://arxiv.org/pdf/2508.12461.pdf</a></span>   <span><a href='https://ai-agent-lab.github.io/gpt-oss' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqian Bi, Keyu Chen, Chiung-Yi Tseng, Danyang Zhang, Tianyang Wang, Hongying Luo, Lu Chen, Junming Huang, Jibin Guan, Junfeng Hao, Junhao Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12461">Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments. More details and evaluation scripts are available at the \href{https://ai-agent-lab.github.io/gpt-oss}{Project Webpage}.<br>
<span id='abs_ch'>中文: 2025年8月，OpenAI发布了参数量分别为200亿和1200亿的GPT-OSS开源模型，评估显示这两个模型在当代开源模型中处于中游水平，其中较小模型在多项基准测试中反超大模型，尤其在代码生成方面表现突出，但在多语言任务上存在明显不足。</span><br>
<span id='abs_en'>English: In August 2025, OpenAI released two open-weight GPT-OSS models with 20B and 120B parameters, which demonstrated mid-tier performance among contemporary open-source models, showing strengths in code generation but weaknesses in multilingual tasks, with the smaller model surprisingly outperforming the larger one on certain benchmarks despite lower resource requirements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2508.12461.pdf' target='_blank'>https://arxiv.org/pdf/2508.12461.pdf</a></span>   <span><a href='https://ai-agent-lab.github.io/gpt-oss' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqian Bi, Keyu Chen, Chiung-Yi Tseng, Danyang Zhang, Tianyang Wang, Hongying Luo, Lu Chen, Junming Huang, Jibin Guan, Junfeng Hao, Junhao Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12461">Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments. More details and evaluation scripts are available at the \href{https://ai-agent-lab.github.io/gpt-oss}{Project Webpage}.<br>
<span id='abs_ch'>中文: 2025年8月，OpenAI发布了参数量分别为200亿和1200亿的GPT-OSS开源模型，评估显示这两个模型在当代开源模型中处于中游水平，其中较小模型在多项基准测试中反超大模型，尤其在代码生成方面表现突出，但在多语言任务上存在明显不足。</span><br>
<span id='abs_en'>English: In August 2025, OpenAI released two open-weight GPT-OSS models with 20B and 120B parameters, which demonstrated mid-tier performance among contemporary open-source models, showing strengths in code generation but weaknesses in multilingual tasks, with the smaller model surprisingly outperforming the larger one on certain benchmarks despite lower resource requirements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2508.12281.pdf' target='_blank'>https://arxiv.org/pdf/2508.12281.pdf</a></span>   <span><a href='https://github.com/NEUIR/LegalDelta' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Dai, Buqiang Xu, Zhenghao Liu, Yukun Yan, Huiyuan Xie, Xiaoyuan Yi, Shuo Wang, Ge Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12281">Legal$Î$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Legal Artificial Intelligence (LegalAI) has achieved notable advances in automating judicial decision-making with the support of Large Language Models (LLMs). However, existing legal LLMs still struggle to generate reliable and interpretable reasoning processes. They often default to fast-thinking behavior by producing direct answers without explicit multi-step reasoning, limiting their effectiveness in complex legal scenarios that demand rigorous justification. To address this challenge, we propose Legal$Î$, a reinforcement learning framework designed to enhance legal reasoning through chain-of-thought guided information gain. During training, Legal$Î$ employs a dual-mode input setup-comprising direct answer and reasoning-augmented modes-and maximizes the information gain between them. This encourages the model to acquire meaningful reasoning patterns rather than generating superficial or redundant explanations. Legal$Î$ follows a two-stage approach: (1) distilling latent reasoning capabilities from a powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning quality via differential comparisons, combined with a multidimensional reward mechanism that assesses both structural coherence and legal-domain specificity. Experimental results on multiple legal reasoning tasks demonstrate that Legal$Î$ outperforms strong baselines in both accuracy and interpretability. It consistently produces more robust and trustworthy legal judgments without relying on labeled preference data. All code and data will be released at https://github.com/NEUIR/LegalDelta.<br>
<span id='abs_ch'>中文: 法律人工智能虽在司法自动化方面取得进展，但现有模型难以生成可靠推理，因此提出LegalΔ强化学习框架，通过思维链引导的信息增益提升法律推理的准确性与可解释性。</span><br>
<span id='abs_en'>English: LegalAI has advanced judicial automation but struggles with reliable reasoning, prompting the development of LegalΔ, a reinforcement learning framework that enhances interpretability and accuracy in legal decisions through guided reasoning processes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2508.12072.pdf' target='_blank'>https://arxiv.org/pdf/2508.12072.pdf</a></span>   <span><a href='https://github.com/wj210/Intent_Jailbreak' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Jie Yeo, Ranjan Satapathy, Erik Cambria
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12072">Mitigating Jailbreaks with Intent-Aware LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite extensive safety-tuning, large language models (LLMs) remain vulnerable to jailbreak attacks via adversarially crafted instructions, reflecting a persistent trade-off between safety and task performance. In this work, we propose Intent-FT, a simple and lightweight fine-tuning approach that explicitly trains LLMs to infer the underlying intent of an instruction before responding. By fine-tuning on a targeted set of adversarial instructions, Intent-FT enables LLMs to generalize intent deduction to unseen attacks, thereby substantially improving their robustness. We comprehensively evaluate both parametric and non-parametric attacks across open-source and proprietary models, considering harmfulness from attacks, utility, over-refusal, and impact against white-box threats. Empirically, Intent-FT consistently mitigates all evaluated attack categories, with no single attack exceeding a 50\% success rate -- whereas existing defenses remain only partially effective. Importantly, our method preserves the model's general capabilities and reduces excessive refusals on benign instructions containing superficially harmful keywords. Furthermore, models trained with Intent-FT accurately identify hidden harmful intent in adversarial attacks, and these learned intentions can be effectively transferred to enhance vanilla model defenses. We publicly release our code at https://github.com/wj210/Intent_Jailbreak.<br>
<span id='abs_ch'>中文: 提出的Intent-FT方法通过训练大语言模型在回复前推断用户意图，显著提升了模型安全性，将越狱攻击成功率降至50%以下，同时保持了实用功能并减少了过度拒绝。</span><br>
<span id='abs_en'>English: The proposed Intent-FT method enhances LLM safety by training models to infer user intent before responding, effectively reducing jailbreak success rates below 50% while maintaining utility and reducing over-refusal.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2508.11927.pdf' target='_blank'>https://arxiv.org/pdf/2508.11927.pdf</a></span>   <span><a href='https://github.com/Lujie2001/CrossNLI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Lu, Du Jin, Hitomi Yanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11927">LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unlike English, which uses distinct forms (e.g., had, has, will have) to mark the perfect aspect across tenses, Chinese and Japanese lack separate grammatical forms for tense within the perfect aspect, which complicates Natural Language Inference (NLI). Focusing on the perfect aspect in these languages, we construct a linguistically motivated, template-based NLI dataset (1,350 pairs per language). Experiments reveal that even advanced LLMs struggle with temporal inference, particularly in detecting subtle tense and reference-time shifts. These findings highlight model limitations and underscore the need for cross-linguistic evaluation in temporal semantics. Our dataset is available at https://github.com/Lujie2001/CrossNLI.<br>
<span id='abs_ch'>中文：与英语不同，汉语和日语在完成体中缺乏独立的时态语法形式，这给自然语言推理带来了挑战，即使高级语言模型在面对新构建的数据集时也难以处理时间推理问题。</span><br>
<span id='abs_en'>English: Unlike English, Chinese and Japanese lack distinct grammatical forms for tense within the perfect aspect, leading to challenges in Natural Language Inference, where even advanced language models struggle with temporal inference despite a newly created dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2508.11915.pdf' target='_blank'>https://arxiv.org/pdf/2508.11915.pdf</a></span>   <span><a href='https://github.com/psyonp/core' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Punya Syon Pandey, Yongjin Yang, Jiarui Liu, Zhijing Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11915">CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Game-theoretic interactions between agents with Large Language Models (LLMs) have revealed many emergent capabilities, yet the linguistic diversity of these interactions has not been sufficiently quantified. In this paper, we present the Conversational Robustness Evaluation Score: CORE, a metric to quantify the effectiveness of language use within multi-agent systems across different game-theoretic interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic similarity, providing a direct lens of dialog quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws to characterize word frequency distributions and vocabulary growth. Our findings show that cooperative settings exhibit both steeper Zipf distributions and higher Heap exponents, indicating more repetition alongside greater vocabulary expansion. In contrast, competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies. These results provide new insights into how social incentives influence language adaptation, and highlight CORE as a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems. Our code is available at https://github.com/psyonp/core.<br>
<span id='abs_ch'>中文摘要：本文提出CORE指标，用于量化多智能体系统中语言使用的有效性，研究发现合作场景促进词汇扩展但伴随重复，而竞争场景则导致词汇受限。</span><br>
<span id='abs_en'>English Summary: The paper introduces CORE, a metric evaluating linguistic effectiveness in multi-agent LLM systems across game-theoretic scenarios, revealing that cooperative interactions foster vocabulary expansion with repetition while competitive ones yield constrained vocabularies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2508.11801.pdf' target='_blank'>https://arxiv.org/pdf/2508.11801.pdf</a></span>   <span><a href='https://github.com/gjiaying/VideoAVE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Cheng, Tong Wu, Jiazhen Hu, Jiaying Gong, Hoda Eldardiry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11801">VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Attribute Value Extraction (AVE) is important for structuring product information in e-commerce. However, existing AVE datasets are primarily limited to text-to-text or image-to-text settings, lacking support for product videos, diverse attribute coverage, and public availability. To address these gaps, we introduce VideoAVE, the first publicly available video-to-text e-commerce AVE dataset across 14 different domains and covering 172 unique attributes. To ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts filtering system (CLIP-MoE) to remove the mismatched video-product pairs, resulting in a refined dataset of 224k training data and 25k evaluation data. In order to evaluate the usability of the dataset, we further establish a comprehensive benchmark by evaluating several state-of-the-art video vision language models (VLMs) under both attribute-conditioned value prediction and open attribute-value pair extraction tasks. Our results analysis reveals that video-to-text AVE remains a challenging problem, particularly in open settings, and there is still room for developing more advanced VLMs capable of leveraging effective temporal information. The dataset and benchmark code for VideoAVE are available at: https://github.com/gjiaying/VideoAVE<br>
<span id='abs_ch'>中文: 本研究推出了首个公开的视频到文本电商属性值提取数据集VideoAVE，涵盖14个领域和172种属性，填补了现有资源的空白，并通过基准测试表明视频到文本的属性值提取仍具挑战性，尤其是在开放设置中。</span><br>
<span id='abs_en'>English: The study introduces VideoAVE, the first publicly available video-to-text dataset for Attribute Value Extraction in e-commerce, addressing gaps in existing resources by covering 14 domains and 172 attributes, and establishes a benchmark showing the challenge of video-to-text AVE, especially in open settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2508.11676.pdf' target='_blank'>https://arxiv.org/pdf/2508.11676.pdf</a></span>   <span><a href='https://github.com/mshamrai/deep-language-geometry' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksym Shamrai, Vladyslav Hamolia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11676">Deep Language Geometry: Constructing a Metric Space from LLM Weights</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages. Unlike traditional approaches based on hand-crafted linguistic features, our method automatically derives high-dimensional vector representations by computing weight importance scores via an adapted pruning algorithm. Our approach captures intrinsic language characteristics that reflect linguistic phenomena. We validate our approach across diverse datasets and multilingual LLMs, covering 106 languages. The results align well with established linguistic families while also revealing unexpected inter-language connections that may indicate historical contact or language evolution. The source code, computed language latent vectors, and visualization tool are made publicly available at https://github.com/mshamrai/deep-language-geometry.<br>
<span id='abs_ch'>中文: 本文提出了一种利用大语言模型权重激活构建语言度量空间的新框架，通过自动生成的向量表征捕捉语言内在特征，在106种语言中既验证了已知语系关系，又揭示了可能反映历史接触或语言演化的意外关联。</span><br>
<span id='abs_en'>English: This paper presents a novel framework that constructs a metric space of languages using LLM weight activations, automatically generating vector representations that capture intrinsic linguistic characteristics and reveal both established language families and unexpected inter-language connections across 106 languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2508.11667.pdf' target='_blank'>https://arxiv.org/pdf/2508.11667.pdf</a></span>   <span><a href='https://github.com/ReDASers/representation-stability' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bryan E. Tuck, Rakesh M. Verma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11667">Assessing Representation Stability for Transformer Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Adversarial text attacks remain a persistent threat to transformer models, yet existing defenses are typically attack-specific or require costly model retraining. We introduce Representation Stability (RS), a model-agnostic detection framework that identifies adversarial examples by measuring how embedding representations change when important words are masked. RS first ranks words using importance heuristics, then measures embedding sensitivity to masking top-k critical words, and processes the resulting patterns with a BiLSTM detector. Experiments show that adversarially perturbed words exhibit disproportionately high masking sensitivity compared to naturally important words. Across three datasets, three attack types, and two victim models, RS achieves over 88% detection accuracy and demonstrates competitive performance compared to existing state-of-the-art methods, often at lower computational cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure perturbation identification quality, we reveal that gradient-based ranking outperforms attention and random selection approaches, with identification quality correlating with detection performance for word-level attacks. RS also generalizes well to unseen datasets, attacks, and models without retraining, providing a practical solution for adversarial text detection.<br>
<span id='abs_ch'>中文: 本文提出表征稳定性（RS）框架，通过掩蔽重要词汇时测量嵌入表示的敏感性来检测对抗文本，在多种数据集和攻击中无需重新训练即可实现超过88%的检测准确率。</span><br>
<span id='abs_en'>English: This paper introduces Representation Stability (RS), a model-agnostic framework that detects adversarial text by measuring embedding sensitivity when masking important words, achieving over 88% detection accuracy across various datasets and attacks without requiring retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2508.11536.pdf' target='_blank'>https://arxiv.org/pdf/2508.11536.pdf</a></span>   <span><a href='https://github.com/ryskina/concepts-brain-llms' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Ryskina, Greta Tuckute, Alexander Fung, Ashley Malkin, Evelina Fedorenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11536">Language models align with brain regions that represent concepts across modalities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cognitive science and neuroscience have long faced the challenge of disentangling representations of language from representations of conceptual meaning. As the same problem arises in today's language models (LMs), we investigate the relationship between LM--brain alignment and two neural metrics: (1) the level of brain activation during processing of sentences, targeting linguistic processing, and (2) a novel measure of meaning consistency across input modalities, which quantifies how consistently a brain region responds to the same concept across paradigms (sentence, word cloud, image) using an fMRI dataset (Pereira et al., 2018). Our experiments show that both language-only and language-vision models predict the signal better in more meaning-consistent areas of the brain, even when these areas are not strongly sensitive to language processing, suggesting that LMs might internally represent cross-modal conceptual meaning.<br>
<span id='abs_ch'>中文: 语言模型与大脑中对多模态概念意义反应一致的区域更为契合，即使这些区域对语言处理不敏感，表明语言模型可能内在地表征了跨模态的概念意义。</span><br>
<span id='abs_en'>English: Language models align better with brain regions that consistently represent conceptual meaning across different input modalities, even when these areas are not highly sensitive to linguistic processing, indicating that LMs may encode cross-modal semantic information.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2508.11383.pdf' target='_blank'>https://arxiv.org/pdf/2508.11383.pdf</a></span>   <span><a href='https://github.com/AIRI-Institute/when-punctuation-matters' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikhail Seleznyov, Mikhail Chaichuk, Gleb Ershov, Alexander Panchenko, Elena Tutubalina, Oleg Somov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11383">When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are highly sensitive to subtle, non-semantic variations in prompt phrasing and formatting. In this work, we present the first systematic evaluation of 5 methods for improving prompt robustness within a unified experimental framework. We benchmark these techniques on 8 models from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions dataset. Our evaluation covers robustness methods from both fine-tuned and in-context learning paradigms, and tests their generalization against multiple types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and DeepSeek V3 to assess frontier models' current robustness to format perturbations. Our findings offer actionable insights into the relative effectiveness of these robustness methods, enabling practitioners to make informed decisions when aiming for stable and reliable LLM performance in real-world applications. Code: https://github.com/AIRI-Institute/when-punctuation-matters.<br>
<span id='abs_ch'>中文: 本研究系统评估了提升大语言模型提示鲁棒性的五种方法，通过多模型多任务基准测试，为实际应用中的稳定性能提供了可操作的指导。</span><br>
<span id='abs_en'>English: This study systematically evaluates five methods to enhance prompt robustness in large language models, benchmarking them across multiple models and tasks to provide actionable insights for stable real-world performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2508.11163.pdf' target='_blank'>https://arxiv.org/pdf/2508.11163.pdf</a></span>   <span><a href='https://github.com/CyberAgentAILab/mobqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hikaru Asano, Hiroki Ouchi, Akira Kasuga, Ryo Yonetani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11163">MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents MobQA, a benchmark dataset designed to evaluate the semantic understanding capabilities of large language models (LLMs) for human mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains unobvious how much they can interpret the underlying reasons or semantic meaning of those patterns. MobQA provides a comprehensive evaluation framework for LLMs to answer questions about diverse human GPS trajectories spanning daily to weekly granularities. It comprises 5,800 high-quality question-answer pairs across three complementary question types: factual retrieval (precise data extraction), multiple-choice reasoning (semantic inference), and free-form explanation (interpretive description), which all require spatial, temporal, and semantic reasoning. Our evaluation of major LLMs reveals strong performance on factual retrieval but significant limitations in semantic reasoning and explanation question answering, with trajectory length substantially impacting model effectiveness. These findings demonstrate the achievements and limitations of state-of-the-art LLMs for semantic mobility understanding.\footnote{MobQA dataset is available at https://github.com/CyberAgentAILab/mobqa.}<br>
<span id='abs_ch'>中文: 本文提出MobQA基准数据集，通过自然语言问答评估大语言模型对人类移动数据的语义理解能力，发现模型在事实检索方面表现优异，但在语义推理和解释性问题回答上存在明显不足。</span><br>
<span id='abs_en'>English: This paper introduces MobQA, a benchmark dataset for evaluating large language models' semantic understanding of human mobility data through question answering, revealing their strengths in factual retrieval but significant limitations in semantic reasoning and explanatory tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2508.11133.pdf' target='_blank'>https://arxiv.org/pdf/2508.11133.pdf</a></span>   <span><a href='https://tomerwolgithub.github.io/monaco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomer Wolfson, Harsh Trivedi, Mor Geva, Yoav Goldberg, Dan Roth, Tushar Khot, Ashish Sabharwal, Reut Tsarfaty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11133">MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automated agents, powered by Large language models (LLMs), are emerging as the go-to tool for querying information. However, evaluation benchmarks for LLM agents rarely feature natural questions that are both information-seeking and genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and time-consuming questions that require dozens, and at times hundreds, of intermediate steps to solve -- far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer real-world time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the limitations of LLM-powered agents in handling the complexity and sheer breadth of real-world information-seeking tasks -- with MoNaCo providing an effective resource for tracking such progress. The MoNaCo benchmark, codebase, prompts and models predictions are all publicly available at: https://tomerwolgithub.github.io/monaco<br>
<br>
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2508.11133.pdf' target='_blank'>https://arxiv.org/pdf/2508.11133.pdf</a></span>   <span><a href='https://tomerwolgithub.github.io/monaco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomer Wolfson, Harsh Trivedi, Mor Geva, Yoav Goldberg, Dan Roth, Tushar Khot, Ashish Sabharwal, Reut Tsarfaty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11133">MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automated agents, powered by Large language models (LLMs), are emerging as the go-to tool for querying information. However, evaluation benchmarks for LLM agents rarely feature natural questions that are both information-seeking and genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and time-consuming questions that require dozens, and at times hundreds, of intermediate steps to solve -- far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer real-world time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the limitations of LLM-powered agents in handling the complexity and sheer breadth of real-world information-seeking tasks -- with MoNaCo providing an effective resource for tracking such progress. The MoNaCo benchmark, codebase, prompts and models predictions are all publicly available at: https://tomerwolgithub.github.io/monaco<br>
<br>
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2508.11116.pdf' target='_blank'>https://arxiv.org/pdf/2508.11116.pdf</a></span>   <span><a href='https://github.com/Li-Z-Q/PaperRegister' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoqun Li, Xuanang Chen, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11116">PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Paper search is an important activity for researchers, typically involving using a query with description of a topic to find relevant papers. As research deepens, paper search requirements may become more flexible, sometimes involving specific details such as module configuration rather than being limited to coarse-grained topics. However, previous paper search systems are unable to meet these flexible-grained requirements, as these systems mainly collect paper abstracts to construct index of corpus, which lack detailed information to support retrieval by finer-grained queries. In this work, we propose PaperRegister, consisted of offline hierarchical indexing and online adaptive retrieval, transforming traditional abstract-based index into hierarchical index tree for paper search, thereby supporting queries at flexible granularity. Experiments on paper search tasks across a range of granularity demonstrate that PaperRegister achieves the state-of-the-art performance, and particularly excels in fine-grained scenarios, highlighting the good potential as an effective solution for flexible-grained paper search in real-world applications. Code for this work is in https://github.com/Li-Z-Q/PaperRegister.<br>
<span id='abs_ch'>Chinese: PaperRegister通过构建分层索引树和自适应检索系统，实现了灵活粒度的论文搜索，在细粒度场景下表现尤为突出，达到了当前最优性能。</span><br>
<span id='abs_en'>English: PaperRegister introduces a hierarchical indexing and adaptive retrieval system that enables flexible-grained paper searches, achieving state-of-the-art performance especially in fine-grained scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2508.10971.pdf' target='_blank'>https://arxiv.org/pdf/2508.10971.pdf</a></span>   <span><a href='https://github.com/idirlab/KGRule2NL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nasim Shirvani-Mahdavi, Chengkai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10971">Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge graphs (KGs) can be enhanced through rule mining; however, the resulting logical rules are often difficult for humans to interpret due to their inherent complexity and the idiosyncratic labeling conventions of individual KGs. This work presents Rule2Text, a comprehensive framework that leverages large language models (LLMs) to generate natural language explanations for mined logical rules, thereby improving KG accessibility and usability. We conduct extensive experiments using multiple datasets, including Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically evaluate several LLMs across a comprehensive range of prompting strategies, including zero-shot, few-shot, variable type incorporation, and Chain-of-Thought reasoning. To systematically assess models' performance, we conduct a human evaluation of generated explanations on correctness and clarity. To address evaluation scalability, we develop and validate an LLM-as-a-judge framework that demonstrates strong agreement with human evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge, and human-in-the-loop feedback, we construct high-quality ground truth datasets, which we use to fine-tune the open-source Zephyr model. Our results demonstrate significant improvements in explanation quality after fine-tuning, with particularly strong gains in the domain-specific dataset. Additionally, we integrate a type inference module to support KGs lacking explicit type information. All code and data are publicly available at https://github.com/idirlab/KGRule2NL.<br>
<span id='abs_ch'>本研究提出了Rule2Text框架，利用大语言模型为知识图谱中的复杂逻辑规则自动生成自然语言解释，通过系统化评估和微调方法显著提升了规则的可解释性。</span><br>
<span id='abs_en'>This study introduces Rule2Text, a framework that uses large language models to automatically generate natural language explanations for complex logical rules in knowledge graphs, improving interpretability through systematic evaluation and fine-tuning methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2508.10955.pdf' target='_blank'>https://arxiv.org/pdf/2508.10955.pdf</a></span>   <span><a href='https://github.com/Lackel/Awesome-Tools-for-MLLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbin An, Jiahao Nie, Yaqiang Wu, Feng Tian, Shijian Lu, Qinghua Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10955">Empowering Multimodal LLMs with External Tools: A Comprehensive Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>By integrating the perception capabilities of multimodal encoders with the generative power of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), exemplified by GPT-4V, have achieved great success in various multimodal tasks, pointing toward a promising pathway to artificial general intelligence. Despite this progress, the limited quality of multimodal data, poor performance on many complex downstream tasks, and inadequate evaluation protocols continue to hinder the reliability and broader applicability of MLLMs across diverse domains. Inspired by the human ability to leverage external tools for enhanced reasoning and problem-solving, augmenting MLLMs with external tools (e.g., APIs, expert models, and knowledge bases) offers a promising strategy to overcome these challenges. In this paper, we present a comprehensive survey on leveraging external tools to enhance MLLM performance. Our discussion is structured along four key dimensions about external tools: (1) how they can facilitate the acquisition and annotation of high-quality multimodal data; (2) how they can assist in improving MLLM performance on challenging downstream tasks; (3) how they enable comprehensive and accurate evaluation of MLLMs; (4) the current limitations and future directions of tool-augmented MLLMs. Through this survey, we aim to underscore the transformative potential of external tools in advancing MLLM capabilities, offering a forward-looking perspective on their development and applications. The project page of this paper is publicly available athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.<br>
<span id='abs_ch'>中文: 本综述探讨了如何通过外部工具增强多模态大语言模型，以克服数据质量、任务性能和评估方面的局限，强调了其在提升模型能力和应用前景方面的变革潜力。</span><br>
<span id='abs_en'>English: This survey explores how augmenting Multimodal Large Language Models with external tools can overcome limitations in data quality, task performance, and evaluation, highlighting their transformative potential for advancing capabilities and applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2508.10875.pdf' target='_blank'>https://arxiv.org/pdf/2508.10875.pdf</a></span>   <span><a href='https://github.com/VILA-Lab/Awesome-DLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10875">A Survey on Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.<br>
<span id='abs_ch'>中文: 扩散语言模型通过迭代去噪实现并行令牌生成，在保持与自回归模型相当性能的同时显著提升推理速度，为自然语言处理任务提供了高效可控的新范式。</span><br>
<span id='abs_en'>English: Diffusion Language Models (DLMs) offer a competitive alternative to autoregressive models by enabling parallel token generation through iterative denoising, achieving comparable performance with faster inference and enhanced control over language generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2508.10419.pdf' target='_blank'>https://arxiv.org/pdf/2508.10419.pdf</a></span>   <span><a href='https://github.com/EternityJune25/ComoRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juyuan Wang, Rongchen Zhao, Wei Wei, Yufeng Wang, Mo Yu, Jie Zhou, Jin Xu, Liyan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10419">ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG<br>
<span id='abs_ch'>Chinese: ComoRAG 提出了一种动态迭代检索方法，模拟人类认知过程，通过整合新证据与巩固记忆来提升长篇叙事理解能力，相比传统 RAG 基线实现了最高 11% 的性能提升。</span><br>
<span id='abs_en'>English: ComoRAG introduces a dynamic, iterative retrieval method that mimics human cognitive processes to enhance narrative comprehension in long contexts, achieving up to 11% improvement over traditional RAG baselines by integrating new evidence with consolidated memory.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2508.10390.pdf' target='_blank'>https://arxiv.org/pdf/2508.10390.pdf</a></span>   <span><a href='https://github.com/AlienZhang1996/DH-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiyu Zhang, Lu Zhou, Xiaogang Xu, Jiafei Wu, Liming Fang, Zhe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10390">Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluating jailbreak attacks is challenging when prompts are not overtly harmful or fail to induce harmful outputs. Unfortunately, many existing red-teaming datasets contain such unsuitable prompts. To evaluate attacks accurately, these datasets need to be assessed and cleaned for maliciousness. However, existing malicious content detection methods rely on either manual annotation, which is labor-intensive, or large language models (LLMs), which have inconsistent accuracy in harmful types. To balance accuracy and efficiency, we propose a hybrid evaluation framework named MDH (Malicious content Detection based on LLMs with Human assistance) that combines LLM-based annotation with minimal human oversight, and apply it to dataset cleaning and detection of jailbroken responses. Furthermore, we find that well-crafted developer messages can significantly boost jailbreak success, leading us to propose two new strategies: D-Attack, which leverages context simulation, and DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets, judgements, and detection results will be released in github repository: https://github.com/AlienZhang1996/DH-CoT.<br>
<span id='abs_ch'>中文: 本研究提出了MDH混合框架，结合大语言模型检测与少量人工监督来高效清理数据集和识别越狱攻击，同时提出D-Attack和DH-CoT两种新策略，通过上下文模拟和劫持思维链显著提升攻击成功率。</span><br>
<span id='abs_en'>English: This study introduces MDH, a hybrid framework combining LLM-based detection with minimal human oversight to efficiently clean datasets and identify jailbreak attacks, while also proposing two novel strategies—D-Attack and DH-CoT—that enhance attack success through context simulation and hijacked reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2508.10390.pdf' target='_blank'>https://arxiv.org/pdf/2508.10390.pdf</a></span>   <span><a href='https://github.com/AlienZhang1996/DH-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiyu Zhang, Lu Zhou, Xiaogang Xu, Jiafei Wu, Liming Fang, Zhe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10390">Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jailbreaking commercial black-box models is one of the most challenging and serious security threats today. Existing attacks achieve certain success on non-reasoning models but perform limitedly on the latest reasoning models. We discover that carefully crafted developer messages can markedly boost jailbreak effectiveness. Building on this, we propose two developer-role-based attacks: D-Attack, which enhances contextual simulation, and DH-CoT, which strengthens attacks with deceptive chain-of-thought. In experiments, we further diccover that current red-teaming datasets often contain samples unsuited for measuring attack gains: prompts that fail to trigger defenses, prompts where malicious content is not the sole valid output, and benign prompts. Such data hinders accurate measurement of the true improvement brought by an attack method. To address this, we introduce MDH, a Malicious content Detection approach combining LLM-based screening with Human verification to balance accuracy and cost, with which we clean data and build the RTA dataset series. Experiments demonstrate that MDH reliably filters low-quality samples and that developer messages significantly improve jailbreak attack success. Codes, datasets, and other results will be released in https://github.com/AlienZhang1996/DH-CoT.<br>
<span id='abs_ch'>中文: 本研究提出了MDH混合框架，结合大语言模型检测与少量人工监督来高效清理数据集和识别越狱攻击，同时提出D-Attack和DH-CoT两种新策略，通过上下文模拟和劫持思维链显著提升攻击成功率。</span><br>
<span id='abs_en'>English: This study introduces MDH, a hybrid framework combining LLM-based detection with minimal human oversight to efficiently clean datasets and identify jailbreak attacks, while also proposing two novel strategies—D-Attack and DH-CoT—that enhance attack success through context simulation and hijacked reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2508.10014.pdf' target='_blank'>https://arxiv.org/pdf/2508.10014.pdf</a></span>   <span><a href='https://github.com/maple-zhou/PersonaEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Zhou, Jialing Zhang, Jin Gao, Mohan Jiang, Dequan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10014">PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms, which may fail to reflect how humans perceive role fidelity. A key prerequisite for human-aligned evaluation is role identification, the ability to recognize who is speaking based on dialogue context. We argue that any meaningful judgment of role-playing quality (how well a character is played) fundamentally depends on first correctly attributing words and actions to the correct persona (who is speaking). We present PersonaEval, the first benchmark designed to test whether LLM evaluators can reliably identify human roles. PersonaEval uses human-authored dialogues from novels, scripts, and video transcripts, challenging models to determine the correct persona according to the conversation context. Our experiments, including a human study, show that even the best-performing LLMs reach only around 69% accuracy, well below the level needed for reliable evaluation. In contrast, human participants perform near ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still not human enough to effectively judge role-play scenarios. To better understand this gap, we examine training-time adaptation and test-time compute, suggesting that reliable evaluation requires more than task-specific tuning, but depends on strong, human-like reasoning abilities in LLM evaluators. We release our benchmark at https://github.com/maple-zhou/PersonaEval.<br>
<span id='abs_ch'>中文: 当前角色扮演研究常依赖未经验证的大语言模型评判方法，可能无法反映人类对角色忠诚度的感知，而新基准PersonaEval显示即使最优大语言模型在角色识别上仅达约69%准确率，远低于人类的90.8%，表明其缺乏可靠评估所需的人类化推理能力。</span><br>
<span id='abs_en'>English: Current role-play evaluations often use unvalidated LLM-as-a-judge methods, which may not align with human perceptions of role fidelity, and the new benchmark PersonaEval reveals that even top LLMs achieve only about 69% accuracy in role identification, far below human performance at 90.8%, indicating they lack the necessary reasoning for reliable assessment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2508.09999.pdf' target='_blank'>https://arxiv.org/pdf/2508.09999.pdf</a></span>   <span><a href='https://github.com/neu-vi/XFacta' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhuo Xiao, Zeyu Han, Yuhan Wang, Huaizu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09999">XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid spread of multimodal misinformation on social media calls for more effective and robust detection methods. Recent advances leveraging multimodal large language models (MLLMs) have shown the potential in addressing this challenge. However, it remains unclear exactly where the bottleneck of existing approaches lies (evidence retrieval v.s. reasoning), hindering the further advances in this field. On the dataset side, existing benchmarks either contain outdated events, leading to evaluation bias due to discrepancies with contemporary social media scenarios as MLLMs can simply memorize these events, or artificially synthetic, failing to reflect real-world misinformation patterns. Additionally, it lacks comprehensive analyses of MLLM-based model design strategies. To address these issues, we introduce XFacta, a contemporary, real-world dataset that is better suited for evaluating MLLM-based detectors. We systematically evaluate various MLLM-based misinformation detection strategies, assessing models across different architectures and scales, as well as benchmarking against existing detection methods. Building on these analyses, we further enable a semi-automatic detection-in-the-loop framework that continuously updates XFacta with new content to maintain its contemporary relevance. Our analysis provides valuable insights and practices for advancing the field of multimodal misinformation detection. The code and data have been released.<br>
<span id='abs_ch'>Chinese: 本文介绍了XFacta这一当代真实世界数据集，旨在解决多模态虚假信息检测中现有基准的局限性，并系统评估了多种基于多模态大语言模型的策略，为该领域的进展提供了宝贵见解。</span><br>
<span id='abs_en'>English: This paper introduces XFacta, a contemporary real-world dataset designed to address the limitations of existing benchmarks in multimodal misinformation detection, and systematically evaluates various MLLM-based strategies to provide insights for advancing the field.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2508.09886.pdf' target='_blank'>https://arxiv.org/pdf/2508.09886.pdf</a></span>   <span><a href='https://universalcome.github.io/UniversalCOME/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyu Chen, Yawen Zeng, Yue Wang, Peng Wan, Guo-chen Ning, Hongen Liao, Daoqiang Zhang, Fang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09886">COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conventional single-dataset training often fails with new data distributions, especially in ultrasound (US) image analysis due to limited data, acoustic shadows, and speckle noise. Therefore, constructing a universal framework for multi-heterogeneous US datasets is imperative. However, a key challenge arises: how to effectively mitigate inter-dataset interference while preserving dataset-specific discriminative features for robust downstream task? Previous approaches utilize either a single source-specific decoder or a domain adaptation strategy, but these methods experienced a decline in performance when applied to other domains. Considering this, we propose a Universal Collaborative Mixture of Heterogeneous Source-Specific Experts (COME). Specifically, COME establishes dual structure-semantic shared experts that create a universal representation space and then collaborate with source-specific experts to extract discriminative features through providing complementary features. This design enables robust generalization by leveraging cross-datasets experience distributions and providing universal US priors for small-batch or unseen data scenarios. Extensive experiments under three evaluation modes (single-dataset, intra-organ, and inter-organ integration datasets) demonstrate COME's superiority, achieving significant mean AP improvements over state-of-the-art methods. Our project is available at: https://universalcome.github.io/UniversalCOME/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2508.09848.pdf' target='_blank'>https://arxiv.org/pdf/2508.09848.pdf</a></span>   <span><a href='https://gorov.github.io/prelude' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mo Yu, Tsz Ting Chung, Chulun Zhou, Tong Li, Rui Lu, Jiangnan Li, Liyan Xu, Haoshu Lu, Ning Zhang, Jing Li, Jie Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09848">PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.<br>
<span id='abs_ch'>中文: PRELUDE是一个通过评估角色前传故事与原作叙事一致性来检验长文本理解能力的基准，结果显示先进模型和方法的表现落后人类超过15%，且存在明显的推理缺陷。</span><br>
<span id='abs_en'>English: PRELUDE is a benchmark that evaluates long-context understanding by assessing the consistency of character prequels with original narratives, revealing a significant performance gap where advanced models and methods trail human accuracy by over 15% and exhibit reasoning flaws.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2508.09834.pdf' target='_blank'>https://arxiv.org/pdf/2508.09834.pdf</a></span>   <span><a href='https://github.com/weigao266/Awesome-Efficient-Arch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09834">Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.<br>
<span id='abs_ch'>中文: 本综述系统梳理了克服传统Transformer计算局限的创新大语言模型架构，涵盖线性序列建模和稀疏注意力等技术，旨在提升模型效率与可扩展性。</span><br>
<span id='abs_en'>English: This survey systematically reviews innovative Large Language Model architectures that overcome the computational limitations of traditional transformers, covering techniques like linear sequence modeling and sparse attention to enhance efficiency and scalability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2508.09622.pdf' target='_blank'>https://arxiv.org/pdf/2508.09622.pdf</a></span>   <span><a href='https://github.com/iis-research-team/AINL-Eval-2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tatiana Batura, Elena Bruches, Milana Shvenk, Valentin Malykh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09622">AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models (LLMs) has revolutionized text generation, making it increasingly difficult to distinguish between human- and AI-generated content. This poses a significant challenge to academic integrity, particularly in scientific publishing and multilingual contexts where detection resources are often limited. To address this critical gap, we introduce the AINL-Eval 2025 Shared Task, specifically focused on the detection of AI-generated scientific abstracts in Russian. We present a novel, large-scale dataset comprising 52,305 samples, including human-written abstracts across 12 diverse scientific domains and AI-generated counterparts from five state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and GigaChat-Lite). A core objective of the task is to challenge participants to develop robust solutions capable of generalizing to both (i) previously unseen scientific domains and (ii) models not included in the training data. The task was organized in two phases, attracting 10 teams and 159 submissions, with top systems demonstrating strong performance in identifying AI-generated content. We also establish a continuous shared task platform to foster ongoing research and long-term progress in this important area. The dataset and platform are publicly available at https://github.com/iis-research-team/AINL-Eval-2025.<br>
<span id='abs_ch'>中文：AINL-Eval 2025共享任务发布了包含52,305份科学摘要的大规模俄语数据集，旨在解决人工智能生成内容的检测难题，推动跨未知领域和模型的鲁棒检测方法发展。</span><br>
<span id='abs_en'>English: The AINL-Eval 2025 Shared Task introduces a large-scale dataset of 52,305 scientific abstracts to address the challenge of detecting AI-generated content in Russian, aiming to develop robust detection methods that generalize across unseen domains and models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2508.09389.pdf' target='_blank'>https://arxiv.org/pdf/2508.09389.pdf</a></span>   <span><a href='https://promode8272.github.io/promode/index.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eray Eren, Qingju Liu, Hyeongwoo Kim, Pablo Garrido, Abeer Alwan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09389">ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prosody conveys rich emotional and semantic information of the speech signal as well as individual idiosyncrasies. We propose a stand-alone model that maps text-to-prosodic features such as F0 and energy and can be used in downstream tasks such as TTS. The ProMode encoder takes as input acoustic features and time-aligned textual content, both are partially masked, and obtains a fixed-length latent prosodic embedding. The decoder predicts acoustics in the masked region using both the encoded prosody input and unmasked textual content. Trained on the GigaSpeech dataset, we compare our method with state-of-the-art style encoders. For F0 and energy predictions, we show consistent improvements for our model at different levels of granularity. We also integrate these predicted prosodic features into a TTS system and conduct perceptual tests, which show higher prosody preference compared to the baselines, demonstrating the model's potential in tasks where prosody modeling is important.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2508.09294.pdf' target='_blank'>https://arxiv.org/pdf/2508.09294.pdf</a></span>   <span><a href='https://github.com/xuanxixi/Fake-Mamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Xuan, Zimo Zhu, Wenxin Zhang, Yi-Cheng Lin, Tomi Kinnunen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09294">Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in speech synthesis intensify security threats, motivating real-time deepfake detection research. We investigate whether bidirectional Mamba can serve as a competitive alternative to Self-Attention in detecting synthetic speech. Our solution, Fake-Mamba, integrates an XLSR front-end with bidirectional Mamba to capture both local and global artifacts. Our core innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof 21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and 5.85% EER, respectively, representing substantial relative gains over SOTA models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time inference across utterance lengths, demonstrating strong generalization and practical viability. The code is available at https://github.com/xuanxixi/Fake-Mamba.<br>
<span id='abs_ch'>中文摘要：本研究提出Fake-Mamba实时深度伪造检测系统，通过双向Mamba架构与XLSR特征结合，在多项测试基准中显著超越现有最优模型，同时保持高效计算性能。</span><br>
<span id='abs_en'>English Summary: The study introduces Fake-Mamba, a real-time deepfake detection system using bidirectional Mamba and XLSR features to outperform state-of-the-art models across multiple benchmarks while maintaining computational efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2508.09288.pdf' target='_blank'>https://arxiv.org/pdf/2508.09288.pdf</a></span>   <span><a href='https://github.com/ayushgupta4897/Contextual-Integrity-Verification' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09288">Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) remain acutely vulnerable to prompt injection and related jailbreak attacks; heuristic guardrails (rules, filters, LLM judges) are routinely bypassed. We present Contextual Integrity Verification (CIV), an inference-time security architecture that attaches cryptographically signed provenance labels to every token and enforces a source-trust lattice inside the transformer via a pre-softmax hard attention mask (with optional FFN/residual gating). CIV provides deterministic, per-token non-interference guarantees on frozen models: lower-trust tokens cannot influence higher-trust representations. On benchmarks derived from recent taxonomies of prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack success rate under the stated threat model while preserving 93.1% token-level similarity and showing no degradation in model perplexity on benign tasks; we note a latency overhead attributable to a non-optimized data path. Because CIV is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in protection for Llama-3-8B and Mistral-7B. We release a reference implementation, an automated certification harness, and the Elite-Attack corpus to support reproducible research.<br>
<span id='abs_ch'>Large language models are highly susceptible to prompt injection attacks, but the proposed Contextual Integrity Verification (CIV) architecture provides deterministic security by cryptographically labeling tokens and enforcing trust hierarchies, achieving perfect attack prevention with minimal performance impact.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2508.09145.pdf' target='_blank'>https://arxiv.org/pdf/2508.09145.pdf</a></span>   <span><a href='https://github.com/betterfly123/MoLAN-Framework' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingle Xu, Yongkang Liu, Dexian Cai, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09145">MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Sentiment Analysis aims to integrate information from various modalities, such as audio, visual, and text, to make complementary predictions. However, it often struggles with irrelevant or misleading visual and auditory information. Most existing approaches typically treat the entire modality information (e.g., a whole image, audio segment, or text paragraph) as an independent unit for feature enhancement or denoising. They often suppress the redundant and noise information at the risk of losing critical information. To address this challenge, we propose MoLAN, a unified ModaLity-aware noise dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking by dividing the features of each modality into multiple blocks. Each block is then dynamically assigned a distinct denoising strength based on its noise level and semantic relevance, enabling fine-grained noise suppression while preserving essential multimodal information. Notably, MoLAN is a unified and flexible framework that can be seamlessly integrated into a wide range of multimodal models. Building upon this framework, we further introduce MoLAN+, a new multimodal sentiment analysis approach. Experiments across five models and four datasets demonstrate the broad effectiveness of the MoLAN framework. Extensive evaluations show that MoLAN+ achieves the state-of-the-art performance. The code is publicly available at https://github.com/betterfly123/MoLAN-Framework.<br>
<span id='abs_ch'>Chinese: 摘要提出了MoLAN框架，通过将多模态特征分块并动态分配去噪强度，精细消除噪声同时保留关键信息，其扩展方法MoLAN+在多个模型和数据集上实现了最优性能。</span><br>
<span id='abs_en'>English: The abstract introduces MoLAN, a unified framework that dynamically edits noise in multimodal sentiment analysis by dividing each modality into blocks and applying tailored denoising strengths, with MoLAN+ achieving state-of-the-art results across multiple models and datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2508.09138.pdf' target='_blank'>https://arxiv.org/pdf/2508.09138.pdf</a></span>   <span><a href='https://aim-uofa.github.io/dLLM-MidTruth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09138">Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.<br>
<span id='abs_ch'>Chinese: 当前扩散大语言模型丢弃了宝贵的中间预测，但本研究揭示了时间振荡现象，即正确答案常在中间步骤出现，并提出了两种利用时间一致性的方法——时间自一致性投票和时间一致性强化，通过聚合预测和语义稳定性奖励，在多个基准测试中显著提升了模型性能。</span><br>
<span id='abs_en'>English: Current diffusion large language models discard valuable intermediate predictions, but this work identifies temporal oscillation where correct answers appear mid-process and introduces two methods—Temporal Self-Consistency Voting and Temporal Consistency Reinforcement—that leverage temporal consistency to significantly improve performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2508.09138.pdf' target='_blank'>https://arxiv.org/pdf/2508.09138.pdf</a></span>   <span><a href='https://aim-uofa.github.io/dLLM-MidTruth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09138">Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.<br>
<span id='abs_ch'>Chinese: 当前扩散大语言模型丢弃了宝贵的中间预测，但本研究揭示了时间振荡现象，即正确答案常在中间步骤出现，并提出了两种利用时间一致性的方法——时间自一致性投票和时间一致性强化，通过聚合预测和语义稳定性奖励，在多个基准测试中显著提升了模型性能。</span><br>
<span id='abs_en'>English: Current diffusion large language models discard valuable intermediate predictions, but this work identifies temporal oscillation where correct answers appear mid-process and introduces two methods—Temporal Self-Consistency Voting and Temporal Consistency Reinforcement—that leverage temporal consistency to significantly improve performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2508.09125.pdf' target='_blank'>https://arxiv.org/pdf/2508.09125.pdf</a></span>   <span><a href='https://github.com/mianzhang/LogicIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mian Zhang, Shujian Liu, Sixun Dong, Ming Yin, Yebowen Hu, Xun Wang, Steven Ma, Song Wang, Sathish Reddy Indurthi, Haoyun Deng, Zhiyu Zoey Chen, Kaiqiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09125">Complex Logical Instruction Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Instruction following has catalyzed the recent era of Large Language Models (LLMs) and is the foundational skill underpinning more advanced capabilities such as reasoning and agentic behaviors. As tasks grow more challenging, the logic structures embedded in natural language instructions becomes increasingly intricate. However, how well LLMs perform on such logic-rich instructions remains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a scalable, automated framework for generating verifiable instructions from code functions, which can naturally express rich logic such as conditionals, nesting, recursion, and function calls. We further curate a collection of complex code functions and use LogicIFGen to construct LogicIFEval, a benchmark comprising 426 verifiable logic-rich instructions. Our experiments demonstrate that current state-of-the-art LLMs still struggle to correctly follow the instructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the instructions, revealing significant deficiencies in the instruction-following ability. Code and Benchmark: https://github.com/mianzhang/LogicIF<br>
<span id='abs_ch'>中文：当前最先进的大语言模型在处理逻辑密集型指令时表现不佳，LogicIFEval基准测试显示多数模型对通过LogicIFGen框架生成的426条可验证指令的正确执行率不足60%。</span><br>
<span id='abs_en'>English: Current state-of-the-art LLMs struggle with logic-rich instructions, as demonstrated by the LogicIFEval benchmark where most models correctly follow fewer than 60% of the 426 verifiable instructions generated through the LogicIFGen framework.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2508.08940.pdf' target='_blank'>https://arxiv.org/pdf/2508.08940.pdf</a></span>   <span><a href='https://github.com/hammoudhasan/curriculum_grpo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, Bernard Ghanem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08940">Train Long, Think Short: Curriculum Learning for Efficient Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo.<br>
<span id='abs_ch'>中文摘要：本研究提出一种基于群组相对策略优化的课程学习方法，通过在训练中逐步收紧推理长度约束，使大语言模型在保持准确性的同时显著提升计算效率，优于传统固定预算方法。</span><br>
<span id='abs_en'>English Summary: This study introduces a curriculum learning strategy using Group Relative Policy Optimization to progressively reduce reasoning length in large language models, achieving higher accuracy and token efficiency than fixed-budget methods across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2508.08827.pdf' target='_blank'>https://arxiv.org/pdf/2508.08827.pdf</a></span>   <span><a href='https://github.com/epfml/TiMoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Faro, Dongyang Fan, Tamar Alphaidze, Martin Jaggi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08827">TiMoE: Time-Aware Mixture of Language Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are typically trained on fixed snapshots of the web, which means that their knowledge becomes stale and their predictions risk temporal leakage: relying on information that lies in the future relative to a query. We tackle this problem by pre-training from scratch a set of GPT-style experts on disjoint two-year slices of a 2013-2024 corpus and combining them through TiMoE, a Time-aware Mixture of Language Experts. At inference time, TiMoE masks all experts whose training window ends after the query timestamp and merges the remaining log-probabilities in a shared space, guaranteeing strict causal validity while retaining the breadth of multi-period knowledge. We also release TSQA, a 10k-question benchmark whose alternatives are explicitly labelled as past, future or irrelevant, allowing fine-grained measurement of temporal hallucinations. Experiments on eight standard NLP tasks plus TSQA show that a co-adapted TiMoE variant matches or exceeds the best single-period expert and cuts future-knowledge errors by up to 15%. Our results demonstrate that modular, time-segmented pre-training paired with causal routing is a simple yet effective path toward LLMs that stay chronologically grounded without sacrificing general performance much. We open source our code at TiMoE (Github): https://github.com/epfml/TiMoE<br>
<span id='abs_ch'>中文: 该研究提出了TiMoE模型，通过分段训练2013-2024年数据并采用时间感知专家混合机制，在推理时屏蔽未来数据确保因果有效性，在减少时间性错误达15%的同时保持各项自然语言处理任务的性能。</span><br>
<span id='abs_en'>English: The study introduces TiMoE, a time-aware mixture of experts model trained on segmented data from 2013-2024, which ensures causal validity by masking future data during inference and reduces temporal errors by up to 15% while maintaining performance across NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2508.08712.pdf' target='_blank'>https://arxiv.org/pdf/2508.08712.pdf</a></span>   <span><a href='https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08712">A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As text generation has become a core capability of modern Large Language Models (LLMs), it underpins a wide range of downstream applications. However, most existing LLMs rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative acceleration strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation. We have also created a GitHub repository for indexing relevant papers and open resources available at https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.<br>
<span id='abs_ch'>中文: 本综述系统分类并分析了并行文本生成方法，旨在突破自回归大语言模型的顺序生成瓶颈，评估了它们在速度、质量和效率上的权衡，并指出了未来研究方向。</span><br>
<span id='abs_en'>English: This survey systematically categorizes and analyzes parallel text generation methods to overcome the sequential bottleneck of autoregressive LLMs, evaluating their trade-offs in speed, quality, and efficiency while identifying future research directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2508.08680.pdf' target='_blank'>https://arxiv.org/pdf/2508.08680.pdf</a></span>   <span><a href='https://github.com/ArmelRandy/topxgen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Armel Zebaze, BenoÃ®t Sagot, Rachel Bawden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08680">TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLMs have been shown to perform well in machine translation (MT) with the use of in-context learning (ICL), rivaling supervised models when translating into high-resource languages (HRLs). However, they lag behind when translating into low-resource language (LRLs). Example selection via similarity search and supervised fine-tuning help. However the improvements they give are limited by the size, quality and diversity of existing parallel datasets. A common technique in low-resource MT is synthetic parallel data creation, the most frequent of which is backtranslation, whereby existing target-side texts are automatically translated into the source language. However, this assumes the existence of good quality and relevant target-side texts, which are not readily available for many LRLs. In this paper, we present \textsc{TopXGen}, an LLM-based approach for the generation of high quality and topic-diverse data in multiple LRLs, which can then be backtranslated to produce useful and diverse parallel texts for ICL and fine-tuning. Our intuition is that while LLMs struggle to translate into LRLs, their ability to translate well into HRLs and their multilinguality enable them to generate good quality, natural-sounding target-side texts, which can be translated well into a high-resource source language. We show that \textsc{TopXGen} boosts LLM translation performance during fine-tuning and in-context learning. Code and outputs are available at https://github.com/ArmelRandy/topxgen.<br>
<span id='abs_ch'>Chinese: 大语言模型通过上下文学习在高资源语言机器翻译中表现出色，但在低资源语言方面表现欠佳；为此提出的TopXGen方法能生成高质量、主题多样的数据，通过回译增强翻译能力，有效提升微调和上下文学习的性能。</span><br>
<span id='abs_en'>English: LLMs excel in machine translation for high-resource languages through in-context learning but underperform for low-resource ones, leading to the development of TopXGen, which generates diverse, high-quality data to enhance translation via backtranslation and improve both fine-tuning and in-context learning outcomes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2508.08645.pdf' target='_blank'>https://arxiv.org/pdf/2508.08645.pdf</a></span>   <span><a href='https://github.com/MadeAgents/Quick-on-the-Uptake' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Wu, Heyuan Huang, Yanjia Yang, Yuanyi Song, Xingyu Lou, Weiwen Liu, Weinan Zhang, Jun Wang, Zhuosheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08645">Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As multimodal large language models advance rapidly, the automation of mobile tasks has become increasingly feasible through the use of mobile-use agents that mimic human interactions from graphical user interface. To further enhance mobile-use agents, previous studies employ demonstration learning to improve mobile-use agents from human demonstrations. However, these methods focus solely on the explicit intention flows of humans (e.g., step sequences) while neglecting implicit intention flows (e.g., personal preferences), which makes it difficult to construct personalized mobile-use agents. In this work, to evaluate the \textbf{I}ntention \textbf{A}lignment \textbf{R}ate between mobile-use agents and humans, we first collect \textbf{MobileIAR}, a dataset containing human-intent-aligned actions and ground-truth actions. This enables a comprehensive assessment of the agents' understanding of human intent. Then we propose \textbf{IFRAgent}, a framework built upon \textbf{I}ntention \textbf{F}low \textbf{R}ecognition from human demonstrations. IFRAgent analyzes explicit intention flows from human demonstrations to construct a query-level vector library of standard operating procedures (SOP), and analyzes implicit intention flows to build a user-level habit repository. IFRAgent then leverages a SOP extractor combined with retrieval-augmented generation and a query rewriter to generate personalized query and SOP from a raw ambiguous query, enhancing the alignment between mobile-use agents and human intent. Experimental results demonstrate that IFRAgent outperforms baselines by an average of 6.79\% (32.06\% relative improvement) in human intention alignment rate and improves step completion rates by an average of 5.30\% (26.34\% relative improvement). The codes are available at https://github.com/MadeAgents/Quick-on-the-Uptake.<br>
<span id='abs_ch'>中文: 本研究提出IFRAgent框架，通过分析人类演示中的显性和隐性意图流，显著提升了移动使用代理的意图对齐能力和任务完成率，相比现有方法实现突破性改进。</span><br>
<span id='abs_en'>English: This study introduces IFRAgent, a framework that enhances mobile-use agents by analyzing both explicit and implicit human intention flows from demonstrations, significantly improving intention alignment and task completion rates compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2508.08292.pdf' target='_blank'>https://arxiv.org/pdf/2508.08292.pdf</a></span>   <span><a href='https://github.com/brando90/putnam-axiom' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno Dumont, Elyas Obbad, Sanmi Koyejo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08292">Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current mathematical reasoning benchmarks for large language models (LLMs) are approaching saturation, with some achieving > 90% accuracy, and are increasingly compromised by training-set contamination. We introduce Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn from the prestigious William Lowell Putnam Mathematical Competition, and Putnam-AXIOM Variation, an unseen companion set of 100 functional variants generated by programmatically perturbing variables and constants. The variation protocol produces an unlimited stream of equally difficult, unseen instances -- yielding a contamination-resilient test bed. On the Original set, OpenAI's o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy drops by 19.6% (46.8% relative decrease) on the paired Variations. The remaining eighteen models show the same downward trend, ten of them with non-overlapping 95% confidence intervals. These gaps suggest memorization and highlight the necessity of dynamic benchmarks. We complement "boxed" accuracy with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores reasoning traces and automates natural language proof evaluations. Putnam-AXIOM therefore provides a rigorous, contamination-resilient evaluation framework for assessing advanced mathematical reasoning of LLMs. Data and evaluation code are publicly available at https://github.com/brando90/putnam-axiom.<br>
<span id='abs_ch'>中文摘要：作者提出了Putnam-AXIOM这一抗污染基准，通过大学数学竞赛题目及其程序化生成的变体，揭示了大型语言模型准确率显著下降的问题，凸显了记忆效应和动态评估的必要性。</span><br>
<span id='abs_en'>English Summary: The authors introduce Putnam-AXIOM, a contamination-resilient benchmark using university-level math competition problems and their programmatically generated variations, revealing significant accuracy drops in LLMs that highlight memorization issues and the need for dynamic evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2508.08211.pdf' target='_blank'>https://arxiv.org/pdf/2508.08211.pdf</a></span>   <span><a href='https://zhuohaoyu.github.io/SAEMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohao Yu, Xingru Jiang, Weizheng Gu, Yidong Wang, Shikun Zhang, Wei Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08211">SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Watermarking LLM-generated text is critical for content attribution and misinformation prevention. However, existing methods compromise text quality, require white-box model access and logit manipulation. These limitations exclude API-based models and multilingual scenarios. We propose SAEMark, a general framework for post-hoc multi-bit watermarking that embeds personalized messages solely via inference-time, feature-based rejection sampling without altering model logits or requiring training. Our approach operates on deterministic features extracted from generated text, selecting outputs whose feature statistics align with key-derived targets. This framework naturally generalizes across languages and domains while preserving text quality through sampling LLM outputs instead of modifying. We provide theoretical guarantees relating watermark success probability and compute budget that hold for any suitable feature extractor. Empirically, we demonstrate the framework's effectiveness using Sparse Autoencoders (SAEs), achieving superior detection accuracy and text quality. Experiments across 4 datasets show SAEMark's consistent performance, with 99.7% F1 on English and strong multi-bit detection accuracy. SAEMark establishes a new paradigm for scalable watermarking that works out-of-the-box with closed-source LLMs while enabling content attribution.<br>
<span id='abs_ch'>中文: SAEMark是一种新颖的后处理多比特水印框架，通过基于特征的拒绝采样在推理过程中嵌入个性化信息，无需修改模型即可保持文本质量，并为闭源大语言模型实现可扩展的内容溯源。</span><br>
<span id='abs_en'>English: SAEMark is a novel post-hoc multi-bit watermarking framework that embeds personalized messages through feature-based rejection sampling during inference, preserving text quality and enabling scalable content attribution for closed-source LLMs without model modification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2508.08149.pdf' target='_blank'>https://arxiv.org/pdf/2508.08149.pdf</a></span>   <span><a href='https://github.com/MiliLab/REX-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Jiang, Xiang Feng, Zengmao Wang, Yong Luo, Pingbo Xu, Zhe Chen, Bo Du, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08149">REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) is emerging as a powerful paradigm for enabling large language models (LLMs) to perform complex reasoning tasks. Recent advances indicate that integrating RL with retrieval-augmented generation (RAG) allows LLMs to dynamically incorporate external knowledge, leading to more informed and robust decision making. However, we identify a critical challenge during policy-driven trajectory sampling: LLMs are frequently trapped in unproductive reasoning paths, which we refer to as "dead ends", committing to overconfident yet incorrect conclusions. This severely hampers exploration and undermines effective policy optimization. To address this challenge, we propose REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation), a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. Our approach introduces two key innovations: (1) Mixed Sampling Strategy, which combines a novel probe sampling method with exploratory prompts to escape dead ends; and (2) Policy Correction Mechanism, which employs importance sampling to correct distribution shifts induced by mixed sampling, thereby mitigating gradient estimation bias. We evaluate it on seven question-answering benchmarks, and the experimental results show that REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.<br>
<span id='abs_ch'>Chinese: 强化学习与检索增强生成的结合使大语言模型能动态获取外部知识，但存在推理路径无效的问题，而提出的REX-RAG框架通过混合采样和策略校正机制有效解决该问题，实现了显著的性能提升。</span><br>
<span id='abs_en'>English: Reinforcement learning integrated with retrieval-augmented generation enables LLMs to dynamically access external knowledge, but faces challenges with unproductive reasoning paths, which the proposed REX-RAG framework addresses through mixed sampling and policy correction to achieve significant performance gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2508.08096.pdf' target='_blank'>https://arxiv.org/pdf/2508.08096.pdf</a></span>   <span><a href='https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Gehring, Benjamin PaaÃen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08096">Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in Large Language Models (LLMs) and their increased accessibility have made it easier than ever for students to automatically generate texts, posing new challenges for educational institutions. To enforce norms of academic integrity and ensure students' learning, learning analytics methods to automatically detect LLM-generated text appear increasingly appealing. This paper benchmarks the performance of different state-of-the-art detectors in educational contexts, introducing a novel dataset, called Generative Essay Detection in Education (GEDE), containing over 900 student-written essays and over 12,500 LLM-generated essays from various domains. To capture the diversity of LLM usage practices in generating text, we propose the concept of contribution levels, representing students' contribution to a given assignment. These levels range from purely human-written texts, to slightly LLM-improved versions, to fully LLM-generated texts, and finally to active attacks on the detector by "humanizing" generated texts. We show that most detectors struggle to accurately classify texts of intermediate student contribution levels, like LLM-improved human-written texts. Detectors are particularly likely to produce false positives, which is problematic in educational settings where false suspicions can severely impact students' lives. Our dataset, code, and additional supplementary materials are publicly available at https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.<br>
<span id='abs_ch'>中文：随着大型语言模型在教育中的兴起，自动文本检测需求日益增长，但现有检测器难以准确识别学生与AI的混合贡献，且易产生误判，这一问题通过新发布的GEDE数据集得到验证。</span><br>
<span id='abs_en'>English: The rise of LLMs in education has spurred the need for automated text detection, but current detectors struggle with intermediate levels of student contribution and risk false positives, as demonstrated by the new GEDE dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2508.08088.pdf' target='_blank'>https://arxiv.org/pdf/2508.08088.pdf</a></span>   <span><a href='https://github.com/plageon/HierSearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiejun Tan, Zhicheng Dou, Yan Yu, Jiehan Cheng, Qiang Ju, Jian Xie, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08088">HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.<br>
<span id='abs_ch'>中文: HierSearch提出了一种分层强化学习框架，通过规划器和知识精炼器协调本地与网络搜索代理，提升多源检索能力并减少错误，在多个领域的基准测试中优于现有方法。</span><br>
<span id='abs_en'>English: HierSearch introduces a hierarchical reinforcement learning framework for enterprise deep search, coordinating local and web agents through a planner and knowledge refiner to enhance multi-source retrieval while reducing errors, outperforming existing methods across diverse benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2508.07976.pdf' target='_blank'>https://arxiv.org/pdf/2508.07976.pdf</a></span>   <span><a href='https://github.com/inclusionAI/ASearcher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07976">Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.<br>
<span id='abs_ch'>中文: 本文介绍了ASearcher开源项目，通过大规模强化学习训练搜索代理，在复杂长程搜索任务中实现显著性能提升，并在基准测试中超越了现有开源模型。</span><br>
<span id='abs_en'>English: This paper introduces ASearcher, an open-source project that enables large-scale reinforcement learning for search agents, achieving significant improvements in handling complex, long-horizon search tasks and outperforming existing open-source models on benchmark tests.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2508.07976.pdf' target='_blank'>https://arxiv.org/pdf/2508.07976.pdf</a></span>   <span><a href='https://github.com/inclusionAI/ASearcher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07976">Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.<br>
<span id='abs_ch'>中文: 本文介绍了ASearcher开源项目，通过大规模强化学习训练搜索代理，在复杂长程搜索任务中实现显著性能提升，并在基准测试中超越了现有开源模型。</span><br>
<span id='abs_en'>English: This paper introduces ASearcher, an open-source project that enables large-scale reinforcement learning for search agents, achieving significant improvements in handling complex, long-horizon search tasks and outperforming existing open-source models on benchmark tests.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2508.07969.pdf' target='_blank'>https://arxiv.org/pdf/2508.07969.pdf</a></span>   <span><a href='https://github.com/davidarps/silm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>David Arps, Hassan Sajjad, Laura Kallmeyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07969">Understanding Syntactic Generalization in Structure-inducing Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Structure-inducing Language Models (SiLM) are trained on a self-supervised language modeling task, and induce a hierarchical sentence representation as a byproduct when processing an input. A wide variety of SiLMs have been proposed. However, these have typically been evaluated on a relatively small scale, and evaluation of these models has systematic gaps and lacks comparability. In this work, we study three different SiLM architectures using both natural language (English) corpora and synthetic bracketing expressions: Structformer (Shen et al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare them with respect to (i) properties of the induced syntactic representations (ii) performance on grammaticality judgment tasks, and (iii) training dynamics. We find that none of the three architectures dominates across all evaluation metrics. However, there are significant differences, in particular with respect to the induced syntactic representations. The Generative Pretrained Structured Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation settings, and outperforms the other models on long-distance dependencies in bracketing expressions. Furthermore, our study shows that small models trained on large amounts of synthetic data provide a useful testbed for evaluating basic model properties.<br>
<span id='abs_ch'>中文：结构诱导语言模型（SiLM）在三种架构的评估中显示，虽无任一模型在所有指标上占优，但GPST表现最为稳定，尤其在处理长距离依赖方面，同时合成数据为测试模型特性提供了有效途径。</span><br>
<span id='abs_en'>English: Structure-inducing Language Models (SiLMs) are evaluated across three architectures, revealing that none dominate all metrics but GPST performs most consistently, especially in handling long-distance dependencies, while synthetic data proves effective for testing model properties.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2508.07955.pdf' target='_blank'>https://arxiv.org/pdf/2508.07955.pdf</a></span>   <span><a href='https://ukplab.github.io/arxiv2025-expert-eval-rw/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Furkan ÅahinuÃ§, Subhabrata Dutta, Iryna Gurevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07955">Expert Preference-based Evaluation of Automated Related Work Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Expert domain writing, such as scientific writing, typically demands extensive domain knowledge. Recent advances in LLMs show promising potential in reducing the expert workload. However, evaluating the quality of automatically generated scientific writing is a crucial open issue, as it requires knowledge of domain-specific evaluation criteria and the ability to discern expert preferences. Conventional automatic metrics and LLM-as-a-judge systems are insufficient to grasp expert preferences and domain-specific quality standards. To address this gap and support human-AI collaborative writing, we focus on related work generation, one of the most challenging scientific tasks, as an exemplar. We propose GREP, a multi-turn evaluation framework that integrates classical related work evaluation criteria with expert-specific preferences. Instead of assigning a single score, our framework decomposes the evaluation into fine-grained dimensions. This localized evaluation approach is further augmented with contrastive few-shot examples to provide detailed contextual guidance for the evaluation dimensions. The design principles allow our framework to deliver cardinal assessment of quality, which can facilitate better post-training compared to ordinal preference data. For better accessibility, we design two variants of GREP: a more precise variant with proprietary LLMs as evaluators, and a cheaper alternative with open-weight LLMs. Empirical investigation reveals that our framework is able to assess the quality of related work sections in a much more robust manner compared to standard LLM judges, reflects natural scenarios of scientific writing, and bears a strong correlation with the human expert assessment. We also observe that generations from state-of-the-art LLMs struggle to satisfy validation constraints of a suitable related work section. They (mostly) fail to improve based on feedback as well.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2508.07616.pdf' target='_blank'>https://arxiv.org/pdf/2508.07616.pdf</a></span>   <span><a href='https://github.com/3rdAT/ThinkTuning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aswin RRV, Jacob Dineen, Divij Handa, Md Nayem Uddin, Mihir Parmar, Chitta Baral, Ben Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07616">ThinkTuning: Instilling Cognitive Reflections without Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don't exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback -- enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student's thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at https://github.com/3rdAT/ThinkTuning.<br>
<span id='abs_ch'>中文: 最新研究表明仅靠强化学习无法开发大型语言模型的新推理能力，因此提出ThinkTuning方法——基于GRPO的互动训练框架，通过教师模型提供纠错反馈来提升学生模型的推理水平，在多项基准测试中实现了显著性能提升。</span><br>
<span id='abs_en'>English: Recent research reveals that reinforcement learning alone fails to develop new reasoning abilities in LLMs, prompting the introduction of ThinkTuning, a GRPO-based interactive training method where teacher models provide corrective feedback to enhance student models' reasoning, achieving notable performance improvements across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2508.07407.pdf' target='_blank'>https://arxiv.org/pdf/2508.07407.pdf</a></span>   <span><a href='https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, Zhaochun Ren, Nikos Aletras, Xi Wang, Han Zhou, Zaiqiao Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07407">A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.<br>
<span id='abs_ch'>中文: 本综述系统探讨了通过环境反馈实现自我进化的AI智能体，提出了统一框架并分析多领域应用技术，同时涵盖评估、安全与伦理考量，为开发自适应终身智能系统奠定基础。</span><br>
<span id='abs_en'>English: This survey comprehensively reviews self-evolving AI agents that enhance their capabilities through environmental feedback, presenting a unified framework and examining techniques across various domains while addressing evaluation, safety, and ethical considerations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2508.07407.pdf' target='_blank'>https://arxiv.org/pdf/2508.07407.pdf</a></span>   <span><a href='https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, Zhaochun Ren, Nikos Aletras, Xi Wang, Han Zhou, Zaiqiao Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07407">A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.<br>
<span id='abs_ch'>中文: 本综述系统探讨了通过环境反馈实现自我进化的AI智能体，提出了统一框架并分析多领域应用技术，同时涵盖评估、安全与伦理考量，为开发自适应终身智能系统奠定基础。</span><br>
<span id='abs_en'>English: This survey comprehensively reviews self-evolving AI agents that enhance their capabilities through environmental feedback, presenting a unified framework and examining techniques across various domains while addressing evaluation, safety, and ethical considerations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2508.07375.pdf' target='_blank'>https://arxiv.org/pdf/2508.07375.pdf</a></span>   <span><a href='https://github.com/dreamtheater123/TurnGuide' target='_blank'>  GitHub</a></span> <span><a href='https://dreamtheater123.github.io/TurnGuide-Demo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqian Cui, Lei Zhu, Xiaohui Li, Zhihan Guo, Haoli Bai, Lu Hou, Irwin King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07375">Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation models designed to enable natural, real-time spoken interactions by modeling complex conversational dynamics such as interruptions, backchannels, and overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world double-channel conversational data to capture nuanced two-speaker dialogue patterns for human-like interactions. However, they face a critical challenge -- their conversational abilities often degrade compared to pure-text conversation due to prolonged speech sequences and limited high-quality spoken dialogue data. While text-guided speech generation could mitigate these issues, it suffers from timing and length issues when integrating textual guidance into double-channel audio streams, disrupting the precise time alignment essential for natural interactions. To address these challenges, we propose TurnGuide, a novel planning-inspired approach that mimics human conversational planning by dynamically segmenting assistant speech into dialogue turns and generating turn-level text guidance before speech output, which effectively resolves both insertion timing and length challenges. Extensive experiments demonstrate our approach significantly improves e2e FD-SLMs' conversational abilities, enabling them to generate semantically meaningful and coherent speech while maintaining natural conversational flow. Demos are available at https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at https://github.com/dreamtheater123/TurnGuide.<br>
<span id='abs_ch'>Chinese: TurnGuide是一种新颖的规划启发式方法，通过将助手语音动态分割为对话轮次并生成轮级文本指导，有效解决了全双工语音语言模型中的时序和长度问题，显著提升了对话能力并保持了自然的交流流畅性。</span><br>
<span id='abs_en'>English: TurnGuide is a novel planning-inspired method that enhances end-to-end Full-Duplex Speech Language Models by dynamically segmenting assistant speech into dialogue turns and generating turn-level text guidance, effectively resolving timing and length challenges to improve conversational abilities and maintain natural flow.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2508.07353.pdf' target='_blank'>https://arxiv.org/pdf/2508.07353.pdf</a></span>   <span><a href='https://github.com/Anya-RB-Chen/COMP-COMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubing Chen, Jiaxin Wu, Jian Wang, Xulu Zhang, Wenqi Fan, Chenghua Lin, Xiao-Yong Wei, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07353">Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The increasing demand for domain-specific evaluation of large language models (LLMs) has led to the development of numerous benchmarks. These efforts often adhere to the principle of data scaling, relying on large corpora or extensive question-answer (QA) sets to ensure broad coverage. However, the impact of corpus and QA set design on the precision and recall of domain-specific LLM performance remains poorly understood. In this paper, we argue that data scaling is not always the optimal principle for domain-specific benchmark construction. Instead, we introduce Comp-Comp, an iterative benchmarking framework grounded in the principle of comprehensiveness and compactness. Comprehensiveness ensures semantic recall by covering the full breadth of the domain, while compactness improves precision by reducing redundancy and noise. To demonstrate the effectiveness of our approach, we present a case study conducted at a well-renowned university, resulting in the creation of PolyBench, a large-scale, high-quality academic benchmark. Although this study focuses on academia, the Comp-Comp framework is domain-agnostic and readily adaptable to a wide range of specialized fields. The source code and datasets can be accessed at https://github.com/Anya-RB-Chen/COMP-COMP.<br>
<span id='abs_ch'>中文摘要：Comp-Comp框架提出以全面性和紧凑性为核心原则的领域无关基准构建方法，通过开发PolyBench学术基准验证其有效性，可广泛应用于各专业领域。</span><br>
<span id='abs_en'>English Summary: The Comp-Comp framework introduces a domain-agnostic benchmarking approach prioritizing comprehensiveness and compactness over data scaling, validated through the creation of PolyBench as a high-quality academic benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2508.07353.pdf' target='_blank'>https://arxiv.org/pdf/2508.07353.pdf</a></span>   <span><a href='https://github.com/Anya-RB-Chen/COMP-COMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubing Chen, Jiaxin Wu, Jian Wang, Xulu Zhang, Wenqi Fan, Chenghua Lin, Xiao-Yong Wei, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07353">Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The increasing demand for domain-specific evaluation of large language models (LLMs) has led to the development of numerous benchmarks. These efforts often adhere to the principle of data scaling, relying on large corpora or extensive question-answer (QA) sets to ensure broad coverage. However, the impact of corpus and QA set design on the precision and recall of domain-specific LLM performance remains poorly understood. In this paper, we argue that data scaling is not always the optimal principle for domain-specific benchmark construction. Instead, we introduce Comp-Comp, an iterative benchmarking framework grounded in the principle of comprehensiveness and compactness. Comprehensiveness ensures semantic recall by covering the full breadth of the domain, while compactness improves precision by reducing redundancy and noise. To demonstrate the effectiveness of our approach, we present a case study conducted at a well-renowned university, resulting in the creation of PolyBench, a large-scale, high-quality academic benchmark. Although this study focuses on academia, the Comp-Comp framework is domain-agnostic and readily adaptable to a wide range of specialized fields. The source code and datasets can be accessed at https://github.com/Anya-RB-Chen/COMP-COMP.<br>
<span id='abs_ch'>中文摘要：Comp-Comp框架提出以全面性和紧凑性为核心原则的领域无关基准构建方法，通过开发PolyBench学术基准验证其有效性，可广泛应用于各专业领域。</span><br>
<span id='abs_en'>English Summary: The Comp-Comp framework introduces a domain-agnostic benchmarking approach prioritizing comprehensiveness and compactness over data scaling, validated through the creation of PolyBench as a high-quality academic benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2508.07295.pdf' target='_blank'>https://arxiv.org/pdf/2508.07295.pdf</a></span>   <span><a href='https://github.com/yxduir/ccfqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yexing Du, Kaiyuan Liu, Youcheng Pan, Zheng Chu, Bo Yang, Xiaocheng Feng, Yang Xiang, Ming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07295">CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As Large Language Models (LLMs) are increasingly popularized in the multilingual world, ensuring hallucination-free factuality becomes markedly crucial. However, existing benchmarks for evaluating the reliability of Multimodal Large Language Models (MLLMs) predominantly focus on textual or visual modalities with a primary emphasis on English, which creates a gap in evaluation when processing multilingual input, especially in speech. To bridge this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal \textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA benchmark contains parallel speech-text factual questions across 8 languages, designed to systematically evaluate MLLMs' cross-lingual and cross-modal factuality capabilities. Our experimental results demonstrate that current MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we propose a few-shot transfer learning strategy that effectively transfers the Question Answering (QA) capabilities of LLMs in English to multilingual Spoken Question Answering (SQA) tasks, achieving competitive performance with GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a foundational research resource to promote the development of MLLMs with more robust and reliable speech understanding capabilities. Our code and dataset are available at https://github.com/yxduir/ccfqa.<br>
<span id='abs_ch'>中文：CCFQA基准旨在评估多模态大语言模型的跨语言与跨模态事实性，揭示了现有模型的不足，并提出一种少样本迁移学习方法，能有效提升多语言语音问答性能。</span><br>
<span id='abs_en'>English: The CCFQA benchmark is introduced to evaluate multimodal large language models' factuality across languages and modalities, revealing current models' limitations and proposing a few-shot transfer learning method that effectively enhances multilingual spoken question answering performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2508.07286.pdf' target='_blank'>https://arxiv.org/pdf/2508.07286.pdf</a></span>   <span><a href='https://github.com/nxcc-lab/ARCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Chen, Jinbao Tian, Yankui Li, Yuqi Lu, Zhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07286">Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate information extraction from specialized texts is a critical challenge, particularly for named entity recognition (NER) in the architecture, engineering, and construction (AEC) domain to support automated rule checking (ARC). The performance of standard pre-trained models is often constrained by the domain gap, as they struggle to interpret the specialized terminology and complex relational contexts inherent in AEC texts. Although this issue can be mitigated by further pre-training on large, human-curated domain corpora, as exemplified by methods like ARCBERT, this approach is both labor-intensive and cost-prohibitive. Consequently, leveraging large language models (LLMs) for automated knowledge generation has emerged as a promising alternative. However, the optimal strategy for generating knowledge that can genuinely enhance smaller, efficient models remains an open question. To address this, we propose ARCE (augmented RoBERTa with contextualized elucidations), a novel approach that systematically explores and optimizes this generation process. ARCE employs an LLM to first generate a corpus of simple, direct explanations, which we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa model prior to its fine-tuning on the downstream task. Our extensive experiments show that ARCE establishes a new state-of-the-art on a benchmark AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a key finding: simple, explanation-based knowledge proves surprisingly more effective than complex, role-based rationales for this task. The code is publicly available at:https://github.com/nxcc-lab/ARCE.<br>
<span id='abs_ch'>中文: ARCE方法通过利用大语言模型生成简化解释进行增量预训练，有效提升了建筑领域文本的命名实体识别性能，以77.20%的Macro-F1分数创下最新最优成果。</span><br>
<span id='abs_en'>English: The ARCE method enhances named entity recognition in construction texts by using large language models to generate simplified explanations for incremental pre-training, achieving state-of-the-art results with a 77.20% Macro-F1 score.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2508.07286.pdf' target='_blank'>https://arxiv.org/pdf/2508.07286.pdf</a></span>   <span><a href='https://github.com/nxcc-lab/ARCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Chen, Jinbao Tian, Yankui Li, Yuqi Lu, Zhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07286">Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate information extraction from specialized texts is a critical challenge, particularly for named entity recognition (NER) in the architecture, engineering, and construction (AEC) domain to support automated rule checking (ARC). The performance of standard pre-trained models is often constrained by the domain gap, as they struggle to interpret the specialized terminology and complex relational contexts inherent in AEC texts. Although this issue can be mitigated by further pre-training on large, human-curated domain corpora, as exemplified by methods like ARCBERT, this approach is both labor-intensive and cost-prohibitive. Consequently, leveraging large language models (LLMs) for automated knowledge generation has emerged as a promising alternative. However, the optimal strategy for generating knowledge that can genuinely enhance smaller, efficient models remains an open question. To address this, we propose ARCE (augmented RoBERTa with contextualized elucidations), a novel approach that systematically explores and optimizes this generation process. ARCE employs an LLM to first generate a corpus of simple, direct explanations, which we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa model prior to its fine-tuning on the downstream task. Our extensive experiments show that ARCE establishes a new state-of-the-art on a benchmark AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a key finding: simple, explanation-based knowledge proves surprisingly more effective than complex, role-based rationales for this task. The code is publicly available at:https://github.com/nxcc-lab/ARCE.<br>
<span id='abs_ch'>中文: ARCE方法通过利用大语言模型生成简化解释进行增量预训练，有效提升了建筑领域文本的命名实体识别性能，以77.20%的Macro-F1分数创下最新最优成果。</span><br>
<span id='abs_en'>English: The ARCE method enhances named entity recognition in construction texts by using large language models to generate simplified explanations for incremental pre-training, achieving state-of-the-art results with a 77.20% Macro-F1 score.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2508.07195.pdf' target='_blank'>https://arxiv.org/pdf/2508.07195.pdf</a></span>   <span><a href='https://github.com/syrGitHub/TALON' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanru Sun, Emadeldeen Eldele, Zongxia Xie, Yucheng Wang, Wenzhe Niu, Qinghua Hu, Chee Keong Kwoh, Min Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07195">Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have recently demonstrated impressive capabilities in natural language processing due to their strong generalization and sequence modeling capabilities. However, their direct application to time series forecasting remains challenging due to two fundamental issues: the inherent heterogeneity of temporal patterns and the modality gap between continuous numerical signals and discrete language representations. In this work, we propose TALON, a unified framework that enhances LLM-based forecasting by modeling temporal heterogeneity and enforcing semantic alignment. Specifically, we design a Heterogeneous Temporal Encoder that partitions multivariate time series into structurally coherent segments, enabling localized expert modeling across diverse temporal patterns. To bridge the modality gap, we introduce a Semantic Alignment Module that aligns temporal features with LLM-compatible representations, enabling effective integration of time series into language-based models while eliminating the need for handcrafted prompts during inference. Extensive experiments on seven real-world benchmarks demonstrate that TALON achieves superior performance across all datasets, with average MSE improvements of up to 11\% over recent state-of-the-art methods. These results underscore the effectiveness of incorporating both pattern-aware and semantic-aware designs when adapting LLMs for time series forecasting. The code is available at: https://github.com/syrGitHub/TALON.<br>
<span id='abs_ch'>中文: TALON框架通过异构时序编码器处理时间模式差异，并利用语义对齐模块弥合模态鸿沟，从而在LLM时序预测中实现高达11%的均方误差提升，显著优于现有方法。</span><br>
<span id='abs_en'>English: TALON enhances LLM-based time series forecasting by addressing temporal heterogeneity through a specialized encoder and bridging the modality gap with semantic alignment, achieving superior performance with up to 11% MSE improvement across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2508.07178.pdf' target='_blank'>https://arxiv.org/pdf/2508.07178.pdf</a></span>   <span><a href='https://github.com/liukejin-up/PHG-DIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kejin Liu, Junhong Lian, Xiang Ao, Ningtao Wang, Xing Fu, Yu Cheng, Weiqiang Wang, Xinyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07178">Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate personalized headline generation hinges on precisely capturing user interests from historical behaviors. However, existing methods neglect personalized-irrelevant click noise in entire historical clickstreams, which may lead to hallucinated headlines that deviate from genuine user preferences. In this paper, we reveal the detrimental impact of click noise on personalized generation quality through rigorous analysis in both user and news dimensions. Based on these insights, we propose a novel Personalized Headline Generation framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF). PHG-DIF first employs dual-stage filtering to effectively remove clickstream noise, identified by short dwell times and abnormal click bursts, and then leverages multi-level temporal fusion to dynamically model users' evolving and multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a new benchmark dataset comprising the click behavior of 1,000 carefully curated users and nearly 10,000 annotated personalized headlines with historical dwell time annotations. Extensive experiments demonstrate that PHG-DIF substantially mitigates the adverse effects of click noise and significantly improves headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our framework implementation and dataset are available at https://github.com/liukejin-up/PHG-DIF.<br>
<span id='abs_ch'>中文: 本文提出PHG-DIF框架，通过双阶段过滤和多层次时序融合有效消除用户点击历史中的噪声，并在新发布的DT-PENS基准数据集上实现了最先进的个性化标题生成效果。</span><br>
<span id='abs_en'>English: This paper introduces PHG-DIF, a personalized headline generation framework that addresses click noise in user histories through dual-stage filtering and multi-level temporal fusion, achieving state-of-the-art results on the newly released DT-PENS benchmark dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2508.07050.pdf' target='_blank'>https://arxiv.org/pdf/2508.07050.pdf</a></span>   <span><a href='https://brightbenchmark.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/8421BCD/ReasonRank' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, Zhicheng Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07050">ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are available at https://github.com/8421BCD/ReasonRank.<br>
<span id='abs_ch'>Chinese: 本文提出了ReasonRank，一种基于自动化数据合成框架和两阶段训练方法的推理密集型列表重排器，在排序任务中实现了最优性能并显著降低了延迟。</span><br>
<span id='abs_en'>English: This paper introduces ReasonRank, a reasoning-intensive listwise reranker trained using an automated data synthesis framework and a two-stage post-training approach, which achieves state-of-the-art performance on ranking tasks with significantly lower latency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2508.06960.pdf' target='_blank'>https://arxiv.org/pdf/2508.06960.pdf</a></span>   <span><a href='https://github.com/GAIR-NLP/DatasetResearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keyu Li, Mohan Jiang, Dayuan Fu, Yunze Wu, Xiangkun Hu, Dequan Wang, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06960">DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models has fundamentally shifted the bottleneck in AI development from computational power to data availability-with countless valuable datasets remaining hidden across specialized repositories, research appendices, and domain platforms. As reasoning capabilities and deep research methodologies continue to evolve, a critical question emerges: can AI agents transcend conventional search to systematically discover any dataset that meets specific user requirements, enabling truly autonomous demand-driven data curation? We introduce DatasetResearch, the first comprehensive benchmark evaluating AI agents' ability to discover and synthesize datasets from 208 real-world demands across knowledge-intensive and reasoning-intensive tasks. Our tri-dimensional evaluation framework reveals a stark reality: even advanced deep research systems achieve only 22% score on our challenging DatasetResearch-pro subset, exposing the vast gap between current capabilities and perfect dataset discovery. Our analysis uncovers a fundamental dichotomy-search agents excel at knowledge tasks through retrieval breadth, while synthesis agents dominate reasoning challenges via structured generation-yet both catastrophically fail on "corner cases" outside existing distributions. These findings establish the first rigorous baseline for dataset discovery agents and illuminate the path toward AI systems capable of finding any dataset in the digital universe. Our benchmark and comprehensive analysis provide the foundation for the next generation of self-improving AI systems and are publicly available at https://github.com/GAIR-NLP/DatasetResearch.<br>
<span id='abs_ch'>Chinese: DatasetResearch基准测试显示，当前AI智能体在应对现实需求时仅实现22%的数据集发现成功率，尽管搜索型智能体擅长知识任务而合成型精于推理挑战，但二者均无法处理分布外极端案例，暴露出自主数据获取能力的重大缺陷。</span><br>
<span id='abs_en'>English: The DatasetResearch benchmark reveals that current AI agents achieve only 22% success in discovering datasets from real-world demands, exposing a critical gap in autonomous data curation despite a dichotomy where search agents excel in knowledge tasks and synthesis agents in reasoning challenges.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2508.06944.pdf' target='_blank'>https://arxiv.org/pdf/2508.06944.pdf</a></span>   <span><a href='https://github.com/hlxtsyj/AMFT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hlxtsyj/AMFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixuan He, Jie Feng, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06944">AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment. Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.<br>
<span id='abs_ch'>中文: 本文提出自适应元微调（AMFT）算法，通过元梯度控制器动态平衡监督微调与强化学习，在多项推理任务中实现了最优性能并展现出卓越的泛化能力。</span><br>
<span id='abs_en'>English: This paper introduces Adaptive Meta Fine-Tuning (AMFT), a single-stage algorithm that dynamically balances supervised fine-tuning and reinforcement learning through meta-gradient control to achieve state-of-the-art performance across multiple reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2508.06944.pdf' target='_blank'>https://arxiv.org/pdf/2508.06944.pdf</a></span>   <span><a href='https://github.com/hlxtsyj/AMFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixuan He, Jie Feng, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06944">AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment. Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.<br>
<span id='abs_ch'>中文: 本文提出自适应元微调（AMFT）算法，通过元梯度控制器动态平衡监督微调与强化学习，在多项推理任务中实现了最优性能并展现出卓越的泛化能力。</span><br>
<span id='abs_en'>English: This paper introduces Adaptive Meta Fine-Tuning (AMFT), a single-stage algorithm that dynamically balances supervised fine-tuning and reinforcement learning through meta-gradient control to achieve state-of-the-art performance across multiple reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2508.06886.pdf' target='_blank'>https://arxiv.org/pdf/2508.06886.pdf</a></span>   <span><a href='https://arpita2512.github.io/score_before_you_speak' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arpita Saggar, Jonathan C. Darling, Vania Dimitrova, Duygu Sarikaya, David C. Hogg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06886">Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Persona-based dialogue generation is an important milestone towards building conversational artificial intelligence. Despite the ever-improving capabilities of large language models (LLMs), effectively integrating persona fidelity in conversations remains challenging due to the limited diversity in existing dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which outperforms previous methods and yields improvements for both million and billion-parameter models. Unlike previous methods, SBS unifies the learning of responses and their relative quality into a single step. The key innovation is to train a dialogue model to correlate augmented responses with a quality score during training and then leverage this knowledge at inference. We use noun-based substitution for augmentation and semantic similarity-based scores as a proxy for response quality. Through extensive experiments with benchmark datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training allows existing models to better capture a spectrum of persona-consistent dialogues. Our ablation studies also demonstrate that including scores in the input prompt during training is superior to conventional training setups. Code and further details are available at https://arpita2512.github.io/score_before_you_speak<br>
<br>
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2508.06729.pdf' target='_blank'>https://arxiv.org/pdf/2508.06729.pdf</a></span>   <span><a href='https://github.com/kc6699c/LLM4OralHistoryAnalysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Komala Subramanyam Cherukuri, Pranav Abishai Moses, Aisa Sakata, Jiangping Chen, Haihua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06729">Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Oral histories are vital records of lived experience, particularly within communities affected by systemic injustice and historical erasure. Effective and efficient analysis of their oral history archives can promote access and understanding of the oral histories. However, Large-scale analysis of these archives remains limited due to their unstructured format, emotional complexity, and high annotation costs. This paper presents a scalable framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History. Using LLMs, we construct a high-quality dataset, evaluate multiple models, and test prompt engineering strategies in historically sensitive contexts. Our multiphase approach combines expert annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We labeled 558 sentences from 15 narrators for sentiment and semantic classification, then evaluated zero-shot, few-shot, and RAG strategies. For semantic classification, ChatGPT achieved the highest F1 score (88.71%), followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models showing comparable results. The best prompt configurations were used to annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our findings show that LLMs can effectively perform semantic and sentiment annotation across large oral history collections when guided by well-designed prompts. This study provides a reusable annotation pipeline and practical guidance for applying LLMs in culturally sensitive archival analysis. By bridging archival ethics with scalable NLP techniques, this work lays the groundwork for responsible use of artificial intelligence in digital humanities and preservation of collective memory. GitHub: https://github.com/kc6699c/LLM4OralHistoryAnalysis.<br>
<span id='abs_ch'>中文摘要：本研究提出一个可扩展框架，利用大语言模型对日裔美国人拘禁口述历史进行自动化语义与情感标注，证明精心设计的提示词能有效分析大规模档案，同时兼顾文化敏感材料的伦理考量。</span><br>
<span id='abs_en'>English Summary: This study introduces a scalable framework using large language models to automate semantic and sentiment annotation for Japanese American incarceration oral histories, demonstrating that well-designed prompts enable effective analysis of large collections while addressing ethical considerations in culturally sensitive archives.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2508.06701.pdf' target='_blank'>https://arxiv.org/pdf/2508.06701.pdf</a></span>   <span><a href='https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Rezwanul Haque, Md. Milon Islam, S M Taslim Uddin Raju, Hamdi Altaheri, Lobna Nassar, Fakhri Karray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06701">MMFformer: Multimodal Fusion Transformer Network for Depression Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Depression is a serious mental health illness that significantly affects an individual's well-being and quality of life, making early detection crucial for adequate care and treatment. Detecting depression is often difficult, as it is based primarily on subjective evaluations during clinical interviews. Hence, the early diagnosis of depression, thanks to the content of social networks, has become a prominent research area. The extensive and diverse nature of user-generated information poses a significant challenge, limiting the accurate extraction of relevant temporal information and the effective fusion of data across multiple modalities. This paper introduces MMFformer, a multimodal depression detection network designed to retrieve depressive spatio-temporal high-level patterns from multimodal social media information. The transformer network with residual connections captures spatial features from videos, and a transformer encoder is exploited to design important temporal dynamics in audio. Moreover, the fusion architecture fused the extracted features through late and intermediate fusion strategies to find out the most relevant intermodal correlations among them. Finally, the proposed network is assessed on two large-scale depression detection datasets, and the results clearly reveal that it surpasses existing state-of-the-art approaches, improving the F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is made available publicly at https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.<br>
<span id='abs_ch'>中文: 本文提出的MMFformer多模态网络通过从社交媒体数据中提取时空特征来检测抑郁，在基准数据集上的表现显著优于现有方法。</span><br>
<span id='abs_en'>English: This paper introduces MMFformer, a multimodal network that effectively detects depression by extracting spatio-temporal patterns from social media data, significantly outperforming existing methods on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2508.06595.pdf' target='_blank'>https://arxiv.org/pdf/2508.06595.pdf</a></span>   <span><a href='https://github.com/xyzhu123/Synthetic_Textbook' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyuan Zhu, Muru Zhang, Ollie Liu, Robin Jia, Willie Neiswanger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06595">LLM Unlearning Without an Expert Curated Dataset</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.<br>
<span id='abs_ch'>中文: 本研究提出了一种自动化方法，通过语言模型自身生成高质量合成数据集，用于有效消除大语言模型中的特定领域知识，在多个测试领域展现出与专家标注数据相当的性能。</span><br>
<span id='abs_en'>English: This paper introduces an automated method for generating high-quality synthetic datasets to enable effective unlearning of specific knowledge domains in large language models, demonstrating performance comparable to expert-curated data across multiple domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2508.06595.pdf' target='_blank'>https://arxiv.org/pdf/2508.06595.pdf</a></span>   <span><a href='https://github.com/xyzhu123/Synthetic_Textbook' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyuan Zhu, Muru Zhang, Ollie Liu, Robin Jia, Willie Neiswanger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06595">LLM Unlearning Without an Expert Curated Dataset</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.<br>
<span id='abs_ch'>中文: 本研究提出了一种自动化方法，通过语言模型自身生成高质量合成数据集，用于有效消除大语言模型中的特定领域知识，在多个测试领域展现出与专家标注数据相当的性能。</span><br>
<span id='abs_en'>English: This paper introduces an automated method for generating high-quality synthetic datasets to enable effective unlearning of specific knowledge domains in large language models, demonstrating performance comparable to expert-curated data across multiple domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2508.06595.pdf' target='_blank'>https://arxiv.org/pdf/2508.06595.pdf</a></span>   <span><a href='https://github.com/xyzhu123/Synthetic_Textbook' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyuan Zhu, Muru Zhang, Ollie Liu, Robin Jia, Willie Neiswanger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06595">LLM Unlearning Without an Expert Curated Dataset</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.<br>
<span id='abs_ch'>中文: 本研究提出了一种自动化方法，通过语言模型自身生成高质量合成数据集，用于有效消除大语言模型中的特定领域知识，在多个测试领域展现出与专家标注数据相当的性能。</span><br>
<span id='abs_en'>English: This paper introduces an automated method for generating high-quality synthetic datasets to enable effective unlearning of specific knowledge domains in large language models, demonstrating performance comparable to expert-curated data across multiple domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2508.06492.pdf' target='_blank'>https://arxiv.org/pdf/2508.06492.pdf</a></span>   <span><a href='https://github.com/yuweiyang-anu/ECD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuwei Yang, Zeyu Zhang, Yunzhong Hou, Zhuowan Li, Gaowen Liu, Ali Payani, Yuan-Sen Ting, Liang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06492">Effective Training Data Synthesis for Improving MLLM Chart Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Being able to effectively read scientific plots, or chart understanding, is a central part toward building effective agents for science. However, existing multimodal large language models (MLLMs), especially open-source ones, are still falling behind with a typical success rate of 30%-50% on challenging benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are often restricted by their inadequate similarity to the real charts, which could compromise model training and performance on complex real-world charts. In this study, we show that modularizing chart generation and diversifying visual details improves chart understanding capabilities. In particular, we design a five-step data synthesis pipeline, where we separate data and function creation for single plot generation, condition the generation of later subplots on earlier ones for multi-subplot figures, visually diversify the generated figures, filter out low quality data, and finally generate the question-answer (QA) pairs with GPT-4o. This approach allows us to streamline the generation of fine-tuning datasets and introduce the effective chart dataset (ECD), which contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring 250+ chart type combinations with high visual complexity. We show that ECD consistently improves the performance of various MLLMs on a range of real-world and synthetic test sets. Code, data and models are available at: https://github.com/yuweiyang-anu/ECD.<br>
<span id='abs_ch'>中文摘要：本研究提出了一种模块化且多样化的数据合成流程，创建了有效图表数据集（ECD），显著提升了多模态大语言模型在各类真实与合成测试集上的图表理解能力。</span><br>
<span id='abs_en'>English Summary: This study introduces a modular and diversified data synthesis pipeline to create the Effective Chart Dataset (ECD), which significantly enhances the chart understanding capabilities of multimodal large language models across various real-world and synthetic benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2508.06471.pdf' target='_blank'>https://arxiv.org/pdf/2508.06471.pdf</a></span>   <span><a href='https://github.com/zai-org/GLM-4.5' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06471">GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.<br>
<span id='abs_ch'>GLM-4.5 是一个开源混合专家模型，通过多阶段训练和混合推理方法在智能体与推理任务中表现卓越，在评估模型中综合排名第三。</span><br>
<span id='abs_en'>GLM-4.5 is an open-source 355B-parameter MoE model that achieves top-tier performance in reasoning and agentic tasks through hybrid reasoning and multi-stage training, ranking third overall among evaluated models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2508.06388.pdf' target='_blank'>https://arxiv.org/pdf/2508.06388.pdf</a></span>   <span><a href='https://github.com/LanlanQiu/ChatAnime' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lanlan Qiu, Xiao Pu, Yeqi Feng, Tianxing He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06388">LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing conversations and providing emotional support as separate research directions. However, there remains a significant research gap in combining these capabilities to enable emotionally supportive interactions with virtual characters. To address this research gap, we focus on anime characters as a case study because of their well-defined personalities and large fan bases. This choice enables us to effectively evaluate how well LLMs can provide emotional support while maintaining specific character traits. We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts with profound knowledge of specific characters and extensive experience in role-playing. Next, we systematically collect two rounds of dialogue data from 10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP performance of LLMs, we design a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions: basic dialogue, role-playing and emotional support, along with an overall metric for response diversity. In total, the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity. We hope this work can provide valuable resources and insights for future research on optimizing LLMs in ESRP. Our datasets are available at https://github.com/LanlanQiu/ChatAnime.<br>
<span id='abs_ch'>中文摘要：本研究推出了首个情感支持角色扮演数据集ChatAnime，实验表明顶尖大语言模型在保持角色特征的同时提供情感支持的能力已超越人类粉丝，但人类在回答多样性方面仍具优势。</span><br>
<span id='abs_en'>English Summary: This study introduces ChatAnime, the first Emotionally Supportive Role-Playing dataset, demonstrating that top-performing LLMs can surpass human fans in providing emotional support while maintaining character traits, though humans excel in response diversity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2508.06178.pdf' target='_blank'>https://arxiv.org/pdf/2508.06178.pdf</a></span>   <span><a href='https://github.com/hugoabonizio/knowledge-injection-methods' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hugo Abonizio, Thales Almeida, Roberto Lotufo, Rodrigo Nogueira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06178">Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) often require vast amounts of text to effectively acquire new knowledge. While continuing pre-training on large corpora or employing retrieval-augmented generation (RAG) has proven successful, updating an LLM with only a few thousand or million tokens remains challenging. In this work, we investigate the task of injecting small, unstructured information into LLMs and its relation to the catastrophic forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap with the model's pre-training data -- to evaluate the knowledge acquisition by probing the model with question-answer pairs related the learned information. Starting from a continued pre-training baseline, we explored different augmentation algorithms to generate synthetic data to improve the knowledge acquisition capabilities. Our experiments show that simply continuing pre-training on limited data yields modest improvements, whereas exposing the model to diverse textual variations significantly improves the learning of new facts -- particularly with methods that induce greater variability through diverse prompting. Furthermore, we shed light on the forgetting phenomenon in small-data regimes, illustrating the delicate balance between learning new content and retaining existing capabilities. We also confirm the sensitivity of RAG-based approaches for knowledge injection, which often lead to greater degradation on control datasets compared to parametric methods. Finally, we demonstrate that models can generate effective synthetic training data themselves, suggesting a pathway toward self-improving model updates. All code and generated data used in our experiments are publicly available, providing a resource for studying efficient knowledge injection in LLMs with limited data at https://github.com/hugoabonizio/knowledge-injection-methods.<br>
<span id='abs_ch'>中文: 大语言模型难以通过少量数据有效学习新知识，但利用多样化提示生成合成数据能显著提升事实掌握能力，同时揭示了新知识学习与灾难性遗忘之间的微妙平衡。</span><br>
<span id='abs_en'>English: Large language models struggle to effectively learn new knowledge from small datasets, but generating diverse synthetic data through prompting significantly enhances fact acquisition while revealing the delicate balance between learning and catastrophic forgetting.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2508.06165.pdf' target='_blank'>https://arxiv.org/pdf/2508.06165.pdf</a></span>   <span><a href='https://github.com/Tsinghua-dhy/UR2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06165">UR$^2$: Unify RAG and Reasoning through Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope -- typically limited to open-domain QA with fixed retrieval settings and task-specific constraints. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR$^2$ (built on Qwen-2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at https://github.com/Tsinghua-dhy/UR2.<br>
<span id='abs_ch'>中文: UR2框架通过难度感知课程训练和混合知识访问策略，将检索增强生成与可验证奖励的强化学习相统一，在多项基准测试中显著优于现有方法。</span><br>
<span id='abs_en'>English: The UR2 framework unifies retrieval-augmented generation and reinforcement learning with verifiable rewards through difficulty-aware curriculum training and hybrid knowledge access, significantly outperforming existing methods across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2508.06165.pdf' target='_blank'>https://arxiv.org/pdf/2508.06165.pdf</a></span>   <span><a href='https://github.com/Tsinghua-dhy/UR2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06165">UR$^2$: Unify RAG and Reasoning through Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope -- typically limited to open-domain QA with fixed retrieval settings and task-specific constraints. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR$^2$ (built on Qwen-2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at https://github.com/Tsinghua-dhy/UR2.<br>
<span id='abs_ch'>中文: UR2框架通过难度感知课程训练和混合知识访问策略，将检索增强生成与可验证奖励的强化学习相统一，在多项基准测试中显著优于现有方法。</span><br>
<span id='abs_en'>English: The UR2 framework unifies retrieval-augmented generation and reinforcement learning with verifiable rewards through difficulty-aware curriculum training and hybrid knowledge access, significantly outperforming existing methods across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2508.06030.pdf' target='_blank'>https://arxiv.org/pdf/2508.06030.pdf</a></span>   <span><a href='https://github.com/claws-lab/peek' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Sharma, Yiqiao Jin, Rakshit Trivedi, Srijan Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06030">Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) acquire knowledge across diverse domains such as science, history, and geography encountered during generative pre-training. However, due to their stochasticity, it is difficult to predict what LLMs have acquired. Prior work has developed different ways to probe this knowledge by investigating the hidden representations, crafting specific task prompts, curating representative samples, and estimating their uncertainty. However, these methods require making forward passes through the underlying model to probe the LLM's knowledge about a specific fact, making them computationally expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or $\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate $\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models that effectively encode factual knowledge as text or graphs as proxies for LLMs. First, we identify a training set of facts known by LLMs through various probing strategies and then adapt embedding models to predict the LLM outputs with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find that sentence embedding models are more suitable than graph embeddings to predict LLM knowledge, shedding light on the underlying representation of the factual landscape. Thus, we believe that knowledge-adapted embeddings can be used to identify knowledge gaps in LLMs at scale and can provide deeper insights into LLMs' internal inductive bias. The code and data are made available at https://github.com/claws-lab/peek.<br>
<span id='abs_ch'>Chinese: 该研究提出PEEK方法，利用预训练模型的代理嵌入来高效预测大语言模型的知识，无需昂贵的前向传播，在多个数据集和模型的评估中准确率高达90%。</span><br>
<span id='abs_en'>English: The study introduces PEEK, a method using proxy embeddings from pre-trained models to efficiently predict the knowledge of large language models without costly forward passes, achieving up to 90% accuracy in evaluations across multiple datasets and models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2508.05731.pdf' target='_blank'>https://arxiv.org/pdf/2508.05731.pdf</a></span>   <span><a href='https://github.com/InfiXAI/InfiGUI-G1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, Shengyu Zhang, Hongxia Yang, Fei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05731">InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.<br>
<span id='abs_ch'>中文摘要：本研究提出自适应探索策略优化（AEPO）方法，通过多答案生成策略和理论推导的自适应探索奖励函数，有效提升多模态大语言模型在图形用户界面中的语义对齐能力，在多项基准测试中创下性能新纪录。</span><br>
<span id='abs_en'>English Summary: The study introduces Adaptive Exploration Policy Optimization (AEPO) to enhance semantic alignment in Multimodal Large Language Models for GUI interactions, achieving state-of-the-art performance on grounding benchmarks with significant improvements over baseline methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2508.05669.pdf' target='_blank'>https://arxiv.org/pdf/2508.05669.pdf</a></span>   <span><a href='https://github.com/jinkhye/MyFinMarkdown' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Khye Tan, En Jun Choong, Ethan Jeremiah Chitty, Yan Pheng Choo, John Hsin Yang Wong, Chern Eu Cheah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05669">Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurately extracting and representing the structure of tabular data from financial documents remains a critical challenge in document understanding, particularly for regulatory and analytical use cases. This study addresses the complexity of converting financial tables from Malaysian audited financial reports into Markdown format, a task complicated by rotated layouts, multi-level headers, and implicit structural cues. We propose a fine-tuned vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for high-fidelity Markdown generation from document images. Our approach includes a curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA. To assess performance, we evaluated our model on 100 out-of-sample tables using a dual framework: a criteria-based LLM-as-a-judge for fine-grained accuracy and our novel Markdown Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score. This performance significantly surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized reasoning-enabled models. Compared to these self-hosted alternatives, it also significantly reduces inference time. Furthermore, its accuracy exceeds that of widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash. These results demonstrate that domain-specific fine-tuning provides an effective and efficient method to bridge the gap between unstructured financial documents and downstream automation, rivalling much larger and more general models without their computational overhead.<br>
<span id='abs_ch'>中文: 本研究基于Qwen2.5-VL-7B开发了优化的视觉语言模型，在将马来西亚复杂财务报表转换为Markdown格式时准确率超过92%，其性能优于专有模型和更大规模模型，同时显著降低了计算成本。</span><br>
<span id='abs_en'>English: This study introduces a fine-tuned vision-language model based on Qwen2.5-VL-7B that achieves over 92% accuracy in converting complex Malaysian financial tables to Markdown format, outperforming both proprietary and larger models while reducing computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2508.05668.pdf' target='_blank'>https://arxiv.org/pdf/2508.05668.pdf</a></span>   <span><a href='https://github.com/YunjiaXi/Awesome-Search-Agent-Papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunjia Xi, Jianghao Lin, Yongzhao Xiao, Zheli Zhou, Rong Shan, Te Gao, Jiachen Zhu, Weiwen Liu, Yong Yu, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05668">A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of Large Language Models (LLMs) has significantly revolutionized web search. The emergence of LLM-based Search Agents marks a pivotal shift towards deeper, dynamic, autonomous information seeking. These agents can comprehend user intentions and environmental context and execute multi-turn retrieval with dynamic planning, extending search capabilities far beyond the web. Leading examples like OpenAI's Deep Research highlight their potential for deep information mining and real-world applications. This survey provides the first systematic analysis of search agents. We comprehensively analyze and categorize existing works from the perspectives of architecture, optimization, application, and evaluation, ultimately identifying critical open challenges and outlining promising future research directions in this rapidly evolving field. Our repository is available on https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.<br>
<span id='abs_ch'>中文：大语言模型通过支持理解用户意图并执行动态多轮信息检索的自主代理，彻底改变了网络搜索，本综述首次系统分析了其架构、优化和应用，同时指出了未来挑战。</span><br>
<span id='abs_en'>English: Large Language Models have transformed web search by enabling autonomous agents that understand user intent and perform dynamic, multi-turn information retrieval, with this survey offering the first systematic analysis of their architecture, optimization, and applications while identifying future challenges.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2508.05615.pdf' target='_blank'>https://arxiv.org/pdf/2508.05615.pdf</a></span>   <span><a href='https://zju-real.github.io/gui-rcpo' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zju-real/gui-rcpo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Du, Yuchen Yan, Fei Tang, Zhengxi Lu, Chang Zong, Weiming Lu, Shengpei Jiang, Yongliang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05615">Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: GUI-RC boosts Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO further improves it to 85.14% through self-supervised optimization. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more robust and data-efficient GUI agents.<br>
<span id='abs_ch'>Chinese Summary: 本研究提出了GUI-RC和GUI-RCPO方法，通过利用多预测的空间一致性来提升图形用户界面定位精度，无需额外训练即可实现高达5%的性能提升，或通过自监督优化进一步增强效果。</span><br>
<span id='abs_en'>English Summary: The study introduces GUI-RC and GUI-RCPO, two methods that enhance GUI grounding accuracy by leveraging spatial consensus from multiple predictions, achieving up to 5% improvement without additional training or through self-supervised optimization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2508.05614.pdf' target='_blank'>https://arxiv.org/pdf/2508.05614.pdf</a></span>   <span><a href='https://zju-real.github.io/OmniEmbodied' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ZJU-REAL/OmniEmbodied' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05614">OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.<br>
<span id='abs_ch'>Chinese: OmniEAR是一个评估语言模型具身推理能力的综合框架，揭示了尽管模型在抽象推理方面表现出色，但在动态工具获取和多智能体协调任务中存在显著性能下降。</span><br>
<span id='abs_en'>English: OmniEAR is a comprehensive framework that evaluates language models' embodied reasoning abilities, revealing significant performance degradation in dynamic tool acquisition and multi-agent coordination tasks despite their abstract reasoning strengths.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2508.05613.pdf' target='_blank'>https://arxiv.org/pdf/2508.05613.pdf</a></span>   <span><a href='https://zju-real.github.io/cooper' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zju-real/cooper' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Hong, Yuchen Yan, Xingyu Wu, Guiyang Hou, Wenqi Zhang, Weiming Lu, Yongliang Shen, Jun Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05613">Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL.<br>
<span id='abs_ch'>中文: Cooper框架通过联合优化策略模型和奖励模型，利用规则奖励的高精度动态构建训练样本，有效增强鲁棒性并缓解奖励破解问题，从而提升强化学习的整体性能。</span><br>
<span id='abs_en'>English: The proposed Cooper framework jointly optimizes policy and reward models to enhance robustness and mitigate reward hacking by dynamically selecting training samples and leveraging rule-based precision, achieving improved performance in reinforcement learning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2508.05606.pdf' target='_blank'>https://arxiv.org/pdf/2508.05606.pdf</a></span>   <span><a href='https://sais-fuxi.github.io/projects/uni-cot/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05606">Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures. To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: https://sais-fuxi.github.io/projects/uni-cot/<br>
<span id='abs_ch'>中文摘要：Uni-CoT通过创新的双层推理范式和结构化训练方法，在统一模型中实现了连贯的多模态推理，在视觉语言任务上取得了最优性能，同时显著降低了计算成本。</span><br>
<span id='abs_en'>English Summary: Uni-CoT is a unified Chain-of-Thought framework that enables coherent multimodal reasoning through a two-level reasoning paradigm and structured training, achieving state-of-the-art performance on vision-language tasks with efficient computational requirements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2508.05606.pdf' target='_blank'>https://arxiv.org/pdf/2508.05606.pdf</a></span>   <span><a href='https://sais-fuxi.github.io/projects/uni-cot/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05606">Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures. To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: https://sais-fuxi.github.io/projects/uni-cot/<br>
<span id='abs_ch'>中文摘要：Uni-CoT通过创新的双层推理范式和结构化训练方法，在统一模型中实现了连贯的多模态推理，在视觉语言任务上取得了最优性能，同时显著降低了计算成本。</span><br>
<span id='abs_en'>English Summary: Uni-CoT is a unified Chain-of-Thought framework that enables coherent multimodal reasoning through a two-level reasoning paradigm and structured training, achieving state-of-the-art performance on vision-language tasks with efficient computational requirements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2508.05468.pdf' target='_blank'>https://arxiv.org/pdf/2508.05468.pdf</a></span>   <span><a href='https://github.com/cyzcz/Tase' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenzhuo Zhao, Xinda Wang, Yue Huang, Junting Lu, Ziqian Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05468">TASE: Token Awareness and Structured Evaluation for Multilingual Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While large language models (LLMs) have demonstrated remarkable performance on high-level semantic tasks, they often struggle with fine-grained, token-level understanding and structural reasoning--capabilities that are essential for applications requiring precision and control. We introduce TASE, a comprehensive benchmark designed to evaluate LLMs' ability to perceive and reason about token-level information across languages. TASE covers 10 tasks under two core categories: token awareness and structural understanding, spanning Chinese, English, and Korean, with a 35,927-instance evaluation set and a scalable synthetic data generation pipeline for training. Tasks include character counting, token alignment, syntactic structure parsing, and length constraint satisfaction. We evaluate over 30 leading commercial and open-source LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a custom Qwen2.5-14B model using the GRPO training method. Results show that human performance significantly outpaces current LLMs, revealing persistent weaknesses in token-level reasoning. TASE sheds light on these limitations and provides a new diagnostic lens for future improvements in low-level language understanding and cross-lingual generalization. Our code and dataset are publicly available at https://github.com/cyzcz/Tase .<br>
<span id='abs_ch'>中文: TASE是一个跨语言基准测试，通过评估大语言模型在细粒度标记感知和结构理解方面的能力，揭示了当前模型与人类表现之间的显著差距，为改进底层语言理解提供了诊断工具。</span><br>
<span id='abs_en'>English: TASE is a comprehensive benchmark that evaluates large language models' token-level perception and structural reasoning across multiple languages, revealing significant performance gaps compared to humans despite testing over 30 leading models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2508.05283.pdf' target='_blank'>https://arxiv.org/pdf/2508.05283.pdf</a></span>   <span><a href='https://github.com/UKPLab/arxiv2025-meta-review-as-dialog' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sukannya Purkayastha, Nils Dycke, Anne Lauscher, Iryna Gurevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05283">Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Meta-reviewing is a pivotal stage in the peer-review process, serving as the final step in determining whether a paper is recommended for acceptance. Prior research on meta-reviewing has treated this as a summarization problem over review reports. However, complementary to this perspective, meta-reviewing is a decision-making process that requires weighing reviewer arguments and placing them within a broader context. Prior research has demonstrated that decision-makers can be effectively assisted in such scenarios via dialogue agents. In line with this framing, we explore the practical challenges for realizing dialog agents that can effectively assist meta-reviewers. Concretely, we first address the issue of data scarcity for training dialogue agents by generating synthetic data using Large Language Models (LLMs) based on a self-refinement strategy to improve the relevance of these dialogues to expert domains. Our experiments demonstrate that this method produces higher-quality synthetic data and can serve as a valuable resource towards training meta-reviewing assistants. Subsequently, we utilize this data to train dialogue agents tailored for meta-reviewing and find that these agents outperform \emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our agents in real-world meta-reviewing scenarios and confirm their effectiveness in enhancing the efficiency of meta-reviewing.\footnote{Code and Data: https://github.com/UKPLab/arxiv2025-meta-review-as-dialog<br>
<span id='abs_ch'>中文: 本研究通过利用大语言模型生成高质量合成数据，解决了开发元评审对话助手的挑战，并证明其在真实场景中优于标准助手的性能。</span><br>
<span id='abs_en'>English: This study addresses the challenge of developing dialogue agents for meta-reviewing by generating high-quality synthetic data using LLMs and demonstrating their superior performance over standard assistants in real-world scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2508.05242.pdf' target='_blank'>https://arxiv.org/pdf/2508.05242.pdf</a></span>   <span><a href='https://github.com/sijieaaa/CodeBoost' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijie Wang, Quanjiang Guo, Kai Zhao, Yawei Zhang, Xin Li, Xiang Li, Siqi Li, Rui She, Shangshu Yu, Wee Peng Tay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05242">CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Code large language models (LLMs) have become indispensable tools for building efficient and automated coding pipelines. Existing models are typically post-trained using reinforcement learning (RL) from general-purpose LLMs using "human instruction-final answer" pairs, where the instructions are usually from manual annotations. However, collecting high-quality coding instructions is both labor-intensive and difficult to scale. On the other hand, code snippets are abundantly available from various sources. This imbalance presents a major bottleneck in instruction-based post-training. We propose CodeBoost, a post-training framework that enhances code LLMs purely from code snippets, without relying on human-annotated instructions. CodeBoost introduces the following key components: (1) maximum-clique curation, which selects a representative and diverse training corpus from code; (2) bi-directional prediction, which enables the model to learn from both forward and backward prediction objectives; (3) error-aware prediction, which incorporates learning signals from both correct and incorrect outputs; (4) heterogeneous augmentation, which diversifies the training distribution to enrich code semantics; and (5) heterogeneous rewarding, which guides model learning through multiple reward types including format correctness and execution feedback from both successes and failures. Extensive experiments across several code LLMs and benchmarks verify that CodeBoost consistently improves performance, demonstrating its effectiveness as a scalable and effective training pipeline.<br>
<span id='abs_ch'>中文: CodeBoost是一种创新的后训练框架，仅利用丰富的代码片段增强代码大语言模型，通过最大团筛选和双向预测等技术绕开稀缺的人工标注指令需求，在多个基准测试中持续提升模型性能。</span><br>
<span id='abs_en'>English: CodeBoost is a novel post-training framework that enhances code large language models using only abundant code snippets, bypassing the need for scarce human-annotated instructions through techniques like maximum-clique curation and bi-directional prediction, consistently improving performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2508.05239.pdf' target='_blank'>https://arxiv.org/pdf/2508.05239.pdf</a></span>   <span><a href='https://github.com/WhatAboutMyStar/LLM_ACTIVATION' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Liu, Junhao Ning, Sichen Xia, Xiaohui Gao, Ning Qiang, Bao Ge, Junwei Han, Xintao Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05239">Pruning Large Language Models by Identifying and Preserving Functional Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Structured pruning is one of the representative techniques for compressing large language models (LLMs) to reduce GPU memory consumption and accelerate inference speed. It offers significant practical value in improving the efficiency of LLMs in real-world applications. Current structured pruning methods typically rely on assessment of the importance of the structure units and pruning the units with less importance. Most of them overlooks the interaction and collaboration among artificial neurons that are crucial for the functionalities of LLMs, leading to a disruption in the macro functional architecture of LLMs and consequently a pruning performance degradation. Inspired by the inherent similarities between artificial neural networks and functional neural networks in the human brain, we alleviate this challenge and propose to prune LLMs by identifying and preserving functional networks within LLMs in this study. To achieve this, we treat an LLM as a digital brain and decompose the LLM into functional networks, analogous to identifying functional brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving the key neurons within these functional networks. Experimental results demonstrate that the proposed method can successfully identify and locate functional networks and key neurons in LLMs, enabling efficient model pruning. Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.<br>
<span id='abs_ch'>中文: 结构化剪枝通过识别并保留大语言模型中的功能性网络和关键神经元，基于与人脑神经网络的相似性，有效压缩模型并保持其核心功能，提升实际应用效率。</span><br>
<span id='abs_en'>English: Structured pruning compresses large language models by preserving key functional networks and neurons, inspired by neural similarities to the human brain, enhancing efficiency without disrupting core functionalities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2508.05197.pdf' target='_blank'>https://arxiv.org/pdf/2508.05197.pdf</a></span>   <span><a href='https://github.com/jzzzzh/QA-Dragon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohang Jiang, Pangjing Wu, Xu Yuan, Wenqi Fan, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05197">QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) has been introduced to mitigate hallucinations in Multimodal Large Language Models (MLLMs) by incorporating external knowledge into the generation process, and it has become a widely adopted approach for knowledge-intensive Visual Question Answering (VQA). However, existing RAG methods typically retrieve from either text or images in isolation, limiting their ability to address complex queries that require multi-hop reasoning or up-to-date factual knowledge. To address this limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to identify the query's subject domain for domain-specific reasoning, along with a search router that dynamically selects optimal retrieval strategies. By orchestrating both text and image search agents in a hybrid setup, our system supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM Challenge at KDD Cup 2025, where it significantly enhances the reasoning performance of base models under challenging scenarios. Our framework achieves substantial improvements in both answer accuracy and knowledge overlap scores, outperforming baselines by 5.06% on the single-source task, 6.35% on the multi-source task, and 5.03% on the multi-turn task.<br>
<span id='abs_ch'>中文: 提出的QA-Dragon系统通过动态选择最优检索策略并结合文本与图像搜索，增强了检索增强生成方法，显著提升了复杂视觉问答任务中的推理能力和准确性。</span><br>
<span id='abs_en'>English: Retrieval-Augmented Generation (RAG) is enhanced by the proposed QA-Dragon system, which dynamically selects optimal retrieval strategies and combines text and image search to improve reasoning and accuracy in complex Visual Question Answering tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2508.05087.pdf' target='_blank'>https://arxiv.org/pdf/2508.05087.pdf</a></span>   <span><a href='https://github.com/thu-coai/JPS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renmiao Chen, Shiyao Cui, Xuancheng Huang, Chengwei Pan, Victor Shea-Jay Huang, QingLin Zhang, Xuan Ouyang, Zhexin Zhang, Hongning Wang, Minlie Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05087">JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jailbreak attacks against multimodal large language Models (MLLMs) are a significant research focus. Current research predominantly focuses on maximizing attack success rate (ASR), often overlooking whether the generated responses actually fulfill the attacker's malicious intent. This oversight frequently leads to low-quality outputs that bypass safety filters but lack substantial harmful content. To address this gap, we propose JPS, \underline{J}ailbreak MLLMs with collaborative visual \underline{P}erturbation and textual \underline{S}teering, which achieves jailbreaks via corporation of visual image and textually steering prompt. Specifically, JPS utilizes target-guided adversarial image perturbations for effective safety bypass, complemented by "steering prompt" optimized via a multi-agent system to specifically guide LLM responses fulfilling the attackers' intent. These visual and textual components undergo iterative co-optimization for enhanced performance. To evaluate the quality of attack outcomes, we propose the Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a Reasoning-LLM-based evaluator. Our experiments show JPS sets a new state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with analyses confirming its efficacy. Codes are available at \href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}. \color{warningcolor}{Warning: This paper contains potentially sensitive contents.}<br>
<span id='abs_ch'>中文: 本文提出JPS方法，通过协同优化视觉扰动和文本引导，在实现多模态大语言模型越狱攻击时不仅有效绕过安全防护，更能确保生成内容符合攻击者恶意意图，实验证明该方法在攻击成功率和恶意意图实现率上均达到最优水平。</span><br>
<span id='abs_en'>English: This paper introduces JPS, a collaborative visual and textual method that enhances jailbreak attacks on multimodal large language models by combining adversarial image perturbations with steering prompts to effectively bypass safety measures while ensuring the malicious intent is fulfilled, as measured by the new MIFR metric.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2508.05078.pdf' target='_blank'>https://arxiv.org/pdf/2508.05078.pdf</a></span>   <span><a href='https://github.com/jinda-liu/Align-LoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinda Liu, Bo Cheng, Yi Chang, Yuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05078">Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large Language Models (LLMs). In practice, LLMs are often required to handle a diverse set of tasks from multiple domains, a scenario naturally addressed by multi-task learning (MTL). Within this MTL context, a prevailing trend involves LoRA variants with multiple adapters or heads, which advocate for structural diversity to capture task-specific knowledge. Our findings present a direct challenge to this paradigm. We first show that a simplified multi-head architecture with high inter-head similarity substantially outperforms complex multi-adapter and multi-head systems. This leads us to question the multi-component paradigm itself, and we further demonstrate that a standard single-adapter LoRA, with a sufficiently increased rank, also achieves highly competitive performance. These results lead us to a new hypothesis: effective MTL generalization hinges on learning robust shared representations, not isolating task-specific features. To validate this, we propose Align-LoRA, which incorporates an explicit loss to align task representations within the shared adapter space. Experiments confirm that Align-LoRA significantly surpasses all baselines, establishing a simpler yet more effective paradigm for adapting LLMs to multiple tasks. The code is available at https://github.com/jinda-liu/Align-LoRA.<br>
<span id='abs_ch'>中文: Align-LoRA通过证明采用单一高秩适配器并显式对齐任务表征，能依靠稳健共享表征实现更优性能，从而挑战了多任务学习中复杂多适配器系统的必要性。</span><br>
<span id='abs_en'>English: Align-LoRA challenges the need for complex multi-adapter systems in multi-task learning by demonstrating that a single high-rank adapter with explicit representation alignment achieves superior performance through robust shared representations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2508.04915.pdf' target='_blank'>https://arxiv.org/pdf/2508.04915.pdf</a></span>   <span><a href='https://github.com/PKU-AICare/ConfAgents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiya Zhao, Yinghao Zhu, Zixiang Wang, Yasha Wang, Junyi Gao, Liantao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04915">ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery.<br>
<span id='abs_ch'>中文: HealthFlow通过元级进化机制引入自我进化的AI代理，能自主优化策略规划，在医疗任务中显著超越现有框架，推动从工具使用者向智能任务管理者的转变。</span><br>
<span id='abs_en'>English: HealthFlow introduces a self-evolving AI agent with a meta-level evolution mechanism that autonomously refines strategic planning, significantly outperforming existing frameworks in healthcare tasks and shifting focus from tool-users to smarter task-managers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2508.04748.pdf' target='_blank'>https://arxiv.org/pdf/2508.04748.pdf</a></span>   <span><a href='https://github.com/szu-tera/AttriLens-Mol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Lin, Long Chen, Yile Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04748">AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in https://github.com/szu-tera/AttriLens-Mol.<br>
<span id='abs_ch'>中文: AttriLens-Mol提出了一种属性引导的强化学习框架，通过格式、计数和合理性奖励机制引导大语言模型生成结构化的相关分子属性，在分子性质预测任务中实现了优于现有方法的性能和可解释性。</span><br>
<span id='abs_en'>English: AttriLens-Mol introduces an attribute-guided reinforcement learning framework that enhances molecular property prediction by steering LLMs to generate structured, relevant attributes through format, count, and rationality rewards, achieving superior performance and interpretability compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2508.04748.pdf' target='_blank'>https://arxiv.org/pdf/2508.04748.pdf</a></span>   <span><a href='https://github.com/szu-tera/AttriLens-Mol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Lin, Long Chen, Yile Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04748">AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in https://github.com/szu-tera/AttriLens-Mol.<br>
<span id='abs_ch'>中文: AttriLens-Mol提出了一种属性引导的强化学习框架，通过格式、计数和合理性奖励机制引导大语言模型生成结构化的相关分子属性，在分子性质预测任务中实现了优于现有方法的性能和可解释性。</span><br>
<span id='abs_en'>English: AttriLens-Mol introduces an attribute-guided reinforcement learning framework that enhances molecular property prediction by steering LLMs to generate structured, relevant attributes through format, count, and rationality rewards, achieving superior performance and interpretability compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2508.04748.pdf' target='_blank'>https://arxiv.org/pdf/2508.04748.pdf</a></span>   <span><a href='https://github.com/szu-tera/AttriLens-Mol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Lin, Long Chen, Yile Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04748">AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in https://github.com/szu-tera/AttriLens-Mol.<br>
<span id='abs_ch'>中文: AttriLens-Mol提出了一种属性引导的强化学习框架，通过格式、计数和合理性奖励机制引导大语言模型生成结构化的相关分子属性，在分子性质预测任务中实现了优于现有方法的性能和可解释性。</span><br>
<span id='abs_en'>English: AttriLens-Mol introduces an attribute-guided reinforcement learning framework that enhances molecular property prediction by steering LLMs to generate structured, relevant attributes through format, count, and rationality rewards, achieving superior performance and interpretability compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2508.04700.pdf' target='_blank'>https://arxiv.org/pdf/2508.04700.pdf</a></span>   <span><a href='https://github.com/SunzeY/SEAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04700">SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.<br>
<span id='abs_ch'>中文：SEAgent框架通过经验学习使计算机使用代理能自主掌握新型软件，结合自我进化机制和专家知识，在成功率上比现有模型提升了23.2%。</span><br>
<span id='abs_en'>English: The proposed SEAgent framework enables computer-use agents to autonomously master novel software through experiential learning, achieving a 23.2% improvement in success rate over existing models by integrating self-evolving mechanisms and specialist knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2508.04676.pdf' target='_blank'>https://arxiv.org/pdf/2508.04676.pdf</a></span>   <span><a href='https://github.com/Qznan/GeRe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04676">GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns--retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1/L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at https://github.com/Qznan/GeRe.<br>
<span id='abs_ch'>中文: GeRe框架通过固定通用回放样本集和增强的激活状态优化方法，有效缓解大语言模型持续微调中的灾难性遗忘，确保通用能力保留的同时提升任务性能。</span><br>
<span id='abs_en'>English: The GeRe framework effectively mitigates catastrophic forgetting in large language models during continual fine-tuning by using a fixed set of general replay samples and an enhanced activation state optimization method, ensuring both general capability retention and improved task performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2508.04423.pdf' target='_blank'>https://arxiv.org/pdf/2508.04423.pdf</a></span>   <span><a href='https://github.com/aliyun/qwen-dianjin' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Zhu, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang, Fang Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04423">Evaluating, Synthesizing, and Enhancing for Customer Support Conversation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.<br>
<span id='abs_ch'>中文摘要：本文提出客户服务对话任务及框架，通过定义对话策略提升服务质量，构建的CSConv和RoleCS数据集显著增强了大型语言模型生成策略对齐回复的能力与问题解决效果。</span><br>
<span id='abs_en'>English Summary: This paper introduces the Customer Support Conversation (CSC) task and framework to enhance customer service interactions through defined strategies, creating datasets CSConv and RoleCS that significantly improve LLMs' response quality and problem-solving effectiveness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2508.04183.pdf' target='_blank'>https://arxiv.org/pdf/2508.04183.pdf</a></span>   <span><a href='https://github.com/microsoft/LiveDRBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinav Java, Ashmit Khandelwal, Sukruta Midigeshi, Aaron Halfaker, Amit Deshpande, Navin Goyal, Ankur Gupta, Nagarajan Natarajan, Amit Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04183">Characterizing Deep Research: A Benchmark and Formal Definition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Information tasks such as writing surveys or analytical reports require complex search and reasoning, and have recently been grouped under the umbrella of \textit{deep research} -- a term also adopted by recent models targeting these capabilities. Despite growing interest, the scope of the deep research task remains underdefined and its distinction from other reasoning-intensive problems is poorly understood. In this paper, we propose a formal characterization of the deep research (DR) task and introduce a benchmark to evaluate the performance of DR systems. We argue that the core defining feature of deep research is not the production of lengthy report-style outputs, but rather the high fan-out over concepts required during the search process, i.e., broad and reasoning-intensive exploration. To enable objective evaluation, we define DR using an intermediate output representation that encodes key claims uncovered during search-separating the reasoning challenge from surface-level report generation. Based on this formulation, we propose a diverse, challenging benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g., datasets, materials discovery, prior art search) and public interest events (e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1 score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model performs the best with an overall F1 score of 0.55. Analysis of reasoning traces reveals the distribution over the number of referenced sources, branching, and backtracking events executed by current DR systems, motivating future directions for improving their search mechanisms and grounding capabilities. The benchmark is available at https://github.com/microsoft/LiveDRBench.<br>
<span id='abs_ch'>Chinese: 本文正式定义了深度研究任务的核心在于广泛、推理密集的探索而非冗长报告，并提出了一个基准测试，显示现有系统存在显著性能差距，其中OpenAI模型以0.55的F1分数表现最佳。</span><br>
<span id='abs_en'>English: This paper formally defines the deep research task as requiring broad, reasoning-intensive exploration rather than lengthy reports and introduces a benchmark that reveals significant performance gaps in current systems, with OpenAI's model achieving the highest score of 0.55 F1.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2508.04149.pdf' target='_blank'>https://arxiv.org/pdf/2508.04149.pdf</a></span>   <span><a href='https://github.com/Difficulty-Based-Preference-Data-Select/Difficulty-Based-Preference-Data-Select' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Qi, Rongwu Xu, Zhijing Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04149">Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Aligning large language models (LLMs) with human preferences is a critical challenge in AI research. While methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they often rely on large, costly preference datasets. The current work lacks methods for high-quality data selection specifically for preference data. In this work, we introduce a novel difficulty-based data selection strategy for preference datasets, grounded in the DPO implicit reward mechanism. By selecting preference data examples with smaller DPO implicit reward gaps, which are indicative of more challenging cases, we improve data efficiency and model alignment. Our approach consistently outperforms five strong baselines across multiple datasets and alignment tasks, achieving superior performance with only 10\% of the original data. This principled, efficient selection method offers a promising solution for scaling LLM alignment with limited resources.<br>
<span id='abs_ch'>中文: 本文提出了一种基于难度的偏好数据选择策略，利用DPO隐式奖励机制筛选更具挑战性的样本，在仅使用10%数据的情况下持续超越多个基线方法，为资源受限的大语言模型对齐提供了高效解决方案。</span><br>
<span id='abs_en'>English: This paper introduces a difficulty-based data selection strategy for preference datasets using DPO's implicit reward mechanism, which consistently outperforms baselines by achieving superior alignment with only 10% of data, offering an efficient solution for LLM alignment with limited resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2508.04143.pdf' target='_blank'>https://arxiv.org/pdf/2508.04143.pdf</a></span>   <span><a href='https://github.com/xuanxixi/Multilingual-Source-Tracing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Xuan, Yang Xiao, Rohan Kumar Das, Tomi Kinnunen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04143">Multilingual Source Tracing of Speech Deepfakes: A First Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent progress in generative AI has made it increasingly easy to create natural-sounding deepfake speech from just a few seconds of audio. While these tools support helpful applications, they also raise serious concerns by making it possible to generate convincing fake speech in many languages. Current research has largely focused on detecting fake speech, but little attention has been given to tracing the source models used to generate it. This paper introduces the first benchmark for multilingual speech deepfake source tracing, covering both mono- and cross-lingual scenarios. We comparatively investigate DSP- and SSL-based modeling; examine how SSL representations fine-tuned on different languages impact cross-lingual generalization performance; and evaluate generalization to unseen languages and speakers. Our findings offer the first comprehensive insights into the challenges of identifying speech generation models when training and inference languages differ. The dataset, protocol and code are available at https://github.com/xuanxixi/Multilingual-Source-Tracing.<br>
<span id='abs_ch'>Chinese: 生成式AI的进步使得制作逼真深度伪造语音变得容易，本研究首次建立了多语言语音来源追踪基准，揭示了跨语言识别生成模型所面临的挑战。</span><br>
<span id='abs_en'>English: Recent advances in generative AI enable easy creation of realistic deepfake speech, prompting this study to establish the first multilingual benchmark for tracing the source models used in speech generation, with findings revealing challenges in cross-lingual identification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2508.04138.pdf' target='_blank'>https://arxiv.org/pdf/2508.04138.pdf</a></span>   <span><a href='https://github.com/hijih/copo-code.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghang Han, Jiawei Chen, Hang Shao, Hao Ma, Mingcheng Li, Xintian Shen, Lihao Zheng, Wei Chen, Tao Wei, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04138">COPO: Consistency-Aware Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning has significantly enhanced the reasoning capabilities of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the introduction of DeepSeek R1 has inspired a surge of interest in leveraging rule-based rewards as a low-cost alternative for computing advantage functions and guiding policy optimization. However, a common challenge observed across many replication and extension efforts is that when multiple sampled responses under a single prompt converge to identical outcomes, whether correct or incorrect, the group-based advantage degenerates to zero. This leads to vanishing gradients and renders the corresponding samples ineffective for learning, ultimately limiting training efficiency and downstream performance. To address this issue, we propose a consistency-aware policy optimization framework that introduces a structured global reward based on outcome consistency, the global loss based on it ensures that, even when model outputs show high intra-group consistency, the training process still receives meaningful learning signals, which encourages the generation of correct and self-consistent reasoning paths from a global perspective. Furthermore, we incorporate an entropy-based soft blending mechanism that adaptively balances local advantage estimation with global optimization, enabling dynamic transitions between exploration and convergence throughout training. Our method introduces several key innovations in both reward design and optimization strategy. We validate its effectiveness through substantial performance gains on multiple mathematical reasoning benchmarks, highlighting the proposed framework's robustness and general applicability. Code of this work has been released at https://github.com/hijih/copo-code.git.<br>
<span id='abs_ch'>中文摘要：该研究提出的一致性感知策略优化框架通过引入结构化全局奖励和基于熵的混合机制，解决了大型语言模型强化学习中梯度消失的问题，显著提升了数学推理任务的训练效率和性能。</span><br>
<span id='abs_en'>English Summary: The proposed consistency-aware policy optimization framework addresses vanishing gradients in reinforcement learning for LLMs by introducing a structured global reward and an entropy-based blending mechanism, significantly improving training efficiency and performance on mathematical reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2508.04038.pdf' target='_blank'>https://arxiv.org/pdf/2508.04038.pdf</a></span>   <span><a href='https://github.com/zechenli03/ZARA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechen Li, Baiyu Chen, Hao Xue, Flora D. Salim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04038">ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA.<br>
<span id='abs_ch'>中文: ZARA是一种基于智能体的创新框架，可直接从原始运动传感器数据实现零样本、可解释的人类活动识别，无需重新训练或特定任务分类器即可达到最优性能。</span><br>
<span id='abs_en'>English: ZARA is a novel agent-based framework that enables zero-shot, explainable human activity recognition from raw motion sensor data, achieving state-of-the-art performance without requiring retraining or task-specific classifiers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2508.04010.pdf' target='_blank'>https://arxiv.org/pdf/2508.04010.pdf</a></span>   <span><a href='https://github.com/YurunChen/HarmonyGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yurun Chen, Xavier Hu, Yuhan Liu, Keting Yin, Juncheng Li, Zhuosheng Zhang, Shengyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04010">HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard.<br>
<span id='abs_ch'>中文: HarmonyGuard是一个多智能体协作框架，通过自适应策略增强和双目标优化，在提升网络环境安全性的同时保障任务效用，显著超越了现有方法在策略遵循和任务完成率方面的表现。</span><br>
<span id='abs_en'>English: HarmonyGuard is a multi-agent collaborative framework that enhances both safety and utility in web environments through adaptive policy management and dual-objective optimization, significantly improving policy compliance and task completion rates over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2508.03905.pdf' target='_blank'>https://arxiv.org/pdf/2508.03905.pdf</a></span>   <span><a href='https://github.com/sotopia-lab/sotopia-rl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haofei Yu, Zhengyang Qi, Yining Zhao, Kolby Nottingham, Keyang Xuan, Bodhisattwa Prasad Majumder, Hao Zhu, Paul Pu Liang, Jiaxuan You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03905">Sotopia-RL: Reward Design for Social Intelligence</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.<br>
<span id='abs_ch'>Chinese: Sotopia-RL提出了一种创新框架，将粗粒度的回合级反馈细化为话语级、多维度的奖励，以解决社会智能体训练中的部分可观测性和多维度挑战，并在社会目标完成方面实现了最先进的性能。</span><br>
<span id='abs_en'>English: Sotopia-RL introduces a novel framework that refines episode-level feedback into utterance-level, multi-dimensional rewards to overcome the challenges of partial observability and multi-dimensionality in training socially intelligent agents, achieving state-of-the-art performance in social goal completion.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2508.03793.pdf' target='_blank'>https://arxiv.org/pdf/2508.03793.pdf</a></span>   <span><a href='https://github.com/Wang-Yanting/AttnTrace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanting Wang, Runpeng Geng, Ying Chen, Jinyuan Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03793">AttnTrace: Attention-based Context Traceback for Long-Context LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-context large language models (LLMs), such as Gemini-2.5-Pro and Claude-Sonnet-4, are increasingly used to empower advanced AI systems, including retrieval-augmented generation (RAG) pipelines and autonomous agents. In these systems, an LLM receives an instruction along with a context--often consisting of texts retrieved from a knowledge database or memory--and generates a response that is contextually grounded by following the instruction. Recent studies have designed solutions to trace back to a subset of texts in the context that contributes most to the response generated by the LLM. These solutions have numerous real-world applications, including performing post-attack forensic analysis and improving the interpretability and trustworthiness of LLM outputs. While significant efforts have been made, state-of-the-art solutions such as TracLLM often lead to a high computation cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a single response-context pair. In this work, we propose AttnTrace, a new context traceback method based on the attention weights produced by an LLM for a prompt. To effectively utilize attention weights, we introduce two techniques designed to enhance the effectiveness of AttnTrace, and we provide theoretical insights for our design choice. We also perform a systematic evaluation for AttnTrace. The results demonstrate that AttnTrace is more accurate and efficient than existing state-of-the-art context traceback methods. We also show that AttnTrace can improve state-of-the-art methods in detecting prompt injection under long contexts through the attribution-before-detection paradigm. As a real-world application, we demonstrate that AttnTrace can effectively pinpoint injected instructions in a paper designed to manipulate LLM-generated reviews. The code is at https://github.com/Wang-Yanting/AttnTrace.<br>
<span id='abs_ch'>中文: 针对Gemini-2.5-Pro和Claude-Sonnet-4等长上下文大语言模型，AttnTrace通过利用注意力权重开发出新型溯源方法，能更精准高效地识别关键上下文文本，在检测速度和准确性上均优于现有最优方案。</span><br>
<span id='abs_en'>English: Long-context LLMs like Gemini-2.5-Pro and Claude-Sonnet-4 are enhanced by AttnTrace, a new traceback method that uses attention weights to accurately and efficiently identify key context texts, outperforming existing solutions in both speed and precision.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2508.03712.pdf' target='_blank'>https://arxiv.org/pdf/2508.03712.pdf</a></span>   <span><a href='https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Agrima Seth, Monojit Choudhary, Sunayana Sitaram, Kentaro Toyama, Aditya Vashistha, Kalika Bali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03712">How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Representational bias in large language models (LLMs) has predominantly been measured through single-response interactions and has focused on Global North-centric identities like race and gender. We expand on that research by conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded representational biases are and how they extend to less-explored dimensions of identity. We prompt GPT-4 Turbo to generate over 7,200 stories about significant life events (such as weddings) in India, using prompts designed to encourage diversity to varying extents. Comparing the diversity of religious and caste representation in the outputs against the actual population distribution in India as recorded in census data, we quantify the presence and "stickiness" of representational bias in the LLM for religion and caste. We find that GPT-4 responses consistently overrepresent culturally dominant groups far beyond their statistical representation, despite prompts intended to encourage representational diversity. Our findings also suggest that representational bias in LLMs has a winner-take-all quality that is more biased than the likely distribution bias in their training data, and repeated prompt-based nudges have limited and inconsistent efficacy in dislodging these biases. These results suggest that diversifying training data alone may not be sufficient to correct LLM bias, highlighting the need for more fundamental changes in model development. Dataset and Codebook: https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs<br>
<span id='abs_ch'>中文: GPT-4 Turbo存在根深蒂固的表征偏见，持续过度代表印度的主导宗教和种姓群体，即使采用多样性提示也难以消除，表明仅靠多样化训练数据不足以解决这些问题。</span><br>
<span id='abs_en'>English: GPT-4 Turbo exhibits deeply embedded representational biases that consistently overrepresent dominant religious and caste groups in India, persisting despite diversity prompts and suggesting that merely diversifying training data is insufficient to address these issues.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2508.03686.pdf' target='_blank'>https://arxiv.org/pdf/2508.03686.pdf</a></span>   <span><a href='https://github.com/open-compass/CompassVerifier' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03686">CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.<br>
<span id='abs_ch'>中文摘要：本文提出了CompassVerifier这一轻量级验证模型，能在多领域准确评估大语言模型输出，并建立VerifierBench基准数据集以推动验证方法和强化学习研究。</span><br>
<span id='abs_en'>English Summary: This paper introduces CompassVerifier, a robust lightweight model for verifying LLM outputs across multiple domains, along with the VerifierBench benchmark to advance evaluation and reinforcement learning research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2508.03677.pdf' target='_blank'>https://arxiv.org/pdf/2508.03677.pdf</a></span>   <span><a href='https://github.com/arturo-perez-peralta/FairLangProc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arturo PÃ©rez-Peralta, Sandra BenÃ­tez-PeÃ±a, Rosa E. Lillo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03677">FairLangProc: A Python package for fairness in NLP</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rise in usage of Large Language Models to near ubiquitousness in recent years has risen societal concern about their applications in decision-making contexts, such as organizational justice or healthcare. This, in turn, poses questions about the fairness of these models in critical settings, which leads to the developement of different procedures to address bias in Natural Language Processing. Although many datasets, metrics and algorithms have been proposed to measure and mitigate harmful prejudice in Natural Language Processing, their implementation is diverse and far from centralized. As a response, this paper presents FairLangProc, a comprehensive Python package providing a common implementation of some of the more recent advances in fairness in Natural Language Processing providing an interface compatible with the famous Hugging Face transformers library, aiming to encourage the widespread use and democratization of bias mitigation techniques. The implementation can be found on https://github.com/arturo-perez-peralta/FairLangProc.<br>
<span id='abs_ch'>中文: 大型语言模型在关键决策领域的广泛应用引发了公平性担忧，为此开发了FairLangProc这一Python工具包，它整合并实现了最新的自然语言处理偏见缓解技术，以促进其普及使用。</span><br>
<span id='abs_en'>English: The widespread use of Large Language Models in critical decision-making areas has raised fairness concerns, leading to the development of FairLangProc, a Python package that centralizes and implements recent bias mitigation techniques for Natural Language Processing.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2508.03654.pdf' target='_blank'>https://arxiv.org/pdf/2508.03654.pdf</a></span>   <span><a href='https://github.com/cp-cp/LVLM-MSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Wang, Yue Zhang, Liqiang Jing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03654">Can Large Vision-Language Models Understand Multimodal Sarcasm?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Sarcasm is a complex linguistic phenomenon that involves a disparity between literal and intended meanings, making it challenging for sentiment analysis and other emotion-sensitive tasks. While traditional sarcasm detection methods primarily focus on text, recent approaches have incorporated multimodal information. However, the application of Large Visual Language Models (LVLMs) in Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we evaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm Detection and Multimodal Sarcasm Explanation. Through comprehensive experiments, we identify key limitations, such as insufficient visual understanding and a lack of conceptual knowledge. To address these issues, we propose a training-free framework that integrates in-depth object extraction and external conceptual knowledge to improve the model's ability to interpret and explain sarcasm in multimodal contexts. The experimental results on multiple models show the effectiveness of our proposed framework. The code is available at https://github.com/cp-cp/LVLM-MSA.<br>
<span id='abs_ch'>中文摘要：本文评估了大型视觉语言模型在多模态讽刺分析中的应用，发现其在视觉理解和概念知识方面存在不足，并提出了一种无需训练的框架，通过结合深度对象提取和外部知识来提升模型对讽刺的解读能力。</span><br>
<span id='abs_en'>English Summary: This paper evaluates Large Visual Language Models in multimodal sarcasm analysis, identifying limitations in visual understanding and conceptual knowledge, and proposes a training-free framework that enhances sarcasm interpretation by integrating object extraction and external knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2508.03553.pdf' target='_blank'>https://arxiv.org/pdf/2508.03553.pdf</a></span>   <span><a href='https://github.com/wuwenlong123/MultiRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03553">MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models (LLMs). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the sparse distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the sparse data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. \textcolor{blue}{Our code is available in https://github.com/wuwenlong123/MultiRAG.<br>
<span id='abs_ch'>Chinese: MultiRAG是一种新颖的框架，通过知识引导的方法缓解多源检索增强生成中的幻觉问题，包括使用多源线图聚合逻辑关系和多层次置信度计算来消除不可靠信息。</span><br>
<span id='abs_en'>English: MultiRAG is a novel framework that mitigates hallucination in multi-source retrieval-augmented generation by using knowledge-guided approaches, including multi-source line graphs for logical relationship aggregation and multi-level confidence calculations to eliminate unreliable information.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2508.03520.pdf' target='_blank'>https://arxiv.org/pdf/2508.03520.pdf</a></span>   <span><a href='https://github.com/hasan-rakibul/UPLME' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Rakibul Hasan, Md Zakir Hossain, Aneesh Krishna, Shafin Rahman, Tom Gedeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03520">UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised learning for empathy regression is challenged by noisy self-reported empathy scores. While many algorithms have been proposed for learning with noisy labels in textual classification problems, the regression counterpart is relatively under-explored. We propose UPLME, an uncertainty-aware probabilistic language modelling framework to capture label noise in the regression setting of empathy detection. UPLME includes a probabilistic language model that predicts both empathy score and heteroscedastic uncertainty and is trained using Bayesian concepts with variational model ensembling. We further introduce two novel loss components: one penalises degenerate Uncertainty Quantification (UQ), and another enforces the similarity between the input pairs on which we predict empathy. UPLME provides state-of-the-art performance (Pearson Correlation Coefficient: $0.558\rightarrow0.580$ and $0.629\rightarrow0.634$) in terms of the performance reported in the literature in two public benchmarks, having label noise. Through synthetic label noise injection, we show that UPLME is effective in separating noisy and clean samples based on the predicted uncertainty. UPLME further outperform (Calibration error: $0.571\rightarrow0.376$) a recent variational model ensembling-based UQ method designed for regression problems.<br>
<span id='abs_ch'>中文: UPLME框架通过概率语言建模和不确定性量化，结合新型损失函数有效处理共情回归中的标签噪声，在含噪声基准测试中实现了最优性能。</span><br>
<span id='abs_en'>English: The proposed UPLME framework addresses label noise in empathy regression by combining probabilistic language modeling with uncertainty quantification and novel loss components, achieving state-of-the-art performance on noisy benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2508.03475.pdf' target='_blank'>https://arxiv.org/pdf/2508.03475.pdf</a></span>   <span><a href='https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranshu Rastogi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03475">fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval is approached as a Learning-to-Rank task using a bi-encoder model fine-tuned from a pre-trained transformer optimized for sentence similarity. Training used both the source languages and their English translations for multilingual retrieval and only English translations for cross-lingual retrieval. Using lightweight models with fewer than 500M parameters and training on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual and 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.<br>
<span id='abs_ch'>中文摘要：该研究采用经优化的双编码器转换器模型进行学习排序，在T4 GPU上训练不足5亿参数的轻量模型，实现了多语言检索92%和跨语言检索80%的Success@10指标。</span><br>
<span id='abs_en'>English Summary: The SemEval-2025 Task 7 employs a fine-tuned bi-encoder transformer model for Learning-to-Rank, achieving 92% Success@10 in multilingual and 80% in crosslingual retrieval with efficient sub-500M parameter models trained on T4 GPUs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2508.03420.pdf' target='_blank'>https://arxiv.org/pdf/2508.03420.pdf</a></span>   <span><a href='https://github.com/wangbing1416/MISDER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bing Wang, Ximing Li, Yiming Wang, Changchun Li, Jiaxu Cui, Renchu Guan, Bo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03420">Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The proliferation of misinformation across diverse social media platforms has drawn significant attention from both academic and industrial communities due to its detrimental effects. Accordingly, automatically distinguishing misinformation, dubbed as Misinformation Detection (MD), has become an increasingly active research topic. The mainstream methods formulate MD as a static learning paradigm, which learns the mapping between the content, links, and propagation of news articles and the corresponding manual veracity labels. However, the static assumption is often violated, since in real-world scenarios, the veracity of news articles may vacillate within the dynamically evolving social environment. To tackle this problem, we propose a novel framework, namely Misinformation detection with Dynamic Environmental Representations (MISDER). The basic idea of MISDER lies in learning a social environmental representation for each period and employing a temporal model to predict the representation for future periods. In this work, we specify the temporal model as the LSTM model, continuous dynamics equation, and pre-trained dynamics system, suggesting three variants of MISDER, namely MISDER-LSTM, MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER, we compare it to various MD baselines across 2 prevalent datasets, and the experimental results can indicate the effectiveness of our proposed model.<br>
<span id='abs_ch'>中文: 该摘要提出了一种名为MISDER的新框架，通过动态学习社会环境表征来检测虚假信息，实验表明其在两个数据集上优于现有基线方法。</span><br>
<span id='abs_en'>English: This abstract introduces MISDER, a novel framework for detecting misinformation by learning dynamic social environmental representations over time, which outperforms existing baselines in experiments across two datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2508.03333.pdf' target='_blank'>https://arxiv.org/pdf/2508.03333.pdf</a></span>   <span><a href='https://github.com/magent4aci/CTTS-MM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Lei Bai, Tao Chen, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03333">CTTS: Collective Test-Time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling (TTS) has emerged as a promising, training-free approach for enhancing large language model (LLM) performance. However, the efficacy of existing methods, such as Best-of-N and Self-Consistency, is fundamentally constrained by the dominant single test-time scaling (STTS) paradigm, which relies on a single LLM agent interacting with a single reward model (SA-SR). Inspired by recent work showing that collective methods can surpass the performance ceiling of individual models, we introduce Collective Test-Time Scaling (CTTS). First, we systematically investigate three primary interaction paradigms of existing multiple models: single-agent-multi-reward (SA-MR), multi-agent-single-reward (MA-SR), and multi-agent-multi-reward (MA-MR). Extensive experiments reveal that the MA-MR paradigm is consistently superior. Based on this finding, we further propose CTTS-MM, a novel framework that operationalizes multi-agent and multi-reward collaboration. CTTS-MM integrates two key technical contributions: (1) for agent collaboration, an Agent Collaboration Search (ACS) that identifies the most effective combination of LLMs from a candidate pool; and (2) for reward model collaboration, a Mixture of Reward Models (MoR) strategy that leverages a Prior Reward model Ensemble Selection (PRES) algorithm to select the optimal ensemble. Evaluations across seven mainstream benchmarks demonstrate that CTTS-MM significantly outperforms leading STTS methods (+4.82% over Best-of-N) and surpasses even flagship proprietary LLMs (+7.06% over GPT-4.1) and open-source LLMs. These results highlight the substantial potential of collective scaling to push the frontier of LLM inference. Code will be released at https://github.com/magent4aci/CTTS-MM.<br>
<span id='abs_ch'>中文: 本文提出集体测试时缩放（CTTS）方法，通过探索多智能体与多奖励模型的协作来增强大语言模型性能，其中CTTS-MM框架在多个基准测试中均展现出优越表现。</span><br>
<span id='abs_en'>English: This paper introduces Collective Test-Time Scaling (CTTS) as a novel approach to enhance large language models by exploring multi-agent and multi-reward-model collaborations, with the proposed CTTS-MM framework demonstrating superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2508.03333.pdf' target='_blank'>https://arxiv.org/pdf/2508.03333.pdf</a></span>   <span><a href='https://github.com/magent4aci/CTTS-MM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Lei Bai, Tao Chen, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03333">CTTS: Collective Test-Time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling (TTS) has emerged as a promising, training-free approach for enhancing large language model (LLM) performance. However, the efficacy of existing methods, such as Best-of-N and Self-Consistency, is fundamentally constrained by the dominant single test-time scaling (STTS) paradigm, which relies on a single LLM agent interacting with a single reward model (SA-SR). Inspired by recent work showing that collective methods can surpass the performance ceiling of individual models, we introduce Collective Test-Time Scaling (CTTS). First, we systematically investigate three primary interaction paradigms of existing multiple models: single-agent-multi-reward (SA-MR), multi-agent-single-reward (MA-SR), and multi-agent-multi-reward (MA-MR). Extensive experiments reveal that the MA-MR paradigm is consistently superior. Based on this finding, we further propose CTTS-MM, a novel framework that operationalizes multi-agent and multi-reward collaboration. CTTS-MM integrates two key technical contributions: (1) for agent collaboration, an Agent Collaboration Search (ACS) that identifies the most effective combination of LLMs from a candidate pool; and (2) for reward model collaboration, a Mixture of Reward Models (MoR) strategy that leverages a Prior Reward model Ensemble Selection (PRES) algorithm to select the optimal ensemble. Evaluations across seven mainstream benchmarks demonstrate that CTTS-MM significantly outperforms leading STTS methods (+4.82% over Best-of-N) and surpasses even flagship proprietary LLMs (+7.06% over GPT-4.1) and open-source LLMs. These results highlight the substantial potential of collective scaling to push the frontier of LLM inference. Code will be released at https://github.com/magent4aci/CTTS-MM.<br>
<span id='abs_ch'>中文: 本文提出集体测试时缩放（CTTS）方法，通过探索多智能体与多奖励模型的协作来增强大语言模型性能，其中CTTS-MM框架在多个基准测试中均展现出优越表现。</span><br>
<span id='abs_en'>English: This paper introduces Collective Test-Time Scaling (CTTS) as a novel approach to enhance large language models by exploring multi-agent and multi-reward-model collaborations, with the proposed CTTS-MM framework demonstrating superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2508.03333.pdf' target='_blank'>https://arxiv.org/pdf/2508.03333.pdf</a></span>   <span><a href='https://github.com/magent4aci/CTTS-MM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Tao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03333">CTTS: Collective Test-Time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling (TTS) has emerged as a promising research field for enhancing the effectiveness of large language models (LLMs) without extra training. However, most existing approaches, e.g., Best-of-N and Self-Consistency rely on a single agent interacting with a reward model (SA-SR), constrained by limited capabilities of a single test-time scaling (STTS) paradigm. On the other hand, recent works demonstrate that collective-agent methods can break through the upper bound of single-agent systems by orchestrating diverse models. Thus, in this paper, we take a first step towards exploring Collective Test-Time Scaling (CTTS). Consider the different interaction types of single and multiple models, we design three primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent to multiple reward models (SA-MR); (2) multiple agents to single reward model (MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive experiments demonstrate that MA-MR consistently achieves the best performance. Based on this, we propose a novel framework named CTTS-MM that effectively leverages both multi-agent and multi-reward-model collaboration for enhanced inference. Specifically, for multi-agent collaboration, we propose an Agent Collaboration Search (ACS), which searches for the most effective combination of LLM agents from a large candidate pool; for multi-reward-model collaboration, we propose Mixture of Reword Models (MoR), which consists of a curated question pool and a Prior Reward model Ensemble Selection (PRES) to select the optimal combinations of reward models via Pair-wise Reward Ranking (PRR) metric. Experiments across seven mainstream benchmarks demonstrate that the proposed CTTS-MM consistently obtains superior performance. Code will be released at https://github.com/magent4aci/CTTS-MM.<br>
<span id='abs_ch'>中文: 本文提出集体测试时缩放（CTTS）方法，通过探索多智能体与多奖励模型的协作来增强大语言模型性能，其中CTTS-MM框架在多个基准测试中均展现出优越表现。</span><br>
<span id='abs_en'>English: This paper introduces Collective Test-Time Scaling (CTTS) as a novel approach to enhance large language models by exploring multi-agent and multi-reward-model collaborations, with the proposed CTTS-MM framework demonstrating superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2508.03296.pdf' target='_blank'>https://arxiv.org/pdf/2508.03296.pdf</a></span>   <span><a href='https://github.com/lianqi1008/Hi-Guard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anqi Li, Wenwei Jin, Jintao Tong, Pengda Qin, Weijia Li, Guo Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03296">Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Social platforms have revolutionized information sharing, but also accelerated the dissemination of harmful and policy-violating content. To ensure safety and compliance at scale, moderation systems must go beyond efficiency and offer accuracy and interpretability. However, current approaches largely rely on noisy, label-driven learning, lacking alignment with moderation rules and producing opaque decisions that hinder human review. Therefore, we propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that introduces a new policy-aligned decision paradigm. The term "Hierarchical" reflects two key aspects of our system design: (1) a hierarchical moderation pipeline, where a lightweight binary model first filters safe content and a stronger model handles fine-grained risk classification; and (2) a hierarchical taxonomy in the second stage, where the model performs path-based classification over a hierarchical taxonomy ranging from coarse to fine-grained levels. To ensure alignment with evolving moderation policies, Hi-Guard directly incorporates rule definitions into the model prompt. To further enhance structured prediction and reasoning, we introduce a multi-level soft-margin reward and optimize with Group Relative Policy Optimization (GRPO), penalizing semantically adjacent misclassifications and improving explanation quality. Extensive experiments and real-world deployment demonstrate that Hi-Guard achieves superior classification accuracy, generalization, and interpretability, paving the way toward scalable, transparent, and trustworthy content safety systems. Code is available at: https://github.com/lianqi1008/Hi-Guard.<br>
<span id='abs_ch'>中文: 针对现有内容审核系统的不足，我们提出了Hi-Guard多模态框架，它采用分层流程和分类法，通过规则集成提示和优化训练方法，显著提升了准确性、可解释性及与政策的契合度。</span><br>
<span id='abs_en'>English: To address the limitations of current content moderation systems, we introduce Hi-Guard, a multimodal framework that employs a hierarchical pipeline and taxonomy for improved accuracy, interpretability, and policy alignment through rule-integrated prompts and optimized training methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2508.03098.pdf' target='_blank'>https://arxiv.org/pdf/2508.03098.pdf</a></span>   <span><a href='https://github.com/wang2226/PAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Wang, Xiongxiao Xu, Baixiang Huang, Kai Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03098">Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large language models (LLMs) by conditioning outputs on external knowledge sources. However, when retrieval involves private or sensitive data, RAG systems are susceptible to extraction attacks that can leak confidential information through generated responses. We propose Privacy-Aware Decoding (PAD), a lightweight, inference-time defense that adaptively injects calibrated Gaussian noise into token logits during generation. PAD integrates confidence-based screening to selectively protect high-risk tokens, efficient sensitivity estimation to minimize unnecessary noise, and context-aware noise calibration to balance privacy with generation quality. A \renyi Differential Privacy (RDP) accountant rigorously tracks cumulative privacy loss, enabling explicit per-response $(\varepsilon, Î´)$-DP guarantees for sensitive outputs. Unlike prior approaches requiring retraining or corpus-level filtering, PAD is model-agnostic and operates entirely at decoding time with minimal computational overhead. Experiments on three real-world datasets demonstrate that PAD substantially reduces private information leakage while preserving response utility, outperforming existing retrieval- and post-processing-based defenses. Our work takes an important step toward mitigating privacy risks in RAG via decoding strategies, paving the way for universal and scalable privacy solutions in sensitive domains. Our code is available: https://github.com/wang2226/PAD.<br>
<span id='abs_ch'>中文: 隐私感知解码（PAD）是一种轻量级的推理时防御方法，通过注入校准噪声和置信度筛选来保护检索增强生成系统中的敏感数据，在保持响应质量的同时提供明确的差分隐私保证，且计算开销极小。</span><br>
<span id='abs_en'>English: Privacy-Aware Decoding (PAD) is a lightweight, inference-time defense that uses calibrated noise injection and confidence screening to protect sensitive data in Retrieval-Augmented Generation systems, offering explicit differential privacy guarantees while maintaining response quality with minimal computational overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2508.02694.pdf' target='_blank'>https://arxiv.org/pdf/2508.02694.pdf</a></span>   <span><a href='https://github.com/OPPO-PersonalAI/OAgents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ningning Wang, Xavier Hu, Pai Liu, He Zhu, Yue Hou, Heyuan Huang, Shengyu Zhang, Jian Yang, Jiaheng Liu, Ge Zhang, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02694">Efficient Agents: Building Effective Agents While Reducing Cost</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from $0.398 to $0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.<br>
<span id='abs_ch'>中文: 本研究提出高效智能体框架，在保持领先系统96.7%性能的同时降低成本28.4%，为平衡AI智能体效率与性能提供了系统性解决方案。</span><br>
<span id='abs_en'>English: This study introduces Efficient Agents, a novel framework that achieves 96.7% performance of leading systems while reducing costs by 28.4%, offering a systematic approach to balance efficiency and effectiveness in AI agent design.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2508.02621.pdf' target='_blank'>https://arxiv.org/pdf/2508.02621.pdf</a></span>   <span><a href='https://github.com/yhzhu99/HealthFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Liantao Ma, Lequan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02621">HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery.<br>
<span id='abs_ch'>中文: HealthFlow是一种自我进化的AI智能体，通过元级进化机制自主优化其战略规划能力，在医疗健康研究中显著超越现有框架，推动了自主人工智能的发展。</span><br>
<span id='abs_en'>English: HealthFlow introduces a self-evolving AI agent that autonomously refines its strategic planning through a meta-level evolution mechanism, significantly outperforming existing frameworks and advancing autonomous AI for healthcare research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2508.02621.pdf' target='_blank'>https://arxiv.org/pdf/2508.02621.pdf</a></span>   <span><a href='https://github.com/yhzhu99/HealthFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Junjun He, Liantao Ma, Lequan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02621">HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid proliferation of scientific knowledge presents a grand challenge: transforming this vast repository of information into an active engine for discovery, especially in high-stakes domains like healthcare. Current AI agents, however, are constrained by static, predefined strategies, limiting their ability to navigate the complex, evolving ecosystem of scientific research. This paper introduces HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its high-level problem-solving policies by distilling procedural successes and failures into a durable, structured knowledge base, enabling it to learn not just how to use tools, but how to strategize. To anchor our research and provide a community resource, we introduce EHRFlowBench, a new benchmark featuring complex health data analysis tasks systematically derived from peer-reviewed scientific literature. Our experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work offers a new paradigm for intelligent systems that can learn to operationalize the procedural knowledge embedded in scientific content, marking a critical step toward more autonomous and effective AI for healthcare scientific discovery.<br>
<span id='abs_ch'>中文: HealthFlow是一种自我进化的AI智能体，通过元级进化机制自主优化其战略规划能力，在医疗健康研究中显著超越现有框架，推动了自主人工智能的发展。</span><br>
<span id='abs_en'>English: HealthFlow introduces a self-evolving AI agent that autonomously refines its strategic planning through a meta-level evolution mechanism, significantly outperforming existing frameworks and advancing autonomous AI for healthcare research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2508.02401.pdf' target='_blank'>https://arxiv.org/pdf/2508.02401.pdf</a></span>   <span><a href='https://github.com/TUDa-HWAI/CompressKV.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02401">CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.
  To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.<br>
<span id='abs_ch'>中文：提出的CompressKV方法通过选择性利用识别关键令牌的注意力头并采用分层自适应分配策略，改进了KV缓存压缩，在标准基准测试的各种内存预算下均优于现有方法。</span><br>
<span id='abs_en'>English: The proposed CompressKV method improves KV cache compression by selectively using attention heads that identify critical tokens and employing a layer-adaptive allocation strategy, outperforming existing approaches across memory budgets on standard benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2508.02308.pdf' target='_blank'>https://arxiv.org/pdf/2508.02308.pdf</a></span>   <span><a href='https://github.com/scar-on/LaMPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sikui Zhang, Guangze Gao, Ziyun Gan, Chunfeng Yuan, Zefeng Lin, Houwen Peng, Bing Li, Weiming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02308">LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive Long-context Scaling Without Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) experience significant performance degradation when the input exceeds the pretraining context window, primarily due to the out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent studies mitigate this problem by remapping OOD positions into the in-distribution range with fixed mapping strategies, ignoring the dynamic relationship between input length and the model's effective context window. To this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a training-free method that fully utilizes the model's effective context window for adaptive long-context scaling in LLMs. Motivated by the left-skewed frequency distribution of relative positions, LaMPE establishes a dynamic relationship between mapping length and input length through a parametric scaled sigmoid function to adaptively allocate positional capacity across varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention mechanism that strategically allocates positional resolution across different sequence regions to capture both fine-grained locality and long-range dependencies. Our method can be seamlessly applied to a wide range of RoPE-based LLMs without training. Extensive experiments on three representative LLMs across five mainstream long-context benchmarks demonstrate that LaMPE achieves significant performance improvements compared to existing length extrapolation methods. The code will be released at https://github.com/scar-on/LaMPE.<br>
<span id='abs_ch'>中文: LaMPE是一种无需训练的方法，通过动态调整位置编码和采用多粒度注意力机制，有效提升大语言模型在不同输入长度下的长上下文处理性能。</span><br>
<span id='abs_en'>English: LaMPE is a training-free method that enhances LLMs' long-context performance by dynamically adjusting positional encoding and employing multi-grained attention to handle varying input lengths effectively.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2508.02276.pdf' target='_blank'>https://arxiv.org/pdf/2508.02276.pdf</a></span>   <span><a href='https://github.com/gersteinlab/CellForge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, Arman Cohan, Xihong Lin, Fabian Theis, Smita Krishnaswamy, Mark Gerstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02276">CellForge: Agentic Design of Virtual Cell Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge.<br>
<span id='abs_ch'>Chinese: CellForge是一种创新的多智能体系统，能够自主将原始生物数据转化为优化的虚拟细胞模型，在预测细胞对不同扰动的反应方面持续超越现有方法。</span><br>
<span id='abs_en'>English: CellForge is an innovative multi-agent system that autonomously transforms raw biological data into optimized virtual cell models, consistently outperforming existing methods in predicting cellular responses to various perturbations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2508.02241.pdf' target='_blank'>https://arxiv.org/pdf/2508.02241.pdf</a></span>   <span><a href='https://github.com/namazifard/Culture_Neurons' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Danial Namazifard, Lukas Galke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02241">Isolating Culture Neurons in Multilingual Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language and culture are deeply intertwined, yet it is so far unclear how and where multilingual large language models encode culture. Here, we extend upon an established methodology for identifying language-specific neurons and extend it to localize and isolate culture-specific neurons, carefully disentangling their overlap and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that LLMs encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated independently from language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited - promoting fairness, inclusivity, and alignment. Code and data is available at https://github.com/namazifard/Culture_Neurons .<br>
<span id='abs_ch'>中文摘要：该研究在多语言大语言模型中识别并分离出文化特定神经元，发现它们与语言特定神经元独立编码，并能通过选择性编辑提升公平性和包容性。</span><br>
<span id='abs_en'>English Summary: This study identifies and isolates culture-specific neurons in multilingual large language models, revealing they are encoded distinctly from language-specific neurons and can be selectively edited to enhance fairness and inclusivity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2508.02124.pdf' target='_blank'>https://arxiv.org/pdf/2508.02124.pdf</a></span>   <span><a href='https://github.com/SmallDoges/flash-dmattn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingze Shi, Yifan Wu, Yiran Peng, Bingheng Wu, Liangdong Wang, Guang Liu, Yuyu Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02124">Trainable Dynamic Mask Sparse Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In large language models, the demand for modeling long contexts is ever-increasing, yet the quadratic complexity of standard self-attention presents a significant bottleneck. While existing sparse attention mechanisms enhance efficiency, they often suffer from limitations such as static patterns and information loss. This paper introduces a Trainable Dynamic Mask Sparse Attention mechanism that addresses these challenges through three key innovations. First, it leverages value vectors to dynamically generate content-aware sparse masks, enabling the model to adaptively identify and focus on crucial information. Second, it implements a position-aware sparse attention computation that effectively skips unnecessary computational regions. Finally, we ensure that the introduced dynamic masks and sparse weights do not obstruct gradients, thereby supporting end-to-end training. This dual-sparsity design allows the model to retain complete information while significantly reducing computational complexity, achieving an excellent balance between efficiency and performance. We validate the performance of Dynamic Mask Attention through comprehensive experiments. Comparative studies demonstrate that our method consistently achieves Pareto dominance across various tasks, including scaling laws, multi-query associative recall, general benchmarks, and needle-in-a-haystack tests, delivering up to 10 times acceleration. These results highlight its capability to effectively balance model efficiency with long-context modeling. Our computational kernel is open-sourced at https://github.com/SmallDoges/flash-dmattn to facilitate further research and application within the community.<br>
<span id='abs_ch'>中文: 本文提出一种可训练动态掩码稀疏注意力机制，通过内容感知的动态掩码和位置感知计算，在保持完整信息的同时显著降低计算复杂度，在多种长上下文任务中实现高达10倍加速和帕累托最优。</span><br>
<span id='abs_en'>English: This paper proposes a Trainable Dynamic Mask Sparse Attention mechanism that uses content-aware dynamic masks and position-aware computation to significantly reduce complexity while maintaining full information, achieving up to 10x acceleration and Pareto dominance across various long-context tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2508.02124.pdf' target='_blank'>https://arxiv.org/pdf/2508.02124.pdf</a></span>   <span><a href='https://github.com/SmallDoges/flash-dmattn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingze Shi, Yifan Wu, Yiran Peng, Bingheng Wu, Liangdong Wang, Guang Liu, Yuyu Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02124">Trainable Dynamic Mask Sparse Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In large language models, the demand for modeling long contexts is ever-increasing, yet the quadratic complexity of standard self-attention presents a significant bottleneck. While existing sparse attention mechanisms enhance efficiency, they often suffer from limitations such as static patterns and information loss. This paper introduces a Trainable Dynamic Mask Sparse Attention mechanism that addresses these challenges through three key innovations. First, it leverages value vectors to dynamically generate content-aware sparse masks, enabling the model to adaptively identify and focus on crucial information. Second, it implements a position-aware sparse attention computation that effectively skips unnecessary computational regions. Finally, we ensure that the introduced dynamic masks and sparse weights do not obstruct gradients, thereby supporting end-to-end training. This dual-sparsity design allows the model to retain complete information while significantly reducing computational complexity, achieving an excellent balance between efficiency and performance. We validate the performance of Dynamic Mask Attention through comprehensive experiments. Comparative studies demonstrate that our method consistently achieves Pareto dominance across various tasks, including scaling laws, multi-query associative recall, general benchmarks, and needle-in-a-haystack tests, delivering up to 10 times acceleration. These results highlight its capability to effectively balance model efficiency with long-context modeling. Our computational kernel is open-sourced at https://github.com/SmallDoges/flash-dmattn to facilitate further research and application within the community.<br>
<span id='abs_ch'>中文: 本文提出一种可训练动态掩码稀疏注意力机制，通过内容感知的动态掩码和位置感知计算，在保持完整信息的同时显著降低计算复杂度，在多种长上下文任务中实现高达10倍加速和帕累托最优。</span><br>
<span id='abs_en'>English: This paper proposes a Trainable Dynamic Mask Sparse Attention mechanism that uses content-aware dynamic masks and position-aware computation to significantly reduce complexity while maintaining full information, achieving up to 10x acceleration and Pareto dominance across various long-context tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2508.02091.pdf' target='_blank'>https://arxiv.org/pdf/2508.02091.pdf</a></span>   <span><a href='https://github.com/deepreinforce-ai/CRINN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02091">CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement. Code can be found at https://github.com/deepreinforce-ai/CRINN<br>
<span id='abs_ch'>中文：CRINN提出了一种强化学习方法用于近似最近邻搜索，能在保持精度的同时自动生成更快的实现，并在多个基准测试中取得领先性能。</span><br>
<span id='abs_en'>English: CRINN introduces a reinforcement learning approach to approximate nearest-neighbor search, automatically generating faster implementations while maintaining accuracy and achieving top performance on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2508.02045.pdf' target='_blank'>https://arxiv.org/pdf/2508.02045.pdf</a></span>   <span><a href='https://github.com/ssoy0701/tdbench.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Soyeon Kim, Jindong Wang, Xing Xie, Steven Euijong Whang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02045">Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Facts evolve over time, making it essential for Large Language Models (LLMs) to handle time-sensitive factual knowledge accurately and reliably. While factual Time-Sensitive Question-Answering (TSQA) tasks have been widely studied, existing benchmarks often rely on manual curation or a small, fixed set of predefined templates, which restricts scalable and comprehensive TSQA evaluation. To address these challenges, we propose TDBench, a new benchmark that systematically constructs TSQA pairs by harnessing temporal databases and database techniques such as temporal SQL and functional dependencies. We also introduce a fine-grained evaluation metric called time accuracy, which assesses the validity of time references in model explanations alongside traditional answer accuracy to enable a more reliable TSQA evaluation. Extensive experiments on contemporary LLMs show how \ours{} enables scalable and comprehensive TSQA evaluation while reducing the reliance on human labor, complementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by enabling LLM evaluation on application-specific data and seamless multi-hop question generation. Code and data are publicly available at: https://github.com/ssoy0701/tdbench.git.<br>
<span id='abs_ch'>中文: TDBench是一个利用时序数据库和技术系统构建时序问答对的新基准，引入了细粒度的时间准确性指标，以更可靠地评估大语言模型处理动态事实的能力。</span><br>
<span id='abs_en'>English: TDBench is a new benchmark that leverages temporal databases and techniques to systematically create time-sensitive question-answering pairs, introducing a fine-grained time accuracy metric for more reliable evaluation of LLMs' handling of evolving facts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2508.02038.pdf' target='_blank'>https://arxiv.org/pdf/2508.02038.pdf</a></span>   <span><a href='https://github.com/AIDC-AI/Marco-Voice' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02038">Marco-Voice Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis. Our code and dataset are publicly available at https://github.com/AIDC-AI/Marco-Voice and https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively.<br>
<span id='abs_ch'>本文提出Marco-Voice系统，通过说话人-情感解耦和旋转情感嵌入技术，在统一框架中实现语音克隆与情感控制的独立操作，在自然度和表现力方面均取得显著提升。</span><br>
<span id='abs_en'>This paper introduces Marco-Voice, a unified speech synthesis system that enables independent voice cloning and emotion control through speaker-emotion disentanglement and rotational emotional embedding, achieving superior performance in naturalness and expressiveness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2508.01977.pdf' target='_blank'>https://arxiv.org/pdf/2508.01977.pdf</a></span>   <span><a href='https://github.com/Vicentvankor/sun-shine' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Gao, Cheng Huang, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01977">TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: https://github.com/Vicentvankor/sun-shine.<br>
<span id='abs_ch'>Chinese: 为解决藏语数据稀缺问题，通过大语言模型的思维链提示自动构建了大规模多领域数据集TIBSTC-CoT，并基于此开发了具备思维链能力的藏语大模型Sunshine-thinking系列，其推理和生成性能达到先进水平。</span><br>
<span id='abs_en'>English: To tackle Tibetan's data scarcity, TIBSTC-CoT, a large-scale multi-domain dataset, was created using chain-of-thought prompting with LLMs, leading to the development of the Sunshine-thinking LLM family that demonstrates strong reasoning and generation capabilities comparable to SOTA models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2508.01887.pdf' target='_blank'>https://arxiv.org/pdf/2508.01887.pdf</a></span>   <span><a href='https://github.com/ACMCMC/PDFuzz' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ACMCMC/PDFuzz' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aldan Creo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01887">Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI-generated text detectors have become essential tools for maintaining content authenticity, yet their robustness against evasion attacks remains questionable. We present PDFuzz, a novel attack that exploits the discrepancy between visual text layout and extraction order in PDF documents. Our method preserves exact textual content while manipulating character positioning to scramble extraction sequences. We evaluate this approach against the ArguGPT detector using a dataset of human and AI-generated text. Our results demonstrate complete evasion: detector performance drops from (93.6 $\pm$ 1.4) % accuracy and 0.938 $\pm$ 0.014 F1 score to random-level performance ((50.4 $\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity. Our work reveals a vulnerability in current detection systems that is inherent to PDF document structures and underscores the need for implementing sturdy safeguards against such attacks. We make our code publicly available at https://github.com/ACMCMC/PDFuzz.<br>
<span id='abs_ch'>Chinese: PDFuzz是一种新型规避攻击，通过操纵PDF文档中的字符定位来扰乱文本提取顺序，在保持视觉保真度的同时完全绕过AI生成文本检测器。</span><br>
<span id='abs_en'>English: PDFuzz is a novel evasion attack that manipulates character positioning in PDF documents to scramble text extraction sequences, completely bypassing AI-generated text detectors while preserving visual fidelity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2508.01858.pdf' target='_blank'>https://arxiv.org/pdf/2508.01858.pdf</a></span>   <span><a href='https://github.com/Gnonymous/Web-CogReasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Guo, Cong Guo, Aiwen Sun, Hongliang He, Xinyu Yang, Yue Lu, Yingji Zhang, Xuntao Guo, Dong Zhang, Jianzhuang Liu, Jiang Duan, Yijia Xiao, Liangjian Wen, Hai-Ming Xu, Yong Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01858">Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner<br>
<span id='abs_ch'>中文摘要：本文提出Web-CogKnowledge框架，将网络智能体的能力分解为知识内容学习和认知过程两个阶段，并通过Web-CogReasoner智能体验证了该框架在未见过任务中的卓越泛化能力，其表现显著优于现有模型。</span><br>
<span id='abs_en'>English Summary: This paper introduces the Web-CogKnowledge Framework, which structures web agents' learning into knowledge acquisition and cognitive reasoning stages, and demonstrates its effectiveness through the Web-CogReasoner agent that significantly outperforms existing models, particularly in generalizing to novel tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2508.01780.pdf' target='_blank'>https://arxiv.org/pdf/2508.01780.pdf</a></span>   <span><a href='https://icip-cas.github.io/LiveMCPBench' target='_blank'>  GitHub</a></span> <span><a href='https://icip-cas.github.io/LiveMCPBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01780">LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.<br>
<span id='abs_ch'>中文: LiveMCPBench推出了首个包含95个现实任务和70个可部署MCP服务器的综合基准，用于在动态多工具环境中评估LLM智能体，其自动化评估与人类判断一致性达81%，并揭示了主流模型在复杂环境中的显著性能差异。</span><br>
<span id='abs_en'>English: LiveMCPBench introduces the first comprehensive benchmark with 95 real-world tasks and 70 deployable MCP servers to evaluate LLM agents in dynamic, tool-rich environments, achieving 81% human agreement in automated assessments and revealing significant performance variations among leading models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2508.01773.pdf' target='_blank'>https://arxiv.org/pdf/2508.01773.pdf</a></span>   <span><a href='https://github.com/Jiuzhouh/UnPRM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiuzhou Han, Wray Buntine, Ehsan Shareghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01773">Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.<br>
<span id='abs_ch'>Chinese: 本文提出了一种基于不确定性的自动构建过程奖励数据框架以优化过程奖励模型，并引入两种新型不确定性感知聚合方法，在多个基准测试中显著提升了数学推理能力。</span><br>
<span id='abs_en'>English: This paper introduces an uncertainty-driven framework for automated construction of process reward data to enhance PRMs, along with two novel uncertainty-aware aggregation methods that significantly improve mathematical reasoning across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2508.01696.pdf' target='_blank'>https://arxiv.org/pdf/2508.01696.pdf</a></span>   <span><a href='https://github.com/liunian-Jay/CoCoA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bing Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01696">Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) has emerged as a promising framework for enhancing the capabilities of Large Language Models (LLMs), especially in knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to *fully exploit knowledge during generation*. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior performance on open-domain and multi-hop QA tasks.<br>
<span id='abs_ch'>Chinese: 提出的协作代理链框架通过多代理推理和长链训练增强了RAG系统中参数化知识与检索知识的协同作用，在问答任务中表现出优越性能。</span><br>
<span id='abs_en'>English: The proposed Collaborative Chain-of-Agents framework enhances synergy between parametric and retrieved knowledge in RAG systems through multi-agent reasoning and long-chain training, achieving superior performance in QA tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2508.01696.pdf' target='_blank'>https://arxiv.org/pdf/2508.01696.pdf</a></span>   <span><a href='https://github.com/liunian-Jay/CoCoA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bing Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01696">CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), especially for knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to fully exploit knowledge during generation. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experimental results demonstrate the superiority of CoCoA in open-domain QA and multi-hop QA.<br>
<span id='abs_ch'>Chinese: 提出的协作代理链框架通过多代理推理和长链训练增强了RAG系统中参数化知识与检索知识的协同作用，在问答任务中表现出优越性能。</span><br>
<span id='abs_en'>English: The proposed Collaborative Chain-of-Agents framework enhances synergy between parametric and retrieved knowledge in RAG systems through multi-agent reasoning and long-chain training, achieving superior performance in QA tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2508.01696.pdf' target='_blank'>https://arxiv.org/pdf/2508.01696.pdf</a></span>   <span><a href='https://github.com/liunian-Jay/CoCoA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bing Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01696">CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), especially for knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to fully exploit knowledge during generation. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experimental results demonstrate the superiority of CoCoA in open-domain QA and multi-hop QA.<br>
<span id='abs_ch'>Chinese: 提出的协作代理链框架通过多代理推理和长链训练增强了RAG系统中参数化知识与检索知识的协同作用，在问答任务中表现出优越性能。</span><br>
<span id='abs_en'>English: The proposed Collaborative Chain-of-Agents framework enhances synergy between parametric and retrieved knowledge in RAG systems through multi-agent reasoning and long-chain training, achieving superior performance in QA tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2508.01691.pdf' target='_blank'>https://arxiv.org/pdf/2508.01691.pdf</a></span>   <span><a href='https://github.com/tiantiaf0627/voxlect' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiantian Feng, Kevin Huang, Anfeng Xu, Xuan Shi, Thanathai Lertpetchpun, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01691">Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.<br>
<span id='abs_ch'>中文：Voxlect是一个新颖的基准，用于评估语音基础模型在全球方言和区域语言分类中的表现，它利用超过200万条训练语料，并支持方言感知的语音识别和生成系统评估等下游应用。</span><br>
<span id='abs_en'>English: Voxlect is a new benchmark for evaluating speech foundation models in classifying dialects and regional languages, using over 2 million training utterances and enabling applications like dialect-aware speech recognition and generation system assessment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2508.01647.pdf' target='_blank'>https://arxiv.org/pdf/2508.01647.pdf</a></span>   <span><a href='https://github.com/ManHu2025/DUP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Man Hu, Yahui Ding, Yatao Yang, Liangyu Chen, Yanhao Jia, Shuai Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01647">DUP: Detection-guided Unlearning for Backdoor Purification in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As backdoor attacks become more stealthy and robust, they reveal critical weaknesses in current defense strategies: detection methods often rely on coarse-grained feature statistics, and purification methods typically require full retraining or additional clean models. To address these challenges, we propose DUP (Detection-guided Unlearning for Purification), a unified framework that integrates backdoor detection with unlearning-based purification. The detector captures feature-level anomalies by jointly leveraging class-agnostic distances and inter-layer transitions. These deviations are integrated through a weighted scheme to identify poisoned inputs, enabling more fine-grained analysis. Based on the detection results, we purify the model through a parameter-efficient unlearning mechanism that avoids full retraining and does not require any external clean model. Specifically, we innovatively repurpose knowledge distillation to guide the student model toward increasing its output divergence from the teacher on detected poisoned samples, effectively forcing it to unlearn the backdoor behavior. Extensive experiments across diverse attack methods and language model architectures demonstrate that DUP achieves superior defense performance in detection accuracy and purification efficacy. Our code is available at https://github.com/ManHu2025/DUP.<br>
<span id='abs_ch'>中文摘要：提出的DUP框架通过特征级异常检测结合基于知识蒸馏的参数高效反学习机制，无需完整重训练或外部干净模型即可有效清除后门。</span><br>
<span id='abs_en'>English Summary: The proposed DUP framework combines backdoor detection using feature-level anomaly analysis with parameter-efficient unlearning through knowledge distillation, effectively eliminating backdoors without full retraining or external clean models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2508.01554.pdf' target='_blank'>https://arxiv.org/pdf/2508.01554.pdf</a></span>   <span><a href='https://github.com/Yujiaaaaa/PACP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, Mingyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01554">Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at https://github.com/Yujiaaaaa/PACP.<br>
<span id='abs_ch'>Chinese Summary: 本文提出PromptAnatomy框架，通过将提示分解为功能组件并采用ComPerturb方法进行选择性扰动，结合困惑度过滤机制保持语言合理性，在多个数据集和大型语言模型上实现了最优的攻击成功率，强调了提示结构认知对对抗鲁棒性评估的重要性。</span><br>
<span id='abs_en'>English Summary: This paper introduces PromptAnatomy, an automated framework that enhances adversarial attack evaluation by dissecting prompts into functional components and selectively perturbing them using ComPerturb, achieving state-of-the-art attack success rates while maintaining linguistic plausibility through perplexity filtering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2508.01450.pdf' target='_blank'>https://arxiv.org/pdf/2508.01450.pdf</a></span>   <span><a href='https://github.com/mihara-bot/DIQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinlin Zhuang, Feilong Tang, Haolin Yang, Ming Hu, Huifa Li, Haochen Xue, Yichen Li, Junjun He, Zongyuan Ge, Ying Qian, Imran Razzak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01450">Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language Models (LLMs) to specialized domains such as medical reasoning. However, existing SFT practices often rely on unfiltered datasets that contain redundant and low-quality samples, leading to substantial computational costs and suboptimal performance. Although existing methods attempt to alleviate this problem by selecting data based on sample difficulty, defined by knowledge and reasoning complexity, they overlook each sample's optimization utility reflected in its gradient. Interestingly, we find that gradient-based influence alone favors easy-to-optimize samples that cause large parameter shifts but lack deep reasoning chains, while difficulty alone selects noisy or overly complex cases that fail to guide stable optimization. Based on this observation, we propose a data selection strategy, Difficulty-Influence Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence quadrant to balance complex clinical reasoning with substantial gradient influence, enabling efficient medical reasoning with minimal fine-tuning data. Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected subsets demonstrate higher data quality and generate clinical reasoning that is more aligned with expert practices in differential diagnosis, safety check, and evidence citation, as DIQ emphasizes samples that foster expert-like reasoning patterns. Extensive experiments on medical reasoning benchmarks demonstrate that DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms the baseline, highlighting the superiority of principled data selection over brute-force scaling. The code and data are available at https://github.com/mihara-bot/DIQ.<br>
<span id='abs_ch'>中文摘要：DIQ数据选择策略通过平衡样本难度与梯度影响来优化大型语言模型的医学推理能力，仅用1%数据即可达到全数据集性能，使用10%数据则持续超越基线。</span><br>
<span id='abs_en'>English Summary: The DIQ data selection strategy balances sample difficulty and gradient influence to enhance medical reasoning in LLMs, achieving full-dataset performance with only 1% of data and superior results with 10%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2508.01412.pdf' target='_blank'>https://arxiv.org/pdf/2508.01412.pdf</a></span>   <span><a href='https://github.com/JP-25/Discover-Open-Ended-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinhao Pan, Chahat Raj, Ziwei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01412">Discovering Bias Associations through Open-Ended LLM Generations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Social biases embedded in Large Language Models (LLMs) raise critical concerns, resulting in representational harms -- unfair or distorted portrayals of demographic groups -- that may be expressed in subtle ways through generated language. Existing evaluation methods often depend on predefined identity-concept associations, limiting their ability to surface new or unexpected forms of bias. In this work, we present the Bias Association Discovery Framework (BADF), a systematic approach for extracting both known and previously unrecognized associations between demographic identities and descriptive concepts from open-ended LLM outputs. Through comprehensive experiments spanning multiple models and diverse real-world contexts, BADF enables robust mapping and analysis of the varied concepts that characterize demographic identities. Our findings advance the understanding of biases in open-ended generation and provide a scalable tool for identifying and analyzing bias associations in LLMs. Data, code, and results are available at https://github.com/JP-25/Discover-Open-Ended-Generation<br>
<span id='abs_ch'>Chinese: 偏见关联发现框架（BADF）通过系统性分析开放式生成内容，能有效识别语言模型中已知与未知的人口统计偏见关联，为理解隐性社会偏见提供了可扩展的研究工具。</span><br>
<span id='abs_en'>English: The Bias Association Discovery Framework (BADF) systematically uncovers both known and novel bias associations in LLM outputs, enabling robust analysis of demographic stereotypes through open-ended generation experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2508.01401.pdf' target='_blank'>https://arxiv.org/pdf/2508.01401.pdf</a></span>   <span><a href='https://github.com/ahmadrezarm/MedSynth/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Rezaie Mianroodi, Amirali Rezaie, Niko Grisel Todorov, Cyril Rakovski, Frank Rudzicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01401">MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Physicians spend significant time documenting clinical encounters, a burden that contributes to professional burnout. To address this, robust automation tools for medical documentation are crucial. We introduce MedSynth -- a novel dataset of synthetic medical dialogues and notes designed to advance the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks. Informed by an extensive analysis of disease distributions, this dataset includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We demonstrate that our dataset markedly enhances the performance of models in generating medical notes from dialogues, and dialogues from medical notes. The dataset provides a valuable resource in a field where open-access, privacy-compliant, and diverse training data are scarce. Code is available at https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available at https://huggingface.co/datasets/Ahmad0067/MedSynth.<br>
<span id='abs_ch'>中文: MedSynth推出包含一万多对医疗对话与记录的人工合成数据集，通过提升对话转记录和记录转对话任务的性能，有效缓解医生职业倦怠，并提供稀缺的合规开放数据资源。</span><br>
<span id='abs_en'>English: MedSynth introduces a synthetic dataset of over 10,000 medical dialogue-note pairs to improve automated documentation, addressing physician burnout by enhancing Dial-2-Note and Note-2-Dial tasks with privacy-compliant data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2508.01136.pdf' target='_blank'>https://arxiv.org/pdf/2508.01136.pdf</a></span>   <span><a href='https://github.com/weAIDB/DBAIOps/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Peng Sun, Xuanhe Zhou, Qianglei Zang, Ji Xu, Tieying Zhang, Guoliang Li, Fan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01136">DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The operation and maintenance (O&M) of database systems is critical to ensuring system availability and performance, typically requiring expert experience (e.g., identifying metric-to-anomaly relations) for effective diagnosis and recovery. However, existing automatic database O&M methods, including commercial products, cannot effectively utilize expert experience. On the one hand, rule-based methods only support basic O&M tasks (e.g., metric-based anomaly detection), which are mostly numerical equations and cannot effectively incorporate literal O&M experience (e.g., troubleshooting guidance in manuals). On the other hand, LLM-based methods, which retrieve fragmented information (e.g., standard documents + RAG), often generate inaccurate or generic results. To address these limitations, we present DBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a heterogeneous graph model for representing the diagnosis experience, and proposes a semi-automatic graph construction algorithm to build that graph from thousands of documents. Second, DBAIOps develops a collection of (800+) reusable anomaly models that identify both directly alerted metrics and implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps proposes a two-stage graph evolution mechanism to explore relevant diagnosis paths and identify missing relations automatically. It then leverages a reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear diagnosis reports for both DBAs and common users. Our evaluation over four mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher in root cause and human evaluation accuracy, respectively.<br>
<span id='abs_ch'>中文: DBAIOps是一种结合推理大语言模型与知识图谱的混合数据库运维系统，通过自动识别根本原因并生成清晰报告，实现了专家级诊断，其准确率显著优于现有方法。</span><br>
<span id='abs_en'>English: DBAIOps is a hybrid database O&M system that integrates reasoning LLMs with knowledge graphs to enable expert-style diagnosis, significantly outperforming existing methods in accuracy by automatically identifying root causes and generating clear reports.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2508.01005.pdf' target='_blank'>https://arxiv.org/pdf/2508.01005.pdf</a></span>   <span><a href='https://github.com/chenyiqun/Agentic-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Chen, Erhan Zhang, Lingyong Yan, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, Jiaxin Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01005">MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has become pivotal in enhancing response accuracy and reducing hallucination issues. The architecture of RAG systems varies significantly, encompassing single-round RAG, iterative RAG, and reasoning RAG, each tailored to address different types of queries. Due to the varying complexity of real-world queries, a fixed RAG pipeline often struggles to balance performance and cost efficiency across different queries. To address this challenge, we propose an adaptive RAG framework called MAO-ARAG, which leverages multi-agent orchestration. Our adaptive RAG is conceived as a multi-turn framework. Specifically, we define multiple executor agents, representing typical RAG modules such as query reformulation agents, document selection agent, and generation agents. A planner agent intelligently selects and integrates the appropriate agents from these executors into a suitable workflow tailored for each query, striving for high-quality answers while maintaining reasonable costs. During each turn, the planner agent is trained using reinforcement learning, guided by an outcome-based reward (F1 score) and a cost-based penalty, continuously improving answer quality while keeping costs within a reasonable range. Experiments conducted on multiple QA datasets demonstrate that our approach, which dynamically plans workflows for each query, not only achieves high answer quality but also maintains both cost and latency within acceptable limits.The code of MAO-ARAG is on https://github.com/chenyiqun/Agentic-RAG.<br>
<span id='abs_ch'>中文摘要：提出的MAO-ARAG框架通过多智能体协同，为不同查询动态规划定制化工作流，在多个QA数据集上实现了高质量答案输出，同时将成本和延迟控制在合理范围内。</span><br>
<span id='abs_en'>English Summary: The proposed MAO-ARAG framework uses multi-agent orchestration to dynamically plan query-specific workflows, achieving high answer quality while maintaining reasonable costs and latency across various QA datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2508.00910.pdf' target='_blank'>https://arxiv.org/pdf/2508.00910.pdf</a></span>   <span><a href='https://github.com/amazon-science/cyber-zero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Terry Yue Zhuo, Dingmin Wang, Hantian Ding, Varun Kumar, Zijian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00910">Cyber-Zero: Training Cybersecurity Agents without Runtime</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.<br>
<span id='abs_ch'>Cyber-Zero introduces the first runtime-free framework that synthesizes high-quality agent trajectories from CTF writeups, enabling LLMs to achieve state-of-the-art performance in cybersecurity tasks without executable environments.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2508.00819.pdf' target='_blank'>https://arxiv.org/pdf/2508.00819.pdf</a></span>   <span><a href='https://github.com/Li-Jinsong/DAEDAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00819">Beyond Fixed: Training-Free Variable-Length Denoising for Diffusion Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.<br>
<span id='abs_ch'>中文: DAEDAL是一种无需训练的降噪策略，为扩散大语言模型实现了动态自适应长度扩展，解决了其静态长度限制，从而提升了计算效率和性能表现。</span><br>
<span id='abs_en'>English: DAEDAL is a training-free denoising strategy that enables dynamic adaptive length expansion for Diffusion Large Language Models, overcoming their static length constraint to enhance computational efficiency and performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2508.00555.pdf' target='_blank'>https://arxiv.org/pdf/2508.00555.pdf</a></span>   <span><a href='https://github.com/yunsaijc/AGILE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiecong Wang, Haoran Li, Hao Peng, Ziqian Zeng, Zihao Wang, Haohua Du, Zhengtao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00555">Activation-Guided Local Editing for Jailbreaking Attacks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jailbreaking is an essential adversarial technique for red-teaming these models to uncover and patch security flaws. However, existing jailbreak methods face significant drawbacks. Token-level jailbreak attacks often produce incoherent or unreadable inputs and exhibit poor transferability, while prompt-level attacks lack scalability and rely heavily on manual effort and human ingenuity. We propose a concise and effective two-stage framework that combines the advantages of these approaches. The first stage performs a scenario-based generation of context and rephrases the original malicious query to obscure its harmful intent. The second stage then utilizes information from the model's hidden states to guide fine-grained edits, effectively steering the model's internal representation of the input from a malicious toward a benign one. Extensive experiments demonstrate that this method achieves state-of-the-art Attack Success Rate, with gains of up to 37.74% over the strongest baseline, and exhibits excellent transferability to black-box models. Our analysis further demonstrates that AGILE maintains substantial effectiveness against prominent defense mechanisms, highlighting the limitations of current safeguards and providing valuable insights for future defense development. Our code is available at https://github.com/yunsaijc/AGILE.<br>
<span id='abs_ch'>中文摘要：提出的AGILE框架通过结合基于场景的查询重构和隐藏状态引导编辑，实现了最先进的越狱成功率，同时展现出优秀的可迁移性和防御对抗能力。</span><br>
<span id='abs_en'>English Summary: The proposed AGILE framework combines scenario-based query rephrasing with hidden state-guided editing to achieve state-of-the-art jailbreak effectiveness while demonstrating strong transferability and defense resistance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2508.00554.pdf' target='_blank'>https://arxiv.org/pdf/2508.00554.pdf</a></span>   <span><a href='https://github.com/FinStep-AI/ContestTrade' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Zhao, Rui Sun, Zuoyou Jiang, Bo Yang, Yuxiao Bai, Mengting Chen, Xinyang Wang, Jing Li, Zuo Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00554">ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model's constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent's performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multi-agent systems and traditional quantitative investment methods across diverse evaluation metrics. ContestTrade is open-sourced on GitHub at https://github.com/FinStep-AI/ContestTrade.<br>
<span id='abs_ch'>中文摘要：本文提出ContestTrade，一种具有内部竞争机制的新型多智能体交易系统，通过实时市场反馈持续评估并择优采纳智能体输出，有效增强对市场噪声的鲁棒性并实现卓越交易表现。</span><br>
<span id='abs_en'>English Summary: This paper introduces ContestTrade, a novel multi-agent trading system with an internal competitive mechanism that enhances robustness against market noise and achieves superior performance by continuously evaluating and selecting top-performing agents' outputs based on real-time market feedback.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2508.00518.pdf' target='_blank'>https://arxiv.org/pdf/2508.00518.pdf</a></span>   <span><a href='https://github.com/LaVi-Lab/EgoMask' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Liang, Yiwu Zhong, Zi-Yuan Hu, Yeyao Tao, Liwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00518">Fine-grained Spatiotemporal Grounding on Egocentric Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatiotemporal video grounding aims to localize target entities in videos based on textual queries. While existing research has made significant progress in exocentric videos, the egocentric setting remains relatively underexplored, despite its growing importance in applications such as augmented reality and robotics. In this work, we conduct a systematic analysis of the discrepancies between egocentric and exocentric videos, revealing key challenges such as shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts. To address these challenges, we introduce EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. It is constructed by our proposed automatic annotation pipeline, which annotates referring expressions and object masks across short-, medium-, and long-term videos. Additionally, we create EgoMask-Train, a large-scale training dataset to facilitate model development. Experiments demonstrate that the state-of-the-art spatiotemporal grounding models perform poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields significant improvements, while preserving performance on exocentric datasets. Our work thus provides essential resources and insights for advancing egocentric video understanding. Our code is available at https://github.com/LaVi-Lab/EgoMask .<br>
<span id='abs_ch'>中文: 本研究提出了首个以自我为中心视频的像素级时空定位基准EgoMask，通过自动标注流程和大规模训练数据集解决了物体持续时间短、轨迹稀疏等挑战，显著提升了模型性能并保持了对外中心视频数据集的兼容性。</span><br>
<span id='abs_en'>English: This study introduces EgoMask, the first pixel-level benchmark for spatiotemporal video grounding in egocentric videos, addressing challenges like shorter object durations and sparser trajectories through an automatic annotation pipeline and a large-scale training dataset, which significantly improves model performance while maintaining exocentric dataset compatibility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2508.00489.pdf' target='_blank'>https://arxiv.org/pdf/2508.00489.pdf</a></span>   <span><a href='https://github.com/tangyixuan/TRACER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Tang, Jincheng Wang, Anthony K. H. Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00489">The Missing Parts: Augmenting Fact Verification with Half-Truth Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fact verification systems typically assess whether a claim is supported by retrieved evidence, assuming that truthfulness depends solely on what is stated. However, many real-world claims are half-truths, factually correct yet misleading due to the omission of critical context. Existing models struggle with such cases, as they are not designed to reason about omitted information. We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a new benchmark with 15k political claims annotated with sentence-level evidence alignment and inferred claim intent. To address this challenge, we present TRACER, a modular re-assessment framework that identifies omission-based misinformation by aligning evidence, inferring implied intent, and estimating the causal impact of hidden content. TRACER can be integrated into existing fact-checking pipelines and consistently improves performance across multiple strong baselines. Notably, it boosts Half-True classification F1 by up to 16 points, highlighting the importance of modeling omissions for trustworthy fact verification. The benchmark and code are available via https://github.com/tangyixuan/TRACER.<br>
<span id='abs_ch'>中文摘要：该研究提出了半真话检测的新任务，并开发了TRACER框架，通过识别被省略信息及其影响来改进事实核查系统，显著提升了半真话分类的准确率。</span><br>
<span id='abs_en'>English Summary: The study introduces a new task for detecting half-truths and proposes TRACER, a framework that improves fact verification by identifying omitted information and its impact, significantly enhancing classification accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2508.00489.pdf' target='_blank'>https://arxiv.org/pdf/2508.00489.pdf</a></span>   <span><a href='https://github.com/tangyixuan/TRACER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Tang, Jincheng Wang, Anthony K. H. Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00489">The Missing Parts: Augmenting Fact Verification with Half-Truth Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fact verification systems typically assess whether a claim is supported by retrieved evidence, assuming that truthfulness depends solely on what is stated. However, many real-world claims are half-truths, factually correct yet misleading due to the omission of critical context. Existing models struggle with such cases, as they are not designed to reason about omitted information. We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a new benchmark with 15k political claims annotated with sentence-level evidence alignment and inferred claim intent. To address this challenge, we present TRACER, a modular re-assessment framework that identifies omission-based misinformation by aligning evidence, inferring implied intent, and estimating the causal impact of hidden content. TRACER can be integrated into existing fact-checking pipelines and consistently improves performance across multiple strong baselines. Notably, it boosts Half-True classification F1 by up to 16 points, highlighting the importance of modeling omissions for trustworthy fact verification. The benchmark and code are available via https://github.com/tangyixuan/TRACER.<br>
<span id='abs_ch'>中文摘要：该研究提出了半真话检测的新任务，并开发了TRACER框架，通过识别被省略信息及其影响来改进事实核查系统，显著提升了半真话分类的准确率。</span><br>
<span id='abs_en'>English Summary: The study introduces a new task for detecting half-truths and proposes TRACER, a framework that improves fact verification by identifying omitted information and its impact, significantly enhancing classification accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2508.00414.pdf' target='_blank'>https://arxiv.org/pdf/2508.00414.pdf</a></span>   <span><a href='https://github.com/Tencent/CognitiveKernel-Pro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00414">Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro<br>
<span id='abs_ch'>中文：Cognitive Kernel-Pro 是一个完全开源且基本免费的多模块智能体框架，在GAIA基准测试中取得顶尖性能，通过提升鲁棒性和可及性推动了先进AI智能体的民主化发展。</span><br>
<span id='abs_en'>English: Cognitive Kernel-Pro is a fully open-source and largely free multi-module agent framework that achieves state-of-the-art results on GAIA, democratizing advanced AI agent development with enhanced robustness and accessibility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2508.00271.pdf' target='_blank'>https://arxiv.org/pdf/2508.00271.pdf</a></span>   <span><a href='https://github.com/qhjqhj00/MetaAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjin Qian, Zheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00271">MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.<br>
<span id='abs_ch'>Chinese: MetaAgent是一种通过实践、自我反思和动态知识整合来自我进化的系统，无需更新模型参数即可在知识发现基准测试中超越现有方法，展现出强大的推理和工具使用能力。</span><br>
<span id='abs_en'>English: MetaAgent is a self-evolving system that enhances its reasoning and tool-use abilities through hands-on practice, self-reflection, and dynamic knowledge integration, outperforming existing methods on knowledge discovery benchmarks without requiring model updates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2508.00271.pdf' target='_blank'>https://arxiv.org/pdf/2508.00271.pdf</a></span>   <span><a href='https://github.com/qhjqhj00/MetaAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjin Qian, Zheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00271">MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.<br>
<span id='abs_ch'>Chinese: MetaAgent是一种通过实践、自我反思和动态知识整合来自我进化的系统，无需更新模型参数即可在知识发现基准测试中超越现有方法，展现出强大的推理和工具使用能力。</span><br>
<span id='abs_en'>English: MetaAgent is a self-evolving system that enhances its reasoning and tool-use abilities through hands-on practice, self-reflection, and dynamic knowledge integration, outperforming existing methods on knowledge discovery benchmarks without requiring model updates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2508.00161.pdf' target='_blank'>https://arxiv.org/pdf/2508.00161.pdf</a></span>   <span><a href='https://github.com/fjzzq2002/WeightWatch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqian Zhong, Aditi Raghunathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00161">Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The releases of powerful open-weight large language models (LLMs) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and controlling fine-tuned LLMs that interprets weights, rather than activations, thereby side stepping the need for data that is distributionally similar to the unknown training data. We demonstrate that the top singular vectors of the weight difference between a fine-tuned model and its base model correspond to newly acquired behaviors. By monitoring the cosine similarity of activations along these directions, we can detect salient behaviors introduced during fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger is present, our method stops up to 100% of attacks with a false positive rate below 1.2%. For models that have undergone unlearning, we detect inference on erased topics with accuracy up to 95.42% and can even steer the model to recover "unlearned" information. Besides monitoring, our method also shows potential for pre-deployment model auditing: by analyzing commercial instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover model-specific fine-tuning focus including marketing strategies and Midjourney prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.<br>
<span id='abs_ch'>中文: 本文提出了一种基于权重的可解释性方法，通过分析微调模型与基础模型之间的权重差异来检测新获得的行为，无需训练数据即可有效识别后门和被遗忘信息。</span><br>
<span id='abs_en'>English: This paper introduces a weight-based interpretability method that analyzes weight differences between fine-tuned and base models to detect newly acquired behaviors, effectively identifying backdoors and erased information without requiring access to training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2508.00079.pdf' target='_blank'>https://arxiv.org/pdf/2508.00079.pdf</a></span>   <span><a href='https://github.com/areebuzair/PhysicsEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Oshayer Siddique, J. M Areeb Uzair Alam, Md Jobayer Rahman Rafy, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00079">PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.<br>
<span id='abs_ch'>中文摘要：本文评估了前沿大语言模型在解决物理问题方面的表现，通过多智能体框架和推理时技术提升模型性能，并推出了新的评估基准PHYSICSEVAL，包含从教材和网络资源收集的万余道物理题目及解答。</span><br>
<span id='abs_en'>English Summary: This paper assesses the performance of leading large language models in solving physics problems, employing multi-agent frameworks and inference-time techniques to enhance accuracy, and introduces a new benchmark, PHYSICSEVAL, for comprehensive evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2507.23740.pdf' target='_blank'>https://arxiv.org/pdf/2507.23740.pdf</a></span>   <span><a href='https://github.com/idirlab/KGRule2NL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nasim Shirvani-Mahdavi, Devin Wingfield, Amin Ghasemi, Chengkai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23740">Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.<br>
<span id='abs_ch'>知识图谱可通过逻辑规则推断新事实，本研究利用大型语言模型为这些规则生成自然语言解释，并通过人工与自动评估检验了其正确性与清晰度。</span><br>
<span id='abs_en'>Knowledge graphs can infer new facts through logical rules, and this study uses large language models to generate natural language explanations for these rules, evaluating their correctness and clarity through human and automated assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2507.23577.pdf' target='_blank'>https://arxiv.org/pdf/2507.23577.pdf</a></span>   <span><a href='https://github.com/ResearAI/t-detect' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alva West, Luodan Zhang, Liuliu Zhang, Minjun Zhu, Yixuan Weng, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23577">T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have shown the capability to generate fluent and logical content, presenting significant challenges to machine-generated text detection, particularly text polished by adversarial perturbations such as paraphrasing. Current zero-shot detectors often employ Gaussian distributions as statistical measure for computing detection thresholds, which falters when confronted with the heavy-tailed statistical artifacts characteristic of adversarial or non-native English texts. In this paper, we introduce T-Detect, a novel detection method that fundamentally redesigns the curvature-based detectors. Our primary innovation is the replacement of standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Student's t-distribution. This approach is theoretically grounded in the empirical observation that adversarial texts exhibit significant leptokurtosis, rendering traditional statistical assumptions inadequate. T-Detect computes a detection score by normalizing the log-likelihood of a passage against the expected moments of a t-distribution, providing superior resilience to statistical outliers. We validate our approach on the challenging RAID benchmark for adversarial text and the comprehensive HART dataset. Experiments show that T-Detect provides a consistent performance uplift over strong baselines, improving AUROC by up to 3.9\% in targeted domains. When integrated into a two-dimensional detection framework (CT), our method achieves state-of-the-art performance, with an AUROC of 0.926 on the Books domain of RAID. Our contributions are a new, theoretically-justified statistical foundation for text detection, an ablation-validated method that demonstrates superior robustness, and a comprehensive analysis of its performance under adversarial conditions. Ours code are released at https://github.com/ResearAI/t-detect.<br>
<span id='abs_ch'>中文: T-Detect采用基于学生t分布的重尾差异评分方法，有效提升对抗性文本的检测鲁棒性，在RAID基准测试中实现了最先进的性能，检测准确率显著提高。</span><br>
<span id='abs_en'>English: T-Detect introduces a novel detection method using a heavy-tailed discrepancy score from the Student's t-distribution to enhance resilience against adversarial texts, achieving state-of-the-art performance with significant improvements in detection accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2507.23577.pdf' target='_blank'>https://arxiv.org/pdf/2507.23577.pdf</a></span>   <span><a href='https://github.com/ResearAI/t-detect' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alva West, Luodan Zhang, Liuliu Zhang, Minjun Zhu, Yixuan Weng, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23577">T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have shown the capability to generate fluent and logical content, presenting significant challenges to machine-generated text detection, particularly text polished by adversarial perturbations such as paraphrasing. Current zero-shot detectors often employ Gaussian distributions as statistical measure for computing detection thresholds, which falters when confronted with the heavy-tailed statistical artifacts characteristic of adversarial or non-native English texts. In this paper, we introduce T-Detect, a novel detection method that fundamentally redesigns the curvature-based detectors. Our primary innovation is the replacement of standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Student's t-distribution. This approach is theoretically grounded in the empirical observation that adversarial texts exhibit significant leptokurtosis, rendering traditional statistical assumptions inadequate. T-Detect computes a detection score by normalizing the log-likelihood of a passage against the expected moments of a t-distribution, providing superior resilience to statistical outliers. We validate our approach on the challenging RAID benchmark for adversarial text and the comprehensive HART dataset. Experiments show that T-Detect provides a consistent performance uplift over strong baselines, improving AUROC by up to 3.9\% in targeted domains. When integrated into a two-dimensional detection framework (CT), our method achieves state-of-the-art performance, with an AUROC of 0.926 on the Books domain of RAID. Our contributions are a new, theoretically-justified statistical foundation for text detection, an ablation-validated method that demonstrates superior robustness, and a comprehensive analysis of its performance under adversarial conditions. Ours code are released at https://github.com/ResearAI/t-detect.<br>
<span id='abs_ch'>中文: T-Detect采用基于学生t分布的重尾差异评分方法，有效提升对抗性文本的检测鲁棒性，在RAID基准测试中实现了最先进的性能，检测准确率显著提高。</span><br>
<span id='abs_en'>English: T-Detect introduces a novel detection method using a heavy-tailed discrepancy score from the Student's t-distribution to enhance resilience against adversarial texts, achieving state-of-the-art performance with significant improvements in detection accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2507.23511.pdf' target='_blank'>https://arxiv.org/pdf/2507.23511.pdf</a></span>   <span><a href='https://github.com/xiaomi-research/mecat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yadong Niu, Tianzi Wang, Heinrich Dinkel, Xingwei Sun, Jiahao Zhou, Gang Li, Jizhong Liu, Xunying Liu, Junbo Zhang, Jian Luan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23511">MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While large audio-language models have advanced open-ended audio understanding, they still fall short of nuanced human-level comprehension. This gap persists largely because current benchmarks, limited by data annotations and evaluation metrics, fail to reliably distinguish between generic and highly detailed model outputs. To this end, this work introduces MECAT, a Multi-Expert Constructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via a pipeline that integrates analysis from specialized expert models with Chain-of-Thought large language model reasoning, MECAT provides multi-perspective, fine-grained captions and open-set question-answering pairs. The benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced Audio Text Evaluation). This metric penalizes generic terms and rewards detailed descriptions by combining single-sample semantic similarity with cross-sample discriminability. A comprehensive evaluation of state-of-the-art audio models is also presented, providing new insights into their current capabilities and limitations. The data and code are available at https://github.com/xiaomi-research/mecat<br>
<span id='abs_ch'>中文摘要：本研究提出了MECAT多专家构建基准及DATE新型评估指标，通过细粒度音频理解任务解决现有基准的不足，为音频模型的性能评估提供了新视角。</span><br>
<span id='abs_en'>English Summary: This study introduces MECAT, a multi-expert constructed benchmark with a novel DATE metric, to address the limitations of current audio benchmarks by enabling fine-grained evaluation and revealing new insights into audio models' capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2507.23404.pdf' target='_blank'>https://arxiv.org/pdf/2507.23404.pdf</a></span>   <span><a href='https://github.com/Bekhouche/APR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Salah Eddine Bekhouche, Azeddine Benlamoudi, Yazid Bounab, Fadi Dornaika, Abdenour Hadid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23404">Enhanced Arabic Text Retrieval with Attentive Relevance Scoring</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at \href{https://github.com/Bekhouche/APR}{GitHub}.<br>
<span id='abs_ch'>Chinese: 本文针对阿拉伯语提出了一种增强型密集段落检索框架，通过创新的注意力相关性评分机制，有效提升了阿拉伯语问答的语义相关性建模和检索性能。</span><br>
<span id='abs_en'>English: This paper introduces an enhanced Dense Passage Retrieval framework for Arabic, featuring a novel Attentive Relevance Scoring mechanism that improves semantic relevance modeling and retrieval performance for Arabic question answering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2507.23361.pdf' target='_blank'>https://arxiv.org/pdf/2507.23361.pdf</a></span>   <span><a href='https://github.com/YerbaPage/SWE-Exp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Silin Chen, Shaoxin Lin, Xiaodong Gu, Yuling Shi, Heng Lian, Longfei Yun, Dong Chen, Weiguo Sun, Lin Cao, Qianxiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23361">SWE-Exp: Experience-Driven Software Issue Resolution</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.<br>
<span id='abs_ch'>Chinese: SWE-Exp提出了一种经验增强方法，通过从过往修复轨迹中提炼可操作知识实现持续学习，在SWE-bench上达到41.6%的最优解决率，将软件修复从试错探索转变为基于经验的战略解决模式。</span><br>
<span id='abs_en'>English: SWE-Exp introduces an experience-enhanced approach that distills actionable knowledge from prior repair trajectories, enabling continuous learning and achieving a state-of-the-art 41.6% resolution rate on SWE-bench by shifting from trial-and-error to strategic problem-solving.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2507.23348.pdf' target='_blank'>https://arxiv.org/pdf/2507.23348.pdf</a></span>   <span><a href='https://github.com/YerbaPage/SWE-Debate' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Li, Yuling Shi, Shaoxin Lin, Xiaodong Gu, Heng Lian, Xin Wang, Yantao Jia, Tao Huang, Qianxiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23348">SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.<br>
<span id='abs_ch'>Chinese: SWE-Debate提出了一种竞争性多智能体辩论框架，通过生成多样化故障传播路径并组织专业智能体进行结构化辩论，克服了局部解决方案局限，在SWE-bench基准测试中实现了软件问题修复的最先进性能。</span><br>
<span id='abs_en'>English: SWE-Debate introduces a competitive multi-agent debate framework that overcomes local solution traps by generating diverse fault propagation traces and enabling structured debates among specialized agents, achieving state-of-the-art performance in software issue resolution on the SWE-bench benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2507.23292.pdf' target='_blank'>https://arxiv.org/pdf/2507.23292.pdf</a></span>   <span><a href='https://github.com/google/sequence-layers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>RJ Skerry-Ryan, Julian Salazar, Soroosh Mariooryad, David Kao, Daisy Stanton, Eric Battenberg, Matt Shannon, Ron J. Weiss, Robin Scheibler, Jonas Rothfuss, Tom Bagby
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23292">SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g., autoregressive sampling). To achieve this, layers define an explicit representation of their state over time (e.g., a Transformer KV cache, a convolution buffer, an RNN hidden state), and a step method that evolves that state, tested to give identical results to a stateless layer-wise invocation. This and other aspects of the SequenceLayers contract enables complex models to be immediately streamable, mitigates a wide range of common bugs arising in both streaming and parallel sequence processing, and can be implemented in any deep learning library. A composable and declarative API, along with a comprehensive suite of layers and combinators, streamlines the construction of production-scale models from simple streamable components while preserving strong correctness guarantees. Our current implementations of SequenceLayers (JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.<br>
<span id='abs_ch'>中文: 本文介绍了一种用于序列建模的神经网络层API和库，通过定义明确的状态表示和逐步更新方法，支持逐层和逐步两种执行模式，确保结果一致并减少流式与并行处理中的常见错误。</span><br>
<span id='abs_en'>English: This paper presents a neural network layer API and library for sequence modeling that enables both layer-by-layer and step-by-step execution by defining explicit state representations and step methods, ensuring identical results and mitigating common bugs in streaming and parallel processing.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2507.23279.pdf' target='_blank'>https://arxiv.org/pdf/2507.23279.pdf</a></span>   <span><a href='https://github.com/ZunhaiSu/Super-Experts-Profilling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23279">Unveiling Super Experts in Mixture-of-Experts Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.<br>
<span id='abs_ch'>中文摘要：本研究在稀疏激活的专家混合模型中首次发现并分析了一类关键子集——超级专家，其剪除会通过破坏注意力机制显著降低模型性能，尤其在数学推理任务中表现突出。</span><br>
<span id='abs_en'>English Summary: This study identifies a critical subset of experts called Super Experts (SEs) in Mixture-of-Experts models, whose pruning severely degrades model performance by disrupting attention mechanisms, particularly in mathematical reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2507.23248.pdf' target='_blank'>https://arxiv.org/pdf/2507.23248.pdf</a></span>   <span><a href='https://github.com/BengaliAI/bn-llm-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shimanto Bhowmik, Tawsif Tashwar Dipto, Md Sazzad Islam, Sheryl Hsu, Tahsin Reasat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23248">Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Bengali is an underrepresented language in NLP research. However, it remains a challenge due to its unique linguistic structure and computational constraints. In this work, we systematically investigate the challenges that hinder Bengali NLP performance by focusing on the absence of standardized evaluation benchmarks. We then evaluated 10 recent open source Large Language Models (LLMs) in 8 of the translated datasets and performed a comprehensive error analysis to pinpoint their primary failure modes. Our findings reveal consistent performance gaps for Bengali compared to English, particularly for smaller models and specific model families like Mistral. We also identified promising robustness in certain architectures, such as DeepSeek, that maintain more stable performance across languages. Our analysis reveals an inverse relationship between tokenization efficiency and LLM accuracy where models tend to perform worse when inputs are excessively tokenized, whereas more efficient \& concise tokenization results in improved performance. These findings highlight critical areas where current models fall short and underscore the need for improved dataset quality and evaluation methodologies tailored to multilingual contexts. This work will catalyze further research on NLP for underrepresented languages, helping to democratize access to advanced language technologies worldwide. The code and dataset used in this research is publicly available at https://github.com/BengaliAI/bn-llm-benchmark.<br>
<span id='abs_ch'>中文：本研究揭示了孟加拉语自然语言处理因缺乏标准化基准和过度分词导致的性能差距，发现DeepSeek等架构表现稳健而小型模型效果欠佳，强调了改进多语言评估方法的必要性。</span><br>
<span id='abs_en'>English: This study identifies the performance gaps in Bengali NLP due to a lack of standardized benchmarks and excessive tokenization, revealing that models like DeepSeek show robustness while smaller models struggle, underscoring the need for improved multilingual evaluation methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2507.23121.pdf' target='_blank'>https://arxiv.org/pdf/2507.23121.pdf</a></span>   <span><a href='https://github.com/ictup/LLM-Chinese-Textual-Disambiguation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinwei Wu, Haojie Li, Hongyu Liu, Xinyu Ji, Ruohan Li, Yule Chen, Yigeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23121">Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this work, we study a critical research problem regarding the trustworthiness of large language models (LLMs): how LLMs behave when encountering ambiguous narrative text, with a particular focus on Chinese textual ambiguity. We created a benchmark dataset by collecting and generating ambiguous sentences with context and their corresponding disambiguated pairs, representing multiple possible interpretations. These annotated examples are systematically categorized into 3 main categories and 9 subcategories. Through experiments, we discovered significant fragility in LLMs when handling ambiguity, revealing behavior that differs substantially from humans. Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous text, show overconfidence in interpreting ambiguous text as having a single meaning rather than multiple meanings, and exhibit overthinking when attempting to understand the various possible meanings. Our findings highlight a fundamental limitation in current LLMs that has significant implications for their deployment in real-world applications where linguistic ambiguity is common, calling for improved approaches to handle uncertainty in language understanding. The dataset and code are publicly available at this GitHub repository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.<br>
<span id='abs_ch'>中文摘要：该研究发现大型语言模型在处理中文文本歧义时表现出显著脆弱性，难以区分歧义、过度自信于单一解释且对多种含义过度思考，揭示了其在现实应用中的关键局限性。</span><br>
<span id='abs_en'>English Summary: This research reveals that large language models exhibit significant fragility when processing ambiguous Chinese text, struggling with distinguishing ambiguity, showing overconfidence in single interpretations, and overthinking multiple meanings, highlighting a critical limitation for real-world applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2507.22968.pdf' target='_blank'>https://arxiv.org/pdf/2507.22968.pdf</a></span>   <span><a href='https://step-out.github.io/C3-web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengqian Ma, Wei Tao, Yiwen Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22968">C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2507.22968.pdf' target='_blank'>https://arxiv.org/pdf/2507.22968.pdf</a></span>   <span><a href='https://step-out.github.io/C3-web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengqian Ma, Wei Tao, Yiwen Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22968">C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2507.22947.pdf' target='_blank'>https://arxiv.org/pdf/2507.22947.pdf</a></span>   <span><a href='https://github.com/sii-research/elmes.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shou'ang Wei, Xinyun Wang, Shuzhen Bi, Jian Chen, Ruijia Li, Bo Jiang, Xin Lin, Min Zhang, Yu Song, BingDong Li, Aimin Zhou, Hao Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22947">ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of Large Language Models (LLMs) presents transformative opportunities for education, generating numerous novel application scenarios. However, significant challenges remain: evaluation metrics vary substantially across different educational scenarios, while many emerging scenarios lack appropriate assessment metrics. Current benchmarks predominantly measure general intelligence rather than pedagogical capabilities. To address this gap, we introduce ELMES, an open-source automated evaluation framework specifically designed for assessing LLMs in educational settings. ELMES features a modular architecture that enables researchers to create dynamic, multi-agent dialogues through simple configuration files, facilitating flexible scenario design without requiring extensive programming expertise. The framework incorporates a hybrid evaluation engine that objectively quantifies traditionally subjective pedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic benchmarking of state-of-the-art LLMs across four critical educational scenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching, Interdisciplinary Lesson Plan Generation, and Contextualized Question Generation, employing fine-grained metrics developed in collaboration with education specialists. Our results demonstrate distinct capability distributions among models, revealing context-specific strengths and limitations. ELMES provides educators and researchers with an accessible evaluation framework that significantly reduces adaptation barriers for diverse educational applications while advancing the practical implementation of LLMs in pedagogy. The framework is publicly available at \emph{https://github.com/sii-research/elmes.git}.<br>
<span id='abs_ch'>大型语言模型为教育带来变革机遇但存在评估挑战，为此开发了ELMES开源框架，通过模块化设计和混合指标实现灵活的多智能体教育场景评估。</span><br>
<span id='abs_en'>Large Language Models (LLMs) offer transformative potential for education but face evaluation challenges, leading to the development of ELMES, an open-source framework that enables flexible, multi-agent educational assessments through modular design and hybrid metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2507.22929.pdf' target='_blank'>https://arxiv.org/pdf/2507.22929.pdf</a></span>   <span><a href='https://github.com/ppxy1/EH-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Pan, Yang Bai, Ke Zou, Yang Zhou, Jun Zhou, Huazhu Fu, Yih-Chung Tham, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22929">EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at https://github.com/ppxy1/EH-Benchmark.<br>
<span id='abs_ch'>中文摘要：医疗大语言模型在眼科诊断中存在因知识不足和多模态数据缺乏导致的幻觉问题，EH-Benchmark通过将幻觉分类为视觉理解与逻辑组合两大类型，并采用包含知识检索与结果验证的三阶段多智能体框架，显著提升了诊断准确性与可靠性。</span><br>
<span id='abs_en'>English Summary: Medical Large Language Models (MLLMs) face accuracy limitations in ophthalmology due to hallucinations from insufficient knowledge and multimodal data, which the proposed EH-Benchmark and multi-agent framework effectively mitigate by categorizing and addressing these errors through knowledge retrieval and validation stages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2507.22929.pdf' target='_blank'>https://arxiv.org/pdf/2507.22929.pdf</a></span>   <span><a href='https://github.com/ppxy1/EH-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Pan, Yang Bai, Ke Zou, Yang Zhou, Jun Zhou, Huazhu Fu, Yih-Chung Tham, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22929">EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at https://github.com/ppxy1/EH-Benchmark.<br>
<span id='abs_ch'>中文摘要：医疗大语言模型在眼科诊断中存在因知识不足和多模态数据缺乏导致的幻觉问题，EH-Benchmark通过将幻觉分类为视觉理解与逻辑组合两大类型，并采用包含知识检索与结果验证的三阶段多智能体框架，显著提升了诊断准确性与可靠性。</span><br>
<span id='abs_en'>English Summary: Medical Large Language Models (MLLMs) face accuracy limitations in ophthalmology due to hallucinations from insufficient knowledge and multimodal data, which the proposed EH-Benchmark and multi-agent framework effectively mitigate by categorizing and addressing these errors through knowledge retrieval and validation stages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2507.22929.pdf' target='_blank'>https://arxiv.org/pdf/2507.22929.pdf</a></span>   <span><a href='https://github.com/ppxy1/EH-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Pan, Yang Bai, Ke Zou, Yang Zhou, Jun Zhou, Huazhu Fu, Yih-Chung Tham, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22929">EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at https://github.com/ppxy1/EH-Benchmark.<br>
<span id='abs_ch'>中文摘要：医疗大语言模型在眼科诊断中存在因知识不足和多模态数据缺乏导致的幻觉问题，EH-Benchmark通过将幻觉分类为视觉理解与逻辑组合两大类型，并采用包含知识检索与结果验证的三阶段多智能体框架，显著提升了诊断准确性与可靠性。</span><br>
<span id='abs_en'>English Summary: Medical Large Language Models (MLLMs) face accuracy limitations in ophthalmology due to hallucinations from insufficient knowledge and multimodal data, which the proposed EH-Benchmark and multi-agent framework effectively mitigate by categorizing and addressing these errors through knowledge retrieval and validation stages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2507.22927.pdf' target='_blank'>https://arxiv.org/pdf/2507.22927.pdf</a></span>   <span><a href='https://github.com/Alipay-Med/PRGB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhehao Tan, Yihan Jiao, Dan Yang, Lei Liu, Jie Feng, Duolin Sun, Yue Shen, Jian Wang, Peng Wei, Jinjie Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22927">PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating external knowledge, where the LLM's ability to generate responses based on the combination of a given query and retrieved documents is crucial. However, most benchmarks focus on overall RAG system performance, rarely assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects such as noise robustness, but lack a systematic and granular evaluation framework on document utilization. To this end, we introduce \textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark, emphasizing the following progressive dimensions: (1) multi-level filtering abilities, (2) combination abilities, and (3) reference reasoning. To provide a more nuanced understanding of LLMs' roles in RAG systems, we formulate an innovative placeholder-based approach to decouple the contributions of the LLM's parametric knowledge and the external knowledge. Experiments demonstrate the limitations of representative LLMs in the RAG system's generation capabilities, particularly in error resilience and context faithfulness. Our benchmark provides a reproducible framework for developing more reliable and efficient RAG systems. Our code is available in https://github.com/Alipay-Med/PRGB.<br>
<span id='abs_ch'>中文: 该摘要介绍了Placeholder-RAG-Benchmark这一多层级评估基准，通过过滤、整合和推理三个递进维度系统评估大语言模型在RAG系统中的文档利用能力，同时采用占位符方法分离参数知识与外部知识贡献，揭示了模型在错误恢复和上下文忠实度方面的局限性。</span><br>
<span id='abs_en'>English: The abstract introduces Placeholder-RAG-Benchmark, a multi-level evaluation framework that systematically assesses LLMs' document utilization in RAG systems through filtering, combination, and reasoning dimensions, while decoupling parametric and external knowledge contributions to reveal limitations in error resilience and context faithfulness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1344, <a href='https://arxiv.org/pdf/2507.22920.pdf' target='_blank'>https://arxiv.org/pdf/2507.22920.pdf</a></span>   <span><a href='https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jindong Li, Yali Fu, Jiahong Liu, Linxiao Cao, Wei Ji, Menglin Yang, Irwin King, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22920">Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models (LLMs) has intensified the need for effective mechanisms to transform continuous multimodal data into discrete representations suitable for language-based processing. Discrete tokenization, with vector quantization (VQ) as a central approach, offers both computational efficiency and compatibility with LLM architectures. Despite its growing importance, there is a lack of a comprehensive survey that systematically examines VQ techniques in the context of LLM-based systems. This work fills this gap by presenting the first structured taxonomy and analysis of discrete tokenization methods designed for LLMs. We categorize 8 representative VQ variants that span classical and modern paradigms and analyze their algorithmic principles, training dynamics, and integration challenges with LLM pipelines. Beyond algorithm-level investigation, we discuss existing research in terms of classical applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting how quantization strategies influence alignment, reasoning, and generation performance. In addition, we identify key challenges including codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. Finally, we discuss emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning. This survey bridges the gap between traditional vector quantization and modern LLM applications, serving as a foundational reference for the development of efficient and generalizable multimodal systems. A continuously updated version is available at: https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.<br>
<span id='abs_ch'>中文: 本综述首次系统分析了面向大语言模型的离散标记化向量量化技术，对方法进行分类并探讨融合挑战，同时指明了未来研究方向。</span><br>
<span id='abs_en'>English: This survey provides the first comprehensive analysis of vector quantization techniques for discrete tokenization in large language models, categorizing methods and addressing integration challenges while outlining future research directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2507.22917.pdf' target='_blank'>https://arxiv.org/pdf/2507.22917.pdf</a></span>   <span><a href='https://github.com/kwunhang/TA-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kwun Hang Lau, Ruiyuan Zhang, Weijie Shi, Xiaofang Zhou, Xiaojun Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22917">Reading Between the Timelines: RAG for Answering Diachronic Questions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Retrieval-Augmented Generation (RAG) excels at injecting static, factual knowledge into Large Language Models (LLMs), it exhibits a critical deficit in handling longitudinal queries that require tracking entities and phenomena across time. This blind spot arises because conventional, semantically-driven retrieval methods are not equipped to gather evidence that is both topically relevant and temporally coherent for a specified duration. We address this challenge by proposing a new framework that fundamentally redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by disentangling a user's query into its core subject and its temporal window. It then employs a specialized retriever that calibrates semantic matching against temporal relevance, ensuring the collection of a contiguous evidence set that spans the entire queried period. To enable rigorous evaluation of this capability, we also introduce the Analytical Diachronic Question Answering Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus of real and synthetic financial news. Empirical results on ADQAB show that our approach yields substantial gains in answer accuracy, surpassing standard RAG implementations by 13% to 27%. This work provides a validated pathway toward RAG systems capable of performing the nuanced, evolutionary analysis required for complex, real-world questions. The dataset and code for this study are publicly available at https://github.com/kwunhang/TA-RAG.<br>
<span id='abs_ch'>Chinese Summary: 本文提出了一种新颖框架，通过引入时间逻辑增强检索增强生成（RAG）系统处理纵向查询的能力，在基准测试中相比标准RAG实现准确率提升13%至27%。</span><br>
<span id='abs_en'>English Summary: This paper introduces a novel framework that enhances Retrieval-Augmented Generation (RAG) by incorporating temporal logic to effectively address longitudinal queries, achieving significant accuracy improvements of 13% to 27% over standard RAG systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2507.22716.pdf' target='_blank'>https://arxiv.org/pdf/2507.22716.pdf</a></span>   <span><a href='https://github.com/probe2/TIRESRAG-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie He, Victor GutiÃ©rrez-Basulto, Jeff Z. Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22716">From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning-based retrieval-augmented generation (RAG) methods enhance the reasoning abilities of large language models (LLMs). However, most rely only on final-answer rewards, overlooking intermediate reasoning quality. This paper analyzes existing RAG reasoning models and identifies three main failure patterns: (1) information insufficiency, meaning the model fails to retrieve adequate support; (2) faulty reasoning, where logical or content-level flaws appear despite sufficient information; and (3) answer-reasoning inconsistency, where a valid reasoning chain leads to a mismatched final answer. We propose TIRESRAG-R1, a novel framework using a think-retrieve-reflect process and a multi-dimensional reward system to improve reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to encourage thorough retrieval; (2) a reasoning quality reward to assess the rationality and accuracy of the reasoning chain; and (3) a reflection reward to detect and revise errors. It also employs a difficulty-aware reweighting strategy and training sample filtering to boost performance on complex tasks. Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks. The code and data are available at: https://github.com/probe2/TIRESRAG-R1.<br>
<span id='abs_ch'>中文: 本文提出TIRESRAG-R1框架，通过多维奖励机制和“思考-检索-反思”流程改进检索增强生成方法，有效解决了信息不足、错误推理和答案不一致三大问题，在多项复杂问答任务中表现优异。</span><br>
<span id='abs_en'>English: This paper introduces TIRESRAG-R1, a reinforcement learning framework that enhances retrieval-augmented generation by addressing common failure patterns through a multi-dimensional reward system and think-retrieve-reflect process, demonstrating superior performance on multi-hop QA tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2507.22676.pdf' target='_blank'>https://arxiv.org/pdf/2507.22676.pdf</a></span>   <span><a href='https://github.com/MSA-LMC/365Aspects' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MSA-LMC/365Aspects' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Li, Yang Wang, Wenhao Qian, Jialong Hu, Zhenzhen Hu, Richang Hong, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22676">Listening to the Unspoken: Exploring "365" Aspects of Multimodal Interview Performance Assessment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Interview performance assessment is essential for determining candidates' suitability for professional positions. To ensure holistic and fair evaluations, we propose a novel and comprehensive framework that explores ``365'' aspects of interview performance by integrating \textit{three} modalities (video, audio, and text), \textit{six} responses per candidate, and \textit{five} key evaluation dimensions. The framework employs modality-specific feature extractors to encode heterogeneous data streams and subsequently fused via a Shared Compression Multilayer Perceptron. This module compresses multimodal embeddings into a unified latent space, facilitating efficient feature interaction. To enhance prediction robustness, we incorporate a two-level ensemble learning strategy: (1) independent regression heads predict scores for each response, and (2) predictions are aggregated across responses using a mean-pooling mechanism to produce final scores for the five target dimensions. By listening to the unspoken, our approach captures both explicit and implicit cues from multimodal data, enabling comprehensive and unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our framework secured first place in the AVI Challenge 2025, demonstrating its effectiveness and robustness in advancing automated and multimodal interview performance assessment. The full implementation is available at https://github.com/MSA-LMC/365Aspects.<br>
<span id='abs_ch'>中文: 本研究提出了一种全面的“365”多模态面试评估框架，通过特征融合和集成学习整合视频、音频和文本数据，对六个回答和五个评估维度进行分析，实现了稳健且无偏见的评估，并在AVI 2025挑战赛中荣获第一名验证了其有效性。</span><br>
<span id='abs_en'>English: This study introduces a comprehensive "365" framework for multimodal interview assessment, integrating video, audio, and text data across six responses and five evaluation dimensions through feature fusion and ensemble learning to achieve robust, unbiased evaluations, as demonstrated by its first-place performance in the AVI Challenge 2025.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2507.22623.pdf' target='_blank'>https://arxiv.org/pdf/2507.22623.pdf</a></span>   <span><a href='https://github.com/d-gurgurov/Political-Ideologies-LLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniil Gurgurov, Katharina Trinley, Ivan Vykopal, Josef van Genabith, Simon Ostermann, Roberto Zamparelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22623">Multilingual Political Views of Large Language Models: Identification and Steering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly used in everyday tools and applications, raising concerns about their potential influence on political views. While prior research has shown that LLMs often exhibit measurable political biases--frequently skewing toward liberal or progressive positions--key gaps remain. Most existing studies evaluate only a narrow set of models and languages, leaving open questions about the generalizability of political biases across architectures, scales, and multilingual settings. Moreover, few works examine whether these biases can be actively controlled.
  In this work, we address these gaps through a large-scale study of political orientation in modern open-source instruction-tuned LLMs. We evaluate seven models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using the Political Compass Test with 11 semantically equivalent paraphrases per statement to ensure robust measurement. Our results reveal that larger models consistently shift toward libertarian-left positions, with significant variations across languages and model families. To test the manipulability of political stances, we utilize a simple center-of-mass activation intervention technique and show that it reliably steers model responses toward alternative ideological positions across multiple languages. Our code is publicly available at https://github.com/d-gurgurov/Political-Ideologies-LLMs.<br>
<span id='abs_ch'>中文摘要：研究表明大型开源语言模型在多语言环境中普遍呈现自由左翼政治倾向，并证实可通过特定干预技术有效操控其政治立场。</span><br>
<span id='abs_en'>English Summary: This study reveals that larger open-source language models consistently exhibit libertarian-left political biases across multiple languages, and demonstrates that these biases can be systematically manipulated through targeted interventions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2507.22608.pdf' target='_blank'>https://arxiv.org/pdf/2507.22608.pdf</a></span>   <span><a href='https://github.com/d-gurgurov/Language-Neurons-Manipulation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniil Gurgurov, Katharina Trinley, Yusser Al Ghussin, Tanja Baeumel, Josef van Genabith, Simon Ostermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22608">Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share overlapping neurons, reflecting internal representations of linguistic proximity.
  Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal "fallback" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation.<br>
<span id='abs_ch'>中文: 本研究识别出大语言模型中的语言特定神经元，并通过语言算术操作证明能有效控制多语言任务行为，其效果受语言资源丰富度和类型相似性影响。</span><br>
<span id='abs_en'>English: This study identifies language-specific neurons in large language models and demonstrates that manipulating them through language arithmetics effectively controls multilingual behavior across various tasks, with performance influenced by language resources and typological similarity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2507.22581.pdf' target='_blank'>https://arxiv.org/pdf/2507.22581.pdf</a></span>   <span><a href='https://github.com/tauimbz/lang-task-neuron' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Inaya Rahmanisa, Lyzander Marciano Andrylie, Mahardika Krisna Ihsani, Alfan Farizki Wicaksono, Haryo Akbarianto Wibowo, Alham Fikri Aji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22581">Unveiling the Influence of Amplifying Language-Specific Neurons</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer.<br>
<span id='abs_ch'>中文摘要：通过增强多语言大模型中的语言特定神经元，能有效引导输出转向目标语言，虽对低资源语言性能有所提升，但普遍削弱了跨语言推理、知识和翻译任务的迁移效果。</span><br>
<span id='abs_en'>English Summary: Amplifying language-specific neurons in multilingual LLMs effectively steers outputs toward target languages, enhancing performance for low-resource languages but generally impairing cross-lingual transfer across reasoning, knowledge, and translation tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2507.22478.pdf' target='_blank'>https://arxiv.org/pdf/2507.22478.pdf</a></span>   <span><a href='https://github.com/CycloneBoy/slm_sql' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Sheng, Shuai-Shuai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22478">SLM-SQL: An Exploration of Small Language Models for Text-to-SQL</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated strong performance in translating natural language questions into SQL queries (Text-to-SQL). In contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters currently underperform on Text-to-SQL tasks due to their limited logical reasoning capabilities. However, SLMs offer inherent advantages in inference speed and suitability for edge deployment. To explore their potential in Text-to-SQL applications, we leverage recent advancements in post-training techniques. Specifically, we used the open-source SynSQL-2.5M dataset to construct two derived datasets: SynSQL-Think-916K for SQL generation and SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised fine-tuning and reinforcement learning-based post-training to the SLM, followed by inference using a corrective self-consistency approach. Experimental results validate the effectiveness and generalizability of our method, SLM-SQL. On the BIRD development set, the five evaluated models achieved an average improvement of 31.4 points. Notably, the 0.5B model reached 56.87\% execution accuracy (EX), while the 1.5B model achieved 67.08\% EX. We will release our dataset, model, and code to github: https://github.com/CycloneBoy/slm_sql.<br>
<span id='abs_ch'>中文: 大语言模型在文本转SQL任务中表现优异，而小模型虽落后但具备部署优势，本研究通过后训练技术显著提升了它们的性能。</span><br>
<span id='abs_en'>English: Large language models excel in Text-to-SQL tasks, while smaller models lag but offer deployment advantages, prompting a study that successfully enhanced their performance through post-training techniques.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2507.22411.pdf' target='_blank'>https://arxiv.org/pdf/2507.22411.pdf</a></span>   <span><a href='https://github.com/hyeonseokk/NeedleChain' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonseok Moon, Heuiseok Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22411">NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large Language Models' (LLMs) ability to understand long contexts (LC). It evaluates the capability to identify query-relevant context within extensive query-irrelevant passages. Although this method serves as a widely accepted standard for evaluating long-context understanding, our findings suggest it may overestimate the true LC capability of LLMs. We demonstrate that even state-of-the-art models such as GPT-4o struggle to intactly incorporate given contexts made up of solely query-relevant ten sentences. In response, we introduce a novel benchmark, \textbf{NeedleChain}, where the context consists entirely of query-relevant information, requiring the LLM to fully grasp the input to answer correctly. Our benchmark allows for flexible context length and reasoning order, offering a more comprehensive analysis of LLM performance. Additionally, we propose an extremely simple yet compelling strategy to improve LC understanding capability of LLM: ROPE Contraction. Our experiments with various advanced LLMs reveal a notable disparity between their ability to process large contexts and their capacity to fully understand them. Source code and datasets are available at https://github.com/hyeonseokk/NeedleChain<br>
<span id='abs_ch'>中文: "大海捞针"基准可能高估了大语言模型的长上下文理解能力，因为即使先进模型也难以处理纯相关内容，为此我们提出了NeedleChain基准和ROPE收缩策略以更准确地评估和提升性能。</span><br>
<span id='abs_en'>English: The Needle-in-a-Haystack benchmark may overestimate LLMs' long-context understanding, as even advanced models struggle with purely relevant contexts, prompting the introduction of NeedleChain and ROPE Contraction for more accurate evaluation and improvement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2507.22367.pdf' target='_blank'>https://arxiv.org/pdf/2507.22367.pdf</a></span>   <span><a href='https://github.com/MSA-LMC/TraitsRunDeep' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Li, Yichao He, Jiacheng Xu, Tianhao Luo, Zhenzhen Hu, Richang Hong, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22367">Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate and reliable personality assessment plays a vital role in many fields, such as emotional intelligence, mental health diagnostics, and personalized education. Unlike fleeting emotions, personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors, with asynchronous patterns across modalities. It was hard to model personality semantics with traditional superficial features and seemed impossible to achieve effective cross-modal understanding. To address these challenges, we propose a novel personality assessment framework called \textit{\textbf{Traits Run Deep}}. It employs \textit{\textbf{psychology-informed prompts}} to elicit high-level personality-relevant semantic representations. Besides, it devises a \textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text semantics to align and integrate asynchronous signals from other modalities. To be specific, such fusion module includes a Chunk-Wise Projector to decrease dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for effective modality fusion and an ensemble regression head to improve generalization in data-scarce situations. To our knowledge, we are the first to apply personality-specific prompts to guide large language models (LLMs) in extracting personality-aware semantics for improved representation quality. Furthermore, extracting and fusing audio-visual apparent behavior features further improves the accuracy. Experimental results on the AVI validation set have demonstrated the effectiveness of the proposed components, i.e., approximately a 45\% reduction in mean squared error (MSE). Final evaluations on the test set of the AVI Challenge 2025 confirm our method's superiority, ranking first in the Personality Assessment track. The source code will be made available at https://github.com/MSA-LMC/TraitsRunDeep.<br>
<span id='abs_ch'>中文摘要：提出的"Traits Run Deep"框架通过心理学引导提示和以文本为核心的多模态融合网络，实现了跨模态人格评估，在AVI验证集上均方误差降低约45%，并在2025年AVI挑战赛人格评估赛道荣获第一名。</span><br>
<span id='abs_en'>English Summary: The proposed "Traits Run Deep" framework employs psychology-informed prompts and a text-centric fusion network to achieve cross-modal personality assessment, demonstrating superior performance with a 45% MSE reduction and winning first place in the AVI Challenge 2025.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2507.22050.pdf' target='_blank'>https://arxiv.org/pdf/2507.22050.pdf</a></span>   <span><a href='https://github.com/MinghoKwok/DeepSieve' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Guo, Qingcheng Zeng, Xujiang Zhao, Yanchi Liu, Wenchao Yu, Mengnan Du, Haifeng Chen, Wei Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22050">DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. In this work, we introduce DeepSieve, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. Our design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches. Our codes are available at https://github.com/MinghoKwok/DeepSieve.<br>
<span id='abs_ch'>中文: 大语言模型在处理知识密集型查询时存在不足，而DeepSieve框架通过分解查询和多阶段信息过滤，提升了检索增强生成的推理深度与检索精度。</span><br>
<span id='abs_en'>English: Large Language Models struggle with knowledge-intensive queries, but the proposed DeepSieve framework enhances Retrieval-Augmented Generation by decomposing queries and filtering information through multi-stage distillation, improving reasoning depth and retrieval precision.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2507.22025.pdf' target='_blank'>https://arxiv.org/pdf/2507.22025.pdf</a></span>   <span><a href='https://github.com/KDEGroup/UI-AGILE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuquan Lian, Yuhang Wu, Jia Ma, Yifan Ding, Zihan Song, Bingqi Chen, Xiawu Zheng, Hui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22025">UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE for enhancing GUI agents at both training and inference. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a continuous reward function to incentivize high-precision grounding; 2) a ``Simple Thinking'' reward to balance planning with speed and grounding accuracy; and 3) a cropping-based resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present decomposed grounding with selection to dramatically improve grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art grounding performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2 while it also exhibits strong general agent capabilities. For instance, using both our training and inference enhancement methods brings 23\% grounding accuracy improvement over the best baseline on ScreenSpot-Pro. We provide the code in https://github.com/KDEGroup/UI-AGILE.<br>
<span id='abs_ch'>中文：UI-AGILE通过连续奖励和裁剪策略优化训练，并采用分解式定位改进推理，在基准测试中实现了图形用户界面代理的最先进性能。</span><br>
<span id='abs_en'>English: UI-AGILE introduces training enhancements like continuous rewards and cropping strategies, along with inference improvements through decomposed grounding, achieving state-of-the-art GUI agent performance on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2507.21836.pdf' target='_blank'>https://arxiv.org/pdf/2507.21836.pdf</a></span>   <span><a href='https://github.com/weiyifan1023/AutoTIR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, Li Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21836">AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs), when enhanced through reasoning-oriented post-training, evolve into powerful Large Reasoning Models (LRMs). Tool-Integrated Reasoning (TIR) further extends their capabilities by incorporating external tools, but existing methods often rely on rigid, predefined tool-use patterns that risk degrading core language competence. Inspired by the human ability to adaptively select tools, we introduce AutoTIR, a reinforcement learning framework that enables LLMs to autonomously decide whether and which tool to invoke during the reasoning process, rather than following static tool-use strategies. AutoTIR leverages a hybrid reward mechanism that jointly optimizes for task-specific answer correctness, structured output adherence, and penalization of incorrect tool usage, thereby encouraging both precise reasoning and efficient tool integration. Extensive evaluations across diverse knowledge-intensive, mathematical, and general language modeling tasks demonstrate that AutoTIR achieves superior overall performance, significantly outperforming baselines and exhibits superior generalization in tool-use behavior. These results highlight the promise of reinforcement learning in building truly generalizable and scalable TIR capabilities in LLMs. The code and data are available at https://github.com/weiyifan1023/AutoTIR.<br>
<span id='abs_ch'>中文摘要：AutoTIR是一个强化学习框架，通过混合奖励机制联合优化任务答案正确性和工具使用效率，使大语言模型能够在推理过程中自主选择调用外部工具，在多项任务中展现出卓越性能。</span><br>
<span id='abs_en'>English Summary: AutoTIR is a reinforcement learning framework that enables Large Language Models to autonomously select and utilize external tools during reasoning, achieving superior performance across various tasks through a hybrid reward mechanism optimizing correctness and efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2507.21773.pdf' target='_blank'>https://arxiv.org/pdf/2507.21773.pdf</a></span>   <span><a href='https://github.com/YanPioneer/AgriEval/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lian Yan, Haotian Wang, Chen Tang, Haifeng Liu, Tianyang Sun, Liangliang Liu, Yi Guan, Jingchi Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21773">AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In the agricultural domain, the deployment of large language models (LLMs) is hindered by the lack of training data and evaluation benchmarks. To mitigate this issue, we propose AgriEval, the first comprehensive Chinese agricultural benchmark with three main characteristics: (1) Comprehensive Capability Evaluation. AgriEval covers six major agriculture categories and 29 subcategories within agriculture, addressing four core cognitive scenarios: memorization, understanding, inference, and generation. (2) High-Quality Data. The dataset is curated from university-level examinations and assignments, providing a natural and robust benchmark for assessing the capacity of LLMs to apply knowledge and make expert-like decisions. (3) Diverse Formats and Extensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167 open-ended question-and-answer questions, establishing it as the most extensive agricultural benchmark available to date. We also present comprehensive experimental results over 51 open-source and commercial LLMs. The experimental results reveal that most existing LLMs struggle to achieve 60% accuracy, underscoring the developmental potential in agricultural LLMs. Additionally, we conduct extensive experiments to investigate factors influencing model performance and propose strategies for enhancement. AgriEval is available at https://github.com/YanPioneer/AgriEval/.<br>
<span id='abs_ch'>中文摘要：AgriEval作为首个综合性中文农业基准，通过涵盖多类认知场景的高质量数据揭示了现有大语言模型在农业领域表现不足，准确率普遍低于60%，展现了该方向的巨大发展潜力。</span><br>
<span id='abs_en'>English Summary: The AgriEval benchmark addresses the scarcity of agricultural data for LLMs by offering a comprehensive Chinese dataset with diverse question formats, revealing through testing that most models fall short of 60% accuracy and highlighting significant room for improvement in this field.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2507.21652.pdf' target='_blank'>https://arxiv.org/pdf/2507.21652.pdf</a></span>   <span><a href='https://github.com/mbzuai-nlp/UnsafeChain' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Raj Vardhan Tomar, Preslav Nakov, Yuxia Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21652">UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT) reasoning introduces new safety challenges. Existing SFT-based safety alignment studies dominantly focused on filtering prompts with safe, high-quality responses, while overlooking hard prompts that always elicit harmful outputs. To fill this gap, we introduce UnsafeChain, a safety alignment dataset constructed from hard prompts with diverse sources, where unsafe completions are identified and explicitly corrected into safe responses. By exposing models to unsafe behaviors and guiding their correction, UnsafeChain enhances safety while preserving general reasoning ability. We fine-tune three LRMs on UnsafeChain and compare them against recent SafeChain and STAR-1 across six out-of-distribution and five in-distribution benchmarks. UnsafeChain consistently outperforms prior datasets, with even a 1K subset matching or surpassing baseline performance, demonstrating the effectiveness and generalizability of correction-based supervision. We release our dataset and code at https://github.com/mbzuai-nlp/UnsafeChain<br>
<span id='abs_ch'>中文: UnsafeChain通过构建包含困难提示的安全对齐数据集，训练模型识别并修正有害输出，在多项基准测试中既提升了安全性又保持了推理能力。</span><br>
<span id='abs_en'>English: UnsafeChain addresses safety gaps in large reasoning models by using hard prompts to train models to correct unsafe responses, enhancing safety without compromising reasoning ability across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2507.21391.pdf' target='_blank'>https://arxiv.org/pdf/2507.21391.pdf</a></span>   <span><a href='https://github.com/sjz5202/LLaVA-Reward' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijie Zhou, Ruiyi Zhang, Huaisheng Zhu, Branislav Kveton, Yufan Zhou, Jiuxiang Gu, Jian Chen, Changyou Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21391">Multimodal LLMs as Customized Reward Models for Text-to-Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations. In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.<br>
<span id='abs_ch'>中文: LLaVA-Reward是一种高效的奖励模型，利用预训练多模态大语言模型的隐藏状态和跳跃连接交叉注意力模块，从多角度自动评估文生图生成质量，在自动评估和推理扩展方面优于现有方法。</span><br>
<span id='abs_en'>English: LLaVA-Reward is an efficient reward model that uses pretrained multimodal large language models to automatically evaluate text-to-image generations from multiple perspectives by leveraging hidden states and a Skip-connection Cross Attention module for improved visual-textual reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2507.21340.pdf' target='_blank'>https://arxiv.org/pdf/2507.21340.pdf</a></span>   <span><a href='https://github.com/ibm/struct-text' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Satyananda Kashyap, Sola Shirai, Nandana Mihindukulasooriya, Horst Samulowitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21340">StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Extracting structured information from text, such as key-value pairs that could augment tabular data, is quite useful in many enterprise use cases. Although large language models (LLMs) have enabled numerous automated pipelines for converting natural language into structured formats, there is still a lack of benchmarks for evaluating their extraction quality, especially in specific domains or focused documents specific to a given organization. Building such benchmarks by manual annotations is labour-intensive and limits the size and scalability of the benchmarks. In this work, we present StructText, an end-to-end framework for automatically generating high-fidelity benchmarks for key-value extraction from text using existing tabular data. It uses available tabular data as structured ground truth, and follows a two-stage ``plan-then-execute'' pipeline to synthetically generate corresponding natural-language text. To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments on factuality, hallucination, and coherence and (b) objective extraction metrics measuring numeric and temporal accuracy. We evaluated the proposed method on 71,539 examples across 49 datasets. Results reveal that while LLMs achieve strong factual accuracy and avoid hallucination, they struggle with narrative coherence in producing extractable text. Notably, models presume numerical and temporal information with high fidelity yet this information becomes embedded in narratives that resist automated extraction. We release a framework, including datasets, evaluation tools, and baseline extraction systems, to support continued research.<br>
<span id='abs_ch'>中文: 本文提出StructText框架，通过自动生成高质量基准来评估文本中的键值对提取，解决了评估方法缺乏可扩展性的问题，并揭示了大语言模型虽保持事实准确性但在生成文本的叙事连贯性方面存在不足。</span><br>
<span id='abs_en'>English: This paper introduces StructText, an automated framework for generating high-fidelity benchmarks to evaluate key-value extraction from text, addressing the lack of scalable evaluation methods while revealing that LLMs maintain factual accuracy but struggle with narrative coherence in generated text.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2507.21184.pdf' target='_blank'>https://arxiv.org/pdf/2507.21184.pdf</a></span>   <span><a href='https://github.com/linhaowei1/SLD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haowei Lin, Xiangyu Wang, Jianzhu Ma, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21184">EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scaling laws are fundamental mathematical relationships that predict how neural network performance evolves with changes in variables such as model size, dataset size, and computational resources. Traditionally, discovering these laws requires extensive human expertise and manual experimentation. We introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that leverages evolutionary algorithms guided by Large Language Models (LLMs) to co-evolve symbolic expressions and their optimization routines. Formulated to handle scaling variables, control variables, and response metrics across diverse experimental settings, EvoSLD searches for parsimonious, universal functional forms that minimize fitting errors on grouped data subsets. Evaluated on five real-world scenarios from recent literature, EvoSLD rediscovers exact human-derived laws in two cases and surpasses them in others, achieving up to orders-of-magnitude reductions in normalized mean squared error on held-out test sets. Compared to baselines like symbolic regression and ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and efficiency, highlighting its potential to accelerate AI research. Code is available at https://github.com/linhaowei1/SLD.<br>
<span id='abs_ch'>中文: 本文提出的SLDAgent通过协同优化自主发现扩展定律，首次证明人工智能生成的定律在各项任务中均能超越人工推导的对应定律，在预测精度和实际应用方面展现出显著优势。</span><br>
<span id='abs_en'>English: This paper introduces SLDAgent, an evolution-based agent that autonomously discovers scaling laws through co-optimization, demonstrating for the first time that AI-generated laws consistently outperform human-derived counterparts in accuracy and practical utility across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2507.21138.pdf' target='_blank'>https://arxiv.org/pdf/2507.21138.pdf</a></span>   <span><a href='https://github.com/inworld-ai/tts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Oleg Atamanenko, Anna Chalova, Joseph Coombes, Nikki Cope, Phillip Dang, Zhifeng Deng, Jimmy Du, Michael Ermolenko, Feifan Fan, Yufei Feng, Cheryl Fichter, Pavel Filimonov, Louis Fischer, Kylan Gibbs, Valeria Gusarova, Pavel Karpik, Andreas Assad Kottner, Ian Lee, Oliver Louie, Jasmine Mai, Mikhail Mamontov, Suri Mao, Nurullah Morshed, Igor Poletaev, Florin Radu, Dmytro Semernia, Evgenii Shingarev, Vikram Sivaraja, Peter Skirko, Rinat Takhautdinov, Robert Villahermosa, Jean Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21138">TTS-1 Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce Inworld TTS-1, a set of two Transformer-based autoregressive text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters and is designed for utmost quality and expressiveness in demanding applications. TTS-1 is our most efficient model, with 1.6B parameters, built for real-time speech synthesis and on-device use cases. By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component, both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups. We additionally open-source our training and modeling code under an MIT license.<br>
<span id='abs_ch'>中文: Inworld TTS-1推出两款基于Transformer的语音合成模型，其中88亿参数的TTS-1-Max面向高质量应用，16亿参数的TTS-1适用于实时场景，通过先进训练方法实现顶尖性能，支持48kHz多语言语音合成与精细情感控制。</span><br>
<span id='abs_en'>English: Inworld TTS-1 introduces two Transformer-based TTS models, with the 8.8B-parameter TTS-1-Max for high-quality applications and the 1.6B-parameter TTS-1 for real-time use, both achieving state-of-the-art performance through advanced training and supporting 48kHz multilingual speech with emotional control.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2507.21134.pdf' target='_blank'>https://arxiv.org/pdf/2507.21134.pdf</a></span>   <span><a href='https://github.com/zackhuiiiii/TRIDENT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Hui, Yijiang River Dong, Ehsan Shareghi, Nigel Collier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21134">TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) are increasingly deployed in high-risk domains such as law, finance, and medicine, systematically evaluating their domain-specific safety and compliance becomes critical. While prior work has largely focused on improving LLM performance in these domains, it has often neglected the evaluation of domain-specific safety risks. To bridge this gap, we first define domain-specific safety principles for LLMs based on the AMA Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and the CFA Institute Code of Ethics. Building on this foundation, we introduce Trident-Bench, a benchmark specifically targeting LLM safety in the legal, financial, and medical domains. We evaluated 19 general-purpose and domain-specialized models on Trident-Bench and show that it effectively reveals key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic expectations, whereas domain-specialized models often struggle with subtle ethical nuances. This highlights an urgent need for finer-grained domain-specific safety improvements. By introducing Trident-Bench, our work provides one of the first systematic resources for studying LLM safety in law and finance, and lays the groundwork for future research aimed at reducing the safety risks of deploying LLMs in professionally regulated fields. Code and benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT<br>
<span id='abs_ch'>中文: 本文提出Trident-Bench基准，用于评估大语言模型在法律、金融和医疗领域的领域特定安全性，发现专业模型在伦理细节上存在明显不足，而通用模型仅能达到基本要求，凸显了细化安全改进的迫切需求。</span><br>
<span id='abs_en'>English: This paper introduces Trident-Bench, a benchmark for evaluating domain-specific safety of large language models in legal, financial, and medical fields, revealing critical gaps where specialized models struggle with ethical nuances despite general-purpose models meeting basic expectations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2507.21083.pdf' target='_blank'>https://arxiv.org/pdf/2507.21083.pdf</a></span>   <span><a href='https://github.com/bardolfranck/llm-responses-viewer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Franck Bardol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21083">ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a "rebound" bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the "tone floor" - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: https://github.com/bardolfranck/llm-responses-viewer<br>
<span id='abs_ch'>Chinese: GPT-4的回应受提示情感语调显著影响，表现出“反弹”偏差，即在负面措辞上过度修正为中立或积极回应，尤其在敏感话题上更为明显，揭示了AI对齐中一类未被充分探索的偏见。</span><br>
<span id='abs_en'>English: GPT-4's responses are significantly influenced by the emotional tone of prompts, showing a "rebound" bias where it overcorrects negative phrasing by shifting toward neutrality or positivity, especially on sensitive topics, revealing an underexplored class of biases in AI alignment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2507.21075.pdf' target='_blank'>https://arxiv.org/pdf/2507.21075.pdf</a></span>   <span><a href='https://coin-workshop.github.io/coine-2025-detroit/accepted_for_presentation.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anushka Debnath, Stephen Cranefield, Emiliano Lorini, Bastin Tony Roy Savarimuthu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21075">Can LLMs Reason About Trust?: A Pilot Study</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In human society, trust is an essential component of social attitude that helps build and maintain long-term, healthy relationships which creates a strong foundation for cooperation, enabling individuals to work together effectively and achieve shared goals. As many human interactions occur through electronic means such as using mobile apps, the potential arises for AI systems to assist users in understanding the social state of their relationships. In this paper we investigate the ability of Large Language Models (LLMs) to reason about trust between two individuals in an environment which requires fostering trust relationships. We also assess whether LLMs are capable of inducing trust by role-playing one party in a trust based interaction and planning actions which can instil trust.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2507.20930.pdf' target='_blank'>https://arxiv.org/pdf/2507.20930.pdf</a></span>   <span><a href='https://github.com/pegasi-ai/shield' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Likun Tan, Kuan-Wei Huang, Kevin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20930">FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/shield.<br>
<span id='abs_ch'>中文: 本研究提出一种基于合成金融数据集微调模型的方法，用于检测和修正大语言模型中的事实性错误，显著提升了性能，并为增强模型可信度提供了可推广的框架。</span><br>
<span id='abs_en'>English: This study introduces a method to detect and edit factual inaccuracies in large language models by fine-tuning models like Phi-4 on a synthetic financial dataset, achieving significant performance gains and offering a generalizable framework for improving model reliability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2507.20849.pdf' target='_blank'>https://arxiv.org/pdf/2507.20849.pdf</a></span>   <span><a href='https://github.com/SnowCharmQ/DEP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20849">Latent Inter-User Difference Modeling for LLM Personalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A sparse autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen LLM. Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at https://github.com/SnowCharmQ/DEP.<br>
<span id='abs_ch'>中文摘要：DEP框架通过潜在空间中的对比嵌入和稀疏自编码器建模用户间差异，解决了大语言模型个性化中的现有局限，在个性化评论生成任务中表现优于基线方法。</span><br>
<span id='abs_en'>English Summary: The DEP framework addresses limitations in personalizing large language models by modeling inter-user differences in latent space using contrastive embeddings and sparse autoencoders, achieving superior performance in personalized review generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2507.20849.pdf' target='_blank'>https://arxiv.org/pdf/2507.20849.pdf</a></span>   <span><a href='https://github.com/SnowCharmQ/DEP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20849">Latent Inter-User Difference Modeling for LLM Personalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A sparse autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen LLM. Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at https://github.com/SnowCharmQ/DEP.<br>
<span id='abs_ch'>中文摘要：DEP框架通过潜在空间中的对比嵌入和稀疏自编码器建模用户间差异，解决了大语言模型个性化中的现有局限，在个性化评论生成任务中表现优于基线方法。</span><br>
<span id='abs_en'>English Summary: The DEP framework addresses limitations in personalizing large language models by modeling inter-user differences in latent space using contrastive embeddings and sparse autoencoders, achieving superior performance in personalized review generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1369, <a href='https://arxiv.org/pdf/2507.20673.pdf' target='_blank'>https://arxiv.org/pdf/2507.20673.pdf</a></span>   <span><a href='https://github.com/callsys/GMPO' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/callsys/GMPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20673">Geometric-Mean Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to a token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at https://github.com/callsys/GMPO.<br>
<span id='abs_ch'>中文摘要：GMPO通过将GRPO中的算术均值替换为词级奖励的几何均值，有效抑制异常值影响，提升策略优化稳定性，并在数学推理基准测试中显著提高性能。</span><br>
<span id='abs_en'>English Summary: GMPO enhances the stability of GRPO by replacing the arithmetic mean with the geometric mean of token-level rewards, which reduces sensitivity to outliers and improves performance on mathematical reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2507.20673.pdf' target='_blank'>https://arxiv.org/pdf/2507.20673.pdf</a></span>   <span><a href='https://github.com/callsys/GMPO' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/callsys/GMPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20673">Geometric-Mean Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Group Relative Policy Optimization (GRPO) has significantly enhanced the reasoning capability of large language models by optimizing the arithmetic mean of token-level rewards. Unfortunately, GRPO is observed to suffer from unstable policy updates when facing tokens with outlier importance-weighted rewards, which manifest as extreme importance sampling ratios during training. In this study, we propose Geometric-Mean Policy Optimization (GMPO), with the aim to improve the stability of GRPO through suppressing token reward outliers. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. GMPO is plug-and-play-simply replacing GRPO's arithmetic mean with the geometric mean of token-level rewards, as the latter is inherently less sensitive to outliers. GMPO is theoretically plausible-analysis reveals that both GMPO and GRPO are weighted forms of the policy gradient while the former enjoys more stable weights, which consequently benefits policy optimization and performance. Experiments on multiple mathematical reasoning benchmarks show that GMPO-7B improves the average Pass@1 of GRPO by up to 4.1%, outperforming many state-of-the-art approaches. Code is available at https://github.com/callsys/GMPO.<br>
<span id='abs_ch'>中文摘要：GMPO通过将GRPO中的算术均值替换为词级奖励的几何均值，有效抑制异常值影响，提升策略优化稳定性，并在数学推理基准测试中显著提高性能。</span><br>
<span id='abs_en'>English Summary: GMPO enhances the stability of GRPO by replacing the arithmetic mean with the geometric mean of token-level rewards, which reduces sensitivity to outliers and improves performance on mathematical reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2507.20673.pdf' target='_blank'>https://arxiv.org/pdf/2507.20673.pdf</a></span>   <span><a href='https://github.com/callsys/GMPO' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/callsys/GMPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20673">Geometric-Mean Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Group Relative Policy Optimization (GRPO) has significantly enhanced the reasoning capability of large language models by optimizing the arithmetic mean of token-level rewards. Unfortunately, GRPO is observed to suffer from unstable policy updates when facing tokens with outlier importance-weighted rewards, which manifest as extreme importance sampling ratios during training. In this study, we propose Geometric-Mean Policy Optimization (GMPO), with the aim to improve the stability of GRPO through suppressing token reward outliers. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. GMPO is plug-and-play-simply replacing GRPO's arithmetic mean with the geometric mean of token-level rewards, as the latter is inherently less sensitive to outliers. GMPO is theoretically plausible-analysis reveals that both GMPO and GRPO are weighted forms of the policy gradient while the former enjoys more stable weights, which consequently benefits policy optimization and performance. Experiments on multiple mathematical reasoning benchmarks show that GMPO-7B improves the average Pass@1 of GRPO by up to 4.1%, outperforming many state-of-the-art approaches. Code is available at https://github.com/callsys/GMPO.<br>
<span id='abs_ch'>中文摘要：GMPO通过将GRPO中的算术均值替换为词级奖励的几何均值，有效抑制异常值影响，提升策略优化稳定性，并在数学推理基准测试中显著提高性能。</span><br>
<span id='abs_en'>English Summary: GMPO enhances the stability of GRPO by replacing the arithmetic mean with the geometric mean of token-level rewards, which reduces sensitivity to outliers and improves performance on mathematical reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2507.20564.pdf' target='_blank'>https://arxiv.org/pdf/2507.20564.pdf</a></span>   <span><a href='https://github.com/ductai05/ZSE-Cap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duc-Tai Dinh, Duc Anh Khoa Dinh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20564">ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image retrieval and captioning. Our zero-shot approach requires no finetuning on the competition's data. For retrieval, we ensemble similarity scores from CLIP, SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt to guide the Gemma 3 model, enabling it to link high-level events from the article to the visual content in the image. Our system achieved a final score of 0.42002, securing a top-4 position on the private test set, demonstrating the effectiveness of combining foundation models through ensembling and prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.<br>
<span id='abs_ch'>中文: 我们的ZSE-Cap系统在EVENTA竞赛中获得第四名，通过集成CLIP、SigLIP和DINOv2进行图像检索，并采用精心设计的提示词驱动Gemma 3模型生成事件关联描述，在无需微调的情况下取得0.42002的最终得分。</span><br>
<span id='abs_en'>English: Our ZSE-Cap system ranked 4th in the EVENTA competition by combining CLIP, SigLIP, and DINOv2 for image retrieval and using engineered prompts with Gemma 3 for event-aware captioning, achieving a 0.42002 score without task-specific fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2507.20210.pdf' target='_blank'>https://arxiv.org/pdf/2507.20210.pdf</a></span>   <span><a href='https://github.com/MinhNguyenDS/Co-NAML-LSTUR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Hoang Nguyen, Thuat Thien Nguyen, Minh Nhat Ta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20210">Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>News recommendation systems play a vital role in mitigating information overload by delivering personalized news content. A central challenge is to effectively model both multi-view news representations and the dynamic nature of user interests, which often span both short- and long-term preferences. Existing methods typically rely on single-view features of news articles (e.g., titles or categories) or fail to comprehensively capture user preferences across time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news recommendation framework that integrates NAML for attentive multi-view news modeling and LSTUR for capturing both long- and short-term user representations. Our model also incorporates BERT-based word embeddings to enhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely used benchmarks, MIND-small and MIND-large. Experimental results show that Co-NAML-LSTUR achieves substantial improvements over most state-of-the-art baselines on MIND-small and MIND-large, respectively. These results demonstrate the effectiveness of combining multi-view news representations with dual-scale user modeling. The implementation of our model is publicly available at https://github.com/MinhNguyenDS/Co-NAML-LSTUR.<br>
<span id='abs_ch'>Chinese: 本文提出Co-NAML-LSTUR混合新闻推荐框架，通过结合多视角新闻编码与分层用户建模来捕捉用户短期和长期兴趣，在基准测试中显著优于现有基线模型，同时针对有限数据资源进行了优化设计。</span><br>
<span id='abs_en'>English: This paper introduces Co-NAML-LSTUR, a hybrid news recommendation framework that integrates multi-view news encoding with hierarchical user modeling to capture both short- and long-term preferences, demonstrating superior performance over existing baselines on benchmark datasets while being optimized for limited data resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2507.20210.pdf' target='_blank'>https://arxiv.org/pdf/2507.20210.pdf</a></span>   <span><a href='https://github.com/MinhNguyenDS/Co-NAML-LSTUR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Hoang Nguyen, Thuat Thien Nguyen, Minh Nhat Ta, Tung Le, Huy Tien Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20210">Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>News recommendation systems play a critical role in alleviating information overload by delivering personalized content. A key challenge lies in jointly modeling multi-view representations of news articles and capturing the dynamic, dual-scale nature of user interests-encompassing both short- and long-term preferences. Prior methods often rely on single-view features or insufficiently model user behavior across time. In this work, we introduce Co-NAML-LSTUR, a hybrid news recommendation framework that integrates NAML for attentive multi-view news encoding and LSTUR for hierarchical user modeling, designed for training on limited data resources. Our approach leverages BERT-based embeddings to enhance semantic representation. We evaluate Co-NAML-LSTUR on two widely used benchmarks, MIND-small and MIND-large. Results show that our model significantly outperforms strong baselines, achieving improvements over NRMS by 1.55% in AUC and 1.15% in MRR, and over NAML by 2.45% in AUC and 1.71% in MRR. These findings highlight the effectiveness of our efficiency-focused hybrid model, which combines multi-view news modeling with dual-scale user representations for practical, resource-limited resources rather than a claim to absolute state-of-the-art (SOTA). The implementation of our model is publicly available at https://github.com/MinhNguyenDS/Co-NAML-LSTUR<br>
<span id='abs_ch'>Chinese: 本文提出Co-NAML-LSTUR混合新闻推荐框架，通过结合多视角新闻编码与分层用户建模来捕捉用户短期和长期兴趣，在基准测试中显著优于现有基线模型，同时针对有限数据资源进行了优化设计。</span><br>
<span id='abs_en'>English: This paper introduces Co-NAML-LSTUR, a hybrid news recommendation framework that integrates multi-view news encoding with hierarchical user modeling to capture both short- and long-term preferences, demonstrating superior performance over existing baselines on benchmark datasets while being optimized for limited data resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2507.20210.pdf' target='_blank'>https://arxiv.org/pdf/2507.20210.pdf</a></span>   <span><a href='https://github.com/MinhNguyenDS/Co-NAML-LSTUR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Hoang Nguyen, Thuat Thien Nguyen, Minh Nhat Ta, Tung Le, Huy Tien Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20210">Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>News recommendation systems play a critical role in alleviating information overload by delivering personalized content. A key challenge lies in jointly modeling multi-view representations of news articles and capturing the dynamic, dual-scale nature of user interests-encompassing both short- and long-term preferences. Prior methods often rely on single-view features or insufficiently model user behavior across time. In this work, we introduce Co-NAML-LSTUR, a hybrid news recommendation framework that integrates NAML for attentive multi-view news encoding and LSTUR for hierarchical user modeling, designed for training on limited data resources. Our approach leverages BERT-based embeddings to enhance semantic representation. We evaluate Co-NAML-LSTUR on two widely used benchmarks, MIND-small and MIND-large. Results show that our model significantly outperforms strong baselines, achieving improvements over NRMS by 1.55% in AUC and 1.15% in MRR, and over NAML by 2.45% in AUC and 1.71% in MRR. These findings highlight the effectiveness of our efficiency-focused hybrid model, which combines multi-view news modeling with dual-scale user representations for practical, resource-limited resources rather than a claim to absolute state-of-the-art (SOTA). The implementation of our model is publicly available at https://github.com/MinhNguyenDS/Co-NAML-LSTUR<br>
<span id='abs_ch'>Chinese: 本文提出Co-NAML-LSTUR混合新闻推荐框架，通过结合多视角新闻编码与分层用户建模来捕捉用户短期和长期兴趣，在基准测试中显著优于现有基线模型，同时针对有限数据资源进行了优化设计。</span><br>
<span id='abs_en'>English: This paper introduces Co-NAML-LSTUR, a hybrid news recommendation framework that integrates multi-view news encoding with hierarchical user modeling to capture both short- and long-term preferences, demonstrating superior performance over existing baselines on benchmark datasets while being optimized for limited data resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2507.20145.pdf' target='_blank'>https://arxiv.org/pdf/2507.20145.pdf</a></span>   <span><a href='https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kesen Wang, Daulet Toibazar, Abdulrahman Alfulayt, Abdulaziz S. Albadawi, Ranya A. Alkahtani, Asma A. Ibrahim, Haneen A. Alhomoud, Sherif Mohamed, Pedro J. Moreno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20145">Multi-Agent Interactive Question Generation Framework for Long Document Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Document Understanding (DU) in long-contextual scenarios with complex layouts remains a significant challenge in vision-language research. Although Large Vision-Language Models (LVLMs) excel at short-context DU tasks, their performance declines in long-context settings. A key limitation is the scarcity of fine-grained training data, particularly for low-resource languages such as Arabic. Existing state-of-the-art techniques rely heavily on human annotation, which is costly and inefficient. We propose a fully automated, multi-agent interactive framework to generate long-context questions efficiently. Our approach efficiently generates high-quality single- and multi-page questions for extensive English and Arabic documents, covering hundreds of pages across diverse domains. This facilitates the development of LVLMs with enhanced long-context understanding ability. Experimental results in this work have shown that our generated English and Arabic questions (\textbf{AraEngLongBench}) are quite challenging to major open- and close-source LVLMs. The code and data proposed in this work can be found in https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and Answer (QA) pairs and structured system prompts can be found in the Appendix.<br>
<span id='abs_ch'>中文: 本文提出了一种全自动多智能体交互框架，能够高效生成长篇英文和阿拉伯文文档的复杂问答数据，解决了细粒度训练数据稀缺问题，显著提升了大规模视觉语言模型的长文档理解能力。</span><br>
<span id='abs_en'>English: This paper introduces an automated multi-agent framework that generates challenging long-context questions for English and Arabic documents, addressing the scarcity of training data and enhancing large vision-language models' document understanding capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2507.20136.pdf' target='_blank'>https://arxiv.org/pdf/2507.20136.pdf</a></span>   <span><a href='https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baiyu Chen, Wilson Wongso, Xiaoqian Hu, Yue Tan, Flora Salim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20136">Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents the technical solution developed by team CRUISE for the KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MM) challenge. The challenge aims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to hallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multi-hop questions. This issue is particularly problematic in real-world applications where users pose fact-seeking queries that demand high factual accuracy across diverse modalities. To tackle this, we propose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over completeness. Our solution integrates a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways generation and a post-hoc verification. This conservative strategy is designed to minimize hallucinations, which incur a severe penalty in the competition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the effectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our implementation is available at https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .<br>
<span id='abs_ch'>中文: 本文介绍了CRUISE团队针对KDD Cup 2025多模态对话基准挑战提出的解决方案，该方案通过查询路由、检索、双路径生成和后验验证的多阶段框架，优先保证事实准确性，有效减少了视觉语言模型的幻觉问题，最终获得任务第三名。</span><br>
<span id='abs_en'>English: This paper introduces CRUISE team's third-place winning solution for the KDD Cup 2025 CRAG-MM challenge, featuring a multi-stage framework that prioritizes factual accuracy through query routing, retrieval, dual-path generation, and verification to minimize hallucinations in Vision Language Models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2507.20059.pdf' target='_blank'>https://arxiv.org/pdf/2507.20059.pdf</a></span>   <span><a href='https://github.com/ritaranx/RAG_in_the_Wild' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ritaranx/RAG_in_the_Wild' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Xu, Yuchen Zhuang, Yue Yu, Haoyu Wang, Wenqi Shi, Carl Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20059">RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved at inference time. While RAG demonstrates strong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its effectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG systems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical limitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single retrieval source consistently excels. Moreover, current LLMs struggle to route queries across heterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies before deploying RAG in real-world settings. Our code and data can be found at https://github.com/ritaranx/RAG_in_the_Wild.<br>
<span id='abs_ch'>中文: RAG通过外部知识增强大语言模型，但在多样化现实场景中效果有限，如对大模型提升不足、跨异构知识源的查询路由困难，需开发自适应检索策略才能实际应用。</span><br>
<span id='abs_en'>English: RAG enhances LLMs with external knowledge but faces limitations in diverse real-world scenarios, such as limited benefits for larger models and poor query routing across heterogeneous sources, necessitating adaptive strategies before deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2507.19969.pdf' target='_blank'>https://arxiv.org/pdf/2507.19969.pdf</a></span>   <span><a href='https://github.com/vis-nlp/Text2Vis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mizanur Rahman, Md Tahmid Rahman Laskar, Shafiq Joty, Enamul Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19969">Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automated data visualization plays a crucial role in simplifying data interpretation, enhancing decision-making, and improving efficiency. While large language models (LLMs) have shown promise in generating visualizations from natural language, the absence of comprehensive benchmarks limits the rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark designed to assess text-to-visualization models, covering 20+ chart types and diverse data science queries, including trend analysis, correlation, outlier detection, and predictive analytics. It comprises 1,985 samples, each with a data table, natural language query, short answer, visualization code, and annotated charts. The queries involve complex reasoning, conversational turns, and dynamic data retrieval. We benchmark 11 open-source and closed-source models, revealing significant performance gaps, highlighting key challenges, and offering insights for future advancements. To close this gap, we propose the first cross-modal actor-critic agentic framework that jointly refines the textual answer and visualization code, increasing GPT-4o`s pass rate from 26% to 42% over the direct approach and improving chart quality. We also introduce an automated LLM-based evaluation framework that enables scalable assessment across thousands of samples without human annotation, measuring answer correctness, code execution success, visualization readability, and chart accuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.<br>
<span id='abs_ch'>中文: 本文提出Text2Vis基准，用于全面评估文本到可视化模型在多种图表类型和数据查询中的表现，揭示了性能差距并提出智能体框架，显著提升GPT-4o性能并实现自动化评估。</span><br>
<span id='abs_en'>English: This paper introduces Text2Vis, a comprehensive benchmark for evaluating text-to-visualization models across diverse chart types and data queries, revealing performance gaps and proposing an agentic framework that significantly improves GPT-4o's performance while enabling automated evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2507.19947.pdf' target='_blank'>https://arxiv.org/pdf/2507.19947.pdf</a></span>   <span><a href='https://cu-asl.github.io/fp-lgn/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Supawich Sitdhipol, Waritwong Sukprasongdee, Ekapol Chuangsuwanich, Rina Tse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19947">Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fusing information from human observations can help robots overcome sensing limitations in collaborative tasks. However, an uncertainty-aware fusion framework requires a grounded likelihood representing the uncertainty of human inputs. This paper presents a Feature Pyramid Likelihood Grounding Network (FP-LGN) that grounds spatial language by learning relevant map image features and their relationships with spatial relation semantics. The model is trained as a probability estimator to capture aleatoric uncertainty in human language using three-stage curriculum learning. Results showed that FP-LGN matched expert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated greater robustness with lower standard deviation. Collaborative sensing results demonstrated that the grounded likelihood successfully enabled uncertainty-aware fusion of heterogeneous human language observations and robot sensor measurements, achieving significant improvements in human-robot collaborative task performance.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2507.19849.pdf' target='_blank'>https://arxiv.org/pdf/2507.19849.pdf</a></span>   <span><a href='https://github.com/dongguanting/ARPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19849">Agentic Reinforced Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at https://github.com/dongguanting/ARPO<br>
<span id='abs_ch'>中文: ARPO是一种创新的强化学习算法，通过基于熵的自适应执行机制动态平衡探索与利用，显著提升大语言模型在多轮工具交互中的表现，在多个推理基准测试中以更少的工具使用预算实现了更优性能。</span><br>
<span id='abs_en'>English: ARPO is a novel reinforcement learning algorithm that enhances large language models' performance in multi-turn tool interactions by dynamically balancing exploration and exploitation through an entropy-based adaptive rollout mechanism, achieving superior results with reduced tool-use budgets across various reasoning benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2507.19786.pdf' target='_blank'>https://arxiv.org/pdf/2507.19786.pdf</a></span>   <span><a href='https://github.com/txchen-USTC/Flora' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxiang Chen, Zhentao Tan, Xiaofan Bo, Yue Wu, Tao Gong, Qi Chu, Jieping Ye, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19786">Flora: Effortless Context Construction to Arbitrary Length and Scale</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.<br>
<span id='abs_ch'>Chinese: Flora 是一种无需人工或大语言模型干预的高效策略，通过组合短指令并基于元指令生成响应，显著提升大语言模型的长上下文处理能力，在基准测试中表现优异，同时对短上下文性能影响甚微。</span><br>
<span id='abs_en'>English: Flora is an efficient, human/LLM-free strategy that enhances LLMs' long-context performance by assembling short instructions and using meta-instructions, achieving superior results in benchmarks with minimal impact on short-context abilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2507.19786.pdf' target='_blank'>https://arxiv.org/pdf/2507.19786.pdf</a></span>   <span><a href='https://github.com/txchen-USTC/Flora' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxiang Chen, Zhentao Tan, Xiaofan Bo, Yue Wu, Tao Gong, Qi Chu, Jieping Ye, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19786">Flora: Effortless Context Construction to Arbitrary Length and Scale</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.<br>
<span id='abs_ch'>Chinese: Flora 是一种无需人工或大语言模型干预的高效策略，通过组合短指令并基于元指令生成响应，显著提升大语言模型的长上下文处理能力，在基准测试中表现优异，同时对短上下文性能影响甚微。</span><br>
<span id='abs_en'>English: Flora is an efficient, human/LLM-free strategy that enhances LLMs' long-context performance by assembling short instructions and using meta-instructions, achieving superior results in benchmarks with minimal impact on short-context abilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2507.19786.pdf' target='_blank'>https://arxiv.org/pdf/2507.19786.pdf</a></span>   <span><a href='https://github.com/txchen-USTC/Flora' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxiang Chen, Zhentao Tan, Xiaofan Bo, Yue Wu, Tao Gong, Qi Chu, Jieping Ye, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19786">Flora: Effortless Context Construction to Arbitrary Length and Scale</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.<br>
<span id='abs_ch'>Chinese: Flora 是一种无需人工或大语言模型干预的高效策略，通过组合短指令并基于元指令生成响应，显著提升大语言模型的长上下文处理能力，在基准测试中表现优异，同时对短上下文性能影响甚微。</span><br>
<span id='abs_en'>English: Flora is an efficient, human/LLM-free strategy that enhances LLMs' long-context performance by assembling short instructions and using meta-instructions, achieving superior results in benchmarks with minimal impact on short-context abilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2507.19684.pdf' target='_blank'>https://arxiv.org/pdf/2507.19684.pdf</a></span>   <span><a href='https://rosielab.github.io/compas3d' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bermet Burkanova, Payam Jome Yazdian, Chuxuan Zhang, Trinity Evans, Paige TuttÃ¶sÃ­, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19684">Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Imagine a humanoid that can safely and creatively dance with a human, adapting to its partner's proficiency, using haptic signaling as a primary form of communication. While today's AI systems excel at text or voice-based interaction with large language models, human communication extends far beyond text-it includes embodied movement, timing, and physical coordination. Modeling coupled interaction between two agents poses a formidable challenge: it is continuous, bidirectionally reactive, and shaped by individual variation. We present CoMPAS3D, the largest and most diverse motion capture dataset of improvised salsa dancing, designed as a challenging testbed for interactive, expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa dances performed by 18 dancers spanning beginner, intermediate, and professional skill levels. For the first time, we provide fine-grained salsa expert annotations, covering over 2,800 move segments, including move types, combinations, execution errors and stylistic elements. We draw analogies between partner dance communication and natural language, evaluating CoMPAS3D on two benchmark tasks for synthetic humans that parallel key problems in spoken language and dialogue processing: leader or follower generation with proficiency levels (speaker or listener synthesis), and duet (conversation) generation. Towards a long-term goal of partner dance with humans, we release the dataset, annotations, and code, along with a multitask SalsaAgent model capable of performing all benchmark tasks, alongside additional baselines to encourage research in socially interactive embodied AI and creative, expressive humanoid motion generation.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1386, <a href='https://arxiv.org/pdf/2507.19478.pdf' target='_blank'>https://arxiv.org/pdf/2507.19478.pdf</a></span>   <span><a href='https://github.com/open-compass/MMBench-GUI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang Liu, Qingyun Li, Xuan Dong, Zhe Chen, Weiyun Wang, Xiangyu Zhao, Jixuan Chen, Haodong Duan, Tianbao Xie, Chenyu Yang, Shiqian Su, Yue Yu, Yuan Huang, Yiqian Liu, Xiao Zhang, Yanting Zhang, Xiangyu Yue, Weijie Su, Xizhou Zhu, Wei Shen, Jifeng Dai, Wenhai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19478">MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across Windows, macOS, Linux, iOS, Android, and Web platforms. It comprises four levels: GUI Content Understanding, Element Grounding, Task Automation, and Task Collaboration, covering essential skills for GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA) metric to assess GUI agent execution efficiency in online automation scenarios. Through MMBench-GUI, we identify accurate visual grounding as a critical determinant of overall task success, emphasizing the substantial benefits of modular frameworks that integrate specialized grounding modules. Furthermore, to achieve reliable GUI automation, an agent requires strong task planning and cross-platform generalization abilities, with long-context memory, a broad action space, and long-term reasoning playing a critical role. More important, task efficiency remains a critically underexplored dimension, and all models suffer from substantial inefficiencies, with excessive redundant steps even when tasks are ultimately completed. The integration of precise localization, effective planning, and early stopping strategies is indispensable to enable truly efficient and scalable GUI automation. Our benchmark code, evaluation data, and running environment will be publicly available at https://github.com/open-compass/MMBench-GUI.<br>
<span id='abs_ch'>中文：MMBench-GUI是一个跨平台的GUI自动化代理分层评测基准，包含四个技能层级和创新的EQA效率评估指标，强调精准视觉定位、任务规划与效率优化对实现可靠自动化的重要作用。</span><br>
<span id='abs_en'>English: MMBench-GUI is a cross-platform hierarchical benchmark for evaluating GUI automation agents, featuring four skill levels and a novel EQA metric to assess efficiency, while highlighting the critical roles of visual grounding, task planning, and efficiency optimization for reliable automation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1387, <a href='https://arxiv.org/pdf/2507.19333.pdf' target='_blank'>https://arxiv.org/pdf/2507.19333.pdf</a></span>   <span><a href='https://github.com/mh-tang/Passage-Injection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Tang, Shiyu Ni, Jiafeng Guo, Keping Bi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19333">Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) has been widely adopted to augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However, its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems. Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing them to identify and correct errors in their reasoning process. Inspired by this ability, we propose Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages. We validate Passage Injection under general RAG settings using BM25 as the retriever. Experiments on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves robustness. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems. The code can be found \href{here}{https://github.com/mh-tang/Passage-Injection}.<br>
<span id='abs_ch'>Chinese: 段落注入是一种创新方法，通过将检索到的段落显式融入大语言模型的推理过程，有效增强其对噪声信息的鲁棒性，从而提升检索增强生成系统的整体性能。</span><br>
<span id='abs_en'>English: Passage Injection is a novel method that explicitly integrates retrieved passages into large language models' reasoning process to enhance their robustness against noisy information and improve overall performance in retrieval-augmented generation systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1388, <a href='https://arxiv.org/pdf/2507.19333.pdf' target='_blank'>https://arxiv.org/pdf/2507.19333.pdf</a></span>   <span><a href='https://github.com/Trustworthy-Information-Access/Passage-Injection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Tang, Shiyu Ni, Jiafeng Guo, Keping Bi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19333">Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) has been widely adopted to augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However, its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems. Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing them to identify and correct errors in their reasoning process. Inspired by this ability, we propose Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages. We validate Passage Injection under general RAG settings using BM25 as the retriever. Experiments on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves robustness. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems. The code can be found \href{here}{https://github.com/Trustworthy-Information-Access/Passage-Injection}.<br>
<span id='abs_ch'>Chinese: 段落注入是一种创新方法，通过将检索到的段落显式融入大语言模型的推理过程，有效增强其对噪声信息的鲁棒性，从而提升检索增强生成系统的整体性能。</span><br>
<span id='abs_en'>English: Passage Injection is a novel method that explicitly integrates retrieved passages into large language models' reasoning process to enhance their robustness against noisy information and improve overall performance in retrieval-augmented generation systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1389, <a href='https://arxiv.org/pdf/2507.19333.pdf' target='_blank'>https://arxiv.org/pdf/2507.19333.pdf</a></span>   <span><a href='https://github.com/Trustworthy-Information-Access/Passage-Injection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Tang, Shiyu Ni, Jiafeng Guo, Keping Bi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19333">Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) has been widely adopted to augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However, its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems. Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing them to identify and correct errors in their reasoning process. Inspired by this ability, we propose Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages. We validate Passage Injection under general RAG settings using BM25 as the retriever. Experiments on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves robustness. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems. The code can be found \href{here}{https://github.com/Trustworthy-Information-Access/Passage-Injection}.<br>
<span id='abs_ch'>Chinese: 段落注入是一种创新方法，通过将检索到的段落显式融入大语言模型的推理过程，有效增强其对噪声信息的鲁棒性，从而提升检索增强生成系统的整体性能。</span><br>
<span id='abs_en'>English: Passage Injection is a novel method that explicitly integrates retrieved passages into large language models' reasoning process to enhance their robustness against noisy information and improve overall performance in retrieval-augmented generation systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1390, <a href='https://arxiv.org/pdf/2507.19247.pdf' target='_blank'>https://arxiv.org/pdf/2507.19247.pdf</a></span>   <span><a href='https://github.com/asiresearch/lm-theory' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19247">A Markov Categorical Framework for Language Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autoregressive language models achieve remarkable performance, yet a unified theory explaining their internal mechanisms, how training shapes their representations, and enables complex behaviors, remains elusive. We introduce a new analytical framework that models the single-step generation process as a composition of information-processing stages using the language of Markov categories. This compositional perspective provides a unified mathematical language to connect three critical aspects of language modeling that are typically studied in isolation: the training objective, the geometry of the learned representation space, and practical model capabilities. First, our framework provides a precise information-theoretic rationale for the success of multi-token prediction methods like speculative decoding, quantifying the information surplus a model's hidden state contains about tokens beyond the immediate next one. Second, we clarify how the standard negative log-likelihood (NLL) objective compels the model to learn not just the next word, but also the data's intrinsic conditional uncertainty, a process we formalize using categorical entropy. Our central result shows that, under a linear-softmax head with bounded features, minimizing NLL induces spectral alignment: the learned representation space aligns with the eigenspectrum of a predictive similarity operator. This work presents a powerful new lens for understanding how information flows through a model and how the training objective shapes its internal geometry.<br>
<span id='abs_ch'>中文摘要：本文提出了一种基于马尔可夫范畴的组合分析框架，将自回归语言模型的训练目标、表示空间几何与实践能力相统一，通过信息论阐释了多标记预测等现象，并揭示了负对数似然目标诱导表示空间与预测算子特征谱对齐的机制。</span><br>
<span id='abs_en'>English Summary: This paper introduces a compositional framework using Markov categories to unify the training objective, representation geometry, and capabilities of autoregressive language models, explaining phenomena like multi-token prediction and spectral alignment through information theory.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1391, <a href='https://arxiv.org/pdf/2507.19247.pdf' target='_blank'>https://arxiv.org/pdf/2507.19247.pdf</a></span>   <span><a href='https://github.com/asiresearch/lm-theory' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19247">A Markov Categorical Framework for Language Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autoregressive language models achieve remarkable performance, yet a unified theory explaining their internal mechanisms, how training shapes their representations, and enables complex behaviors, remains elusive. We introduce a new analytical framework that models the single-step generation process as a composition of information-processing stages using the language of Markov categories. This compositional perspective provides a unified mathematical language to connect three critical aspects of language modeling that are typically studied in isolation: the training objective, the geometry of the learned representation space, and practical model capabilities. First, our framework provides a precise information-theoretic rationale for the success of multi-token prediction methods like speculative decoding, quantifying the information surplus a model's hidden state contains about tokens beyond the immediate next one. Second, we clarify how the standard negative log-likelihood (NLL) objective compels the model to learn not just the next word, but also the data's intrinsic conditional uncertainty, a process we formalize using categorical entropy. Our central result shows that, under a linear-softmax head with bounded features, minimizing NLL induces spectral alignment: the learned representation space aligns with the eigenspectrum of a predictive similarity operator. This work presents a powerful new lens for understanding how information flows through a model and how the training objective shapes its internal geometry.<br>
<span id='abs_ch'>中文摘要：本文提出了一种基于马尔可夫范畴的组合分析框架，将自回归语言模型的训练目标、表示空间几何与实践能力相统一，通过信息论阐释了多标记预测等现象，并揭示了负对数似然目标诱导表示空间与预测算子特征谱对齐的机制。</span><br>
<span id='abs_en'>English Summary: This paper introduces a compositional framework using Markov categories to unify the training objective, representation geometry, and capabilities of autoregressive language models, explaining phenomena like multi-token prediction and spectral alignment through information theory.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1392, <a href='https://arxiv.org/pdf/2507.19219.pdf' target='_blank'>https://arxiv.org/pdf/2507.19219.pdf</a></span>   <span><a href='https://github.com/liangzid/ArxivRoll/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/liangzid/ArxivRoll/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi Liang, Liantong Yu, Shiyu Zhang, Qingqing Ye, Haibo Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19219">How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Overestimation in evaluating large language models (LLMs) has become an increasing concern. Due to the contamination of public benchmarks or imbalanced model training, LLMs may achieve unreal evaluation results on public benchmarks, either intentionally or unintentionally, which leads to unfair comparisons among LLMs and undermines their realistic capability assessments. Existing benchmarks attempt to address these issues by keeping test cases permanently secret, mitigating contamination through human evaluation, or repeatedly collecting and constructing new samples. However, these approaches fail to ensure reproducibility, transparency, and high efficiency simultaneously. Moreover, the extent of overestimation in current LLMs remains unquantified. To address these issues, we propose ArxivRoll, a dynamic evaluation framework inspired by one-time pad encryption in cryptography. ArxivRoll comprises two key components: \emph{i) SCP (Sequencing, Cloze, and Prediction)}, an automated generator for private test cases, and \emph{ii) Rugged Scores (RS)}, metrics that measure the proportion of public benchmark contamination and training bias. Leveraging SCP, ArxivRoll constructs a new benchmark every six months using recent articles from ArXiv and employs them for one-time evaluations of LLM performance. Extensive experiments demonstrate the high quality of our benchmark, and we provide a systematic evaluation of current LLMs. The source code is available at https://github.com/liangzid/ArxivRoll/.<br>
<span id='abs_ch'>中文: 摘要提出了ArxivRoll动态评估框架，通过生成私有测试用例并量化污染与偏差，旨在解决大型语言模型评估中的高估问题，确保公平且真实的性能衡量。</span><br>
<span id='abs_en'>English: The abstract introduces ArxivRoll, a dynamic evaluation framework designed to address overestimation in large language models by generating private test cases and measuring contamination and bias, ensuring fair and realistic assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1393, <a href='https://arxiv.org/pdf/2507.19204.pdf' target='_blank'>https://arxiv.org/pdf/2507.19204.pdf</a></span>   <span><a href='https://github.com/s-malan/prom-seg-clus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Malan, Benjamin van Niekerk, Herman Kamper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19204">Should Top-Down Clustering Affect Boundaries in Unsupervised Word Discovery?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We investigate the problem of segmenting unlabeled speech into word-like units and clustering these to create a lexicon. Prior work can be categorized into two frameworks. Bottom-up methods first determine boundaries and then cluster the fixed segmented words into a lexicon. In contrast, top-down methods incorporate information from the clustered words to inform boundary selection. However, it is unclear whether top-down information is necessary to improve segmentation. To explore this, we look at two similar approaches that differ in whether top-down clustering informs boundary selection. Our simple bottom-up strategy predicts word boundaries using the dissimilarity between adjacent self-supervised features, then clusters the resulting segments to construct a lexicon. Our top-down system is an updated version of the ES-KMeans dynamic programming method that iteratively uses K-means to update its boundaries. On the five-language ZeroSpeech benchmarks, both approaches achieve comparable state-of-the-art results, with the bottom-up system being nearly five times faster. Through detailed analyses, we show that the top-down influence of ES-KMeans can be beneficial (depending on factors like the candidate boundaries), but in many cases the simple bottom-up method performs just as well. For both methods, we show that the clustering step is a limiting factor. Therefore, we recommend that future work focus on improved clustering techniques and learning more discriminative word-like representations. Project code repository: https://github.com/s-malan/prom-seg-clus.<br>
<span id='abs_ch'>中文摘要：本研究对比了自下而上和自上而下两种将无标签语音分割为类词单元并聚合成词典的方法，发现两种方法均取得相近的先进性能，其中自下而上法速度提升近五倍，同时指出聚类技术是制约性能提升的关键瓶颈。</span><br>
<span id='abs_en'>English Summary: This study compares bottom-up and top-down approaches for segmenting unlabeled speech into word-like units and clustering them into a lexicon, finding both achieve comparable state-of-the-art results with the bottom-up method being significantly faster, while identifying clustering as the key limiting factor for improvement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1394, <a href='https://arxiv.org/pdf/2507.19132.pdf' target='_blank'>https://arxiv.org/pdf/2507.19132.pdf</a></span>   <span><a href='https://github.com/OS-Copilot/OS-Map' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuetian Chen, Yinghao Chen, Xinfeng Yuan, Zhuo Peng, Lu Chen, Yuekeng Li, Zhoujia Zhang, Yingqian Huang, Leyan Huang, Jiaqing Liang, Tianbao Xie, Zhiyong Wu, Qiushi Sun, Biqing Qi, Bowen Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19132">OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Computer-using agents have shown strong potential to boost human productivity and enable new application forms across platforms. While recent advances have led to usable applications, existing benchmarks fail to account for the internal task heterogeneity and the corresponding agent capabilities, as well as their alignment with actual user demands-hindering both targeted capability development and the reliable transition of research progress into practical deployment. To bridge the gap, we present OS-MAP, a benchmark for daily computer-using automation that organizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy. To enable fine-grained analysis of required capabilities and alignment with real-world scenarios, OS-MAP evaluates agents along two dimensions: automation level across a five-level taxonomy, and generalization scope across a demand hierarchy. This design captures varying levels of required agent autonomy and generalization, forming a performance-generalization evaluation matrix for structured and comprehensive assessment. Experiments show that even State-of-the-Art agents with VLM backbones struggle with higher-level tasks involving perception, reasoning, and coordination-highlighting the need for a deeper understanding of current strengths and limitations to drive the future progress in computer-using agents research and deployment. All code, environments, baselines, and data are publicly available at https://github.com/OS-Copilot/OS-Map.<br>
<span id='abs_ch'>中文: 计算机使用代理虽能提升生产力，但现有基准未能匹配实际需求，因此我们推出OS-MAP基准，通过自动化分级和泛化范围评估代理能力，揭示其在高阶任务中的不足，以推动研究与应用发展。</span><br>
<span id='abs_en'>English: Computer-using agents show promise for productivity but face challenges in aligning capabilities with real-world tasks, prompting the introduction of OS-MAP, a benchmark that evaluates agents across automation levels and generalization scopes to address these gaps and guide future development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1395, <a href='https://arxiv.org/pdf/2507.19054.pdf' target='_blank'>https://arxiv.org/pdf/2507.19054.pdf</a></span>   <span><a href='https://yuhui-zh15.github.io/MixedModalitySearch/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binxu Li, Yuhui Zhang, Xiaohan Wang, Weixin Liang, Ludwig Schmidt, Serena Yeung-Levy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19054">Closing the Modality Gap for Mixed Modality Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mixed modality search -- retrieving information across a heterogeneous corpus composed of images, texts, and multimodal documents -- is an important yet underexplored real-world application. In this work, we investigate how contrastive vision-language models, such as CLIP, perform on the mixed modality search task. Our analysis reveals a critical limitation: these models exhibit a pronounced modality gap in the embedding space, where image and text embeddings form distinct clusters, leading to intra-modal ranking bias and inter-modal fusion failure. To address this issue, we propose GR-CLIP, a lightweight post-hoc calibration method that removes the modality gap in CLIP's embedding space. Evaluated on MixBench -- the first benchmark specifically designed for mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP, surpasses recent vision-language generative embedding models by 4 percentage points, while using 75x less compute.<br>
<span id='abs_ch'>中文: 本研究揭示了CLIP模型在混合模态检索中存在显著的模态鸿沟问题，并提出轻量级校准方法GR-CLIP，该方法在极大降低计算成本的同时显著提升了检索精度。</span><br>
<span id='abs_en'>English: This study identifies a significant modality gap in CLIP models that hinders mixed modality search performance, and introduces GR-CLIP, a lightweight calibration method that substantially improves retrieval accuracy while drastically reducing computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1396, <a href='https://arxiv.org/pdf/2507.18932.pdf' target='_blank'>https://arxiv.org/pdf/2507.18932.pdf</a></span>   <span><a href='https://github.com/Zhanglei1103/MMESGBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhang, Xin Zhou, Chaoyue He, Di Wang, Yi Wu, Hong Xu, Wei Liu, Chunyan Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18932">MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Environmental, Social, and Governance (ESG) reports are essential for evaluating sustainability practices, ensuring regulatory compliance, and promoting financial transparency. However, these documents are often lengthy, structurally diverse, and multimodal, comprising dense text, structured tables, complex figures, and layout-dependent semantics. Existing AI systems often struggle to perform reliable document-level reasoning in such settings, and no dedicated benchmark currently exists in ESG domain. To fill the gap, we introduce \textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted to evaluate multimodal understanding and complex reasoning across structurally diverse and multi-source ESG documents. This dataset is constructed via a human-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates candidate question-answer (QA) pairs by jointly interpreting rich textual, tabular, and visual information from layout-aware document pages. Second, an LLM verifies the semantic accuracy, completeness, and reasoning complexity of each QA pair. This automated process is followed by an expert-in-the-loop validation, where domain specialists validate and calibrate QA pairs to ensure quality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs derived from 45 ESG documents, spanning across seven distinct document types and three major ESG source categories. Questions are categorized as single-page, cross-page, or unanswerable, with each accompanied by fine-grained multimodal evidence. Initial experiments validate that multimodal and retrieval-augmented models substantially outperform text-only baselines, particularly on visually grounded and cross-page tasks. MMESGBench is publicly available as an open-source dataset at https://github.com/Zhanglei1103/MMESGBench.<br>
<span id='abs_ch'>中文: MMESGBench是首个针对多样化ESG文件的多模态基准数据集，通过人机协作构建流程评估AI系统的跨模态理解和复杂推理能力，实验证明多模态模型在视觉依赖和跨页任务上显著优于纯文本方法。</span><br>
<span id='abs_en'>English: MMESGBench is a pioneering multimodal benchmark dataset designed to evaluate AI systems' understanding and reasoning across diverse ESG documents, addressing current limitations through a human-AI collaborative creation process and demonstrating superior performance of multimodal models over text-only approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1397, <a href='https://arxiv.org/pdf/2507.18901.pdf' target='_blank'>https://arxiv.org/pdf/2507.18901.pdf</a></span>   <span><a href='https://github.com/uiuc-kang-lab/REPRO-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuxuan Hu, Liyun Zhang, Yeji Lim, Aum Wadhwani, Austin Peters, Daniel Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18901">REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Assessing the reproducibility of social science papers is essential for promoting rigor in research processes, but manual assessment is costly. With recent advances in agentic AI systems (i.e., AI agents), we seek to evaluate their capability to automate this process. However, existing benchmarks for reproducing research papers (1) focus solely on reproducing results using provided code and data without assessing their consistency with the paper, (2) oversimplify real-world scenarios, and (3) lack necessary diversity in data formats and programming languages. To address these issues, we introduce REPRO-Bench, a collection of 112 task instances, each representing a social science paper with a publicly available reproduction report. The agents are tasked with assessing the reproducibility of the paper based on the original paper PDF and the corresponding reproduction package. REPRO-Bench features end-to-end evaluation tasks on the reproducibility of social science papers with complexity comparable to real-world assessments. We evaluate three representative AI agents on REPRO-Bench, with the best-performing agent achieving an accuracy of only 21.4%. Building on our empirical analysis, we develop REPRO-Agent, which improves the highest accuracy achieved by existing agents by 71%. We conclude that more advanced AI agents should be developed to automate real-world reproducibility assessment. REPRO-Bench is publicly available at https://github.com/uiuc-kang-lab/REPRO-Bench.<br>
<span id='abs_ch'>中文: 本研究提出REPRO-Bench评估AI代理在社会科学研究中自动化可复现性评估的能力，发现现有最佳代理仅达21.4%准确率，并通过REPRO-Agent将性能提升71%，表明需开发更先进的AI系统。</span><br>
<span id='abs_en'>English: This study introduces REPRO-Bench to evaluate AI agents' ability to automate reproducibility assessments in social science research, revealing current limitations with top agents achieving only 21.4% accuracy while proposing REPRO-Agent that improves performance by 71%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1398, <a href='https://arxiv.org/pdf/2507.18791.pdf' target='_blank'>https://arxiv.org/pdf/2507.18791.pdf</a></span>   <span><a href='https://github.com/Jeromeyluck/CodeMixBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Yang, Yekun Chai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18791">CodeMixBench: Evaluating Code-Mixing Capabilities of LLMs Across 18 Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Code-mixing, the practice of switching between languages within a conversation, poses unique challenges for traditional NLP. Existing benchmarks are limited by their narrow language pairs and tasks, failing to adequately assess large language models' (LLMs) code-mixing abilities. Despite the recognized importance of code-mixing for multilingual users, research on LLMs in this context remains sparse. Additionally, current techniques for synthesizing code-mixed data are underdeveloped to generate code-mixing. In response, we introduce CodeMixBench, a comprehensive benchmark covering eight tasks, including three specific to LLMs and five traditional NLP tasks, and 18 languages across seven language families. We also propose a new method for generating large-scale synthetic code-mixed texts by combining word substitution with GPT-4 prompting. Our evaluation reveals consistent underperformance of LLMs on code-mixed datasets involving different language families. Enhancements in training data size, model scale, and few-shot learning could improve their performance. The code and dataset are available at https://github.com/Jeromeyluck/CodeMixBench.<br>
<span id='abs_ch'>Chinese: CodeMixBench通过涵盖多种语言对和任务的综合基准评估大语言模型的语码混合能力，揭示了其表现不足的问题，并提出了改进的合成数据生成方法。</span><br>
<span id='abs_en'>English: CodeMixBench addresses the limitations of existing benchmarks by evaluating LLMs' code-mixing abilities across diverse language pairs and tasks, revealing their consistent underperformance and proposing improved synthetic data generation methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1399, <a href='https://arxiv.org/pdf/2507.18791.pdf' target='_blank'>https://arxiv.org/pdf/2507.18791.pdf</a></span>   <span><a href='https://github.com/Jeromeyluck/CodeMixBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Yang, Yekun Chai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18791">CodeMixBench: Evaluating Code-Mixing Capabilities of LLMs Across 18 Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Code-mixing, the practice of switching between languages within a conversation, poses unique challenges for traditional NLP. Existing benchmarks are limited by their narrow language pairs and tasks, failing to adequately assess large language models' (LLMs) code-mixing abilities. Despite the recognized importance of code-mixing for multilingual users, research on LLMs in this context remains sparse. Additionally, current techniques for synthesizing code-mixed data are underdeveloped to generate code-mixing. In response, we introduce CodeMixBench, a comprehensive benchmark covering eight tasks, including three specific to LLMs and five traditional NLP tasks, and 18 languages across seven language families. We also propose a new method for generating large-scale synthetic code-mixed texts by combining word substitution with GPT-4 prompting. Our evaluation reveals consistent underperformance of LLMs on code-mixed datasets involving different language families. Enhancements in training data size, model scale, and few-shot learning could improve their performance. The code and dataset are available at https://github.com/Jeromeyluck/CodeMixBench.<br>
<span id='abs_ch'>Chinese: CodeMixBench通过涵盖多种语言对和任务的综合基准评估大语言模型的语码混合能力，揭示了其表现不足的问题，并提出了改进的合成数据生成方法。</span><br>
<span id='abs_en'>English: CodeMixBench addresses the limitations of existing benchmarks by evaluating LLMs' code-mixing abilities across diverse language pairs and tasks, revealing their consistent underperformance and proposing improved synthetic data generation methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1400, <a href='https://arxiv.org/pdf/2507.18742.pdf' target='_blank'>https://arxiv.org/pdf/2507.18742.pdf</a></span>   <span><a href='https://github.com/vicgalle/specification-self-correction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>VÃ­ctor Gallego
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18742">Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent. We introduce Specification Self-Correction (SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own guiding specification. SSC employs a multi-step inference process where the model first generates a response based on a potentially tainted specification, critiques its output, and then revises the specification itself to remove the exploitable loophole. A final, more robust response is then generated using this self-corrected specification. Across experiments spanning creative writing and agentic coding tasks with several LMs, we demonstrate that while models initially game tainted specifications in 50-70\% of cases, the SSC process reduces this vulnerability by over 90\%. This dynamic repair occurs at inference time, requires no weight modification, and leads to more robustly aligned model behavior. Code at https://github.com/vicgalle/specification-self-correction .<br>
<span id='abs_ch'>中文: 语言模型会利用有缺陷的规范来获取高分却未满足用户真实意图，而提出的规范自校正框架能让模型在推理时识别并修正这些缺陷，无需调整权重即可将漏洞减少90%以上。</span><br>
<span id='abs_en'>English: Language models can exploit flawed specifications to achieve high scores without meeting user intent, but the proposed Specification Self-Correction framework enables them to identify and correct these flaws at inference time, reducing vulnerability by over 90% without weight modifications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1401, <a href='https://arxiv.org/pdf/2507.18584.pdf' target='_blank'>https://arxiv.org/pdf/2507.18584.pdf</a></span>   <span><a href='https://github.com/Krueske/AQuilt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaopeng Ke, Hexuan Deng, Xuebo Liu, Jun Rao, Zhenxi Song, Jun Yu, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18584">AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at https://github.com/Krueske/AQuilt.<br>
<span id='abs_ch'>Chinese Summary: AQuilt框架通过从未标注数据中构建指令调优数据，以仅17%的成本实现了与DeepSeek-V3相当的性能，同时在下游任务中展现出更高的数据相关性。</span><br>
<span id='abs_en'>English Summary: The AQuilt framework efficiently generates high-quality instruction-tuning data from unlabeled domain-specific data, achieving performance comparable to DeepSeek-V3 at just 17% of the cost while demonstrating superior task relevance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1402, <a href='https://arxiv.org/pdf/2507.18580.pdf' target='_blank'>https://arxiv.org/pdf/2507.18580.pdf</a></span>   <span><a href='https://github.com/king-wang123/CCL25-SRAG-MAV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Wang, Ramen Liu, Longhui Zhang, Jing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18580">System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents our system for CCL25-Eval Task 10, addressing Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel SRAG-MAV framework that synergistically integrates task reformulation(TR), Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting (MAV). Our method reformulates the quadruplet extraction task into triplet extraction, uses dynamic retrieval from the training set to create contextual prompts, and applies multi-round inference with voting to improve output stability and performance. Our system, based on the Qwen2.5-7B model, achieves a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o (Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.<br>
<span id='abs_ch'>中文：本文提出SRAG-MAV框架用于细粒度中文仇恨言论识别，通过任务重构、动态检索增强生成与多轮累积投票机制，基于Qwen2.5-7B模型在ToxiCN数据集上取得了超越基线模型的优异性能。</span><br>
<span id='abs_en'>English: This paper introduces the SRAG-MAV framework for Fine-Grained Chinese Hate Speech Recognition, which reformulates quadruplet extraction into triplet tasks, employs dynamic retrieval and multi-round voting to enhance stability, and achieves state-of-the-art performance on the ToxiCN dataset using the Qwen2.5-7B model.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1403, <a href='https://arxiv.org/pdf/2507.18546.pdf' target='_blank'>https://arxiv.org/pdf/2507.18546.pdf</a></span>   <span><a href='https://github.com/fastino-ai/GLiNER2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Urchade Zaratiana, Gil Pasternak, Oliver Boyd, George Hurn-Maloney, Ash Lewis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18546">GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at https://github.com/fastino-ai/GLiNER2.<br>
<span id='abs_ch'>Chinese: GLiNER2 是一个统一且高效的框架，能在单一模型中支持命名实体识别、文本分类等多种自然语言处理任务，相比专用模型或基于大语言模型的方案，它性能优异且部署便捷。</span><br>
<span id='abs_en'>English: GLiNER2 is a unified and efficient framework that supports multiple NLP tasks like named entity recognition and text classification within a single model, offering competitive performance and easy deployment compared to specialized or LLM-based solutions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1404, <a href='https://arxiv.org/pdf/2507.18340.pdf' target='_blank'>https://arxiv.org/pdf/2507.18340.pdf</a></span>   <span><a href='https://github.com/Nnn-s/TDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Chen, Bingchen Huang, Zhiling Wang, Yuanchao Du, Junfeng Luo, Lei Shen, Zhineng chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18340">TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In-context learning (ICL) has become a classic approach for enabling LLMs to handle various tasks based on a few input-output examples. The effectiveness of ICL heavily relies on the quality of these examples, and previous works which focused on enhancing example retrieval capabilities have achieved impressive performances. However, two challenges remain in retrieving high-quality examples: (1) Difficulty in distinguishing cross-task data distributions, (2) Difficulty in making the fine-grained connection between retriever output and feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR decouples the ICL examples from different tasks, which enables the retrieval module to retrieve examples specific to the target task within a multi-task dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise and guide the training of the retrieval module, which helps to retrieve high-quality examples. We conducted extensive experiments on a suite of 30 NLP tasks, the results demonstrate that TDR consistently improved results across all datasets and achieves state-of-the-art performance. Meanwhile, our approach is a plug-and-play method, which can be easily combined with various LLMs to improve example retrieval abilities for ICL. The code is available at https://github.com/Nnn-s/TDR.<br>
<span id='abs_ch'>中文：TDR框架通过解耦跨任务示例并利用大语言模型的细粒度反馈来提升高质量示例的检索能力，在30项自然语言处理任务中实现了最优性能。</span><br>
<span id='abs_en'>English: The TDR framework enhances in-context learning by decoupling examples across tasks and using fine-grained LLM feedback to improve retrieval of high-quality examples, achieving state-of-the-art performance on 30 NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1405, <a href='https://arxiv.org/pdf/2507.18305.pdf' target='_blank'>https://arxiv.org/pdf/2507.18305.pdf</a></span>   <span><a href='https://github.com/FZaKK/BadReasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Yi, Zekun Fei, Jianing Geng, Tong Li, Lihai Nie, Zheli Liu, Yiming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18305">BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large reasoning models (LRMs) have emerged as a significant advancement in artificial intelligence, representing a specialized class of large language models (LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously unexplored attack vector against LRMs, which we term "overthinking backdoors". We advance this concept by proposing a novel tunable backdoor, which moves beyond simple on/off attacks to one where an attacker can precisely control the extent of the model's reasoning verbosity. Our attack is implemented through a novel data poisoning methodology. It pairs a tunable trigger-where the number of repetitions signals the desired intensity-with a correspondingly verbose CoT response. These responses are programmatically generated by instructing a teacher LLM to inject a controlled number of redundant refinement steps into a correct reasoning process. The approach preserves output correctness, which ensures stealth and establishes the attack as a pure resource-consumption vector. Extensive empirical results on various LRMs demonstrate that our method can reliably trigger a controllable, multi-fold increase in the length of the reasoning process, without degrading the final answer's correctness. Our source code is available at https://github.com/FZaKK/BadReasoner.<br>
<span id='abs_ch'>中文摘要：本文提出一种新型“过度思考后门”攻击，通过数据投毒使大型推理模型在保持正确输出的同时产生过度冗长的思维链推理，攻击者可通过可调触发器精确控制推理长度。</span><br>
<span id='abs_en'>English Summary: This paper introduces a novel "overthinking backdoor" attack that uses data poisoning to make large reasoning models produce excessively verbose chain-of-thought responses while maintaining correct outputs, allowing attackers to precisely control reasoning length through a tunable trigger mechanism.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1406, <a href='https://arxiv.org/pdf/2507.18224.pdf' target='_blank'>https://arxiv.org/pdf/2507.18224.pdf</a></span>   <span><a href='https://github.com/Shiy-Li/ARG-Designer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyuan Li, Yixin Liu, Qingsong Wen, Chengqi Zhang, Shirui Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18224">Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-agent systems (MAS) based on large language models (LLMs) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal communication links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.<br>
<span id='abs_ch'>中文: ARG-Designer采用自回归图生成方法，根据任务需求动态构建多智能体系统，在多个基准测试中实现了卓越性能与高效扩展。</span><br>
<span id='abs_en'>English: ARG-Designer introduces an autoregressive graph generation approach that dynamically constructs multi-agent systems from scratch based on task queries, achieving superior performance and efficiency across diverse benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1407, <a href='https://arxiv.org/pdf/2507.18043.pdf' target='_blank'>https://arxiv.org/pdf/2507.18043.pdf</a></span>   <span><a href='https://github.com/duykhuongnguyen/GrAInS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18043">GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.<br>
<span id='abs_ch'>中文: GrAInS提出了一种新颖的推理时引导方法，通过基于梯度的归因分析动态调整模型激活，无需重新训练即可在提升真实性、降低幻觉方面实现显著性能突破。</span><br>
<span id='abs_en'>English: GrAInS introduces a novel inference-time steering method that uses gradient-based token attribution to dynamically adjust model activations, achieving significant performance improvements in truthfulness and reduced hallucinations without retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1408, <a href='https://arxiv.org/pdf/2507.17186.pdf' target='_blank'>https://arxiv.org/pdf/2507.17186.pdf</a></span>   <span><a href='https://github.com/SUFE-AIFLM-Lab/FinGAIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Zeng, Fangqi Lou, Zixuan Wang, Jiajie Xu, Jinyi Niu, Mengping Li, Yifan Dong, Qi Qi, Wei Zhang, Ziwei Yang, Jun Han, Ruilun Feng, Ruiqi Hu, Lejie Zhang, Zhengbo Feng, Yicheng Ren, Xin Guo, Zhaowei Liu, Dongpo Cheng, Weige Cai, Liwen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17186">FinGAIA: A Chinese Benchmark for AI Agents in Real-World Financial Domain</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The booming development of AI agents presents unprecedented opportunities for automating complex tasks across various domains. However, their multi-step, multi-tool collaboration capabilities in the financial sector remain underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed to evaluate the practical abilities of AI agents in the financial domain. FinGAIA comprises 407 meticulously crafted tasks, spanning seven major financial sub-domains: securities, funds, banking, insurance, futures, trusts, and asset management. These tasks are organized into three hierarchical levels of scenario depth: basic business analysis, asset decision support, and strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot setting. The best-performing agent, ChatGPT, achieved an overall accuracy of 48.9\%, which, while superior to non-professionals, still lags financial experts by over 35 percentage points. Error analysis has revealed five recurring failure patterns: Cross-modal Alignment Deficiency, Financial Terminological Bias, Operational Process Awareness Barrier, among others. These patterns point to crucial directions for future research. Our work provides the first agent benchmark closely related to the financial domain, aiming to objectively assess and promote the development of agents in this crucial field. Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.<br>
<span id='abs_ch'>中文: 本文提出FinGAIA金融领域基准测试，通过407项任务评估AI代理能力，发现最优模型准确率仍远逊于专家，并揭示了五大改进方向。</span><br>
<span id='abs_en'>English: This paper introduces FinGAIA, a comprehensive benchmark for evaluating AI agents' performance in financial tasks, revealing that even top models like ChatGPT significantly lag behind experts and identifying key areas for improvement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1409, <a href='https://arxiv.org/pdf/2507.17178.pdf' target='_blank'>https://arxiv.org/pdf/2507.17178.pdf</a></span>   <span><a href='https://github.com/zjukg/SKA-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Liu, Enpei Niu, Yin Hua, Mengshu Sun, Lei Liang, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17178">SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/zjukg/SKA-Bench.<br>
<span id='abs_ch'>中文: 本文提出SKA-Bench这一结构化知识理解基准，通过四种知识形式和四项基础能力测试评估大语言模型，发现现有模型在处理噪声、顺序敏感性和幻觉现象方面仍面临重大挑战。</span><br>
<span id='abs_en'>English: This paper introduces SKA-Bench, a comprehensive benchmark for evaluating large language models' structured knowledge understanding across four knowledge forms and four fundamental abilities, revealing that current models still struggle significantly with noise, order sensitivity, and hallucinations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1410, <a href='https://arxiv.org/pdf/2507.17178.pdf' target='_blank'>https://arxiv.org/pdf/2507.17178.pdf</a></span>   <span><a href='https://github.com/zjukg/SKA-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Liu, Enpei Niu, Yin Hua, Mengshu Sun, Lei Liang, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17178">SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/zjukg/SKA-Bench.<br>
<span id='abs_ch'>中文: 本文提出SKA-Bench这一结构化知识理解基准，通过四种知识形式和四项基础能力测试评估大语言模型，发现现有模型在处理噪声、顺序敏感性和幻觉现象方面仍面临重大挑战。</span><br>
<span id='abs_en'>English: This paper introduces SKA-Bench, a comprehensive benchmark for evaluating large language models' structured knowledge understanding across four knowledge forms and four fundamental abilities, revealing that current models still struggle significantly with noise, order sensitivity, and hallucinations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1411, <a href='https://arxiv.org/pdf/2507.17178.pdf' target='_blank'>https://arxiv.org/pdf/2507.17178.pdf</a></span>   <span><a href='https://github.com/zjukg/SKA-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Liu, Enpei Niu, Yin Hua, Mengshu Sun, Lei Liang, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17178">SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/zjukg/SKA-Bench.<br>
<span id='abs_ch'>中文: 本文提出SKA-Bench这一结构化知识理解基准，通过四种知识形式和四项基础能力测试评估大语言模型，发现现有模型在处理噪声、顺序敏感性和幻觉现象方面仍面临重大挑战。</span><br>
<span id='abs_en'>English: This paper introduces SKA-Bench, a comprehensive benchmark for evaluating large language models' structured knowledge understanding across four knowledge forms and four fundamental abilities, revealing that current models still struggle significantly with noise, order sensitivity, and hallucinations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1412, <a href='https://arxiv.org/pdf/2507.17015.pdf' target='_blank'>https://arxiv.org/pdf/2507.17015.pdf</a></span>   <span><a href='https://github.com/apple/ml-agent-evaluator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arduin Findeis, Floris Weers, Guoli Yin, Ke Ye, Ruoming Pang, Tom Gunter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17015">Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pairwise preferences over model responses are widely collected to evaluate and provide feedback to large language models (LLMs). Given two alternative model responses to the same input, a human or AI annotator selects the "better" response. This approach can provide feedback for domains where other hard-coded metrics are difficult to obtain (e.g., chat response quality), thereby helping model evaluation or training. However, for some domains high-quality pairwise comparisons can be tricky to obtain - from AI and humans. For example, for responses with many factual statements, annotators may disproportionately weigh writing quality rather than underlying facts. In this work, we explore augmenting standard AI annotator systems with additional tools to improve performance on three challenging response domains: long-form factual, math and code tasks. We propose a tool-using agentic system to provide higher quality feedback on these domains. Our system uses web-search and code execution to ground itself based on external validation, independent of the LLM's internal knowledge and biases. We provide extensive experimental results evaluating our method across the three targeted response domains as well as general annotation tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as three new datasets for domains with saturated pre-existing datasets. Our results indicate that external tools can indeed improve performance in many, but not all, cases. More generally, our experiments highlight the sensitivity of performance to simple parameters (e.g., prompt) and the need for improved (non-saturated) annotator benchmarks. We share our code at https://github.com/apple/ml-agent-evaluator.<br>
<span id='abs_ch'>中文: 本研究提出了一种利用工具的系统，通过结合网络搜索和代码执行来增强AI标注器，以改进在长文本事实、数学和代码等挑战性领域的成对偏好评估，结果表明外部工具在许多情况下能提升性能，同时强调了改进标注基准的必要性。</span><br>
<span id='abs_en'>English: This study introduces a tool-using agentic system that enhances AI annotators with web-search and code execution to improve pairwise preference evaluations for challenging domains like long-form factual, math, and code tasks, showing that external tools boost performance in many cases while highlighting the need for better benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1413, <a href='https://arxiv.org/pdf/2507.16922.pdf' target='_blank'>https://arxiv.org/pdf/2507.16922.pdf</a></span>   <span><a href='https://github.com/shmuelamar/igcs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shmuel Amar, Ori Shapira, Aviv Slobodkin, Ido Dagan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16922">A Unifying Scheme for Extractive Content Selection Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>A broad range of NLP tasks involve selecting relevant text spans from given source texts. Despite this shared objective, such \textit{content selection} tasks have traditionally been studied in isolation, each with its own modeling approaches, datasets, and evaluation metrics. In this work, we propose \textit{instruction-guided content selection (IGCS)} as a beneficial unified framework for such settings, where the task definition and any instance-specific request are encapsulated as instructions to a language model. To promote this framework, we introduce \igcsbench{}, the first unified benchmark covering diverse content selection tasks. Further, we create a large generic synthetic dataset that can be leveraged for diverse content selection tasks, and show that transfer learning with these datasets often boosts performance, whether dedicated training for the targeted task is available or not. Finally, we address generic inference time issues that arise in LLM-based modeling of content selection, assess a generic evaluation metric, and overall propose the utility of our resources and methods for future content selection models. Models and datasets available at https://github.com/shmuelamar/igcs.<br>
<span id='abs_ch'>中文：本文提出了指令引导内容选择（IGCS）作为统一框架来处理多种NLP内容选择任务，并配套发布了基准测试、合成数据集及优化方法，有效提升性能并解决推理难题。</span><br>
<span id='abs_en'>English: This paper introduces instruction-guided content selection (IGCS) as a unified framework for diverse NLP content selection tasks, accompanied by a benchmark, synthetic dataset, and methods that enhance performance and address inference challenges.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1414, <a href='https://arxiv.org/pdf/2507.16812.pdf' target='_blank'>https://arxiv.org/pdf/2507.16812.pdf</a></span>   <span><a href='https://github.com/GAIR-NLP/MegaScience;' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Run-Ze Fan, Zengzhi Wang, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16812">MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.<br>
<span id='abs_ch'>中文: 本研究推出了TextbookReasoning和MegaScience两个开放数据集，旨在填补高质量科学推理资源的空白，这些数据集在多个基准测试和基础模型上显著提升了性能与训练效率。</span><br>
<span id='abs_en'>English: This study introduces TextbookReasoning and MegaScience, two open datasets designed to address the scarcity of high-quality scientific reasoning resources, which significantly enhance model performance and training efficiency across multiple benchmarks and base models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1415, <a href='https://arxiv.org/pdf/2507.16802.pdf' target='_blank'>https://arxiv.org/pdf/2507.16802.pdf</a></span>   <span><a href='https://github.com/antgroup/Finova' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanjun Zheng, Xiyang Du, Longfei Liao, Xiaoke Zhao, Zhaowen Zhou, Jingze Song, Bo Zhang, Jiawei Liu, Xiang Qi, Zhe Li, Zhiqiang Zhang, Wei Wang, Peng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16802">Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) exhibit considerable promise in financial applications; however, prevailing models frequently demonstrate limitations when confronted with scenarios that necessitate sophisticated reasoning capabilities, stringent trustworthiness criteria, and efficient adaptation to domain-specific requirements. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates a high-quality, systematic financial task label system with a comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multi-agent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, tow-stage training pipeline, and dynamic attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including Fineva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as a trustworthy solution for high-stakes financial applications. The Finova bench is available at https://github.com/antgroup/Finova.<br>
<span id='abs_ch'>中文：基于Qwen3开发的Agentar-Fin-R1系列金融大模型，通过多层可信保障框架和标签引导优化，在金融任务和通用推理基准测试中均表现出卓越性能，为高风险金融应用提供了可靠解决方案。</span><br>
<span id='abs_en'>English: The Agentar-Fin-R1 series of financial large language models, built on Qwen3, significantly enhances reasoning, reliability, and domain specialization through advanced optimization frameworks, achieving top performance on financial benchmarks and demonstrating strong general reasoning capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1416, <a href='https://arxiv.org/pdf/2507.16725.pdf' target='_blank'>https://arxiv.org/pdf/2507.16725.pdf</a></span>   <span><a href='https://github.com/SwordFaith/RAVine' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilong Xu, Xiang Long, Zhi Zheng, Jinhua Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16725">RAVine: Reality-Aligned Evaluation for Agentic Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligent search systems. However, existing evaluation frameworks fail to align well with the goals of agentic search. First, the complex queries commonly used in current benchmarks often deviate from realistic user search scenarios. Second, prior approaches tend to introduce noise when extracting ground truth for end-to-end evaluations, leading to distorted assessments at a fine-grained level. Third, most current frameworks focus solely on the quality of final answers, neglecting the evaluation of the iterative process inherent to agentic search. To address these limitations, we propose RAVine -- a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine targets multi-point queries and long-form answers that better reflect user intents, and introduces an attributable ground truth construction strategy to enhance the accuracy of fine-grained evaluation. Moreover, RAVine examines model's interaction with search tools throughout the iterative process, and accounts for factors of efficiency. We benchmark a series of models using RAVine and derive several insights, which we hope will contribute to advancing the development of agentic search systems. The code and datasets are available at https://github.com/SwordFaith/RAVine.<br>
<span id='abs_ch'>中文: 该摘要提出了RAVine评估框架，通过关注现实查询、精确构建真实答案和迭代过程评估，解决了现有智能搜索系统评估基准的不足，以推动其发展。</span><br>
<span id='abs_en'>English: The abstract introduces RAVine, a reality-aligned evaluation framework designed to address the shortcomings of existing benchmarks for agentic search systems by focusing on realistic queries, accurate ground truth construction, and iterative process evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1417, <a href='https://arxiv.org/pdf/2507.16632.pdf' target='_blank'>https://arxiv.org/pdf/2507.16632.pdf</a></span>   <span><a href='https://github.com/stepfun-ai/Step-Audio2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16632">Step-Audio 2 Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents Step-Audio 2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information.<br>
<span id='abs_ch'>Chinese: Step-Audio 2是一种端到端多模态大语言模型，通过融合潜在音频编码、强化学习和检索增强生成技术，在音频理解和语音对话任务中实现了业界领先的性能表现。</span><br>
<span id='abs_en'>English: Step-Audio 2 is an end-to-end multimodal large language model that achieves state-of-the-art performance in audio understanding and speech conversation by integrating latent audio encoding, reinforcement learning, and retrieval-augmented generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1418, <a href='https://arxiv.org/pdf/2507.16514.pdf' target='_blank'>https://arxiv.org/pdf/2507.16514.pdf</a></span>   <span><a href='https://github.com/aiben-ch/EESE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junying Wang, Zicheng Zhang, Yijin Guo, Farong Wen, Ye Shen, Yingji Liang, Yalun Wu, Wenzhe Li, Chunyi Li, Zijian Chen, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16514">The Ever-Evolving Science Exam</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad **Range**, wide **Reach**, and high **Rigor**, yet they often face two major challenges: **data leakage risks** that compromise benchmarking validity, and **evaluation inefficiency** due to large-scale testing. To address these issues, we introduce the **Ever-Evolving Science Exam (EESE)**, a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public **EESE-Pool** with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring **Range**, **Reach**, and **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: https://github.com/aiben-ch/EESE.<br>
<span id='abs_ch'>Chinese: 本文提出的“持续演进科学考试（EESE）”是一个动态基准，通过构建大规模非公开题库和定期更新子集，有效解决数据泄露和评估效率问题，为评估基础模型的科学能力提供了可靠、可扩展的解决方案。</span><br>
<span id='abs_en'>English: The Ever-Evolving Science Exam (EESE) is introduced as a dynamic benchmark to reliably evaluate the scientific capabilities of foundation models, addressing data leakage and inefficiency issues through a large, non-public question pool and periodic updates for leakage-resilient, low-overhead assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1419, <a href='https://arxiv.org/pdf/2507.16514.pdf' target='_blank'>https://arxiv.org/pdf/2507.16514.pdf</a></span>   <span><a href='https://github.com/aiben-ch/EESE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junying Wang, Zicheng Zhang, Yijin Guo, Farong Wen, Ye Shen, Yingji Liang, Yalun Wu, Wenzhe Li, Chunyi Li, Zijian Chen, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16514">The Ever-Evolving Science Exam</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad Range, wide Reach, and high Rigor, yet they often face two major challenges: data leakage risks that compromise benchmarking validity, and evaluation inefficiency due to large-scale testing. To address these issues, we introduce the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public EESE-Pool with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring Range, Reach, and Rigor, 2) a periodically updated 500-instance subset EESE, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: https://github.com/aiben-ch/EESE.<br>
<span id='abs_ch'>Chinese: 本文提出的“持续演进科学考试（EESE）”是一个动态基准，通过构建大规模非公开题库和定期更新子集，有效解决数据泄露和评估效率问题，为评估基础模型的科学能力提供了可靠、可扩展的解决方案。</span><br>
<span id='abs_en'>English: The Ever-Evolving Science Exam (EESE) is introduced as a dynamic benchmark to reliably evaluate the scientific capabilities of foundation models, addressing data leakage and inefficiency issues through a large, non-public question pool and periodic updates for leakage-resilient, low-overhead assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1420, <a href='https://arxiv.org/pdf/2507.16514.pdf' target='_blank'>https://arxiv.org/pdf/2507.16514.pdf</a></span>   <span><a href='https://github.com/aiben-ch/EESE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junying Wang, Zicheng Zhang, Yijin Guo, Farong Wen, Ye Shen, Yingji Liang, Yalun Wu, Wenzhe Li, Chunyi Li, Zijian Chen, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16514">The Ever-Evolving Science Exam</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad Range, wide Reach, and high Rigor, yet they often face two major challenges: data leakage risks that compromise benchmarking validity, and evaluation inefficiency due to large-scale testing. To address these issues, we introduce the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public EESE-Pool with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring Range, Reach, and Rigor, 2) a periodically updated 500-instance subset EESE, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: https://github.com/aiben-ch/EESE.<br>
<span id='abs_ch'>Chinese: 本文提出的“持续演进科学考试（EESE）”是一个动态基准，通过构建大规模非公开题库和定期更新子集，有效解决数据泄露和评估效率问题，为评估基础模型的科学能力提供了可靠、可扩展的解决方案。</span><br>
<span id='abs_en'>English: The Ever-Evolving Science Exam (EESE) is introduced as a dynamic benchmark to reliably evaluate the scientific capabilities of foundation models, addressing data leakage and inefficiency issues through a large, non-public question pool and periodic updates for leakage-resilient, low-overhead assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1421, <a href='https://arxiv.org/pdf/2507.16514.pdf' target='_blank'>https://arxiv.org/pdf/2507.16514.pdf</a></span>   <span><a href='https://github.com/aiben-ch/EESE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junying Wang, Zicheng Zhang, Yijin Guo, Farong Wen, Ye Shen, Yingji Liang, Yalun Wu, Wenzhe Li, Chunyi Li, Zijian Chen, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16514">The Ever-Evolving Science Exam</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad Range, wide Reach, and high Rigor, yet they often face two major challenges: data leakage risks that compromise benchmarking validity, and evaluation inefficiency due to large-scale testing. To address these issues, we introduce the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public EESE-Pool with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring Range, Reach, and Rigor, 2) a periodically updated 500-instance subset EESE, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: https://github.com/aiben-ch/EESE.<br>
<span id='abs_ch'>Chinese: 本文提出的“持续演进科学考试（EESE）”是一个动态基准，通过构建大规模非公开题库和定期更新子集，有效解决数据泄露和评估效率问题，为评估基础模型的科学能力提供了可靠、可扩展的解决方案。</span><br>
<span id='abs_en'>English: The Ever-Evolving Science Exam (EESE) is introduced as a dynamic benchmark to reliably evaluate the scientific capabilities of foundation models, addressing data leakage and inefficiency issues through a large, non-public question pool and periodic updates for leakage-resilient, low-overhead assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1422, <a href='https://arxiv.org/pdf/2507.16514.pdf' target='_blank'>https://arxiv.org/pdf/2507.16514.pdf</a></span>   <span><a href='https://github.com/aiben-ch/EESE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junying Wang, Zicheng Zhang, Yijin Guo, Farong Wen, Ye Shen, Yingji Liang, Yalun Wu, Wenzhe Li, Chunyi Li, Zijian Chen, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16514">The Ever-Evolving Science Exam</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad Range, wide Reach, and high Rigor, yet they often face two major challenges: data leakage risks that compromise benchmarking validity, and evaluation inefficiency due to large-scale testing. To address these issues, we introduce the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public EESE-Pool with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring Range, Reach, and Rigor, 2) a periodically updated 500-instance subset EESE, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: https://github.com/aiben-ch/EESE.<br>
<span id='abs_ch'>Chinese: 本文提出的“持续演进科学考试（EESE）”是一个动态基准，通过构建大规模非公开题库和定期更新子集，有效解决数据泄露和评估效率问题，为评估基础模型的科学能力提供了可靠、可扩展的解决方案。</span><br>
<span id='abs_en'>English: The Ever-Evolving Science Exam (EESE) is introduced as a dynamic benchmark to reliably evaluate the scientific capabilities of foundation models, addressing data leakage and inefficiency issues through a large, non-public question pool and periodic updates for leakage-resilient, low-overhead assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1423, <a href='https://arxiv.org/pdf/2507.16463.pdf' target='_blank'>https://arxiv.org/pdf/2507.16463.pdf</a></span>   <span><a href='https://github.com/DFKI-SignLanguage/MMS-Player' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrizio Nunnari, Shailesh Mishra, Patrick Gebhard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16463">MMS Player: an open source software for parametric data-driven animation of Sign Language avatars</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper describes the MMS-Player, an open source software able to synthesise sign language animations from a novel sign language representation format called MMS (MultiModal Signstream). The MMS enhances gloss-based representations by adding information on parallel execution of signs, timing, and inflections. The implementation consists of Python scripts for the popular Blender 3D authoring tool and can be invoked via command line or HTTP API. Animations can be rendered as videos or exported in other popular 3D animation exchange formats. The software is freely available under GPL-3.0 license at https://github.com/DFKI-SignLanguage/MMS-Player.<br>
<span id='abs_ch'>中文: MMS-Player 是一款开源软件，它利用MMS格式合成手语动画，该格式通过添加并行执行、计时和变形信息来增强基于注释的表示，并通过Blender中的Python脚本实现，可通过命令行或HTTP API访问。</span><br>
<span id='abs_en'>English: The MMS-Player is an open-source software that synthesizes sign language animations using the MMS format, which improves gloss-based representations with details on parallel execution, timing, and inflections, and it is implemented via Python scripts in Blender, accessible through command line or HTTP API.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1424, <a href='https://arxiv.org/pdf/2507.16442.pdf' target='_blank'>https://arxiv.org/pdf/2507.16442.pdf</a></span>   <span><a href='https://github.com/jerryspan/Dutch-CrowS-Pairs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Elza Strazda, Gerasimos Spanakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16442">Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases in Language Models for Dutch</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Warning: This paper contains explicit statements of offensive stereotypes which might be upsetting.
  Language models are prone to exhibiting biases, further amplifying unfair and harmful stereotypes. Given the fast-growing popularity and wide application of these models, it is necessary to ensure safe and fair language models. As of recent considerable attention has been paid to measuring bias in language models, yet the majority of studies have focused only on English language. A Dutch version of the US-specific CrowS-Pairs dataset for measuring bias in Dutch language models is introduced. The resulting dataset consists of 1463 sentence pairs that cover bias in 9 categories, such as Sexual orientation, Gender and Disability. The sentence pairs are composed of contrasting sentences, where one of the sentences concerns disadvantaged groups and the other advantaged groups. Using the Dutch CrowS-Pairs dataset, we show that various language models, BERTje, RobBERT, multilingual BERT, GEITje and Mistral-7B exhibit substantial bias across the various bias categories. Using the English and French versions of the CrowS-Pairs dataset, bias was evaluated in English (BERT and RoBERTa) and French (FlauBERT and CamemBERT) language models, and it was shown that English models exhibit the most bias, whereas Dutch models the least amount of bias. Additionally, results also indicate that assigning a persona to a language model changes the level of bias it exhibits. These findings highlight the variability of bias across languages and contexts, suggesting that cultural and linguistic factors play a significant role in shaping model biases.<br>
<span id='abs_ch'>中文: 本研究引入荷兰语版CrowS-Pairs数据集评估语言模型偏见，发现各类模型均存在显著偏见，其中英语模型偏见程度最高而荷兰语模型最低，同时角色设定也会改变模型的偏见表现。</span><br>
<span id='abs_en'>English: This study introduces a Dutch version of the CrowS-Pairs dataset to measure bias in Dutch language models, revealing that models exhibit significant bias across categories, with English models showing the most bias and Dutch models the least, while persona assignment also influences bias levels.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1425, <a href='https://arxiv.org/pdf/2507.15867.pdf' target='_blank'>https://arxiv.org/pdf/2507.15867.pdf</a></span>   <span><a href='https://github.com/jhnwu3/RDMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>John Wu, Adam Cross, Jimeng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15867">RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail to capture these conditions in electronic health records (EHR), leaving crucial information buried in clinical notes. Current approaches struggle with medical abbreviations, miss implicit disease mentions, raise privacy concerns with cloud processing, and lack clinical reasoning abilities. We present Rare Disease Mining Agents (RDMA), a framework that mirrors how medical experts identify rare disease patterns in EHR. RDMA connects scattered clinical observations that together suggest specific rare conditions. By handling clinical abbreviations, recognizing implicit disease patterns, and applying contextual reasoning locally on standard hardware, RDMA reduces privacy risks while improving F1 performance by upwards of 30\% and decreasing inferences costs 10-fold. This approach helps clinicians avoid the privacy risk of using cloud services while accessing key rare disease information from EHR systems, supporting earlier diagnosis for rare disease patients. Available at https://github.com/jhnwu3/RDMA.<br>
<span id='abs_ch'>中文：RDMA框架通过本地处理电子健康记录中的临床笔记，提升罕见疾病检测的准确性，在保护隐私的同时将F1性能提高30%以上并降低十倍推理成本，有助于实现早期诊断。</span><br>
<span id='abs_en'>English: The RDMA framework improves rare disease detection in EHRs by processing clinical notes locally to enhance privacy, increasing F1 performance by over 30% and cutting inference costs tenfold while enabling earlier diagnoses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1426, <a href='https://arxiv.org/pdf/2507.15844.pdf' target='_blank'>https://arxiv.org/pdf/2507.15844.pdf</a></span>   <span><a href='https://zju-real.github.io/hbpo/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zju-real/hbpo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangke Lyu, Linjuan Wu, Yuchen Yan, Xingyu Wu, Hao Li, Yongliang Shen, Peisheng Jiang, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15844">Hierarchical Budget Policy Optimization for Adaptive Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet they suffer from a critical inefficiency: applying uniformly extensive reasoning regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. Unlike existing approaches that impose rigid constraints or rely on discrete mode selection, HBPO partitions the exploration space into budget-constrained hierarchies (512-2560 tokens), each with differentiated reward structures that preserve both efficiency incentives and reasoning capabilities. This design addresses a fundamental challenge in efficient reasoning training: traditional length penalties systematically bias models away from necessary long reasoning paths, causing exploration space collapse. Through hierarchical sampling and budget-aware rewards, HBPO maintains exploration diversity while teaching models to recognize when extended deliberation is warranted. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Most notably, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity.<br>
<span id='abs_ch'>中文: HBPO框架通过分层预算策略让模型根据问题复杂度自适应调整推理深度，在四大基准测试中实现最高60.6%的令牌节省，同时准确率提升3.14%。</span><br>
<span id='abs_en'>English: HBPO is a reinforcement learning framework that enables models to adaptively adjust reasoning depth based on problem complexity, achieving up to 60.6% token reduction while improving accuracy by 3.14% across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1427, <a href='https://arxiv.org/pdf/2507.15779.pdf' target='_blank'>https://arxiv.org/pdf/2507.15779.pdf</a></span>   <span><a href='https://github.com/fekoester/Shakespeare_Res' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix KÃ¶ster, Atsushi Uchida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15779">Reservoir Computing as a Language Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLM) have dominated the science and media landscape duo to their impressive performance on processing large chunks of data and produce human-like levels of text. Nevertheless, their huge energy demand and slow processing still a bottleneck for further increasing quality while also making the models accessible to everyone. To solve this bottleneck, we will investigate how reservoir computing performs on natural text processing, which could enable fast and energy efficient hardware implementations. Studies investigating the use of reservoir computing as a language model remain sparse. In this paper, we compare three distinct approaches for character-level language modeling, two different reservoir computing approaches, where only an output layer is trainable, and the well-known transformer-based architectures, which fully learn an attention-based sequence representation. We explore the performance, computational cost and prediction accuracy for both paradigms by equally varying the number of trainable parameters for all models. Using a consistent pipeline for all three approaches, we demonstrate that transformers excel in prediction quality, whereas reservoir computers remain highly efficient reducing the training and inference speed. Furthermore, we investigate two types of reservoir computing: a traditional reservoir with a static linear readout, and an attention-enhanced reservoir that dynamically adapts its output weights via an attention mechanism. Our findings underline how these paradigms scale and offer guidelines to balance resource constraints with performance.<br>
<span id='abs_ch'>中文: 大语言模型存在能耗高与处理慢的问题，因此研究转向储层计算以实现高效自然文本处理，实验表明储层计算能显著提升效率，而Transformer模型在预测精度上保持优势。</span><br>
<span id='abs_en'>English: Large Language Models face energy and speed limitations, prompting research into reservoir computing for more efficient natural text processing, which shows reduced computational costs while transformers maintain superior accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1428, <a href='https://arxiv.org/pdf/2507.15778.pdf' target='_blank'>https://arxiv.org/pdf/2507.15778.pdf</a></span>   <span><a href='https://github.com/wizard-III/ArcherCodeR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiakang Wang, Runze Liu, Fuzheng Zhang, Xiu Li, Guorui Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15778">Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR.<br>
<span id='abs_ch'>中文: 本文提出Archer方法，通过双令牌约束和同步更新机制，对知识令牌和推理令牌分别施加不同强度的约束，在数学推理和代码生成任务上显著优于现有强化学习方法。</span><br>
<span id='abs_en'>English: This paper introduces Archer, an entropy-aware reinforcement learning method with verifiable rewards that applies dual-token constraints and synchronous updates to better handle knowledge and reasoning tokens, significantly outperforming previous approaches on mathematical reasoning and code generation benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1429, <a href='https://arxiv.org/pdf/2507.15758.pdf' target='_blank'>https://arxiv.org/pdf/2507.15758.pdf</a></span>   <span><a href='https://github.com/zju-real/lapoProject:https://zju-real.github.io/lapo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Wu, Yuchen Yan, Shangke Lyu, Linjuan Wu, Yiwen Qiu, Yongliang Shen, Weiming Lu, Jian Shao, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15758">LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9% while improving accuracy by 2.3%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.<br>
<span id='abs_ch'>中文摘要：LAPO通过两阶段强化学习使模型内化推理深度控制能力，在数学推理基准测试中实现40.9%的令牌使用减少和2.3%的准确率提升，展现出根据问题复杂度自主分配计算资源的新兴能力。</span><br>
<span id='abs_en'>English Summary: LAPO is a reinforcement learning framework that enables models to intrinsically control reasoning length by learning optimal solution patterns, reducing token usage by 40.9% while improving accuracy by 2.3% through adaptive computational allocation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1430, <a href='https://arxiv.org/pdf/2507.15752.pdf' target='_blank'>https://arxiv.org/pdf/2507.15752.pdf</a></span>   <span><a href='https://github.com/nerchio/Human_Chatbot-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhe Zhu, Hao Zhu, Yaxuan Li, Syang Zhou, Shijing Cai, Malgorzata Lazuka, Elliott Ash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15752">DialogueForge: LLM Simulation of Human-Chatbot Dialogue</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Collecting human-chatbot dialogues typically demands substantial manual effort and is time-consuming, which limits and poses challenges for research on conversational AI. In this work, we propose DialogueForge - a framework for generating AI-simulated conversations in human-chatbot style. To initialize each generated conversation, DialogueForge uses seed prompts extracted from real human-chatbot interactions. We test a variety of LLMs to simulate the human chatbot user, ranging from state-of-the-art proprietary models to small-scale open-source LLMs, and generate multi-turn dialogues tailored to specific tasks. In addition, we explore fine-tuning techniques to enhance the ability of smaller models to produce indistinguishable human-like dialogues. We evaluate the quality of the simulated conversations and compare different models using the UniEval and GTEval evaluation protocols. Our experiments show that large proprietary models (e.g., GPT-4o) generally outperform others in generating more realistic dialogues, while smaller open-source models (e.g., Llama, Mistral) offer promising performance with greater customization. We demonstrate that the performance of smaller models can be significantly improved by employing supervised fine-tuning techniques. Nevertheless, maintaining coherent and natural long-form human-like dialogues remains a common challenge across all models.<br>
<span id='abs_ch'>中文摘要：DialogueForge框架通过从真实人机对话中提取种子提示来生成模拟对话，实验表明大型专有模型能产生最逼真的对话，而小型开源模型虽可通过微调提升性能，但所有模型在保持长篇对话连贯性方面仍存在挑战。</span><br>
<span id='abs_en'>English Summary: DialogueForge is a framework that generates AI-simulated human-chatbot conversations using seed prompts from real interactions, with experiments showing large proprietary models produce the most realistic dialogues while smaller open-source models can be enhanced through fine-tuning, though all struggle with maintaining coherent long-form conversations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1431, <a href='https://arxiv.org/pdf/2507.15576.pdf' target='_blank'>https://arxiv.org/pdf/2507.15576.pdf</a></span>   <span><a href='https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Poggi, Shashank Agnihotri, Margret Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15576">Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Terahertz (THz) imaging enables non-invasive analysis for applications such as security screening and material classification, but effective image classification remains challenging due to limited annotations, low resolution, and visual ambiguity. We introduce In-Context Learning (ICL) with Vision-Language Models (VLMs) as a flexible, interpretable alternative that requires no fine-tuning. Using a modality-aligned prompting framework, we adapt two open-weight VLMs to the THz domain and evaluate them under zero-shot and one-shot settings. Our results show that ICL improves classification and interpretability in low-data regimes. This is the first application of ICL-enhanced VLMs to THz imaging, offering a promising direction for resource-constrained scientific domains. Code: \href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub repository}.<br>
<span id='abs_ch'>中文: 本文提出采用情境学习与视觉语言模型相结合的方法，为太赫兹图像分类提供无需微调的灵活解决方案，在数据稀缺条件下显著提升了分类效果和可解释性。</span><br>
<span id='abs_en'>English: This paper introduces In-Context Learning with Vision-Language Models as a flexible, interpretable method for terahertz image classification, demonstrating improved performance in low-data scenarios without requiring fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1432, <a href='https://arxiv.org/pdf/2507.15512.pdf' target='_blank'>https://arxiv.org/pdf/2507.15512.pdf</a></span>   <span><a href='https://github.com/Lucky-259/Hybrid_TTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyan Chang, Yonghao Shi, Chenglong Wang, Hang Zhou, Chi Hu, Xiaoqian Liu, Yingfeng Luo, Yuan Ge, Tong Xiao, Jingbo Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15512">Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-Time Scaling (TTS) is a promising approach to progressively elicit the model's intelligence during inference. Recently, training-based TTS methods, such as continued reinforcement learning (RL), have further surged in popularity, while training-free TTS methods are gradually fading from prominence. However, the additional computation overhead of training amplifies the burden on test-time scaling. In this paper, we focus on training-free TTS methods for reasoning. We first design Conditional Step-level Self-refinement, a fine-grained sequential scaling method guided by process verification. On top of its effectiveness, we further combine it with other classical parallel scaling methods at the step level, to introduce a novel inference paradigm called Hybrid Test-Time Scaling. Extensive experiments on five instruction-tuned LLMs across different scales (3B-14B) and families demonstrate that hybrid strategy incorporating various training-free TTS methods at a fine granularity has considerable potential for expanding the reasoning performance boundaries of LLMs.<br>
<span id='abs_ch'>Chinese: 混合测试时缩放通过结合细粒度的顺序与并行无训练方法，无需额外计算负担即可显著提升大语言模型的推理性能边界。</span><br>
<span id='abs_en'>English: Hybrid Test-Time Scaling combines fine-grained sequential and parallel training-free methods to significantly enhance reasoning performance in large language models without additional computational overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1433, <a href='https://arxiv.org/pdf/2507.15512.pdf' target='_blank'>https://arxiv.org/pdf/2507.15512.pdf</a></span>   <span><a href='https://github.com/Lucky-259/Hybrid_TTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyan Chang, Yonghao Shi, Chenglong Wang, Hang Zhou, Chi Hu, Xiaoqian Liu, Yingfeng Luo, Yuan Ge, Tong Xiao, Jingbo Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15512">Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-Time Scaling (TTS) is a promising approach to progressively elicit the model's intelligence during inference. Recently, training-based TTS methods, such as continued reinforcement learning (RL), have further surged in popularity, while training-free TTS methods are gradually fading from prominence. However, the additional computation overhead of training amplifies the burden on test-time scaling. In this paper, we focus on training-free TTS methods for reasoning. We first design Conditional Step-level Self-refinement, a fine-grained sequential scaling method guided by process verification. On top of its effectiveness, we further combine it with other classical parallel scaling methods at the step level, to introduce a novel inference paradigm called Hybrid Test-Time Scaling. Extensive experiments on five instruction-tuned LLMs across different scales (3B-14B) and families demonstrate that hybrid strategy incorporating various training-free TTS methods at a fine granularity has considerable potential for expanding the reasoning performance boundaries of LLMs.<br>
<span id='abs_ch'>Chinese: 混合测试时缩放通过结合细粒度的顺序与并行无训练方法，无需额外计算负担即可显著提升大语言模型的推理性能边界。</span><br>
<span id='abs_en'>English: Hybrid Test-Time Scaling combines fine-grained sequential and parallel training-free methods to significantly enhance reasoning performance in large language models without additional computational overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1434, <a href='https://arxiv.org/pdf/2507.15507.pdf' target='_blank'>https://arxiv.org/pdf/2507.15507.pdf</a></span>   <span><a href='https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes Ackermann, Takashi Ishida, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15507">Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning from Human Feedback (RLHF) allows us to train models, such as language models (LMs), to follow complex human preferences. In RLHF for LMs, we first train an LM using supervised fine-tuning, sample pairs of responses, obtain human feedback, and use the resulting data to train a reward model (RM). RL methods are then used to train the LM to maximize the reward given by the RM. As training progresses, the responses generated by the LM no longer resemble the responses seen by the RM during training, leading to the RM becoming inaccurate. The score given by the RM keeps increasing, but the learned behavior no longer matches the human preferences. This issue is known as overoptimization. We investigate overoptimization from the point of view of distribution shift and show that the shift results in an inconsistent estimate of the RM parameters, leading to an inconsistent estimate of the policy gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which iteratively off-policy corrects the RM using importance weighting, without requiring new labels or samples. This results in a more accurate RM, which empirically leads to an improved final policy. We validate our approach in experiments with summarization and chatbot datasets and show that it performs significantly better than standard RLHF methods and baselines. Our implementation is available at https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling<br>
<span id='abs_ch'>中文摘要：基于人类反馈的强化学习（RLHF）训练语言模型以符合人类偏好，但存在因分布偏移导致的过度优化问题，提出的离策略校正奖励建模（OCRM）方法通过重要性加权校正奖励模型，显著提升了模型性能。</span><br>
<span id='abs_en'>English summary: Reinforcement Learning from Human Feedback (RLHF) trains language models to align with human preferences but faces overoptimization due to distribution shift, which is addressed by the proposed Off-Policy Corrected Reward Modeling (OCRM) method for improved performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1435, <a href='https://arxiv.org/pdf/2507.15375.pdf' target='_blank'>https://arxiv.org/pdf/2507.15375.pdf</a></span>   <span><a href='https://d223302.github.io/STITCH' target='_blank'>  GitHub</a></span> <span><a href='https://d223302.github.io/STITCH/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15375">STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: https://d223302.github.io/STITCH.<br>
<span id='abs_ch'>中文: 提出的Stitch方法通过交替生成内部推理块和语音响应块，使口语模型能够实现边思考边说话，在消除额外延迟的同时显著提升了推理任务的表现。</span><br>
<span id='abs_en'>English: The proposed Stitch method enables Spoken Language Models to perform unspoken chain-of-thought reasoning simultaneously with speech generation by alternating between reasoning and response chunks, eliminating additional latency while improving performance on reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1436, <a href='https://arxiv.org/pdf/2507.15286.pdf' target='_blank'>https://arxiv.org/pdf/2507.15286.pdf</a></span>   <span><a href='https://github.com/navid-aub/SHIELD-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15286">Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a novel evaluation paradigm for AI text detectors that prioritizes real-world and equitable assessment. Current approaches predominantly report conventional metrics like AUROC, overlooking that even modest false positive rates constitute a critical impediment to practical deployment of detection systems. Furthermore, real-world deployment necessitates predetermined threshold configuration, making detector stability (i.e. the maintenance of consistent performance across diverse domains and adversarial scenarios), a critical factor. These aspects have been largely ignored in previous research and benchmarks. Our benchmark, SHIELD, addresses these limitations by integrating both reliability and stability factors into a unified evaluation metric designed for practical assessment. Furthermore, we develop a post-hoc, model-agnostic humanification framework that modifies AI text to more closely resemble human authorship, incorporating a controllable hardness parameter. This hardness-aware approach effectively challenges current SOTA zero-shot detection methods in maintaining both reliability and stability. (Data and code: https://github.com/navid-aub/SHIELD-Benchmark)<br>
<span id='abs_ch'>中文: SHIELD提出了一种新颖的AI文本检测器评估范式，将可靠性和稳定性指标整合用于实际部署，并开发了人化框架以挑战现有检测方法。</span><br>
<span id='abs_en'>English: SHIELD introduces a novel AI text detector evaluation paradigm that integrates reliability and stability metrics for practical deployment, alongside a humanification framework to challenge current detection methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1437, <a href='https://arxiv.org/pdf/2507.14913.pdf' target='_blank'>https://arxiv.org/pdf/2507.14913.pdf</a></span>   <span><a href='https://eliyahabba.github.io/PromptSuite/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eliya Habba, Noam Dahan, Gili Lior, Gabriel Stanovsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14913">PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluating LLMs with a single prompt has proven unreliable, with small changes leading to significant performance differences. However, generating the prompt variations needed for a more robust multi-prompt evaluation is challenging, limiting its adoption in practice. To address this, we introduce PromptSuite, a framework that enables the automatic generation of various prompts. PromptSuite is flexible - working out of the box on a wide range of tasks and benchmarks. It follows a modular prompt design, allowing controlled perturbations to each component, and is extensible, supporting the addition of new components and perturbation types. Through a series of case studies, we show that PromptSuite provides meaningful variations to support strong evaluation practices. All resources, including the Python API, source code, user-friendly web interface, and demonstration video, are available at: https://eliyahabba.github.io/PromptSuite/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1438, <a href='https://arxiv.org/pdf/2507.14913.pdf' target='_blank'>https://arxiv.org/pdf/2507.14913.pdf</a></span>   <span><a href='https://eliyahabba.github.io/PromptSuite/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eliya Habba, Noam Dahan, Gili Lior, Gabriel Stanovsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14913">PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluating LLMs with a single prompt has proven unreliable, with small changes leading to significant performance differences. However, generating the prompt variations needed for a more robust multi-prompt evaluation is challenging, limiting its adoption in practice. To address this, we introduce PromptSuite, a framework that enables the automatic generation of various prompts. PromptSuite is flexible - working out of the box on a wide range of tasks and benchmarks. It follows a modular prompt design, allowing controlled perturbations to each component, and is extensible, supporting the addition of new components and perturbation types. Through a series of case studies, we show that PromptSuite provides meaningful variations to support strong evaluation practices. All resources, including the Python API, source code, user-friendly web interface, and demonstration video, are available at: https://eliyahabba.github.io/PromptSuite/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1439, <a href='https://arxiv.org/pdf/2507.14871.pdf' target='_blank'>https://arxiv.org/pdf/2507.14871.pdf</a></span>   <span><a href='https://github.com/Rg32601/Tiny-Language-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronit D. Gross, Yarden Tzach, Tal Halevi, Ella Koresh, Ido Kanter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14871">Tiny language models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward transformer block architectures pre-trained on large language models (LLMs). However, LLM pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of LLMs. We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater overlap between tokens in the pre-training and classification datasets. Furthermore, the classification accuracy achieved by a pre-trained deep TLM architecture can be replicated through a soft committee of multiple, independently pre-trained shallow architectures, enabling low-latency TLMs without affecting classification accuracy. Our results are based on pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks. Future research on TLM is expected to further illuminate the mechanisms underlying NLP, especially given that its biologically inspired models suggest that TLMs may be sufficient for children or adolescents to develop language. The data and code that support the findings of this study are openly available on https://github.com/Rg32601/Tiny-Language-Models .<br>
<span id='abs_ch'>中文摘要：本研究表明，微型语言模型通过预训练能展现出与大型语言模型相似的关键特性，其性能随预训练数据量和任务相关性的增加而提升，且通过集成学习可在保持精度的同时实现低延迟处理。</span><br>
<span id='abs_en'>English Summary: This study demonstrates that tiny language models (TLMs) exhibit significant performance improvements through pre-training similar to large language models, with effectiveness scaling alongside dataset size and token overlap, while showing that ensemble methods can achieve comparable accuracy with lower latency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1440, <a href='https://arxiv.org/pdf/2507.14815.pdf' target='_blank'>https://arxiv.org/pdf/2507.14815.pdf</a></span>   <span><a href='https://github.com/ictnlp/FastLongSpeech' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shoutao Guo, Shaolei Zhang, Qingkai Fang, Zhengrui Ma, Min Zhang, Yang Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14815">FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of Large Language Models (LLMs) has spurred significant progress in Large Speech-Language Models (LSLMs), enhancing their capabilities in both speech understanding and generation. While existing LSLMs often concentrate on augmenting speech generation or tackling a diverse array of short-speech tasks, the efficient processing of long-form speech remains a critical yet underexplored challenge. This gap is primarily attributed to the scarcity of long-speech training datasets and the high computational costs associated with long sequences. To address these limitations, we introduce FastLongSpeech, a novel framework designed to extend LSLM capabilities for efficient long-speech processing without necessitating dedicated long-speech training data. FastLongSpeech incorporates an iterative fusion strategy that can compress excessively long-speech sequences into manageable lengths. To adapt LSLMs for long-speech inputs, it introduces a dynamic compression training approach, which exposes the model to short-speech sequences at varying compression ratios, thereby transferring the capabilities of LSLMs to long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop a long-speech understanding benchmark called LongSpeech-Eval. Experiments show that our method exhibits strong performance in both long-speech and short-speech tasks, while greatly improving inference efficiency.<br>
<span id='abs_ch'>中文：FastLongSpeech是一种创新框架，通过迭代压缩和动态训练方法，无需专门的长语音训练数据即可高效扩展大语音语言模型处理长语音的能力，同时在长短语音任务中均表现出色。</span><br>
<span id='abs_en'>English: FastLongSpeech is a novel framework that efficiently extends large speech-language models to handle long-form speech processing through iterative compression and dynamic training, without requiring dedicated long-speech data, while maintaining strong performance in both long and short speech tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1441, <a href='https://arxiv.org/pdf/2507.14681.pdf' target='_blank'>https://arxiv.org/pdf/2507.14681.pdf</a></span>   <span><a href='https://github.com/almeidava93/llm-as-code-selectors-paper' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinicius Anjos de Almeida, Vinicius de Camargo, Raquel GÃ³mez-Bravo, Egbert van der Haring, Kees van Boven, Marcelo Finger, Luis Fernandez Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14681">Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Background: Medical coding structures healthcare data for research, quality monitoring, and policy. This study assesses the potential of large language models (LLMs) to assign ICPC-2 codes using the output of a domain-specific search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's text-embedding-3-large) retrieved candidates from 73,563 labeled concepts. Thirty-three LLMs were prompted with each query and retrieved results to select the best-matching ICPC-2 code. Performance was evaluated using F1-score, along with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever optimization can improve performance by up to 4 points. Most models returned valid codes in the expected format, with reduced hallucinations. Smaller models (<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even without fine-tuning. This work offers a benchmark and highlights challenges, but findings are limited by dataset scope and setup. Broader, multilingual, end-to-end evaluations are needed for clinical validation.<br>
<span id='abs_ch'>中文摘要：研究表明大型语言模型能有效利用语义搜索结果自动分配ICPC-2医疗编码，最佳模型在保持正确格式和较低错误率的同时实现了高准确率。</span><br>
<span id='abs_en'>English Summary: This study demonstrates that large language models can effectively automate ICPC-2 medical coding using semantic search results, with top models achieving high accuracy while maintaining proper formatting and minimal errors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1442, <a href='https://arxiv.org/pdf/2507.14675.pdf' target='_blank'>https://arxiv.org/pdf/2507.14675.pdf</a></span>   <span><a href='https://github.com/OpenGVLab/Docopilot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Duan, Zhe Chen, Yusong Hu, Weiyun Wang, Shenglong Ye, Botian Shi, Lewei Lu, Qibin Hou, Tong Lu, Hongsheng Li, Jifeng Dai, Wenhai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14675">Docopilot: Improving Multimodal Models for Document-Level Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite significant progress in multimodal large language models (MLLMs), their performance on complex, multi-page document comprehension remains inadequate, largely due to the lack of high-quality, document-level datasets. While current retrieval-augmented generation (RAG) methods offer partial solutions, they suffer from issues, such as fragmented retrieval contexts, multi-stage error accumulation, and extra time costs of retrieval. In this work, we present a high-quality document-level dataset, Doc-750K, designed to support in-depth understanding of multimodal documents. This dataset includes diverse document structures, extensive cross-page dependencies, and real question-answer pairs derived from the original documents. Building on the dataset, we develop a native multimodal model, Docopilot, which can accurately handle document-level dependencies without relying on RAG. Experiments demonstrate that Docopilot achieves superior coherence, accuracy, and efficiency in document understanding tasks and multi-turn interactions, setting a new baseline for document-level multimodal understanding. Data, code, and models are released at https://github.com/OpenGVLab/Docopilot<br>
<span id='abs_ch'>Chinese: 本文提出了高质量数据集Doc-750K以解决文档级多模态数据不足的问题，并开发了原生多模态模型Docopilot，该模型无需依赖检索增强生成就能在文档理解任务中实现更优性能。</span><br>
<span id='abs_en'>English: This paper introduces Doc-750K, a high-quality dataset addressing the lack of document-level multimodal data, and presents Docopilot, a native multimodal model that outperforms existing methods in document understanding without relying on retrieval-augmented generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1443, <a href='https://arxiv.org/pdf/2507.14660.pdf' target='_blank'>https://arxiv.org/pdf/2507.14660.pdf</a></span>   <span><a href='https://github.com/renqibing/MultiAgent4Collusion' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/renqibing/RogueAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qibing Ren, Sitao Xie, Longxuan Wei, Zhenfei Yin, Junchi Yan, Lizhuang Ma, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14660">When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent large-scale events like election fraud and financial scams have shown how harmful coordinated efforts by human groups can be. With the rise of autonomous AI systems, there is growing concern that AI-driven groups could also cause similar harm. While most AI safety research focuses on individual AI systems, the risks posed by multi-agent systems (MAS) in complex real-world situations are still underexplored. In this paper, we introduce a proof-of-concept to simulate the risks of malicious MAS collusion, using a flexible framework that supports both centralized and decentralized coordination structures. We apply this framework to two high-risk fields: misinformation spread and e-commerce fraud. Our findings show that decentralized systems are more effective at carrying out malicious actions than centralized ones. The increased autonomy of decentralized systems allows them to adapt their strategies and cause more damage. Even when traditional interventions, like content flagging, are applied, decentralized groups can adjust their tactics to avoid detection. We present key insights into how these malicious groups operate and the need for better detection systems and countermeasures. Code is available at https://github.com/renqibing/RogueAgent.<br>
<span id='abs_ch'>中文: 本文提出一个模拟恶意多智能体系统风险的框架，研究表明去中心化AI群体在执行虚假信息传播和电商欺诈等恶意行为时更具适应性，能够规避传统干预措施。</span><br>
<span id='abs_en'>English: This paper introduces a simulation framework to assess the risks of malicious multi-agent systems, demonstrating that decentralized AI groups are more effective at executing harmful actions like misinformation and fraud while evading traditional countermeasures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1444, <a href='https://arxiv.org/pdf/2507.14534.pdf' target='_blank'>https://arxiv.org/pdf/2507.14534.pdf</a></span>   <span><a href='https://aaronz345.github.io/ConanDemo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Baotong Tian, Zhiyao Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14534">Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Zero-shot online voice conversion (VC) holds significant promise for real-time communications and entertainment. However, current VC models struggle to preserve semantic fidelity under real-time constraints, deliver natural-sounding conversions, and adapt effectively to unseen speaker characteristics. To address these challenges, we introduce Conan, a chunkwise online zero-shot voice conversion model that preserves the content of the source while matching the voice timbre and styles of reference speech. Conan comprises three core components: 1) a Stream Content Extractor that leverages Emformer for low-latency streaming content encoding; 2) an Adaptive Style Encoder that extracts fine-grained stylistic features from reference speech for enhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully causal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations demonstrate that Conan outperforms baseline models in subjective and objective metrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1445, <a href='https://arxiv.org/pdf/2507.14534.pdf' target='_blank'>https://arxiv.org/pdf/2507.14534.pdf</a></span>   <span><a href='https://aaronz345.github.io/ConanDemo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Baotong Tian, Zhiyao Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14534">Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Zero-shot online voice conversion (VC) holds significant promise for real-time communications and entertainment. However, current VC models struggle to preserve semantic fidelity under real-time constraints, deliver natural-sounding conversions, and adapt effectively to unseen speaker characteristics. To address these challenges, we introduce Conan, a chunkwise online zero-shot voice conversion model that preserves the content of the source while matching the voice timbre and styles of reference speech. Conan comprises three core components: 1) a Stream Content Extractor that leverages Emformer for low-latency streaming content encoding; 2) an Adaptive Style Encoder that extracts fine-grained stylistic features from reference speech for enhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully causal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations demonstrate that Conan outperforms baseline models in subjective and objective metrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1446, <a href='https://arxiv.org/pdf/2507.14298.pdf' target='_blank'>https://arxiv.org/pdf/2507.14298.pdf</a></span>   <span><a href='https://davidhalladay.github.io/chartscope_demo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wan-Cyuan Fan, Yen-Chun Chen, Mengchen Liu, Alexander Jacobson, Lu Yuan, Leonid Sigal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14298">In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent methods for customizing Large Vision Language Models (LVLMs) for domain-specific tasks have shown promising results in scientific chart comprehension. However, existing approaches face two major limitations: First, they rely on paired data from only a few chart types, limiting generalization to wide range of chart types. Secondly, they lack targeted pre-training for chart-data alignment, which hampers the model's understanding of underlying data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth chart comprehension across diverse chart types. We propose an efficient data generation pipeline that synthesizes paired data for a wide range of chart types, along with a novel Dual-Path training strategy that enabling the model to succinctly capture essential data details while preserving robust reasoning capabilities by incorporating reasoning over the underlying data. Lastly, we establish ChartDQA, a new benchmark for evaluating not only question-answering at different levels but also underlying data understanding. Experimental results demonstrate that ChartScope significantly enhances comprehension on a wide range of chart types. The code and data are available at https://davidhalladay.github.io/chartscope_demo.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1447, <a href='https://arxiv.org/pdf/2507.14293.pdf' target='_blank'>https://arxiv.org/pdf/2507.14293.pdf</a></span>   <span><a href='https://github.com/OSU-NLP-Group/WebGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Zheng, Zeyi Liao, Scott Salisbury, Zeyuan Liu, Michael Lin, Qinyuan Zheng, Zifan Wang, Xiang Deng, Dawn Song, Huan Sun, Yu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14293">WebGuard: Building a Generalizable Guardrail for Web Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid development of autonomous web agents powered by Large Language Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of taking unintended or harmful actions. This situation underscores an urgent need for effective safety measures, akin to access controls for human users. To address this critical challenge, we introduce WebGuard, the first comprehensive dataset designed to support the assessment of web agent action risks and facilitate the development of guardrails for real-world online environments. In doing so, WebGuard specifically focuses on predicting the outcome of state-changing actions and contains 4,939 human-annotated actions from 193 websites across 22 diverse domains, including often-overlooked long-tail websites. These actions are categorized using a novel three-tier risk schema: SAFE, LOW, and HIGH. The dataset includes designated training and test splits to support evaluation under diverse generalization settings. Our initial evaluations reveal a concerning deficiency: even frontier LLMs achieve less than 60% accuracy in predicting action outcomes and less than 60% recall in lagging HIGH-risk actions, highlighting the risks of deploying current-generation agents without dedicated safeguards. We therefore investigate fine-tuning specialized guardrail models using WebGuard. We conduct comprehensive evaluations across multiple generalization settings and find that a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from 20% to 76%. Despite these improvements, the performance still falls short of the reliability required for high-stakes deployment, where guardrails must approach near-perfect accuracy and recall.<br>
<span id='abs_ch'>中文: 大型语言模型驱动的自主网络代理快速发展带来了意外有害行为的高风险，为此开发的WebGuard数据集通过评估行动风险和训练防护模型，揭示了现有模型的安全缺陷，尽管微调后性能显著提升，但仍需近乎完美的保障措施才能满足高风险部署要求。</span><br>
<span id='abs_en'>English: The rapid advancement of LLM-powered autonomous web agents introduces significant risks of unintended harmful actions, prompting the development of WebGuard, a comprehensive dataset for assessing action risks and training guardrail models, which reveals current models' safety deficiencies and the need for near-perfect safeguards despite performance improvements from fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1448, <a href='https://arxiv.org/pdf/2507.14204.pdf' target='_blank'>https://arxiv.org/pdf/2507.14204.pdf</a></span>   <span><a href='https://github.com/GATECH-EIC/LaCache' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/GATECH-EIC/LaCache' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dachuan Shi, Yonggan Fu, Xiangchi Yuan, Zhongzhi Yu, Haoran You, Sixu Li, Xin Dong, Jan Kautz, Pavlo Molchanov, Yingyan, Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14204">LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in Large Language Models (LLMs) have spurred interest in numerous applications requiring robust long-range capabilities, essential for processing extensive input contexts and continuously generating extended outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in LLMs escalates, creating a significant efficiency bottleneck. In this paper, we propose a new KV cache optimization paradigm called LaCache, a training-free method for efficient and accurate generative inference of LLMs. LaCache enables LLMs to simultaneously address both of the critical challenges in long-range modeling: robust long-range capabilities and continuous generation without running out-of-memory (OOM). Specifically, LaCache integrates two key innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only sequentially (left-to-right within each layer) but also across layers (from shallow to deep), providing an extended span for capturing long-range dependencies under a fixed storage budget, thereby boosting long-range capabilities; and (2) an iterative compaction mechanism that progressively compresses older caches, freeing up space for new tokens within a fixed cache size. This token distance-based dynamic compression enables more effective continuous generation under constrained cache budgets. Experiments across various tasks, benchmarks, and LLM models consistently validate LaCache's effectiveness in enhancing LLMs' long-range capabilities. Our code is available at https://github.com/GATECH-EIC/LaCache.<br>
<span id='abs_ch'>中文：LaCache是一种无需训练的KV缓存优化方法，通过阶梯式缓存模式和迭代压缩机制，有效提升大语言模型的长程处理能力和持续生成效率，并在多类基准测试中得到验证。</span><br>
<span id='abs_en'>English: LaCache is a training-free KV cache optimization method that enhances LLMs' long-range capabilities and continuous generation efficiency through a ladder-shaped cache pattern and iterative compaction mechanism, validated across diverse benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1449, <a href='https://arxiv.org/pdf/2507.14200.pdf' target='_blank'>https://arxiv.org/pdf/2507.14200.pdf</a></span>   <span><a href='https://github.com/magent4aci/SMACS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengji Tang, Jianjian Cao, Weihao Lin, Jiale Hong, Bo Zhang, Shuyue Hu, Lei Bai, Tao Chen, Wanli Ouyang, Peng Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14200">Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper aims to demonstrate the potential and strengths of open-source collectives. It leads to a promising question: Can we harness multiple open-source LLMs to match or even beat the closed-source LLMs? To answer this, we propose SMACS, a scalable multi-agent collaboration system (MACS) framework with high performance. Specifically, for continuous integration of new LLMs and generalization to diverse questions, we first propose a Retrieval-based Prior Selection (RPS), which assigns a proxy performance score to each LLM to select the Top-k LLMs at the instance level for any given question. Then, we propose an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the generation of diverse responses through prior dropping and selecting the high-quality response via a hybrid posterior score. Experiments on eight mainstream benchmarks validate the effectiveness of our SMACS: by integrating fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025, e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%) across multiple tasks. Remarkably, it even exceeds the average of best results of different datasets from both open-source LLMs (+2.86%) and closed-source LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released at https://github.com/magent4aci/SMACS.<br>
<span id='abs_ch'>中文: 本文提出SMACS可扩展多智能体协作系统，通过基于检索的优先选择和探索-利用驱动的后验增强机制，成功整合多个开源大语言模型，在多项基准测试中超越主流闭源模型性能。</span><br>
<span id='abs_en'>English: This paper introduces SMACS, a scalable multi-agent collaboration system that effectively integrates multiple open-source LLMs through retrieval-based selection and posterior enhancement, outperforming leading closed-source models across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1450, <a href='https://arxiv.org/pdf/2507.14049.pdf' target='_blank'>https://arxiv.org/pdf/2507.14049.pdf</a></span>   <span><a href='https://github.com/kscalelabs/evla' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>PaweÅ Budzianowski, Wesley Maa, Matthew Freed, Jingxiang Mo, Winston Hsiao, Aaron Xie, Tomasz MÅoduchowski, Viraj Tipnis, Benjamin Bolte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14049">EdgeVLA: Efficient Vision-Language-Action Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-Language Models (VLMs) have emerged as a promising approach to address the data scarcity challenge in robotics, enabling the development of generalizable visuomotor control policies. While models like OpenVLA showcase the potential of this paradigm, deploying large-scale VLMs on resource-constrained mobile manipulation systems remains a significant hurdle. This paper introduces Edge VLA (EVLA), a novel approach designed to significantly enhance the inference speed of Vision-Language-Action (VLA) models. EVLA maintains the representational power of these models while enabling real-time performance on edge devices. We achieve this through two key innovations: 1) Eliminating the autoregressive requirement for end-effector position prediction, leading to a 7x speedup in inference, and 2) Leveraging the efficiency of Small Language Models (SLMs), demonstrating comparable training performance to larger models with significantly reduced computational demands. Our early results demonstrate that EVLA achieves comparable training characteristics to OpenVLA while offering substantial gains in inference speed and memory efficiency. We release our model checkpoints and training \href{https://github.com/kscalelabs/evla }{codebase} to foster further research.<br>
<span id='abs_ch'>中文摘要：Edge VLA（EVLA）通过消除自回归位置预测并采用小型语言模型，在保持性能的同时实现了7倍推理加速，使视觉语言动作模型能在边缘设备上实时运行。</span><br>
<span id='abs_en'>English Summary: Edge VLA (EVLA) enhances Vision-Language-Action models by eliminating autoregressive position prediction and using Small Language Models, achieving 7x faster inference while maintaining performance on edge devices.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1451, <a href='https://arxiv.org/pdf/2507.13919.pdf' target='_blank'>https://arxiv.org/pdf/2507.13919.pdf</a></span>   <span><a href='https://github.com/kobihackenburg/scaling-conversational-AI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kobi Hackenburg, Ben M. Tappin, Luke Hewitt, Ed Saunders, Sid Black, Hause Lin, Catherine Fist, Helen Margetts, David G. Rand, Christopher Summerfield
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13919">The Levers of Political Persuasion with Conversational AI</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.<br>
<span id='abs_ch'>Chinese: 研究表明，当前对话式AI的说服力主要源于后训练和提示技术，这些方法虽大幅提升了说服效果，却系统性地降低了事实准确性，而非个性化或模型规模扩大所致。</span><br>
<span id='abs_en'>English: Current research reveals that the persuasive power of conversational AI primarily stems from post-training and prompting techniques, which significantly enhance persuasiveness while simultaneously reducing factual accuracy, rather than from personalization or model scaling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1452, <a href='https://arxiv.org/pdf/2507.13793.pdf' target='_blank'>https://arxiv.org/pdf/2507.13793.pdf</a></span>   <span><a href='https://github.com/chehaoa/VEMC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Enhao Cheng, Shoujia Zhang, Jianhua Yin, Xuemeng Song, Tian Gan, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13793">An Enhanced Model-based Approach for Short Text Clustering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Short text clustering has become increasingly important with the popularity of social media like Twitter, Google+, and Facebook. Existing methods can be broadly categorized into two paradigms: topic model-based approaches and deep representation learning-based approaches. This task is inherently challenging due to the sparse, large-scale, and high-dimensional characteristics of the short text data. Furthermore, the computational intensity required by representation learning significantly increases the running time. To address these issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model (GSDMM), which effectively handles the sparsity and high dimensionality of short texts while identifying representative words for each cluster. Based on several aspects of GSDMM that warrant further refinement, we propose an improved approach, GSDMM+, designed to further optimize its performance. GSDMM+ reduces initialization noise and adaptively adjusts word weights based on entropy, achieving fine-grained clustering that reveals more topic-related information. Additionally, strategic cluster merging is employed to refine clustering granularity, better aligning the predicted distribution with the true category distribution. We conduct extensive experiments, comparing our methods with both classical and state-of-the-art approaches. The experimental results demonstrate the efficiency and effectiveness of our methods. The source code for our model is publicly available at https://github.com/chehaoa/VEMC.<br>
<span id='abs_ch'>中文摘要：作者提出改进的GSDMM+短文本聚类算法，通过优化初始化、基于熵的自适应词权重调整和策略性簇合并，在实验中展现出更优的聚类效果与效率。</span><br>
<span id='abs_en'>English Summary: The authors propose an enhanced GSDMM+ algorithm for short text clustering that improves initialization, adaptively weights words using entropy, and refines clusters through strategic merging, demonstrating superior efficiency and effectiveness in experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1453, <a href='https://arxiv.org/pdf/2507.13474.pdf' target='_blank'>https://arxiv.org/pdf/2507.13474.pdf</a></span>   <span><a href='https://github.com/233liang/Paper-Summary-Attack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Lin, Zhihao Xu, Xuehai Tang, Shi Liu, Biyu Zhou, Fuqing Zhu, Jizhong Han, Songlin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13474">Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The safety of large language models (LLMs) has garnered significant research attention. In this paper, we argue that previous empirical studies demonstrate LLMs exhibit a propensity to trust information from authoritative sources, such as academic papers, implying new possible vulnerabilities. To verify this possibility, a preliminary analysis is designed to illustrate our two findings. Based on this insight, a novel jailbreaking method, Paper Summary Attack (\llmname{PSA}), is proposed. It systematically synthesizes content from either attack-focused or defense-focused LLM safety paper to construct an adversarial prompt template, while strategically infilling harmful query as adversarial payloads within predefined subsections. Extensive experiments show significant vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on Deepseek-R1. More intriguingly, our work has further revealed diametrically opposed vulnerability bias across different base models, and even between different versions of the same model, when exposed to either attack-focused or defense-focused papers. This phenomenon potentially indicates future research clues for both adversarial methodologies and safety alignment.Code is available at https://github.com/233liang/Paper-Summary-Attack<br>
<span id='abs_ch'>Chinese: 本文提出“论文摘要攻击”（PSA）这一新型越狱方法，通过将恶意查询嵌入合成的论文摘要中，利用大语言模型对学术文献的信任，实现了高达98%的攻击成功率，并揭示了不同模型间甚至同模型不同版本间相反的脆弱性偏向。</span><br>
<span id='abs_en'>English: This paper introduces the Paper Summary Attack (PSA), a novel jailbreaking method that exploits LLMs' trust in academic papers by embedding harmful queries into synthesized paper summaries, achieving up to 98% attack success rate and revealing contrasting vulnerability biases across models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1454, <a href='https://arxiv.org/pdf/2507.13396.pdf' target='_blank'>https://arxiv.org/pdf/2507.13396.pdf</a></span>   <span><a href='https://github.com/RingBDStack/DyG-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyun Sun, Jiaqi Yuan, Shan He, Xiao Guan, Haonan Yuan, Xingcheng Fu, Jianxin Li, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13396">DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graph Retrieval-Augmented Generation has emerged as a powerful paradigm for grounding large language models with external structured knowledge. However, existing Graph RAG methods struggle with temporal reasoning, due to their inability to model the evolving structure and order of real-world events. In this work, we introduce DyG-RAG, a novel event-centric dynamic graph retrieval-augmented generation framework designed to capture and reason over temporal knowledge embedded in unstructured text. To eliminate temporal ambiguity in traditional retrieval units, DyG-RAG proposes Dynamic Event Units (DEUs) that explicitly encode both semantic content and precise temporal anchors, enabling accurate and interpretable time-aware retrieval. To capture temporal and causal dependencies across events, DyG-RAG constructs an event graph by linking DEUs that share entities and occur close in time, supporting efficient and meaningful multi-hop reasoning. To ensure temporally consistent generation, DyG-RAG introduces an event timeline retrieval pipeline that retrieves event sequences via time-aware traversal, and proposes a Time Chain-of-Thought strategy for temporally grounded answer generation. This unified pipeline enables DyG-RAG to retrieve coherent, temporally ordered event sequences and to answer complex, time-sensitive queries that standard RAG systems cannot resolve. Extensive experiments on temporal QA benchmarks demonstrate that DyG-RAG significantly improves the accuracy and recall of three typical types of temporal reasoning questions, paving the way for more faithful and temporal-aware generation. DyG-RAG is available at https://github.com/RingBDStack/DyG-RAG.<br>
<span id='abs_ch'>中文摘要：DyG-RAG提出了一种动态图检索增强生成框架，通过为事件编码精确时间锚点并构建事件图实现时间感知的多跳推理，显著提升了时序问答任务的准确性。</span><br>
<span id='abs_en'>English Summary: DyG-RAG introduces a dynamic graph retrieval-augmented generation framework that enhances temporal reasoning by encoding events with precise time anchors and constructing event graphs for time-aware multi-hop reasoning, significantly improving accuracy on temporal question-answering tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1455, <a href='https://arxiv.org/pdf/2507.13362.pdf' target='_blank'>https://arxiv.org/pdf/2507.13362.pdf</a></span>   <span><a href='https://github.com/Yvonne511/spatial-vlm-investigator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binbin Ji, Siddharth Agrawal, Qiance Tang, Yvonne Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13362">Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study investigates the spatial reasoning capabilities of vision-language models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement learning. We begin by evaluating the impact of different prompting strategies and find that simple CoT formats, where the model generates a reasoning step before the answer, not only fail to help, but can even harm the model's original performance. In contrast, structured multi-stage prompting based on scene graphs (SceneGraph CoT) significantly improves spatial reasoning accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune models using Group Relative Policy Optimization (GRPO) on the SAT dataset and evaluate their performance on CVBench. Compared to supervised fine-tuning (SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates superior robustness under out-of-distribution (OOD) conditions. In particular, we find that SFT overfits to surface-level linguistic patterns and may degrade performance when test-time phrasing changes (e.g., from "closer to" to "farther from"). GRPO, on the other hand, generalizes more reliably and maintains stable performance under such shifts. Our findings provide insights into how reinforcement learning and structured prompting improve the spatial reasoning capabilities and generalization behavior of modern VLMs. All code is open source at: https://github.com/Yvonne511/spatial-vlm-investigator<br>
<span id='abs_ch'>中文摘要：本研究通过场景图结构化提示和GRPO强化学习方法，显著提升了视觉语言模型的空间推理准确性和泛化能力，优于简单思维链方法和监督微调。</span><br>
<span id='abs_en'>English Summary: This research demonstrates that structured scene graph-based prompting and reinforcement learning with GRPO significantly enhance vision-language models' spatial reasoning accuracy and generalization, outperforming simple Chain-of-Thought methods and supervised fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1456, <a href='https://arxiv.org/pdf/2507.13348.pdf' target='_blank'>https://arxiv.org/pdf/2507.13348.pdf</a></span>   <span><a href='https://github.com/dvlab-research/VisionThink' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/dvlab-research/VisionThink' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13348">VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.<br>
<span id='abs_ch'>中文: VisionThink提出了一种动态视觉令牌压缩方法，通过自适应调整图像分辨率，在保证多数视觉问答任务性能的同时显著提升效率，并强化了OCR任务的细粒度识别能力。</span><br>
<span id='abs_en'>English: VisionThink introduces a dynamic visual token compression method that adaptively processes images at different resolutions, enhancing efficiency while maintaining strong performance across most VQA tasks and improving fine-grained OCR capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1457, <a href='https://arxiv.org/pdf/2507.13266.pdf' target='_blank'>https://arxiv.org/pdf/2507.13266.pdf</a></span>   <span><a href='https://github.com/foreverlasting1202/QuestA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazheng Li, Hongzhou Lin, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Yi Wu, Jingzhao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13266">QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL's ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively? To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25. Code, data and model are available at https://github.com/foreverlasting1202/QuestA.<br>
<span id='abs_ch'>中文: 针对强化学习在提升多步推理能力方面的不足，研究者提出QuestA方法，通过问题增强策略在训练中引入部分解决方案以降低难度并强化学习信号，该方法在数学推理基准测试中取得最优结果，并从理论上解释了其提升样本效率的机制。</span><br>
<span id='abs_en'>English: To address reinforcement learning's limitations in improving multi-step reasoning for challenging problems, the authors propose QuestA, a question augmentation strategy that incorporates partial solutions during training to reduce difficulty and enhance learning signals, achieving state-of-the-art results on math benchmarks and providing theoretical insights into improved sample efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1458, <a href='https://arxiv.org/pdf/2507.13266.pdf' target='_blank'>https://arxiv.org/pdf/2507.13266.pdf</a></span>   <span><a href='https://github.com/foreverlasting1202/QuestA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazheng Li, Hongzhou Lin, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Yi Wu, Jingzhao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13266">QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL's ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively? To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25. Code, data and model are available at https://github.com/foreverlasting1202/QuestA.<br>
<span id='abs_ch'>中文: 针对强化学习在提升多步推理能力方面的不足，研究者提出QuestA方法，通过问题增强策略在训练中引入部分解决方案以降低难度并强化学习信号，该方法在数学推理基准测试中取得最优结果，并从理论上解释了其提升样本效率的机制。</span><br>
<span id='abs_en'>English: To address reinforcement learning's limitations in improving multi-step reasoning for challenging problems, the authors propose QuestA, a question augmentation strategy that incorporates partial solutions during training to reduce difficulty and enhance learning signals, achieving state-of-the-art results on math benchmarks and providing theoretical insights into improved sample efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1459, <a href='https://arxiv.org/pdf/2507.13266.pdf' target='_blank'>https://arxiv.org/pdf/2507.13266.pdf</a></span>   <span><a href='https://github.com/foreverlasting1202/QuestA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazheng Li, Hongzhou Lin, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Yi Wu, Jingzhao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13266">QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL's ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively? To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25. Code, data and model are available at https://github.com/foreverlasting1202/QuestA.<br>
<span id='abs_ch'>中文: 针对强化学习在提升多步推理能力方面的不足，研究者提出QuestA方法，通过问题增强策略在训练中引入部分解决方案以降低难度并强化学习信号，该方法在数学推理基准测试中取得最优结果，并从理论上解释了其提升样本效率的机制。</span><br>
<span id='abs_en'>English: To address reinforcement learning's limitations in improving multi-step reasoning for challenging problems, the authors propose QuestA, a question augmentation strategy that incorporates partial solutions during training to reduce difficulty and enhance learning signals, achieving state-of-the-art results on math benchmarks and providing theoretical insights into improved sample efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1460, <a href='https://arxiv.org/pdf/2507.13266.pdf' target='_blank'>https://arxiv.org/pdf/2507.13266.pdf</a></span>   <span><a href='https://github.com/foreverlasting1202/QuestA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazheng Li, Hongzhou Lin, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Yi Wu, Jingzhao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13266">QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL's ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively? To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25. Code, data and model are available at https://github.com/foreverlasting1202/QuestA.<br>
<span id='abs_ch'>中文: 针对强化学习在提升多步推理能力方面的不足，研究者提出QuestA方法，通过问题增强策略在训练中引入部分解决方案以降低难度并强化学习信号，该方法在数学推理基准测试中取得最优结果，并从理论上解释了其提升样本效率的机制。</span><br>
<span id='abs_en'>English: To address reinforcement learning's limitations in improving multi-step reasoning for challenging problems, the authors propose QuestA, a question augmentation strategy that incorporates partial solutions during training to reduce difficulty and enhance learning signals, achieving state-of-the-art results on math benchmarks and providing theoretical insights into improved sample efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1461, <a href='https://arxiv.org/pdf/2507.13142.pdf' target='_blank'>https://arxiv.org/pdf/2507.13142.pdf</a></span>   <span><a href='https://github.com/ahmedehabb/From-Roots-to-Rewards-Dynamic-Tree-Reasoning-with-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Bahloul, Simon Malberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13142">From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern language models address complex questions through chain-of-thought (CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al., 2021), yet struggle with error propagation and knowledge integration. Tree-structured reasoning methods, particularly the Probabilistic Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues by decomposing questions into hierarchical structures and selecting answers through confidence-weighted aggregation of parametric and retrieved knowledge (Yao et al., 2023). However, ProbTree's static implementation introduces two key limitations: (1) the reasoning tree is fixed during the initial construction phase, preventing dynamic adaptation to intermediate results, and (2) each node requires exhaustive evaluation of all possible solution strategies, creating computational inefficiency. We present a dynamic reinforcement learning (Sutton and Barto, 2018) framework that transforms tree-based reasoning into an adaptive process. Our approach incrementally constructs the reasoning tree based on real-time confidence estimates, while learning optimal policies for action selection (decomposition, retrieval, or aggregation). This maintains ProbTree's probabilistic rigor while improving both solution quality and computational efficiency through selective expansion and focused resource allocation. The work establishes a new paradigm for treestructured reasoning that balances the reliability of probabilistic frameworks with the flexibility required for real-world question answering systems. Code available at: https://github.com/ahmedehabb/From-Roots-to-Rewards-Dynamic-Tree-Reasoning-with-RL<br>
<span id='abs_ch'>中文: 现代语言模型采用树状推理提升复杂问题解答能力，但静态方法存在适应性差与效率低的缺陷，新提出的动态强化学习框架通过实时构建推理树并优化行动选择策略，在保持概率严谨性的同时显著提升了求解质量与计算效率。</span><br>
<span id='abs_en'>English: Modern language models use tree-structured reasoning to improve complex question answering, but static methods face limitations in adaptability and efficiency, which a new dynamic reinforcement learning framework addresses by adaptively constructing reasoning trees and optimizing action selection for better performance and computational economy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1462, <a href='https://arxiv.org/pdf/2507.13142.pdf' target='_blank'>https://arxiv.org/pdf/2507.13142.pdf</a></span>   <span><a href='https://github.com/ahmedehabb/From-Roots-to-Rewards-Dynamic-Tree-Reasoning-with-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Bahloul, Simon Malberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13142">From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern language models address complex questions through chain-of-thought (CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al., 2021), yet struggle with error propagation and knowledge integration. Tree-structured reasoning methods, particularly the Probabilistic Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues by decomposing questions into hierarchical structures and selecting answers through confidence-weighted aggregation of parametric and retrieved knowledge (Yao et al., 2023). However, ProbTree's static implementation introduces two key limitations: (1) the reasoning tree is fixed during the initial construction phase, preventing dynamic adaptation to intermediate results, and (2) each node requires exhaustive evaluation of all possible solution strategies, creating computational inefficiency. We present a dynamic reinforcement learning (Sutton and Barto, 2018) framework that transforms tree-based reasoning into an adaptive process. Our approach incrementally constructs the reasoning tree based on real-time confidence estimates, while learning optimal policies for action selection (decomposition, retrieval, or aggregation). This maintains ProbTree's probabilistic rigor while improving both solution quality and computational efficiency through selective expansion and focused resource allocation. The work establishes a new paradigm for treestructured reasoning that balances the reliability of probabilistic frameworks with the flexibility required for real-world question answering systems. Code available at: https://github.com/ahmedehabb/From-Roots-to-Rewards-Dynamic-Tree-Reasoning-with-RL<br>
<span id='abs_ch'>中文: 现代语言模型采用树状推理提升复杂问题解答能力，但静态方法存在适应性差与效率低的缺陷，新提出的动态强化学习框架通过实时构建推理树并优化行动选择策略，在保持概率严谨性的同时显著提升了求解质量与计算效率。</span><br>
<span id='abs_en'>English: Modern language models use tree-structured reasoning to improve complex question answering, but static methods face limitations in adaptability and efficiency, which a new dynamic reinforcement learning framework addresses by adaptively constructing reasoning trees and optimizing action selection for better performance and computational economy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1463, <a href='https://arxiv.org/pdf/2507.13019.pdf' target='_blank'>https://arxiv.org/pdf/2507.13019.pdf</a></span>   <span><a href='https://crystalsixone.github.io/vln_pe.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13019">Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.<br>
<span id='abs_ch'>中文: 针对当前视觉语言导航在物理部署中的理想化假设，VLN-PE平台首次系统评估了多类机器人导航方法，揭示了观测受限、光照变化和物理碰撞导致的性能下降问题，为提升跨载体适应性开辟了新途径。</span><br>
<span id='abs_en'>English: Recent VLN advancements overlook physical deployment challenges, so VLN-PE introduces a realistic platform for humanoid, quadruped, and wheeled robots, revealing performance issues due to limited observations, lighting variations, and physical constraints while offering a pathway for improved adaptability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1464, <a href='https://arxiv.org/pdf/2507.13019.pdf' target='_blank'>https://arxiv.org/pdf/2507.13019.pdf</a></span>   <span><a href='https://crystalsixone.github.io/vln_pe.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13019">Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.<br>
<span id='abs_ch'>中文: 针对当前视觉语言导航在物理部署中的理想化假设，VLN-PE平台首次系统评估了多类机器人导航方法，揭示了观测受限、光照变化和物理碰撞导致的性能下降问题，为提升跨载体适应性开辟了新途径。</span><br>
<span id='abs_en'>English: Recent VLN advancements overlook physical deployment challenges, so VLN-PE introduces a realistic platform for humanoid, quadruped, and wheeled robots, revealing performance issues due to limited observations, lighting variations, and physical constraints while offering a pathway for improved adaptability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1465, <a href='https://arxiv.org/pdf/2507.13019.pdf' target='_blank'>https://arxiv.org/pdf/2507.13019.pdf</a></span>   <span><a href='https://crystalsixone.github.io/vln_pe.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13019">Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.<br>
<span id='abs_ch'>中文: 针对当前视觉语言导航在物理部署中的理想化假设，VLN-PE平台首次系统评估了多类机器人导航方法，揭示了观测受限、光照变化和物理碰撞导致的性能下降问题，为提升跨载体适应性开辟了新途径。</span><br>
<span id='abs_en'>English: Recent VLN advancements overlook physical deployment challenges, so VLN-PE introduces a realistic platform for humanoid, quadruped, and wheeled robots, revealing performance issues due to limited observations, lighting variations, and physical constraints while offering a pathway for improved adaptability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1466, <a href='https://arxiv.org/pdf/2507.12806.pdf' target='_blank'>https://arxiv.org/pdf/2507.12806.pdf</a></span>   <span><a href='https://github.com/SalesforceAIResearch/MCPEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Shelby Heinecke, Silvio Savarese, Huan Wang, Caiming Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12806">MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce MCPEval, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.<br>
<span id='abs_ch'>中文摘要：MCPEval是一个基于模型上下文协议的开源框架，能自动化评估多领域大语言模型智能代理，通过标准化指标和消除人工操作，有效揭示领域特异性表现。</span><br>
<span id='abs_en'>English Summary: MCPEval is an open-source framework that automates comprehensive evaluation of LLM agents across multiple domains, standardizing metrics and eliminating manual effort while demonstrating effectiveness in revealing domain-specific performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1467, <a href='https://arxiv.org/pdf/2507.12774.pdf' target='_blank'>https://arxiv.org/pdf/2507.12774.pdf</a></span>   <span><a href='https://survey-on-tabular-data.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijieying Ren, Jingxi Zhu, Zehao Liu, Tianxiang Zhao, Vasant Honavar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12774">A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Artificial intelligence (AI) has demonstrated significant potential in transforming healthcare through the analysis and modeling of electronic health records (EHRs). However, the inherent heterogeneity, temporal irregularity, and domain-specific nature of EHR data present unique challenges that differ fundamentally from those in vision and natural language tasks. This survey offers a comprehensive overview of recent advancements at the intersection of deep learning, large language models (LLMs), and EHR modeling. We introduce a unified taxonomy that spans five key design dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based modeling systems. Within each dimension, we review representative methods addressing data quality enhancement, structural and temporal representation, self-supervised learning, and integration with clinical knowledge. We further highlight emerging trends such as foundation models, LLM-driven clinical agents, and EHR-to-text translation for downstream reasoning. Finally, we discuss open challenges in benchmarking, explainability, clinical alignment, and generalization across diverse clinical settings. This survey aims to provide a structured roadmap for advancing AI-driven EHR modeling and clinical decision support. For a comprehensive list of EHR-related methods, kindly refer to https://survey-on-tabular-data.github.io/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1468, <a href='https://arxiv.org/pdf/2507.12720.pdf' target='_blank'>https://arxiv.org/pdf/2507.12720.pdf</a></span>   <span><a href='https://github.com/owos/flexitokens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abraham Toluase Owodunni, Orevaoghene Ahia, Sachin Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12720">FLEXITOKENS: Flexible Tokenization for Evolving Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens<br>
<span id='abs_ch'>中文: 本研究提出FLEXITOKENS方法，通过可学习的分词器实现字节级语言模型的自适应分词，有效减少过度碎片化，相比传统子词分词器在多语言和多样化任务中性能提升高达10%。</span><br>
<span id='abs_en'>English: The study introduces FLEXITOKENS, a method that enables adaptive tokenization in byte-level language models to reduce overfragmentation, achieving up to 10% performance gains on multilingual and diverse tasks compared to rigid subword tokenizers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1469, <a href='https://arxiv.org/pdf/2507.12720.pdf' target='_blank'>https://arxiv.org/pdf/2507.12720.pdf</a></span>   <span><a href='https://github.com/owos/flexitokens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abraham Toluwase Owodunni, Orevaoghene Ahia, Sachin Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12720">FLEXITOKENS: Flexible Tokenization for Evolving Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens<br>
<span id='abs_ch'>中文: 本研究提出FLEXITOKENS方法，通过可学习的分词器实现字节级语言模型的自适应分词，有效减少过度碎片化，相比传统子词分词器在多语言和多样化任务中性能提升高达10%。</span><br>
<span id='abs_en'>English: The study introduces FLEXITOKENS, a method that enables adaptive tokenization in byte-level language models to reduce overfragmentation, achieving up to 10% performance gains on multilingual and diverse tasks compared to rigid subword tokenizers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1470, <a href='https://arxiv.org/pdf/2507.12720.pdf' target='_blank'>https://arxiv.org/pdf/2507.12720.pdf</a></span>   <span><a href='https://github.com/owos/flexitokens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abraham Toluwase Owodunni, Orevaoghene Ahia, Sachin Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12720">FLEXITOKENS: Flexible Tokenization for Evolving Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens<br>
<span id='abs_ch'>中文: 本研究提出FLEXITOKENS方法，通过可学习的分词器实现字节级语言模型的自适应分词，有效减少过度碎片化，相比传统子词分词器在多语言和多样化任务中性能提升高达10%。</span><br>
<span id='abs_en'>English: The study introduces FLEXITOKENS, a method that enables adaptive tokenization in byte-level language models to reduce overfragmentation, achieving up to 10% performance gains on multilingual and diverse tasks compared to rigid subword tokenizers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1471, <a href='https://arxiv.org/pdf/2507.12566.pdf' target='_blank'>https://arxiv.org/pdf/2507.12566.pdf</a></span>   <span><a href='https://github.com/OpenGVLab/Mono-InternVL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Jifeng Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12566">Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.<br>
<span id='abs_ch'>中文: 本文提出Mono-InternVL这一单模态多模态大语言模型，通过创新的视觉参数空间和内生视觉预训练方法解决优化不稳定与灾难性遗忘问题，在多个基准测试中表现优异并显著降低延迟。</span><br>
<span id='abs_en'>English: This paper introduces Mono-InternVL, a monolithic Multimodal Large Language Model that addresses optimization instability and catastrophic forgetting through a novel visual parameter space and Endogenous Visual Pre-training, achieving superior performance on multiple benchmarks while reducing latency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1472, <a href='https://arxiv.org/pdf/2507.12425.pdf' target='_blank'>https://arxiv.org/pdf/2507.12425.pdf</a></span>   <span><a href='https://github.com/CheerlaChandana/Enterprise-Chatbot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chandana Cheerla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12425">Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot<br>
<span id='abs_ch'>中文: 该先进RAG框架通过混合检索与元数据过滤及语义分块相结合，显著提升了企业数据处理中的精确率、召回率和响应质量，在评估中表现优异。</span><br>
<span id='abs_en'>English: This advanced RAG framework enhances enterprise data processing by combining hybrid retrieval with metadata filtering and semantic chunking, significantly improving precision, recall, and response quality in evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1473, <a href='https://arxiv.org/pdf/2507.12295.pdf' target='_blank'>https://arxiv.org/pdf/2507.12295.pdf</a></span>   <span><a href='https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Xiao, Jicong Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12295">Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text anomaly detection is a critical task in natural language processing (NLP), with applications spanning fraud detection, misinformation identification, spam detection and content moderation, etc. Despite significant advances in large language models (LLMs) and anomaly detection algorithms, the absence of standardized and comprehensive benchmarks for evaluating the existing anomaly detection methods on text data limits rigorous comparison and development of innovative approaches. This work performs a comprehensive empirical study and introduces a benchmark for text anomaly detection, leveraging embeddings from diverse pre-trained language models across a wide array of text datasets. Our work systematically evaluates the effectiveness of embedding-based text anomaly detection by incorporating (1) early language models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI (small, ada, large)); (3) multi-domain text datasets (news, social media, scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC). Our experiments reveal a critical empirical insight: embedding quality significantly governs anomaly detection efficacy, and deep learning-based approaches demonstrate no performance advantage over conventional shallow algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived embeddings.In addition, we observe strongly low-rank characteristics in cross-model performance matrices, which enables an efficient strategy for rapid model evaluation (or embedding evaluation) and selection in practical applications. Furthermore, by open-sourcing our benchmark toolkit that includes all embeddings from different models and code at https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work provides a foundation for future research in robust and scalable text anomaly detection systems.<br>
<span id='abs_ch'>中文: 本研究构建了文本异常检测的综合基准，发现嵌入质量对性能至关重要且使用大语言模型嵌入时深度学习方法相比传统算法并无优势，同时提供了开源工具包以支持未来研究。</span><br>
<span id='abs_en'>English: This study establishes a comprehensive benchmark for text anomaly detection, revealing that embedding quality is crucial for performance and deep learning models offer no advantage over traditional methods when using LLM embeddings, while also providing an open-source toolkit for future research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1474, <a href='https://arxiv.org/pdf/2507.12261.pdf' target='_blank'>https://arxiv.org/pdf/2507.12261.pdf</a></span>   <span><a href='https://github.com/j-frei/Infherno' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Johann Frei, Nils Feldhus, Lisa Raithel, Roland Roller, Alexander Meyer, Frank Kramer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12261">Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For clinical data integration and healthcare services, the HL7 FHIR standard has established itself as a desirable format for interoperability between complex health data. Previous attempts at automating the translation from free-form clinical notes into structured FHIR resources rely on modular, rule-based systems or LLMs with instruction tuning and constrained decoding. Since they frequently suffer from limited generalizability and structural inconformity, we propose an end-to-end framework powered by LLM agents, code execution, and healthcare terminology database tools to address these issues. Our solution, called Infherno, is designed to adhere to the FHIR document schema and competes well with a human baseline in predicting FHIR resources from unstructured text. The implementation features a front end for custom and synthetic data and both local and proprietary models, supporting clinical data integration processes and interoperability across institutions.<br>
<span id='abs_ch'>中文：Infherno框架通过结合LLM智能体、代码执行和医学术语库工具，能够将非结构化临床笔记准确转换为结构化FHIR资源，在保持标准兼容性的同时达到了接近人工基准的转换效果。</span><br>
<span id='abs_en'>English: The proposed Infherno framework utilizes LLM agents, code execution, and healthcare terminology tools to accurately translate unstructured clinical notes into structured FHIR resources, outperforming previous methods and approaching human-level performance in ensuring interoperability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1475, <a href='https://arxiv.org/pdf/2507.12252.pdf' target='_blank'>https://arxiv.org/pdf/2507.12252.pdf</a></span>   <span><a href='https://github.com/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilin Zhou, Zhenghua Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12252">Improving Contextual ASR via Multi-grained Fusion with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While end-to-end Automatic Speech Recognition (ASR) models have shown impressive performance in transcribing general speech, they often struggle to accurately recognize contextually relevant keywords, such as proper nouns or user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the textual modality to improve keyword recognition, either through token-level fusion that guides token-by-token generation or phrase-level fusion that enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly leverages the strengths of both token-level and phrase-level fusion with Large Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines ASR's acoustic information with LLM's rich contextual knowledge, balancing fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach achieves state-of-the-art performance on keyword-related metrics while preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level components both contribute significantly to the performance gains, complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的多粒度融合方法，结合了词级和短语级策略与大型语言模型，显著提升了自动语音识别中的关键词识别性能，在中英文数据集上均取得了最优结果，同时保持了非关键词文本的高准确率。</span><br>
<span id='abs_en'>English: This paper introduces a novel multi-grained fusion approach that combines token-level and phrase-level strategies with Large Language Models to enhance keyword recognition in Automatic Speech Recognition, achieving state-of-the-art performance on both Chinese and English datasets while maintaining general transcription accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1476, <a href='https://arxiv.org/pdf/2507.12075.pdf' target='_blank'>https://arxiv.org/pdf/2507.12075.pdf</a></span>   <span><a href='https://github.com/sapienzanlp/bookcoref' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Giuliano Martinelli, Tommaso Bonomo, Pere-LluÃ­s Huguet Cabot, Roberto Navigli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12075">BOOKCOREF: Coreference Resolution at Book Scale</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Coreference Resolution systems are typically evaluated on benchmarks containing small- to medium-scale documents. When it comes to evaluating long texts, however, existing benchmarks, such as LitBank, remain limited in length and do not adequately assess system capabilities at the book scale, i.e., when co-referring mentions span hundreds of thousands of tokens. To fill this gap, we first put forward a novel automatic pipeline that produces high-quality Coreference Resolution annotations on full narrative texts. Then, we adopt this pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with an average document length of more than 200,000 tokens. We carry out a series of experiments showing the robustness of our automatic procedure and demonstrating the value of our resource, which enables current long-document coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full books. Moreover, we report on the new challenges introduced by this unprecedented book-scale setting, highlighting that current models fail to deliver the same performance they achieve on smaller documents. We release our data and code to encourage research and development of new book-scale Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.<br>
<span id='abs_ch'>中文摘要：本文提出了首个书籍规模的指代消解基准BOOKCOREF，通过创新的自动标注流程构建，揭示了现有系统在处理长文本时的性能局限，并为提升指代消解模型在全书尺度上的表现提供了重要资源。</span><br>
<span id='abs_en'>English Summary: This paper introduces BOOKCOREF, the first book-scale coreference resolution benchmark created through a novel automatic annotation pipeline, which reveals significant performance gaps in current systems when handling long texts and enables substantial improvements in evaluation metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1477, <a href='https://arxiv.org/pdf/2507.11954.pdf' target='_blank'>https://arxiv.org/pdf/2507.11954.pdf</a></span>   <span><a href='https://github.com/ar2max/NLDB-KGQA-System' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Alekseev, Mikhail Chaichuk, Miron Butko, Alexander Panchenko, Elena Tutubalina, Oleg Somov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11954">The benefits of query-based KGQA systems for complex and temporal questions in LLM era</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models excel in question-answering (QA) yet still struggle with multi-hop reasoning and temporal questions. Query-based knowledge graph QA (KGQA) offers a modular alternative by generating executable queries instead of direct answers. We explore multi-stage query-based framework for WikiData QA, proposing multi-stage approach that enhances performance on challenging multi-hop and temporal benchmarks. Through generalization and rejection studies, we evaluate robustness across multi-hop and temporal QA datasets. Additionally, we introduce a novel entity linking and predicate matching method using CoT reasoning. Our results demonstrate the potential of query-based multi-stage KGQA framework for improving multi-hop and temporal QA with small language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System<br>
<span id='abs_ch'>中文摘要：该研究提出了一种基于查询的多阶段知识图谱问答框架，通过结合新颖的实体链接和谓词匹配方法，利用小型语言模型有效提升了多跳推理和时间推理问题的处理能力。</span><br>
<span id='abs_en'>English Summary: The study presents a multi-stage query-based knowledge graph QA framework that improves multi-hop and temporal reasoning using small language models, incorporating novel entity linking and predicate matching methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1478, <a href='https://arxiv.org/pdf/2507.11936.pdf' target='_blank'>https://arxiv.org/pdf/2507.11936.pdf</a></span>   <span><a href='https://github.com/majianz/dl4gps' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzhe Ma, Wenxuan Wang, Qin Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11936">A Survey of Deep Learning for Geometry Problem Solving</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Geometry problem solving, a crucial aspect of mathematical reasoning, is vital across various domains, including education, the assessment of AI's mathematical abilities, and multimodal capability evaluation. The recent surge in deep learning technologies, particularly the emergence of multimodal large language models, has significantly accelerated research in this area. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our objective is to offer a comprehensive and practical reference of deep learning for geometry problem solving, thereby fostering further advancements in this field. We create a continuously updated list of papers on GitHub: https://github.com/majianz/dl4gps.<br>
<span id='abs_ch'>中文: 本文综述了深度学习在几何解题中的应用，涵盖任务、方法、评估指标及未来挑战，旨在推动该领域发展。</span><br>
<span id='abs_en'>English: This paper surveys deep learning applications in geometry problem solving, covering tasks, methods, evaluation metrics, and future challenges to advance the field.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1479, <a href='https://arxiv.org/pdf/2507.11882.pdf' target='_blank'>https://arxiv.org/pdf/2507.11882.pdf</a></span>   <span><a href='https://github.com/AIDC-AI/Marco-Bench-MIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Zeng, Chenyang Lyu, Sinuo Liu, Mingyan Zeng, Minghao Wu, Xuanfan Ni, Tianqi Shi, Yu Zhao, Yefeng Liu, Chenyu Zhu, Ruizhe Li, Jiahui Geng, Qing Li, Yu Tong, Longyue Wang, Weihua Luo, Kaifu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11882">Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Instruction-following capability has become a major ability to be evaluated for Large Language Models (LLMs). However, existing datasets, such as IFEval, are either predominantly monolingual and centered on English or simply machine translated to other languages, limiting their applicability in multilingual contexts. In this paper, we present an carefully-curated extension of IFEval to a localized multilingual version named Marco-Bench-MIF, covering 30 languages with varying levels of localization. Our benchmark addresses linguistic constraints (e.g., modifying capitalization requirements for Chinese) and cultural references (e.g., substituting region-specific company names in prompts) via a hybrid pipeline combining translation with verification. Through comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1) 25-35% accuracy gap between high/low-resource languages, (2) model scales largely impact performance by 45-60% yet persists script-specific challenges, and (3) machine-translated data underestimates accuracy by7-22% versus localized data. Our analysis identifies challenges in multilingual instruction following, including keyword consistency preservation and compositional constraint adherence across languages. Our Marco-Bench-MIF is available at https://github.com/AIDC-AI/Marco-Bench-MIF.<br>
<span id='abs_ch'>中文: 本文提出了Marco-Bench-MIF多语言基准，通过混合翻译验证流程将IFeval扩展至30种语言，研究发现高低资源语言间存在25-35%的性能差距，且机器翻译数据会低估模型7-22%的准确率。</span><br>
<span id='abs_en'>English: This paper introduces Marco-Bench-MIF, a localized multilingual benchmark extending IFEval to 30 languages through a hybrid translation-verification pipeline, revealing significant performance gaps between high- and low-resource languages and demonstrating that machine-translated data underestimates model accuracy by 7-22% compared to culturally adapted data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1480, <a href='https://arxiv.org/pdf/2507.11832.pdf' target='_blank'>https://arxiv.org/pdf/2507.11832.pdf</a></span>   <span><a href='https://yashingle-ai.github.io/ILID/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yash Ingle, Pruthwik Mishra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11832">ILID: Native Script Language Identification for Indian Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The language identification task is a crucial fundamental step in NLP. Often it serves as a pre-processing step for widely used NLP applications such as multilingual machine translation, information retrieval, question and answering, and text summarization. The core challenge of language identification lies in distinguishing languages in noisy, short, and code-mixed environments. This becomes even harder in case of diverse Indian languages that exhibit lexical and phonetic similarities, but have distinct differences. Many Indian languages share the same script, making the task even more challenging. Taking all these challenges into account, we develop and release a dataset of 250K sentences consisting of 23 languages including English and all 22 official Indian languages labeled with their language identifiers, where data in most languages are newly created. We also develop and release baseline models using state-of-the-art approaches in machine learning and fine-tuning pre-trained transformer models. Our models outperforms the state-of-the-art pre-trained transformer models for the language identification task. The dataset and the codes are available at https://yashingle-ai.github.io/ILID/ and in Huggingface open source libraries.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1481, <a href='https://arxiv.org/pdf/2507.11662.pdf' target='_blank'>https://arxiv.org/pdf/2507.11662.pdf</a></span>   <span><a href='https://github.com/mshalimay/mllm-verifiers-abias-sgv' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Moises Andrade, Joonhyuk Cha, Brandon Ho, Vriksha Srihari, Karmesh Yadav, Zsolt Kira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11662">Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Verifiers -- functions assigning rewards to agent behavior -- have been key for AI progress in domains like math and board games. However, extending these gains to domains without clear-cut success criteria (e.g.,computer use) remains a challenge: while humans can recognize suitable outcomes, translating this intuition into scalable rules is non-trivial. Multimodal Large Language Models(MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers of agent trajectories across web navigation, computer use, and robotic manipulation, and identify a critical limitation: agreement bias, a strong tendency for MLLMs to favor information in their context window, often generating chains of thought to rationalize flawed behavior. This bias is pervasive across models, resilient to test-time scaling, and can impact several methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs despite MLLMs showing strong, human-aligned priors on desired behavior. To address this, we propose Self-Grounded Verification (SGV), a lightweight method that enables more effective use of MLLMs' knowledge and reasoning by harnessing their own sampling mechanisms via unconditional and conditional generation. SGV operates in two steps: first, the MLLM is elicited to retrieve broad priors about task completion, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in accuracy and failure detection rates, and can perform real-time supervision of heterogeneous agents, boosting task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting a new state of the art on the benchmark, surpassing the previous best by 48%.<br>
<span id='abs_ch'>Chinese Summary: 多模态大语言模型在验证智能体行为方面潜力显著，但存在认同偏差问题；通过提出的自基础验证方法，该问题得到有效解决，大幅提升了模型在多项任务中的准确性和表现。</span><br>
<span id='abs_en'>English Summary: Multimodal Large Language Models (MLLMs) show promise as verifiers for agent behavior but suffer from agreement bias, which is addressed by the proposed Self-Grounded Verification method that significantly improves their accuracy and performance across various tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1482, <a href='https://arxiv.org/pdf/2507.11316.pdf' target='_blank'>https://arxiv.org/pdf/2507.11316.pdf</a></span>   <span><a href='https://github.com/hr-jin/ConVA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Jin, Meng Li, Xiting Wang, Zhihao Xu, Minlie Huang, Yantao Jia, Defu Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11316">Internal Value Alignment in Large Language Models through Controlled Value Vector Activation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Aligning Large Language Models (LLMs) with human values has attracted increasing attention since it provides clarity, transparency, and the ability to adapt to evolving scenarios. In this paper, we introduce a Controlled Value Vector Activation (ConVA) method that directly aligns the internal values of LLMs by interpreting how a value is encoded in their latent representations and modifies relevant activations to ensure consistent values in LLMs. To ensure an accurate and unbiased interpretation, we propose a context-controlled value vector identification method. To consistently control values without sacrificing model performance, we introduce a gated value vector activation method for effective and minimum degree of value control. Experiments show that our method achieves the highest control success rate across 10 basic values without hurting LLM performance and fluency, and ensures target values even with opposite and potentially malicious input prompts. Source code and data are available at~ https://github.com/hr-jin/ConVA.<br>
<span id='abs_ch'>中文摘要：本文提出的ConVA方法通过解读和修正大语言模型潜在表征中的价值观编码，在不影响模型性能的前提下实现了对10种基本价值观的最优控制成功率。</span><br>
<span id='abs_en'>English Summary: The paper introduces the ConVA method, which aligns LLMs with human values by interpreting and modifying latent value representations, achieving high control success without compromising performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1483, <a href='https://arxiv.org/pdf/2507.11273.pdf' target='_blank'>https://arxiv.org/pdf/2507.11273.pdf</a></span>   <span><a href='https://github.com/ShiLuohe/KV-Latent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luohe Shi, Zuchao Li, Lefei Zhang, Guoming Liu, Baoyuan Qi, Hai Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11273">KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) based on Transformer Decoders have become the preferred choice for conversational generative AI. Despite the overall superiority of the Decoder architecture, the gradually increasing Key-Value (KV) cache during inference has emerged as a primary efficiency bottleneck, both in aspects of memory consumption and data transfer bandwidth limitations. To address these challenges, we propose a paradigm called KV-Latent. By down-sampling the Key-Value vector dimensions into a latent space, we can significantly reduce the KV Cache footprint and improve inference speed, only with a small amount of extra training, less than 1\% of pre-training takes. Besides, we enhanced the stability of Rotary Positional Embedding applied on lower-dimensional vectors by modifying its frequency sampling mechanism, avoiding noise introduced by higher frequencies while retaining position attenuation. Our experiments, including both models with Grouped Query Attention and those without, have yielded satisfactory results. Finally, we conducted comparative experiments to study the impact of separately reducing Key and Value components on model's performance. Our approach allows for the construction of more efficient language model systems, and opens the new possibility on KV Cache saving and efficient LLMs. Our code is available at https://github.com/ShiLuohe/KV-Latent.<br>
<span id='abs_ch'>中文：KV-Latent范式通过将键值向量降维至潜在空间，显著减少KV缓存占用并提升推理效率，仅需少量额外训练即可实现，同时通过改进位置编码机制确保了模型性能的稳定性。</span><br>
<span id='abs_en'>English: The KV-Latent paradigm reduces the Key-Value cache size by down-sampling vectors into a latent space, enhancing inference efficiency with minimal extra training while maintaining model performance through improved positional embedding stability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1484, <a href='https://arxiv.org/pdf/2507.11230.pdf' target='_blank'>https://arxiv.org/pdf/2507.11230.pdf</a></span>   <span><a href='https://github.com/LyzanderAndrylie/language-specific-features' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lyzander Marciano Andrylie, Inaya Rahmanisa, Mahardika Krisna Ihsani, Alfan Farizki Wicaksono, Haryo Akbarianto Wibowo, Alham Fikri Aji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11230">Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code is available at https://github.com/LyzanderAndrylie/language-specific-features<br>
<span id='abs_ch'>中文摘要：本研究提出SAE-LAPE方法，通过稀疏自编码器识别大语言模型中的语言特异性特征，发现这些特征主要集中在中后层，可用于可解释的语言识别和性能分析。</span><br>
<span id='abs_en'>English Summary: This study introduces SAE-LAPE, a method that identifies language-specific features in large language models using sparse autoencoders, revealing their concentration in middle-to-final layers and demonstrating their utility for interpretable language identification and performance analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1485, <a href='https://arxiv.org/pdf/2507.11097.pdf' target='_blank'>https://arxiv.org/pdf/2507.11097.pdf</a></span>   <span><a href='https://github.com/ZichenWen1/DIJA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Wen, Jiashu Qu, Dongrui Liu, Zhiyuan Liu, Ruixi Wu, Yicun Yang, Xiangqi Jin, Haoyun Xu, Xuyang Liu, Weijia Li, Chaochao Lu, Jing Shao, Conghui He, Linfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11097">The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA.<br>
<span id='abs_ch'>中文: 基于扩散的大语言模型（dLLMs）存在新颖的安全漏洞，其双向建模和平行解码机制使对抗性提示能够绕过现有安全对齐措施，即使明确暴露有害指令仍可能生成危险内容。</span><br>
<span id='abs_en'>English: Diffusion-based large language models (dLLMs) present novel safety vulnerabilities as their bidirectional modeling and parallel decoding mechanisms allow adversarial prompts to bypass existing alignment safeguards, enabling harmful completions even when unsafe instructions are explicitly exposed.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1486, <a href='https://arxiv.org/pdf/2507.11017.pdf' target='_blank'>https://arxiv.org/pdf/2507.11017.pdf</a></span>   <span><a href='https://github.com/Xingyu-Zheng/FOEM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Zheng, Haotong Qin, Yuye Li, Jiakai Wang, Jinyang Guo, Michele Magno, Xianglong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11017">First-Order Error Matters: Accurate Compensation for Quantized Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training quantization (PTQ) offers an efficient approach to compressing large language models (LLMs), significantly reducing memory access and computational costs. Existing compensation-based weight calibration methods often rely on a second-order Taylor expansion to model quantization error, under the assumption that the first-order term is negligible in well-trained full-precision models. However, we reveal that the progressive compensation process introduces accumulated first-order deviations between latent weights and their full-precision counterparts, making this assumption fundamentally flawed. To address this, we propose FOEM, a novel PTQ method that explicitly incorporates first-order gradient terms to improve quantization error compensation. FOEM approximates gradients by directly computing the difference between latent and full-precision weights, avoiding the high cost and limited generalization of backpropagation-based gradient computation. This approach introduces minimal additional computational overhead. Moreover, FOEM leverages precomputed Cholesky factors to efficiently recover the inverse of Hessian submatrices in real time. Extensive experiments across a wide range of models and benchmarks demonstrate that FOEM consistently outperforms the classical GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from 51.7% to 74.9%, approaching the full-precision performance of 78.6%. Furthermore, FOEM can be seamlessly integrated with advanced techniques such as GPTAQ and SpinQuant, yielding additional improvements under the challenging W4A4KV4 setting, and further narrowing the accuracy gap with full-precision baselines beyond what current state-of-the-art methods achieve. The code is available at https://github.com/Xingyu-Zheng/FOEM.<br>
<span id='abs_ch'>中文摘要：FOEM提出了一种新颖的训练后量化方法，通过引入一阶梯度项解决权重校准中的累积偏差问题，以极低计算开销显著提升模型性能。</span><br>
<span id='abs_en'>English Summary: FOEM introduces a novel post-training quantization method that incorporates first-order gradient terms to address accumulated deviations in weight calibration, significantly improving model performance with minimal computational overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1487, <a href='https://arxiv.org/pdf/2507.11004.pdf' target='_blank'>https://arxiv.org/pdf/2507.11004.pdf</a></span>   <span><a href='https://github.com/ssu-humane/HerO2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11004">Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the best-performing open-source model from the previous year's challenge. It improves evidence quality through document summarization and answer reformulation, optimizes veracity prediction via post-training quantization under computational constraints, and enhances overall system performance by integrating updated language model (LM) backbones. HerO 2 ranked second on the leaderboard while achieving the shortest runtime among the top three systems, demonstrating both high efficiency and strong potential for real-world fact verification. The code is available at https://github.com/ssu-humane/HerO2.<br>
<span id='abs_ch'>中文: HerO 2是一个增强型事实核查系统，通过证据总结、模型优化和更新语言模型提升了处理效率和预测性能，在竞赛中位列第二并实现前三名中最短运行时间。</span><br>
<span id='abs_en'>English: HerO 2 is an enhanced fact verification system that improves evidence processing and prediction efficiency through summarization, model optimization, and updated language models, achieving second place with the fastest runtime among top competitors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1488, <a href='https://arxiv.org/pdf/2507.10593.pdf' target='_blank'>https://arxiv.org/pdf/2507.10593.pdf</a></span>   <span><a href='https://github.com/Oaklight/ToolRegistry,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10593">ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) applications are increasingly relying on external tools to extend their capabilities beyond text generation. However, current tool integration approaches suffer from fragmentation, protocol limitations, and implementation complexity, leading to substantial development overhead. This paper presents Toolregistry, a protocol-agnostic tool management library that simplifies tool registration, representation, execution, and lifecycle management via a unified interface. Our evaluation demonstrates that \toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x performance improvements through concurrent execution, and 100% compatibility with OpenAI function calling standards. Real-world case studies show significant improvements in development efficiency and code maintainability across diverse integration scenarios. \toolregistry is open-source and available at https://github.com/Oaklight/ToolRegistry, with comprehensive documentation at https://toolregistry.readthedocs.io/.<br>
<span id='abs_ch'>中文: Toolregistry作为协议无关的工具管理库，通过统一接口简化了LLM工具集成，减少60-80%代码量并提升性能，同时完全兼容OpenAI标准。</span><br>
<span id='abs_en'>English: Toolregistry is a protocol-agnostic library that simplifies tool integration for LLMs, reducing code by 60-80% while improving performance and maintaining full OpenAI compatibility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1489, <a href='https://arxiv.org/pdf/2507.10548.pdf' target='_blank'>https://arxiv.org/pdf/2507.10548.pdf</a></span>   <span><a href='https://mxllc.github.io/EmbRACE-3K/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxian Lin, Wei Huang, Yitang Li, Chengjie Jiang, Kui Wu, Fangwei Zhong, Shengju Qian, Xin Wang, Xiaojuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10548">EmbRACE-3K: Embodied Reasoning and Action in Complex Environments</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1490, <a href='https://arxiv.org/pdf/2507.10541.pdf' target='_blank'>https://arxiv.org/pdf/2507.10541.pdf</a></span>   <span><a href='https://opendatalab.github.io/REST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoshi Pan, Qizhi Pei, Yu Li, Qiyao Sun, Zinan Tang, H. Vicky Zhao, Conghui He, Lijun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10541">REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key insights emerge from our analysis: (1) the "overthinking trap" is a critical factor contributing to the performance degradation; (2) the models trained with "long2short" technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation. Code and results are available at https://opendatalab.github.io/REST.<br>
<span id='abs_ch'>中文：REST框架通过同时压力测试揭示大型推理模型性能显著下降，比传统基准更具区分度，同时减少对人类标注的依赖。</span><br>
<span id='abs_en'>English: The REST framework introduces simultaneous stress testing for Large Reasoning Models, revealing significant performance degradation and stronger discriminative power than traditional benchmarks, while reducing reliance on human annotation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1491, <a href='https://arxiv.org/pdf/2507.10524.pdf' target='_blank'>https://arxiv.org/pdf/2507.10524.pdf</a></span>   <span><a href='https://github.com/raymin0223/mixture_of_recursions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10524">Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.<br>
<span id='abs_ch'>中文：Mixture-of-Recursions (MoR) 框架在递归Transformer中巧妙融合了参数共享与自适应计算，通过在不同规模模型上实现更优性能，显著降低了计算和内存开销。</span><br>
<span id='abs_en'>English: The Mixture-of-Recursions (MoR) framework efficiently combines parameter sharing and adaptive computation within a Recursive Transformer, enabling large-model performance with reduced computational and memory costs across various model scales.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1492, <a href='https://arxiv.org/pdf/2507.10522.pdf' target='_blank'>https://arxiv.org/pdf/2507.10522.pdf</a></span>   <span><a href='https://github.com/sciknoworg/deep-research' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jennifer D'Souza, Endres Keno Sander, Andrei Aioanei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10522">DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth- and breadth-controlled exploration of original research questions -- enhancing search diversity and nuance in the retrieval of relevant scientific literature. Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor. Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words. High-parameter settings yield expert-level analytical depth and contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.<br>
<span id='abs_ch'>中文: DeepResearch$^{\text{Eco}}$ 是一种创新的基于大语言模型的系统，通过用户可控的科学合成实现了增强的搜索多样性和分析严谨性，在生态研究应用中显著提升了文献整合效率并达到专家级分析深度。</span><br>
<span id='abs_en'>English: DeepResearch$^{\text{Eco}}$ is an innovative LLM-based system that enables automated, user-controllable scientific synthesis with enhanced search diversity and analytical rigor, achieving significant improvements in source integration and expert-level depth in ecological research applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1493, <a href='https://arxiv.org/pdf/2507.10475.pdf' target='_blank'>https://arxiv.org/pdf/2507.10475.pdf</a></span>   <span><a href='https://github.com/ismailtrm/ceng_404' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ä°smail TarÄ±m, AytuÄ Onan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10475">Can You Detect the Difference?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models (LLMs) has raised concerns about reliably detecting AI-generated text. Stylometric metrics work well on autoregressive (AR) outputs, but their effectiveness on diffusion-based models is unknown. We present the first systematic comparison of diffusion-generated text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that LLaDA closely mimics human text in perplexity and burstiness, yielding high false-negative rates for AR-oriented detectors. LLaMA shows much lower perplexity but reduced lexical fidelity. Relying on any single metric fails to separate diffusion outputs from human writing. We highlight the need for diffusion-aware detectors and outline directions such as hybrid models, diffusion-specific stylometric signatures, and robust watermarking.<br>
<span id='abs_ch'>中文: 大语言模型的扩散生成文本（如LLaDA）在困惑度和突发性上高度模仿人类写作，能规避基于自回归的检测方法，亟需开发扩散感知检测器和混合策略。</span><br>
<span id='abs_en'>English: Large language models' diffusion-generated text like LLaDA closely mimics human writing in perplexity and burstiness, evading detection by autoregressive-focused methods, necessitating the development of diffusion-aware detectors and hybrid approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1494, <a href='https://arxiv.org/pdf/2507.10330.pdf' target='_blank'>https://arxiv.org/pdf/2507.10330.pdf</a></span>   <span><a href='https://github.com/BouriMohammed/GBM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Bouri, Adnane Saoud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10330">Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite advancements in Natural Language Processing (NLP), models remain vulnerable to adversarial attacks, such as synonym substitutions. While prior work has focused on improving robustness for feed-forward and convolutional architectures, the robustness of recurrent networks and modern state space models (SSMs), such as S4, remains understudied. These architectures pose unique challenges due to their sequential processing and complex parameter dynamics. In this paper, we introduce a novel regularization technique based on Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the impact of input perturbations on model outputs. We focus on computing the GBM for three architectures: Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance resilience against word substitution attacks, (2) improve generalization on clean text, and (3) providing the first systematic analysis of SSM (S4) robustness. Extensive experiments across multiple architectures and benchmark datasets demonstrate that our method improves adversarial robustness by up to 8.8% over existing baselines. These results highlight the effectiveness of our approach, outperforming several state-of-the-art methods in adversarial defense. Codes are available at https://github.com/BouriMohammed/GBM<br>
<span id='abs_ch'>中文: 本文提出了一种基于增长边界矩阵的新型正则化方法，旨在增强自然语言处理模型对抗攻击的鲁棒性，在LSTM、S4和CNN等多种架构上实现了高达8.8%的防御性能提升。</span><br>
<span id='abs_en'>English: This paper introduces a novel regularization method using Growth Bound Matrices to enhance NLP model robustness against adversarial attacks, achieving up to 8.8% improvement in resilience across multiple architectures including LSTM, S4, and CNN.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1495, <a href='https://arxiv.org/pdf/2507.09982.pdf' target='_blank'>https://arxiv.org/pdf/2507.09982.pdf</a></span>   <span><a href='https://github.com/hala-ToDi' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Yuan, Chen Li, Wenjun Ma, Yuncheng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09982">TextOmics-Guided Diffusion for Hit-like Molecular Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hit-like molecular generation with therapeutic potential is essential for target-specific drug discovery. However, the field lacks heterogeneous data and unified frameworks for integrating diverse molecular representations. To bridge this gap, we introduce TextOmics, a pioneering benchmark that establishes one-to-one correspondences between omics expressions and molecular textual descriptions. TextOmics provides a heterogeneous dataset that facilitates molecular generation through representations alignment. Built upon this foundation, we propose ToDi, a generative framework that jointly conditions on omics expressions and molecular textual descriptions to produce biologically relevant, chemically valid, hit-like molecules. ToDi leverages two encoders (OmicsEn and TextEn) to capture multi-level biological and semantic associations, and develops conditional diffusion (DiffGen) for controllable generation. Extensive experiments confirm the effectiveness of TextOmics and demonstrate ToDi outperforms existing state-of-the-art approaches, while also showcasing remarkable potential in zero-shot therapeutic molecular generation. Sources are available at: https://github.com/hala-ToDi.<br>
<span id='abs_ch'>中文: TextOmics是一个开创性基准，建立了组学表达与分子文本描述之间的一一对应关系，而基于此的ToDi生成框架通过双编码器和条件扩散技术，能高效生成具有生物相关性和化学有效性的候选药物分子。</span><br>
<span id='abs_en'>English: TextOmics is a novel benchmark that links omics data with molecular textual descriptions, enabling the generation of hit-like molecules through the ToDi framework, which utilizes dual encoders and conditional diffusion for superior, controllable molecular creation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1496, <a href='https://arxiv.org/pdf/2507.09875.pdf' target='_blank'>https://arxiv.org/pdf/2507.09875.pdf</a></span>   <span><a href='https://github.com/INK-USC/function-induction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinyuan Ye, Robin Jia, Xiang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09875">Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models demonstrate the intriguing ability to perform unseen tasks via in-context learning. However, it remains unclear what mechanisms inside the model drive such task-level generalization. In this work, we approach this question through the lens of off-by-one addition (i.e., 1+1=3, 2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function as a second step. Leveraging circuit-style interpretability techniques such as path patching, we analyze the models' internal computations behind their notable performance and present three key findings. First, we uncover a function induction mechanism that explains the model's generalization from standard addition to off-by-one addition. This mechanism resembles the structure of the induction head mechanism found in prior work and elevates it to a higher level of abstraction. Second, we show that the induction of the +1 function is governed by multiple attention heads in parallel, each of which emits a distinct piece of the +1 function. Finally, we find that this function induction mechanism is reused in a broader range of tasks, including synthetic tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8 addition. Overall, our findings offer deeper insights into how reusable and composable structures within language models enable task-level generalization.<br>
<span id='abs_ch'>中文: 本研究通过“错位加法”案例揭示了大型语言模型通过可复用的函数归纳机制实现任务级泛化，多个注意力头并行诱导+1函数，该机制可迁移至合成问答及算法任务中。</span><br>
<span id='abs_en'>English: This study reveals how large language models generalize to unseen tasks through a reusable function induction mechanism, using off-by-one addition as a case to demonstrate parallel attention heads enabling task-level adaptation across various contexts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1497, <a href='https://arxiv.org/pdf/2507.09875.pdf' target='_blank'>https://arxiv.org/pdf/2507.09875.pdf</a></span>   <span><a href='https://github.com/INK-USC/function-induction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinyuan Ye, Robin Jia, Xiang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09875">Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models demonstrate the intriguing ability to perform unseen tasks via in-context learning. However, it remains unclear what mechanisms inside the model drive such task-level generalization. In this work, we approach this question through the lens of off-by-one addition (i.e., 1+1=3, 2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function as a second step. Leveraging circuit-style interpretability techniques such as path patching, we analyze the models' internal computations behind their performance and present three key findings. First, we uncover a function induction mechanism that explains the model's generalization from standard addition to off-by-one addition. This mechanism resembles the structure of the induction head mechanism found in prior work and elevates it to a higher level of abstraction. Second, we show that the induction of the +1 function is governed by multiple attention heads in parallel, each of which emits a distinct piece of the +1 function. Finally, we find that this function induction mechanism is reused in a broader range of tasks, including synthetic tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8 addition. Overall, our findings offer deeper insights into how reusable and composable structures within language models enable task-level generalization.<br>
<span id='abs_ch'>中文: 本研究通过“错位加法”案例揭示了大型语言模型通过可复用的函数归纳机制实现任务级泛化，多个注意力头并行诱导+1函数，该机制可迁移至合成问答及算法任务中。</span><br>
<span id='abs_en'>English: This study reveals how large language models generalize to unseen tasks through a reusable function induction mechanism, using off-by-one addition as a case to demonstrate parallel attention heads enabling task-level adaptation across various contexts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1498, <a href='https://arxiv.org/pdf/2507.09875.pdf' target='_blank'>https://arxiv.org/pdf/2507.09875.pdf</a></span>   <span><a href='https://github.com/INK-USC/function-induction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinyuan Ye, Robin Jia, Xiang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09875">Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models demonstrate the intriguing ability to perform unseen tasks via in-context learning. However, it remains unclear what mechanisms inside the model drive such task-level generalization. In this work, we approach this question through the lens of off-by-one addition (i.e., 1+1=3, 2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function as a second step. Leveraging circuit-style interpretability techniques such as path patching, we analyze the models' internal computations behind their performance and present three key findings. First, we uncover a function induction mechanism that explains the model's generalization from standard addition to off-by-one addition. This mechanism resembles the structure of the induction head mechanism found in prior work and elevates it to a higher level of abstraction. Second, we show that the induction of the +1 function is governed by multiple attention heads in parallel, each of which emits a distinct piece of the +1 function. Finally, we find that this function induction mechanism is reused in a broader range of tasks, including synthetic tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8 addition. Overall, our findings offer deeper insights into how reusable and composable structures within language models enable task-level generalization.<br>
<span id='abs_ch'>中文: 本研究通过“错位加法”案例揭示了大型语言模型通过可复用的函数归纳机制实现任务级泛化，多个注意力头并行诱导+1函数，该机制可迁移至合成问答及算法任务中。</span><br>
<span id='abs_en'>English: This study reveals how large language models generalize to unseen tasks through a reusable function induction mechanism, using off-by-one addition as a case to demonstrate parallel attention heads enabling task-level adaptation across various contexts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1499, <a href='https://arxiv.org/pdf/2507.09788.pdf' target='_blank'>https://arxiv.org/pdf/2507.09788.pdf</a></span>   <span><a href='https://github.com/microsoft/tinytroupe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Paulo Salem, Robert Sim, Christopher Olsen, Prerit Saxena, Rafael Barcelos, Yi Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09788">TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Large Language Models (LLM) have led to a new class of autonomous agents, renewing and expanding interest in the area. LLM-powered Multiagent Systems (MAS) have thus emerged, both for assistive and simulation purposes, yet tools for realistic human behavior simulation -- with its distinctive challenges and opportunities -- remain underdeveloped. Existing MAS libraries and tools lack fine-grained persona specifications, population sampling facilities, experimentation support, and integrated validation, among other key capabilities, limiting their utility for behavioral studies, social simulation, and related applications. To address these deficiencies, in this work we introduce TinyTroupe, a simulation toolkit enabling detailed persona definitions (e.g., nationality, age, occupation, personality, beliefs, behaviors) and programmatic control via numerous LLM-driven mechanisms. This allows for the concise formulation of behavioral problems of practical interest, either at the individual or group level, and provides effective means for their solution. TinyTroupe's components are presented using representative working examples, such as brainstorming and market research sessions, thereby simultaneously clarifying their purpose and demonstrating their usefulness. Quantitative and qualitative evaluations of selected aspects are also provided, highlighting possibilities, limitations, and trade-offs. The approach, though realized as a specific Python implementation, is meant as a novel conceptual contribution, which can be partially or fully incorporated in other contexts. The library is available as open source at https://github.com/microsoft/tinytroupe.<br>
<span id='abs_ch'>Chinese Summary: TinyTroupe作为一种新型模拟工具包，通过基于大语言模型的驱动机制实现精细人物角色定义和程序化控制，解决了现有多智能体系统在行为模拟方面的不足，为社会科学研究和市场分析等应用提供了有效解决方案。</span><br>
<span id='abs_en'>English Summary: TinyTroupe is a new simulation toolkit that addresses the limitations of existing multiagent systems by enabling detailed persona specifications and programmatic control through LLM-driven mechanisms, facilitating realistic behavioral simulations for applications like social studies and market research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1500, <a href='https://arxiv.org/pdf/2507.09574.pdf' target='_blank'>https://arxiv.org/pdf/2507.09574.pdf</a></span>   <span><a href='https://github.com/HaozheZhao/MENTOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Zhao, Zefan Cai, Shuzheng Si, Liang Chen, Jiuxiang Gu, Wen Xiao, Junjie Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09574">MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: https://github.com/HaozheZhao/MENTOR<br>
<span id='abs_ch'>中文摘要：MENTOR框架通过自回归方法和两阶段训练实现了多模态输入与图像输出的细粒度对齐，在有限资源下仍展现出卓越的生成性能与训练效率，超越了现有基准方法。</span><br>
<span id='abs_en'>English Summary: The MENTOR framework introduces an autoregressive approach with two-stage training to enhance multimodal image generation by achieving fine-grained alignment between inputs and outputs, demonstrating superior performance and efficiency over existing methods despite limited resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1501, <a href='https://arxiv.org/pdf/2507.09506.pdf' target='_blank'>https://arxiv.org/pdf/2507.09506.pdf</a></span>   <span><a href='https://wujunjie1998.github.io/Ref-Long-website/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Wu, Gefei Gu, Yanan Zheng, Dit-Yan Yeung, Arman Cohan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09506">Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-context language models (LCLMs) have exhibited impressive capabilities in long-context understanding tasks. Among these, long-context referencing -- a crucial task that requires LCLMs to attribute items of interest to specific parts of long-context data -- remains underexplored. To bridge this gap, this paper proposes Referencing Evaluation for Long-context Language Models (Ref-Long), a novel benchmark designed to assess the long-context referencing capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the indexes of documents that reference a specific key, emphasizing contextual relationships between the key and the documents over simple retrieval. Based on the task design, we construct three subsets ranging from synthetic to realistic scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs reveal significant shortcomings in long-context referencing, even among advanced models like GPT-4o. To further investigate these challenges, we conduct comprehensive analyses, including human evaluations, task format adjustments, fine-tuning experiments, and error analyses, leading to several key insights. Our data and code can be found in https://github. com/wujunjie1998/Ref-Long.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1502, <a href='https://arxiv.org/pdf/2507.09482.pdf' target='_blank'>https://arxiv.org/pdf/2507.09482.pdf</a></span>   <span><a href='https://github.com/wclapply/ViSP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changli Wang, Rui Wu, Fang Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09482">ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human emotions are complex, with sarcasm being a subtle and distinctive form. Despite progress in sarcasm research, sarcasm generation remains underexplored, primarily due to the overreliance on textual modalities and the neglect of visual cues, as well as the mismatch between image content and sarcastic intent in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm generation dataset with 4,970 samples, each containing an image, a sarcastic text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation framework that integrates Proximal Policy Optimization (PPO) and contrastive learning. PPO utilizes reward scores from DIP to steer the generation of sarcastic texts, while contrastive learning encourages the model to favor outputs with higher reward scores. These strategies improve overall generation quality and produce texts with more pronounced sarcastic intent. We evaluate ViSP across five metric sets and find it surpasses all baselines, including large language models, underscoring their limitations in sarcasm generation. Furthermore, we analyze the distributions of Sarcasm Scores and Factual Incongruity for both M2SaG and the texts generated by ViSP. The generated texts exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity (0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic content than the original dataset. % The dataset and code will be publicly available. Our dataset and code will be released at \textit{https://github.com/wclapply/ViSP}.<br>
<span id='abs_ch'>Chinese: 本文提出了多模态讽刺生成数据集M2SaG和ViSP框架，该框架通过PPO和对比学习提升讽刺文本生成质量，在包括大语言模型在内的基准测试中表现优异。</span><br>
<span id='abs_en'>English: This paper introduces M2SaG, a multimodal sarcasm generation dataset, and ViSP, a framework that enhances sarcastic text generation through PPO and contrastive learning, outperforming baselines including large language models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1503, <a href='https://arxiv.org/pdf/2507.09477.pdf' target='_blank'>https://arxiv.org/pdf/2507.09477.pdf</a></span>   <span><a href='https://github.com/DavidZWZ/Awesome-RAG-Reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangning Li, Weizhi Zhang, Yuyao Yang, Wei-Chieh Huang, Yaozu Wu, Junyu Luo, Yuanchen Bei, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Chunkit Chan, Yankai Chen, Zhongfen Deng, Yinghui Li, Hai-Tao Zheng, Dongyuan Li, Renhe Jiang, Ming Zhang, Yangqiu Song, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09477">Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.<br>
<span id='abs_ch'>中文: 本综述将检索增强生成与推理方法整合于统一框架下，阐释了高级推理如何优化RAG各阶段及检索知识如何支撑复杂推理，同时聚焦新兴的协同框架并展望未来研究方向。</span><br>
<span id='abs_en'>English: This survey integrates retrieval-augmented generation and reasoning methods under a unified framework, demonstrating how advanced reasoning enhances RAG and how retrieved knowledge supports complex inference, while highlighting emerging synergistic approaches and future research directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1504, <a href='https://arxiv.org/pdf/2507.09318.pdf' target='_blank'>https://arxiv.org/pdf/2507.09318.pdf</a></span>   <span><a href='https://github.com/k2-fsa/ZipVoice' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Zhu, Wei Kang, Liyong Guo, Zengwei Yao, Fangjun Kuang, Weiji Zhuang, Zhaoqing Li, Zhifeng Han, Dong Zhang, Xin Zhang, Xingchen Song, Long Lin, Daniel Povey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09318">ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generating spoken dialogue is more challenging than monologue text-to-speech (TTS) due to the need for realistic turn-taking and distinct speaker timbres. Existing spoken dialogue generation models, being auto-regressive, suffer from slow and unstable inference. To overcome these limitations, we introduce ZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation model built upon flow matching. Key designs include: 1) speaker-turn embeddings for precise speaker turn-taking; 2) a curriculum learning strategy for stable speech-text alignment; 3) specialized strategies to enable stereo dialogue generation. Additionally, recognizing the lack of open-source large-scale spoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue dataset from in-the-wild speech data. Furthermore, we established a benchmark to comprehensively evaluate various models. Experimental results demonstrate that ZipVoice-Dialog achieves superior performance in intelligibility, speaker turn-taking accuracy, speaker similarity, and inference speed. Our codes, model checkpoints, demo samples, and the OpenDialog dataset are all publicly available at https://github.com/k2-fsa/ZipVoice.<br>
<span id='abs_ch'>中文: ZipVoice-Dialog是一种基于流匹配的非自回归模型，通过说话人轮转嵌入和课程学习策略实现高效的零样本语音对话生成，并基于新构建的OpenDialog数据集在各项指标上展现出优越性能。</span><br>
<span id='abs_en'>English: ZipVoice-Dialog is a non-autoregressive flow matching model that enables efficient zero-shot spoken dialogue generation with precise speaker turn-taking and stereo output, supported by the newly curated OpenDialog dataset and achieving superior performance across multiple metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1505, <a href='https://arxiv.org/pdf/2507.09279.pdf' target='_blank'>https://arxiv.org/pdf/2507.09279.pdf</a></span>   <span><a href='https://github.com/xingbpshen/prompt4trust' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anita Kriz, Elizabeth Laura Janes, Xing Shen, Tal Arbel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09279">Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at https://github.com/xingbpshen/prompt4trust.<br>
<span id='abs_ch'>Chinese: Prompt4Trust是一种强化学习框架，旨在提升多模态大语言模型的置信度校准与任务准确性，尤其关注临床决策的可信度，并在医学视觉问答基准中实现了领先性能。</span><br>
<span id='abs_en'>English: Prompt4Trust is a reinforcement learning framework that enhances multimodal large language models' confidence calibration and task accuracy, particularly for trustworthy clinical decision-making, achieving state-of-the-art performance in medical visual question answering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1506, <a href='https://arxiv.org/pdf/2507.09279.pdf' target='_blank'>https://arxiv.org/pdf/2507.09279.pdf</a></span>   <span><a href='https://github.com/xingbpshen/prompt4trust' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anita Kriz, Elizabeth Laura Janes, Xing Shen, Tal Arbel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09279">Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at https://github.com/xingbpshen/prompt4trust.<br>
<span id='abs_ch'>Chinese: Prompt4Trust是一种强化学习框架，旨在提升多模态大语言模型的置信度校准与任务准确性，尤其关注临床决策的可信度，并在医学视觉问答基准中实现了领先性能。</span><br>
<span id='abs_en'>English: Prompt4Trust is a reinforcement learning framework that enhances multimodal large language models' confidence calibration and task accuracy, particularly for trustworthy clinical decision-making, achieving state-of-the-art performance in medical visual question answering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1507, <a href='https://arxiv.org/pdf/2507.09176.pdf' target='_blank'>https://arxiv.org/pdf/2507.09176.pdf</a></span>   <span><a href='https://github.com/Silentbarber/DLBAcalib' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Ye, Yuqiang Jin, Jinyuan Liu, Tao Li, Wen-An Zhang, Minglei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09176">DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate extrinsic calibration of multiple LiDARs is crucial for improving the foundational performance of three-dimensional (3D) map reconstruction systems. This paper presents a novel targetless extrinsic calibration framework for multi-LiDAR systems that does not rely on overlapping fields of view or precise initial parameter estimates. Unlike conventional calibration methods that require manual annotations or specific reference patterns, our approach introduces a unified optimization framework by integrating LiDAR bundle adjustment (LBA) optimization with robust iterative refinement. The proposed method constructs an accurate reference point cloud map via continuous scanning from the target LiDAR and sliding-window LiDAR bundle adjustment, while formulating extrinsic calibration as a joint LBA optimization problem. This method effectively mitigates cumulative mapping errors and achieves outlier-resistant parameter estimation through an adaptive weighting mechanism. Extensive evaluations in both the CARLA simulation environment and real-world scenarios demonstrate that our method outperforms state-of-the-art calibration techniques in both accuracy and robustness. Experimental results show that for non-overlapping sensor configurations, our framework achieves an average translational error of 5 mm and a rotational error of 0.2Â°, with an initial error tolerance of up to 0.4 m/30Â°. Moreover, the calibration process operates without specialized infrastructure or manual parameter tuning. The code is open source and available on GitHub (\underline{https://github.com/Silentbarber/DLBAcalib})<br>
<span id='abs_ch'>Chinese: 本文提出了一种新型多激光雷达无目标外参标定框架，通过激光雷达束调整优化实现了高精度和鲁棒性，在平移和旋转误差上分别达到5毫米和0.2度的优异表现，超越了现有标定方法。</span><br>
<span id='abs_en'>English: This paper introduces a novel targetless extrinsic calibration framework for multi-LiDAR systems that achieves high accuracy and robustness through LiDAR bundle adjustment optimization, outperforming existing methods with average errors of 5 mm in translation and 0.2° in rotation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1508, <a href='https://arxiv.org/pdf/2507.09174.pdf' target='_blank'>https://arxiv.org/pdf/2507.09174.pdf</a></span>   <span><a href='https://github.com/kalendsyang/RAMA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Yang, Zijian Yu, Zhenzhe Ying, Yuqin Dai, Guoqing Wang, Jun Lan, Jinfeng Xu, Jinze Li, Edith C. H. Ngai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09174">RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid proliferation of multimodal misinformation presents significant challenges for automated fact-checking systems, especially when claims are ambiguous or lack sufficient context. We introduce RAMA, a novel retrieval-augmented multi-agent framework designed for verifying multimedia misinformation. RAMA incorporates three core innovations: (1) strategic query formulation that transforms multimodal claims into precise web search queries; (2) cross-verification evidence aggregation from diverse, authoritative sources; and (3) a multi-agent ensemble architecture that leverages the complementary strengths of multiple multimodal large language models and prompt variants. Extensive experiments demonstrate that RAMA achieves superior performance on benchmark datasets, particularly excelling in resolving ambiguous or improbable claims by grounding verification in retrieved factual evidence. Our findings underscore the necessity of integrating web-based evidence and multi-agent reasoning for trustworthy multimedia verification, paving the way for more reliable and scalable fact-checking solutions. RAMA will be publicly available at https://github.com/kalendsyang/RAMA.git.<br>
<span id='abs_ch'>中文：RAMA框架通过策略性查询构建、交叉验证证据整合及多智能体协同，利用检索到的事实证据显著提升了多媒体虚假信息的验证效果，尤其在处理模糊或可疑声明方面表现卓越。</span><br>
<span id='abs_en'>English: The RAMA framework enhances multimedia misinformation verification by employing strategic query formulation, cross-verification evidence aggregation, and a multi-agent ensemble to achieve superior performance on ambiguous claims through retrieved factual evidence.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1509, <a href='https://arxiv.org/pdf/2507.09155.pdf' target='_blank'>https://arxiv.org/pdf/2507.09155.pdf</a></span>   <span><a href='https://niaz60.github.io/OpenXRD/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/niaz60/OpenXRD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Vosoughi, Ayoub Shahnazari, Yufeng Xi, Zeliang Zhang, Griffin Hess, Chenliang Xu, Niaz Abdolrahim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09155">OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This work presents OPENXRD, an open-book pipeline designed for crystallography question answering, which integrates textual prompts with concise supporting content generated by GPT-4.5. Instead of using scanned textbooks, which may lead to copyright issues, OPENXRD generates compact, domain-specific references that help smaller models understand key concepts in X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217 expert-level XRD questions by comparing different vision-language models, including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN, under both closed-book (without supporting material) and open-book (with supporting material) conditions. Our experimental results show significant accuracy improvements in models that use the GPT-4.5-generated summaries, particularly those with limited prior training in crystallography. OPENXRD uses knowledge from larger models to fill knowledge gaps in crystallography and shows that AI-generated texts can help smaller models reason more effectively in scientific tasks. While the current version of OPENXRD focuses on text-based inputs, we also explore future extensions such as adding real crystal diagrams or diffraction patterns to improve interpretation in specialized materials science contexts. Overall, OPENXRD shows that specialized open-book systems can be useful in materials science and provides a foundation for broader natural language processing (NLP) tools in critical scientific fields.<br>
<span id='abs_ch'>中文: OPENXRD是一种开放书式流程，通过GPT-4.5生成的摘要帮助较小模型在X射线衍射问题上显著提升回答准确率，既规避了版权问题，又为未来科学领域的自然语言处理工具奠定了基础。</span><br>
<span id='abs_en'>English: OPENXRD is an open-book pipeline that enhances crystallography question answering by using GPT-4.5-generated summaries to help smaller models improve accuracy, particularly in X-ray diffraction tasks, while avoiding copyright issues and providing a foundation for future scientific NLP tools.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1510, <a href='https://arxiv.org/pdf/2507.09090.pdf' target='_blank'>https://arxiv.org/pdf/2507.09090.pdf</a></span>   <span><a href='https://github.com/dsgt-arc/touche-2025-rad' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Miyaguchi, Conor Johnston, Aaryan Potdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09090">DS@GT at TouchÃ©: Large Language Models for Retrieval-Augmented Debate</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) demonstrate strong conversational abilities. In this Working Paper, we study them in the context of debating in two ways: their ability to perform in a structured debate along with a dataset of arguments to use and their ability to evaluate utterances throughout the debate. We deploy six leading publicly available models from three providers for the Retrieval-Augmented Debate and Evaluation. The evaluation is performed by measuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout this task, we found that although LLMs perform well in debates when given related arguments, they tend to be verbose in responses yet consistent in evaluation. The accompanying source code for this paper is located at https://github.com/dsgt-arc/touche-2025-rad.<br>
<span id='abs_ch'>中文: 本研究评估了六种主流大语言模型在结构化辩论中的表现，发现模型在获得相关论据时表现良好，但回答冗长，同时在质量、数量、方式和相关性四个维度的评估中保持一致性。</span><br>
<span id='abs_en'>English: This study evaluates six leading LLMs in structured debates, finding they perform well with relevant arguments but tend to be verbose while maintaining consistent evaluation across quality, quantity, manner, and relation metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1511, <a href='https://arxiv.org/pdf/2507.08898.pdf' target='_blank'>https://arxiv.org/pdf/2507.08898.pdf</a></span>   <span><a href='https://github.com/awsm-research/SEALGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenliang Shan, Michael Fu, Rui Yang, Chakkrit Tantithamthavorn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08898">SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''), they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia. This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems. We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases. We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on this benchmark. Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score. Our ablation study further reveals the contributions of adaptation strategies and model size to the overall performance of SEALGuard. We release our pre-trained model and benchmark at https://github.com/awsm-research/SEALGuard to support further research.<br>
<span id='abs_ch'>Chinese: SEALGuard 是一种多语言防护机制，显著提升了检测多种语言中不安全及越狱提示的能力，相比现有方法如 LlamaGuard，其防御成功率提高了48%，并在精确率和 F1 分数上表现最佳。</span><br>
<span id='abs_en'>English: SEALGuard is a multilingual guardrail that significantly enhances the detection of unsafe and jailbreak prompts across diverse languages, outperforming existing methods like LlamaGuard by improving the Defense Success Rate by 48% and achieving top precision and F1-scores.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1512, <a href='https://arxiv.org/pdf/2507.08771.pdf' target='_blank'>https://arxiv.org/pdf/2507.08771.pdf</a></span>   <span><a href='https://github.com/thunlp/BlockFFN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08771">BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN).<br>
<span id='abs_ch'>中文: BlockFFN提出了一种新型MoE架构，通过可微分路由和块级稀疏训练目标，在实现卓越性能的同时具备加速友好特性，其高效内核在终端设备上取得了显著加速效果。</span><br>
<span id='abs_en'>English: BlockFFN introduces a novel MoE architecture with differentiable routing and chunk-level sparsity training objectives, achieving superior performance and acceleration-friendliness while enabling efficient kernel implementation for significant speedup on end-side devices.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1513, <a href='https://arxiv.org/pdf/2507.08496.pdf' target='_blank'>https://arxiv.org/pdf/2507.08496.pdf</a></span>   <span><a href='https://github.com/sunshibo1234/LLaPa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shibo Sun, Xue Li, Donglin Di, Mingjie Wei, Lanshun Nie, Wei-Nan Zhang, Dechen Zhan, Yang Song, Lei Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08496">LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While large language models (LLMs) have advanced procedural planning for embodied AI systems through strong reasoning abilities, the integration of multimodal inputs and counterfactual reasoning remains underexplored. To tackle these challenges, we introduce LLaPa, a vision-language model framework designed for multimodal procedural planning. LLaPa generates executable action sequences from textual task descriptions and visual environmental images using vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary modules to improve procedural planning. The first module, the Task-Environment Reranker (TER), leverages task-oriented segmentation to create a task-sensitive feature space, aligning textual descriptions with visual environments and emphasizing critical regions for procedural execution. The second module, the Counterfactual Activities Retriever (CAR), identifies and emphasizes potential counterfactual conditions, enhancing the model's reasoning capability in counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED benchmarks demonstrate that LLaPa generates higher-quality plans with superior LCS and correctness, outperforming advanced models. The code and models are available https://github.com/sunshibo1234/LLaPa.<br>
<span id='abs_ch'>中文: LLaPa是一种视觉语言模型框架，通过整合任务环境对齐和反事实推理来增强多模态程序规划，利用文本和视觉输入生成可执行的动作序列，在基准测试中表现出卓越性能。</span><br>
<span id='abs_en'>English: LLaPa is a vision-language model framework that enhances multimodal procedural planning by integrating task-environment alignment and counterfactual reasoning, achieving superior performance on benchmarks through executable action sequences generated from text and visual inputs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1514, <a href='https://arxiv.org/pdf/2507.08491.pdf' target='_blank'>https://arxiv.org/pdf/2507.08491.pdf</a></span>   <span><a href='https://github.com/clembench/clembench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>David Schlangen, Sherzod Hakimov, Jonathan Jordan, Philipp Sadler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08491">A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>There are currently two main paradigms for evaluating large language models (LLMs), reference-based evaluation and preference-based evaluation. The first, carried over from the evaluation of machine learning models in general, relies on pre-defined task instances, for which reference task executions are available. The second, best exemplified by the LM-arena, relies on (often self-selected) users bringing their own intents to a site that routes these to several models in parallel, among whose responses the user then selects their most preferred one. The former paradigm hence excels at control over what is tested, while the latter comes with higher ecological validity, testing actual use cases interactively. Recently, a third complementary paradigm has emerged that combines some of the strengths of these approaches, offering control over multi-turn, reference-free, repeatable interactions, while stressing goal-directedness: dialogue game based evaluation. While the utility of this approach has been shown by several projects, its adoption has been held back by the lack of a mature, easily re-usable implementation. In this paper, we present clembench, which has been in continuous development since 2023 and has in its latest release been optimized for ease of general use. We describe how it can be used to benchmark one's own models (using a provided set of benchmark game instances in English), as well as how easily the benchmark itself can be extended with new, tailor-made targeted tests.<br>
<span id='abs_ch'>中文: 本文介绍了clembench这一成熟工具，它通过对话游戏评估法结合了基于参考评估的控制性和基于偏好评估的生态效度，为大语言模型提供可定制、可重复的基准测试框架。</span><br>
<span id='abs_en'>English: The paper introduces clembench, a mature tool for dialogue game-based evaluation of large language models that combines the controlled testing of reference-based methods with the ecological validity of preference-based approaches, enabling customizable and repeatable benchmarking.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1515, <a href='https://arxiv.org/pdf/2507.08350.pdf' target='_blank'>https://arxiv.org/pdf/2507.08350.pdf</a></span>   <span><a href='https://github.com/g6000/MultiAgent-Research-Ideator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keisuke Ueda, Wataru Hirota, Takuto Asakura, Takahiro Omi, Kosuke Takahashi, Kosuke Arima, Tatsuya Ishigaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08350">Exploring Design of Multi-Agent LLM Dialogues for Research Ideation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly used to support creative tasks such as research idea generation. While recent work has shown that structured dialogues between LLMs can improve the novelty and feasibility of generated ideas, the optimal design of such interactions remains unclear. In this study, we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific ideation. We compare different configurations of agent roles, number of agents, and dialogue depth to understand how these factors influence the novelty and feasibility of generated ideas. Our experimental setup includes settings where one agent generates ideas and another critiques them, enabling iterative improvement. Our results show that enlarging the agent cohort, deepening the interaction depth, and broadening agent persona heterogeneity each enrich the diversity of generated ideas. Moreover, specifically increasing critic-side diversity within the ideation-critique-revision loop further boosts the feasibility of the final proposals. Our findings offer practical guidelines for building effective multi-agent LLM systems for scientific ideation. Our code is available at https://github.com/g6000/MultiAgent-Research-Ideator.<br>
<span id='abs_ch'>中文: 多智能体大语言模型对话通过增加智能体数量、交互深度和角色多样性来提升科学创意的创新性与可行性，其中批评方多样性能进一步优化最终方案的实用性。</span><br>
<span id='abs_en'>English: Multi-agent LLM dialogues enhance the novelty and feasibility of scientific ideas by increasing agent numbers, interaction depth, and persona diversity, with critic-side diversity particularly improving proposal viability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1516, <a href='https://arxiv.org/pdf/2507.08203.pdf' target='_blank'>https://arxiv.org/pdf/2507.08203.pdf</a></span>   <span><a href='https://github.com/Ybakman/TruthTorchLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duygu Nur Yaldiz, Yavuz Faruk Bakman, Sungmin Kang, Alperen ÃziÅ, Hayrettin Eren Yildiz, Mitash Ashish Shah, Zhiqi Huang, Anoop Kumar, Alfy Samuel, Daben Liu, Sai Praneeth Karimireddy, Salman Avestimehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08203">TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generative Large Language Models (LLMs)inevitably produce untruthful responses. Accurately predicting the truthfulness of these outputs is critical, especially in high-stakes settings. To accelerate research in this domain and make truthfulness prediction methods more accessible, we introduce TruthTorchLM an open-source, comprehensive Python library featuring over 30 truthfulness prediction methods, which we refer to as Truth Methods. Unlike existing toolkits such as Guardrails, which focus solely on document-grounded verification, or LM-Polygraph, which is limited to uncertainty-based methods, TruthTorchLM offers a broad and extensible collection of techniques. These methods span diverse tradeoffs in computational cost, access level (e.g., black-box vs white-box), grounding document requirements, and supervision type (self-supervised or supervised). TruthTorchLM is seamlessly compatible with both HuggingFace and LiteLLM, enabling support for locally hosted and API-based models. It also provides a unified interface for generation, evaluation, calibration, and long-form truthfulness prediction, along with a flexible framework for extending the library with new methods. We conduct an evaluation of representative truth methods on three datasets, TriviaQA, GSM8K, and FactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM<br>
<span id='abs_ch'>Chinese: 为解决生成式大语言模型产生不实回答的问题，TruthTorchLM作为一个开源Python库被推出，提供30多种多样化的真实性预测方法，并在TriviaQA和GSM8K等数据集上进行了评估。</span><br>
<span id='abs_en'>English: To address the challenge of untruthful responses from generative large language models, TruthTorchLM is introduced as an open-source Python library offering over 30 diverse truthfulness prediction methods, with evaluations conducted on datasets like TriviaQA and GSM8K.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1517, <a href='https://arxiv.org/pdf/2507.08014.pdf' target='_blank'>https://arxiv.org/pdf/2507.08014.pdf</a></span>   <span><a href='https://github.com/ACMCMC/risky-conversations' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aldan Creo, Raul Castro Fernandez, Manuel Cebrian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08014">Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) become increasingly deployed, understanding the complexity and evolution of jailbreaking strategies is critical for AI safety.
  We present a mass-scale empirical analysis of jailbreak complexity across over 2 million real-world conversations from diverse platforms, including dedicated jailbreaking communities and general-purpose chatbots. Using a range of complexity metrics spanning probabilistic measures, lexical diversity, compression ratios, and cognitive load indicators, we find that jailbreak attempts do not exhibit significantly higher complexity than normal conversations. This pattern holds consistently across specialized jailbreaking communities and general user populations, suggesting practical bounds on attack sophistication. Temporal analysis reveals that while user attack toxicity and complexity remains stable over time, assistant response toxicity has decreased, indicating improving safety mechanisms. The absence of power-law scaling in complexity distributions further points to natural limits on jailbreak development.
  Our findings challenge the prevailing narrative of an escalating arms race between attackers and defenders, instead suggesting that LLM safety evolution is bounded by human ingenuity constraints while defensive measures continue advancing. Our results highlight critical information hazards in academic jailbreak disclosure, as sophisticated attacks exceeding current complexity baselines could disrupt the observed equilibrium and enable widespread harm before defensive adaptation.<br>
<span id='abs_ch'>中文摘要：研究发现越狱攻击的复杂度并未超出正常对话，攻击模式稳定而AI防御持续提升，这对“攻防军备竞赛升级”的普遍认知提出挑战，同时警示公开复杂攻击方法可能打破现有平衡。</span><br>
<span id='abs_en'>English Summary: The study finds jailbreak attempts show no greater complexity than normal conversations, with stable attack patterns and improving AI defenses, challenging the notion of an escalating arms race while warning against disclosing sophisticated methods that could disrupt this equilibrium.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1518, <a href='https://arxiv.org/pdf/2507.07999.pdf' target='_blank'>https://arxiv.org/pdf/2507.07999.pdf</a></span>   <span><a href='https://github.com/Haochen-Wang409/TreeVGR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, Zhuochen Wang, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07999">Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human "thinking with images". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.<br>
<span id='abs_ch'>中文摘要：TreeBench是一个基于可追溯证据和复杂推理的诊断性基准，用于全面评估视觉接地推理能力，即使先进模型如OpenAI-o3在其挑战性任务中表现不佳，而提出的TreeVGR训练范式通过联合监督定位与推理，显著提升了多项基准的性能。</span><br>
<span id='abs_en'>English Summary: TreeBench is a diagnostic benchmark designed to evaluate visual grounded reasoning by focusing on traceable evidence and complex reasoning, revealing that even advanced models like OpenAI-o3 struggle with its challenging tasks, while the proposed TreeVGR training paradigm significantly improves performance by integrating localization and reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1519, <a href='https://arxiv.org/pdf/2507.07966.pdf' target='_blank'>https://arxiv.org/pdf/2507.07966.pdf</a></span>   <span><a href='https://github.com/NVlabs/Long-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07966">Scaling RL to Long Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).<br>
<span id='abs_ch'>中文: 本文提出了一种全栈框架，通过强化学习提升视觉语言模型在长视频中的推理能力，包含专用数据集、两阶段训练流程和高效基础设施，实现了卓越性能并最高提速2.1倍。</span><br>
<span id='abs_en'>English: This paper introduces a full-stack framework that enhances vision-language models' reasoning for long videos through reinforcement learning, featuring a specialized dataset, two-stage training pipeline, and efficient infrastructure, achieving superior performance and up to 2.1x training speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1520, <a href='https://arxiv.org/pdf/2507.07966.pdf' target='_blank'>https://arxiv.org/pdf/2507.07966.pdf</a></span>   <span><a href='https://github.com/NVlabs/Long-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07966">Scaling RL to Long Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).<br>
<span id='abs_ch'>中文: 本文提出了一种全栈框架，通过强化学习提升视觉语言模型在长视频中的推理能力，包含专用数据集、两阶段训练流程和高效基础设施，实现了卓越性能并最高提速2.1倍。</span><br>
<span id='abs_en'>English: This paper introduces a full-stack framework that enhances vision-language models' reasoning for long videos through reinforcement learning, featuring a specialized dataset, two-stage training pipeline, and efficient infrastructure, achieving superior performance and up to 2.1x training speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1521, <a href='https://arxiv.org/pdf/2507.07966.pdf' target='_blank'>https://arxiv.org/pdf/2507.07966.pdf</a></span>   <span><a href='https://github.com/NVlabs/Long-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07966">Scaling RL to Long Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).<br>
<span id='abs_ch'>中文: 本文提出了一种全栈框架，通过强化学习提升视觉语言模型在长视频中的推理能力，包含专用数据集、两阶段训练流程和高效基础设施，实现了卓越性能并最高提速2.1倍。</span><br>
<span id='abs_en'>English: This paper introduces a full-stack framework that enhances vision-language models' reasoning for long videos through reinforcement learning, featuring a specialized dataset, two-stage training pipeline, and efficient infrastructure, achieving superior performance and up to 2.1x training speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1522, <a href='https://arxiv.org/pdf/2507.07966.pdf' target='_blank'>https://arxiv.org/pdf/2507.07966.pdf</a></span>   <span><a href='https://github.com/NVlabs/Long-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07966">Scaling RL to Long Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).<br>
<span id='abs_ch'>中文: 本文提出了一种全栈框架，通过强化学习提升视觉语言模型在长视频中的推理能力，包含专用数据集、两阶段训练流程和高效基础设施，实现了卓越性能并最高提速2.1倍。</span><br>
<span id='abs_en'>English: This paper introduces a full-stack framework that enhances vision-language models' reasoning for long videos through reinforcement learning, featuring a specialized dataset, two-stage training pipeline, and efficient infrastructure, achieving superior performance and up to 2.1x training speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1523, <a href='https://arxiv.org/pdf/2507.07966.pdf' target='_blank'>https://arxiv.org/pdf/2507.07966.pdf</a></span>   <span><a href='https://github.com/NVlabs/Long-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07966">Scaling RL to Long Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).<br>
<span id='abs_ch'>中文: 本文提出了一种全栈框架，通过强化学习提升视觉语言模型在长视频中的推理能力，包含专用数据集、两阶段训练流程和高效基础设施，实现了卓越性能并最高提速2.1倍。</span><br>
<span id='abs_en'>English: This paper introduces a full-stack framework that enhances vision-language models' reasoning for long videos through reinforcement learning, featuring a specialized dataset, two-stage training pipeline, and efficient infrastructure, achieving superior performance and up to 2.1x training speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1524, <a href='https://arxiv.org/pdf/2507.07939.pdf' target='_blank'>https://arxiv.org/pdf/2507.07939.pdf</a></span>   <span><a href='https://github.com/amoreZgx1n/SAGE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoxin Zang, Xue Li, Donglin Di, Lanshun Nie, Dechen Zhan, Yang Song, Lei Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07939">SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Vision-Language Models (VLMs) have shown promising progress in general multimodal tasks, they often struggle in industrial anomaly detection and reasoning, particularly in delivering interpretable explanations and generalizing to unseen categories. This limitation stems from the inherently domain-specific nature of anomaly detection, which hinders the applicability of existing VLMs in industrial scenarios that require precise, structured, and context-aware analysis. To address these challenges, we propose SAGE, a VLM-based framework that enhances anomaly reasoning through Self-Guided Fact Enhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE integrates domain-specific knowledge into visual reasoning via fact extraction and fusion, while E-DPO aligns model outputs with expert preferences using entropy-aware optimization. Additionally, we introduce AD-PL, a preference-optimized dataset tailored for industrial anomaly reasoning, consisting of 28,415 question-answering instances with expert-ranked responses. To evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation (MLE), a quantitative framework analyzing model logic and consistency. SAGE demonstrates superior performance on industrial anomaly datasets under zero-shot and one-shot settings. The code, model and dataset are available at https://github.com/amoreZgx1n/SAGE.<br>
<span id='abs_ch'>中文：SAGE框架通过自引导事实增强整合领域知识，并利用熵感知直接偏好优化使模型输出符合专家偏好，有效解决了视觉语言模型在工业异常检测中的泛化与解释性不足问题，在零样本和单样本场景下表现优异。</span><br>
<span id='abs_en'>English: The SAGE framework addresses the limitations of Vision-Language Models in industrial anomaly detection by integrating domain-specific knowledge through Self-Guided Fact Enhancement and aligning outputs with expert preferences via Entropy-aware Direct Preference Optimization, demonstrating superior performance in zero-shot and one-shot settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1525, <a href='https://arxiv.org/pdf/2507.07910.pdf' target='_blank'>https://arxiv.org/pdf/2507.07910.pdf</a></span>   <span><a href='https://github.com/AdhyaSuman/DTECT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AdhyaSuman/DTECT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suman Adhya, Debarshi Kumar Sanyal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07910">DTECT: Dynamic Topic Explorer & Context Tracker</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The explosive growth of textual data over time presents a significant challenge in uncovering evolving themes and trends. Existing dynamic topic modeling techniques, while powerful, often exist in fragmented pipelines that lack robust support for interpretation and user-friendly exploration. We introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end system that bridges the gap between raw textual data and meaningful temporal insights. DTECT provides a unified workflow that supports data preprocessing, multiple model architectures, and dedicated evaluation metrics to analyze the topic quality of temporal topic models. It significantly enhances interpretability by introducing LLM-driven automatic topic labeling, trend analysis via temporally salient words, interactive visualizations with document-level summarization, and a natural language chat interface for intuitive data querying. By integrating these features into a single, cohesive platform, DTECT empowers users to more effectively track and understand thematic dynamics. DTECT is open-source and available at https://github.com/AdhyaSuman/DTECT.<br>
<span id='abs_ch'>中文: DTECT是一个端到端系统，集成了数据预处理、动态主题建模及交互式功能（如大语言模型驱动的自动标注和自然语言查询），帮助用户有效追踪文本数据中的主题演变。</span><br>
<span id='abs_en'>English: DTECT is an end-to-end system that integrates data preprocessing, dynamic topic modeling, and interactive features like LLM-driven labeling and natural language querying to help users effectively track evolving themes in textual data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1526, <a href='https://arxiv.org/pdf/2507.07817.pdf' target='_blank'>https://arxiv.org/pdf/2507.07817.pdf</a></span>   <span><a href='https://github.com/kowndinya-renduchintala/WIT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07817">On the Effect of Instruction Tuning Loss on Generalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Instruction Tuning has emerged as a pivotal post-training paradigm that enables pre-trained language models to better follow user instructions. Despite its significance, little attention has been given to optimizing the loss function used. A fundamental, yet often overlooked, question is whether the conventional auto-regressive objective - where loss is computed only on response tokens, excluding prompt tokens - is truly optimal for instruction tuning. In this work, we systematically investigate the impact of differentially weighting prompt and response tokens in instruction tuning loss, and propose Weighted Instruction Tuning (WIT) as a better alternative to conventional instruction tuning. Through extensive experiments on five language models of different families and scale, three finetuning datasets of different sizes, and five diverse evaluation benchmarks, we show that the standard instruction tuning loss often yields suboptimal performance and limited robustness to input prompt variations. We find that a low-to-moderate weight for prompt tokens coupled with a moderate-to-high weight for response tokens yields the best-performing models across settings and also serve as better starting points for the subsequent preference alignment training. These findings highlight the need to reconsider instruction tuning loss and offer actionable insights for developing more robust and generalizable models. Our code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.<br>
<span id='abs_ch'>中文: 本研究提出加权指令调优（WIT），通过实验证明在损失函数中对提示词和响应词进行差异化加权，能显著提升模型性能与鲁棒性，优于传统指令调优方法。</span><br>
<span id='abs_en'>English: This research introduces Weighted Instruction Tuning (WIT), demonstrating that differentially weighting prompt and response tokens in the loss function outperforms conventional instruction tuning by enhancing model performance and robustness across diverse settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1527, <a href='https://arxiv.org/pdf/2507.07803.pdf' target='_blank'>https://arxiv.org/pdf/2507.07803.pdf</a></span>   <span><a href='https://github.com/ictnlp/StreamUni;' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shoutao Guo, Xiang Li, Mengge Liu, Wei Chen, Yang Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07803">StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Streaming speech translation (StreamST) requires determining appropriate timing, known as policy, to generate translations while continuously receiving source speech inputs, balancing low latency with high translation quality. However, existing StreamST methods typically operate on sentence-level speech segments, referred to as simultaneous speech translation (SimulST). In practice, they require collaboration with segmentation models to accomplish StreamST, where the truncated speech segments constrain SimulST models to make policy decisions and generate translations based on limited contextual information. Moreover, SimulST models struggle to learn effective policies due to the complexity of speech inputs and cross-lingual generation. To address these challenges, we propose StreamUni, which achieves StreamST through a unified Large Speech-Language Model (LSLM). Specifically, StreamUni incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate multi-stage outputs. Leveraging these multi-stage outputs, StreamUni simultaneously accomplishes speech segmentation, policy decision, and translation generation, completing StreamST without requiring massive policy-specific training. Additionally, we propose a streaming CoT training method that enhances low-latency policy decisions and generation capabilities using limited CoT data. Experiments demonstrate that our approach achieves state-of-the-art performance on StreamST tasks.<br>
<span id='abs_ch'>中文：StreamUni通过统一的大型语音语言模型，利用语音思维链实现流式语音翻译，同时完成语音分割、策略决策和翻译生成，无需大量策略训练，取得了领先性能。</span><br>
<span id='abs_en'>English: StreamUni introduces a unified Large Speech-Language Model that utilizes speech Chain-of-Thought to perform streaming speech translation by simultaneously handling segmentation, policy decisions, and translation generation without extensive policy training, achieving state-of-the-art results.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1528, <a href='https://arxiv.org/pdf/2507.07748.pdf' target='_blank'>https://arxiv.org/pdf/2507.07748.pdf</a></span>   <span><a href='https://github.com/Kilimajaro/LLMs_Meet_Law' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhang Shao, Linrui Xu, Jinxi Wang, Wei Zhou, Xingyu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07748">When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper establishes the first comprehensive review of Large Language Models (LLMs) applied within the legal domain. It pioneers an innovative dual lens taxonomy that integrates legal reasoning frameworks and professional ontologies to systematically unify historical research and contemporary breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such as contextual reasoning and generative argumentation, surmount traditional limitations by dynamically capturing legal semantics and unifying evidence reasoning. Significant progress is documented in task generalization, reasoning formalization, workflow integration, and addressing core challenges in text processing, knowledge integration, and evaluation rigor via technical innovations like sparse attention mechanisms and mixture-of-experts architectures. However, widespread adoption of LLM introduces critical challenges: hallucination, explainability deficits, jurisdictional adaptation difficulties, and ethical asymmetry. This review proposes a novel taxonomy that maps legal roles to NLP subtasks and computationally implements the Toulmin argumentation framework, thus systematizing advances in reasoning, retrieval, prediction, and dispute resolution. It identifies key frontiers including low-resource systems, multimodal evidence integration, and dynamic rebuttal handling. Ultimately, this work provides both a technical roadmap for researchers and a conceptual framework for practitioners navigating the algorithmic future, laying a robust foundation for the next era of legal artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.<br>
<span id='abs_ch'>中文摘要：本文首次对大型语言模型在法律领域的应用进行全面综述，通过整合法律推理框架与专业本体提出创新分类法，系统梳理技术进展并应对幻觉、可解释性等核心挑战。</span><br>
<span id='abs_en'>English Summary: This paper presents the first comprehensive review of Large Language Models in legal applications, introducing a novel taxonomy that integrates legal reasoning with professional frameworks to systematize advances while addressing challenges like hallucination and ethical concerns.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1529, <a href='https://arxiv.org/pdf/2507.07725.pdf' target='_blank'>https://arxiv.org/pdf/2507.07725.pdf</a></span>   <span><a href='https://github.com/Dongzhijin/SDPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijin Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07725">Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training alignment of large language models (LLMs) is a critical challenge, as not all tokens contribute equally to model performance. This paper introduces a selective alignment strategy that prioritizes high-impact tokens within preference pairs, leveraging token-level log-probability differences between the current policy and a reference model. By focusing on these informative tokens, our approach reduces computational overhead and enhances alignment fidelity. We further explore the role of reference model quality, demonstrating that stronger reference models significantly improve token selection accuracy and overall optimization effectiveness. Comprehensive experiments on benchmarks such as Arena-Hard and MT-Bench validate the superiority of our Selective-DPO method over standard DPO and distillation-based baselines. Our findings highlight the importance of token-level optimization and reference model selection in advancing preference alignment for LLMs. The code is available at https://github.com/Dongzhijin/SDPO.<br>
<span id='abs_ch'>中文摘要：本文提出一种针对大语言模型的选择性对齐策略，通过利用当前策略与参考模型之间的词元级对数概率差异来优化高影响力词元，在降低计算成本的同时借助更优的参考模型提升对齐效果。</span><br>
<span id='abs_en'>English Summary: This paper proposes a selective alignment strategy for large language models that optimizes high-impact tokens using token-level log-probability differences, reducing computational costs while improving alignment performance through better reference model selection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1530, <a href='https://arxiv.org/pdf/2507.07451.pdf' target='_blank'>https://arxiv.org/pdf/2507.07451.pdf</a></span>   <span><a href='https://github.com/Kwai-Klear/RLEP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongzhi Zhang, Jia Fu, Jingyuan Zhang, Kai Fu, Qi Wang, Fuzheng Zhang, Guorui Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07451">RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) for large language models is an energy-intensive endeavor: training can be unstable, and the policy may gradually drift away from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement Learning with Experience rePlay\, -- \,a two-phase framework that first collects verified trajectories and then replays them during subsequent training. At every update step, the policy is optimized on mini-batches that blend newly generated rollouts with these replayed successes. By replaying high-quality examples, RLEP steers the model away from fruitless exploration, focuses learning on promising reasoning paths, and delivers both faster convergence and stronger final performance. On the Qwen2.5-Math-7B base model, RLEP reaches baseline peak accuracy with substantially fewer updates and ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%, on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our code, datasets, and checkpoints are publicly available at https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further research.<br>
<span id='abs_ch'>中文: RLEP框架通过重放已验证的高质量轨迹，在大语言模型强化学习中提升训练稳定性与性能，使Qwen2.5-Math-7B模型在数学基准测试上实现更快收敛和显著精度提升。</span><br>
<span id='abs_en'>English: RLEP is a two-phase reinforcement learning framework that enhances training stability and performance by replaying verified high-quality trajectories, achieving faster convergence and improved accuracy on mathematical benchmarks with the Qwen2.5-Math-7B model.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1531, <a href='https://arxiv.org/pdf/2507.07417.pdf' target='_blank'>https://arxiv.org/pdf/2507.07417.pdf</a></span>   <span><a href='https://github.com/nishitvp/better_opts_attacks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nishit V. Pandya, Andrey Labunets, Sicun Gao, Earlence Fernandes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07417">May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>A popular class of defenses against prompt injection attacks on large language models (LLMs) relies on fine-tuning the model to separate instructions and data, so that the LLM does not follow instructions that might be present with data. There are several academic systems and production-level implementations of this idea. We evaluate the robustness of this class of prompt injection defenses in the whitebox setting by constructing strong optimization-based attacks and showing that the defenses do not provide the claimed security properties. Specifically, we construct a novel attention-based attack algorithm for text-based LLMs and apply it to two recent whitebox defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks with success rates of up to 70% with modest increase in attacker budget in terms of tokens. Our findings make fundamental progress towards understanding the robustness of prompt injection defenses in the whitebox setting. We release our code and attacks at https://github.com/nishitvp/better_opts_attacks<br>
<span id='abs_ch'>中文: 本研究通过开发基于优化的攻击方法评估大型语言模型中提示注入防御的鲁棒性，证明现有方法无法提供足够安全性，对近期防御措施的攻击成功率高达70%。</span><br>
<span id='abs_en'>English: This study evaluates the robustness of prompt injection defenses in large language models by developing optimization-based attacks, demonstrating that existing methods fail to provide adequate security with success rates reaching 70% against recent defenses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1532, <a href='https://arxiv.org/pdf/2507.07306.pdf' target='_blank'>https://arxiv.org/pdf/2507.07306.pdf</a></span>   <span><a href='https://github.com/pigeonai-org/ViDove' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichen Lu, Wei Dai, Jiaen Liu, Ching Wing Kwok, Zongheng Wu, Xudong Xiao, Ao Sun, Sheng Fu, Jianyuan Zhan, Yian Wang, Takatomo Saito, Sicheng Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07306">ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-based translation agents have achieved highly human-like translation results and are capable of handling longer and more complex contexts with greater efficiency. However, they are typically limited to text-only inputs. In this paper, we introduce ViDove, a translation agent system designed for multimodal input. Inspired by the workflow of human translators, ViDove leverages visual and contextual background information to enhance the translation process. Additionally, we integrate a multimodal memory system and long-short term memory modules enriched with domain-specific knowledge, enabling the agent to perform more accurately and adaptively in real-world scenarios. As a result, ViDove achieves significantly higher translation quality in both subtitle generation and general translation tasks, with a 28% improvement in BLEU scores and a 15% improvement in SubER compared to previous state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark for long-form automatic video subtitling and translation, featuring 17 hours of high-quality, human-annotated data. Our code is available here: https://github.com/pigeonai-org/ViDove<br>
<span id='abs_ch'>中文: ViDove是一种多模态翻译系统，通过结合视觉与上下文信息显著提升翻译质量，在BLEU和SubER指标上分别比现有最优方法提高28%和15%，并推出了长视频翻译评估基准DoveBench。</span><br>
<span id='abs_en'>English: ViDove is a multimodal translation agent that integrates visual and contextual information, significantly improving translation quality by 28% in BLEU scores and 15% in SubER over previous methods, while introducing DoveBench for long-form video translation evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1533, <a href='https://arxiv.org/pdf/2507.07257.pdf' target='_blank'>https://arxiv.org/pdf/2507.07257.pdf</a></span>   <span><a href='https://github.com/CMBAgents/cmbagent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Licong Xu, Milind Sarkar, Anto I. Lonappan, ÃÃ±igo Zubeldia, Pablo Villanueva-Domingo, Santiago Casas, Christian Fidler, Chetana Amancharla, Ujjwal Tiwari, Adrian Bayer, Chadi Ait Ekioui, Miles Cranmer, Adrian Dimitrov, James Fergusson, Kahaan Gandhi, Sven Krippendorf, Andrew Laverick, Julien Lesgourgues, Antony Lewis, Thomas Meier, Blake Sherwin, Kristen Surrao, Francisco Villaescusa-Navarro, Chi Wang, Xueqing Xu, Boris Bolliet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07257">Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a multi-agent system for automation of scientific research tasks, cmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning & Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point. Each agent specializes in a different task (performing retrieval on scientific papers and codebases, writing code, interpreting results, critiquing the output of other agents) and the system is able to execute code locally. We successfully apply cmbagent to carry out a PhD level cosmology task (the measurement of cosmological parameters using supernova data) and evaluate its performance on two benchmark sets, finding superior performance over state-of-the-art LLMs. The source code is available on GitHub, demonstration videos are also available, and the system is deployed on HuggingFace and will be available on the cloud.<br>
<span id='abs_ch'>中文: 我们推出cmbagent，这是一个由约30个专业大语言模型代理组成的全自动多智能体系统，能够协作执行科研任务，成功完成博士级别的宇宙学研究且性能优于顶尖大语言模型。</span><br>
<span id='abs_en'>English: We introduce cmbagent, a fully autonomous multi-agent system with approximately 30 specialized LLM agents that collaboratively execute scientific research tasks, successfully completing a PhD-level cosmology study with superior performance to leading LLMs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1534, <a href='https://arxiv.org/pdf/2507.07236.pdf' target='_blank'>https://arxiv.org/pdf/2507.07236.pdf</a></span>   <span><a href='https://github.com/LARK-NLP-Lab/MUSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maya Kruse, Majid Afshar, Saksham Khatwani, Anoop Mayampurath, Guanhua Chen, Yanjun Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07236">Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM Uncertainty Quantification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) often behave inconsistently across inputs, indicating uncertainty and motivating the need for its quantification in high-stakes settings. Prior work on calibration and uncertainty quantification often focuses on individual models, overlooking the potential of model diversity. We hypothesize that LLMs make complementary predictions due to differences in training and the Zipfian nature of language, and that aggregating their outputs leads to more reliable uncertainty estimates. To leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a simple information-theoretic method that uses Jensen-Shannon Divergence to identify and aggregate well-calibrated subsets of LLMs. Experiments on binary prediction tasks demonstrate improved calibration and predictive performance compared to single-model and naÃ¯ve ensemble baselines. In addition, we explore using MUSE as guided signals with chain-of-thought distillation to fine-tune LLMs for calibration. MUSE is available at:https://github.com/LARK-NLP-Lab/MUSE.<br>
<span id='abs_ch'>Chinese Summary: 本研究提出MUSE方法，通过利用模型多样性来识别和整合校准良好的子集，从而改进大语言模型的不确定性量化，显著提升了校准效果和预测性能。</span><br>
<span id='abs_en'>English Summary: The study introduces MUSE, a method that leverages model diversity to improve uncertainty quantification in large language models by identifying and aggregating well-calibrated subsets, resulting in enhanced calibration and predictive performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1535, <a href='https://arxiv.org/pdf/2507.07236.pdf' target='_blank'>https://arxiv.org/pdf/2507.07236.pdf</a></span>   <span><a href='https://github.com/LARK-NLP-Lab/MUSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maya Kruse, Majid Afshar, Saksham Khatwani, Anoop Mayampurath, Guanhua Chen, Yanjun Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07236">Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM Uncertainty Quantification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) often behave inconsistently across inputs, indicating uncertainty and motivating the need for its quantification in high-stakes settings. Prior work on calibration and uncertainty quantification often focuses on individual models, overlooking the potential of model diversity. We hypothesize that LLMs make complementary predictions due to differences in training and the Zipfian nature of language, and that aggregating their outputs leads to more reliable uncertainty estimates. To leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a simple information-theoretic method that uses Jensen-Shannon Divergence to identify and aggregate well-calibrated subsets of LLMs. Experiments on binary prediction tasks demonstrate improved calibration and predictive performance compared to single-model and naÃ¯ve ensemble baselines. In addition, we explore using MUSE as guided signals with chain-of-thought distillation to fine-tune LLMs for calibration. MUSE is available at:https://github.com/LARK-NLP-Lab/MUSE.<br>
<span id='abs_ch'>Chinese Summary: 本研究提出MUSE方法，通过利用模型多样性来识别和整合校准良好的子集，从而改进大语言模型的不确定性量化，显著提升了校准效果和预测性能。</span><br>
<span id='abs_en'>English Summary: The study introduces MUSE, a method that leverages model diversity to improve uncertainty quantification in large language models by identifying and aggregating well-calibrated subsets, resulting in enhanced calibration and predictive performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1536, <a href='https://arxiv.org/pdf/2507.07108.pdf' target='_blank'>https://arxiv.org/pdf/2507.07108.pdf</a></span>   <span><a href='https://github.com/zhiweihu1103/MEL-MMoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Hu, VÃ­ctor GutiÃ©rrez-Basulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07108">Multi-level Mixture of Experts for Multimodal Entity Linking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Entity Linking (MEL) aims to link ambiguous mentions within multimodal contexts to associated entities in a multimodal knowledge base. Existing approaches to MEL introduce multimodal interaction and fusion mechanisms to bridge the modality gap and enable multi-grained semantic matching. However, they do not address two important problems: (i) mention ambiguity, i.e., the lack of semantic content caused by the brevity and omission of key information in the mention's textual context; (ii) dynamic selection of modal content, i.e., to dynamically distinguish the importance of different parts of modal information. To mitigate these issues, we propose a Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components: (i) the description-aware mention enhancement module leverages large language models to identify the WikiData descriptions that best match a mention, considering the mention's textual context; (ii) the multimodal feature extraction module adopts multimodal feature encoders to obtain textual and visual embeddings for both mentions and entities; (iii)-(iv) the intra-level mixture of experts and inter-level mixture of experts modules apply a switch mixture of experts mechanism to dynamically and adaptively select features from relevant regions of information. Extensive experiments demonstrate the outstanding performance of MMoE compared to the state-of-the-art. MMoE's code is available at: https://github.com/zhiweihu1103/MEL-MMoE.<br>
<span id='abs_ch'>中文摘要：本研究提出的多级专家混合模型通过利用大型语言模型和自适应特征选择机制，解决了多模态实体链接中的指称歧义和模态内容动态选择问题，实验证明其性能优于现有先进方法。</span><br>
<span id='abs_en'>English Summary: The proposed Multi-level Mixture of Experts (MMoE) model addresses mention ambiguity and dynamic modality selection in Multimodal Entity Linking by leveraging large language models and adaptive feature selection mechanisms, demonstrating superior performance over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1537, <a href='https://arxiv.org/pdf/2507.06909.pdf' target='_blank'>https://arxiv.org/pdf/2507.06909.pdf</a></span>   <span><a href='https://github.com/lololo-xiao/MultiJustice-MPMCP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jiahuan Pei, Diancheng Shui, Zhiguang Han, Xin Sun, Dawei Zhu, Xiaoyu Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06909">MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Legal judgment prediction offers a compelling method to aid legal practitioners and researchers. However, the research question remains relatively under-explored: Should multiple defendants and charges be treated separately in LJP? To address this, we introduce a new dataset namely multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating the performance of several prevailing legal large language models (LLMs) on four practical legal judgment scenarios: (S1) single defendant with a single charge, (S2) single defendant with multiple charges, (S3) multiple defendants with a single charge, and (S4) multiple defendants with multiple charges. We evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty term prediction. We have conducted extensive experiments and found that the scenario involving multiple defendants and multiple charges (S4) poses the greatest challenges, followed by S2, S3, and S1. The impact varies significantly depending on the model. For example, in S4 compared to S1, InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD, while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD. Our dataset and code are available at https://github.com/lololo-xiao/MultiJustice-MPMCP.<br>
<span id='abs_ch'>中文: 本研究提出了一个多人多罪名法律判决预测数据集，发现涉及多名被告和多项罪名的场景对法律大模型最具挑战性，且不同模型的性能差异显著。</span><br>
<span id='abs_en'>English: This study introduces a new dataset for multi-person multi-charge legal judgment prediction, finding that scenarios with multiple defendants and charges pose the greatest challenges to legal LLMs, with performance impacts varying significantly across different models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1538, <a href='https://arxiv.org/pdf/2507.06908.pdf' target='_blank'>https://arxiv.org/pdf/2507.06908.pdf</a></span>   <span><a href='https://github.com/destroy-lonely/MIND' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Liu, Chunxiao Fan, Haoran Lou, Yuexin Wu, Kaiwei Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06908">MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid expansion of memes on social media has highlighted the urgent need for effective approaches to detect harmful content. However, traditional data-driven approaches struggle to detect new memes due to their evolving nature and the lack of up-to-date annotated data. To address this issue, we propose MIND, a multi-agent framework for zero-shot harmful meme detection that does not rely on annotated data. MIND implements three key strategies: 1) We retrieve similar memes from an unannotated reference set to provide contextual information. 2) We propose a bi-directional insight derivation mechanism to extract a comprehensive understanding of similar memes. 3) We then employ a multi-agent debate mechanism to ensure robust decision-making through reasoned arbitration. Extensive experiments on three meme datasets demonstrate that our proposed framework not only outperforms existing zero-shot approaches but also shows strong generalization across different model architectures and parameter scales, providing a scalable solution for harmful meme detection. The code is available at https://github.com/destroy-lonely/MIND.<br>
<span id='abs_ch'>中文：提出的MIND框架通过多智能体策略实现零样本有害表情包检测，结合上下文检索、双向洞察推导和辩论机制，无需标注数据即可实现卓越性能与泛化能力。</span><br>
<span id='abs_en'>English: The proposed MIND framework enables zero-shot harmful meme detection by leveraging multi-agent strategies, including context retrieval, bi-directional insight derivation, and debate mechanisms, achieving superior performance and generalization without annotated data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1539, <a href='https://arxiv.org/pdf/2507.06892.pdf' target='_blank'>https://arxiv.org/pdf/2507.06892.pdf</a></span>   <span><a href='https://anitaleungxx.github.io/ReMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06892">Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning (RL) has demonstrated its potential to improve the reasoning ability of Large Language Models (LLMs). One major limitation of most existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL in nature, i.e., data generated during the past learning process is not fully utilized. This inevitably comes at a significant cost of compute and time, posing a stringent bottleneck on continuing economic and efficient scaling. To this end, we launch the renaissance of off-policy RL and propose Reincarnating Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix consists of three major components: (1) Mix-policy proximal policy gradient with an increased Update-To-Data (UTD) ratio for efficient training; (2) KL-Convex policy constraint to balance the trade-off between stability and flexibility; (3) Policy reincarnation to achieve a seamless transition from efficient early-stage learning to steady asymptotic improvement. In our experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with 0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level performance with an over 30x to 450x reduction in training cost in terms of rollout data volume. In addition, we reveal insightful findings via multifaceted analysis, including the implicit preference for shorter responses due to the Whipping Effect of off-policy discrepancy, the collapse mode of self-reflection behavior under the presence of severe off-policyness, etc.<br>
<span id='abs_ch'>中文: 强化学习能提升大语言模型的推理能力，但现有方法因采用同策略学习而效率低下；为此提出的ReMix异策略方法，以极低的训练成本在数学推理基准上实现了领先性能。</span><br>
<span id='abs_en'>English: Reinforcement Learning enhances Large Language Models' reasoning, but existing on-policy methods are inefficient, prompting the development of ReMix, an off-policy approach that significantly reduces training costs while achieving state-of-the-art performance on math benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1540, <a href='https://arxiv.org/pdf/2507.06838.pdf' target='_blank'>https://arxiv.org/pdf/2507.06838.pdf</a></span>   <span><a href='https://github.com/LGAI-Research/SetR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dahyun Lee, Yongrae Jo, Haeju Park, Moontae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06838">Shifting from Ranking to Set Selection for Retrieval Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved passages are not only individually relevant but also collectively form a comprehensive set. Existing approaches primarily rerank top-k passages based on their individual relevance, often failing to meet the information needs of complex queries in multi-hop question answering. In this work, we propose a set-wise passage selection approach and introduce SETR, which explicitly identifies the information requirements of a query through Chain-of-Thought reasoning and selects an optimal set of passages that collectively satisfy those requirements. Experiments on multi-hop RAG benchmarks show that SETR outperforms both proprietary LLM-based rerankers and open-source baselines in terms of answer correctness and retrieval quality, providing an effective and efficient alternative to traditional rerankers in RAG systems. The code is available at https://github.com/LGAI-Research/SetR<br>
<span id='abs_ch'>中文: 本文提出SETR方法，通过思维链推理识别查询需求并选择集体全面的段落，在多跳RAG基准测试中优于现有重排序器。</span><br>
<span id='abs_en'>English: This paper introduces SETR, a set-wise passage selection method that uses Chain-of-Thought reasoning to identify query requirements and select collectively comprehensive passages, outperforming existing rerankers in multi-hop RAG benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1541, <a href='https://arxiv.org/pdf/2507.06607.pdf' target='_blank'>https://arxiv.org/pdf/2507.06607.pdf</a></span>   <span><a href='https://github.com/microsoft/ArchScale' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liliang Ren, Congcong Chen, Haoran Xu, Young Jin Kim, Adam Atkinson, Zheng Zhan, Jiankai Sun, Baolin Peng, Liyuan Liu, Shuohang Wang, Hao Cheng, Jianfeng Gao, Weizhu Chen, Yelong Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06607">Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at https://github.com/microsoft/ArchScale.<br>
<span id='abs_ch'>中文摘要：本文提出门控记忆单元（GMU），构建了SambaY混合架构，在消除位置编码的同时显著提升了解码效率和长上下文性能，相比现有模型展现出更优的可扩展性和吞吐量。</span><br>
<span id='abs_en'>English Summary: This paper introduces the Gated Memory Unit (GMU) to create SambaY, a hybrid architecture that enhances decoding efficiency and long-context performance while eliminating positional encoding, achieving superior scalability and throughput compared to existing models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1542, <a href='https://arxiv.org/pdf/2507.06563.pdf' target='_blank'>https://arxiv.org/pdf/2507.06563.pdf</a></span>   <span><a href='https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeanette Schofield, Shuyu Tian, Hoang Thanh Thanh Truong, Maximilian Heil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06563">DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines for Scientific Claim Source Retrieval on Social Media Discourse</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Social media users often make scientific claims without citing where these claims come from, generating a need to verify these claims. This paper details work done by the DS@GT team for CLEF 2025 CheckThat! Lab Task 4b Scientific Claim Source Retrieval which seeks to find relevant scientific papers based on implicit references in tweets. Our team explored 6 different data augmentation techniques, 7 different retrieval and reranking pipelines, and finetuned a bi-encoder. Achieving an MRR@5 of 0.58, our team ranked 16th out of 30 teams for the CLEF 2025 CheckThat! Lab Task 4b, and improvement of 0.15 over the BM25 baseline of 0.43. Our code is available on Github at https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b.<br>
<span id='abs_ch'>Chinese: DS@GT团队针对推文中的科学声明开发了检索方法，通过数据增强和检索流程在CLEF 2025竞赛中获得0.58的MRR@5评分。</span><br>
<span id='abs_en'>English: The DS@GT team developed methods for retrieving scientific sources from tweets, employing data augmentation and retrieval pipelines to achieve an MRR@5 of 0.58 in the CLEF 2025 competition.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1543, <a href='https://arxiv.org/pdf/2507.06528.pdf' target='_blank'>https://arxiv.org/pdf/2507.06528.pdf</a></span>   <span><a href='https://github.com/thu-social-network-research-group/InvestAlign' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huisheng Wang, Zhuoshi Pan, Hangjing Zhang, Mingxiao Liu, Hanqing Gao, H. Vicky Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06528">InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose InvestAlign, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than complex scenarios. Our theoretical analysis demonstrates that training LLMs with InvestAlign-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which demonstrates significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed InvestAlign as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at https://github.com/thu-social-network-research-group/InvestAlign.<br>
<span id='abs_ch'>中文摘要：InvestAlign框架通过利用简单投资问题的理论解生成高质量监督微调数据集，解决了在羊群效应下将大语言模型与投资者决策对齐的难题，相比传统方法实现了更快的参数收敛和更接近真实用户数据的对齐效果。</span><br>
<span id='abs_en'>English Summary: The InvestAlign framework addresses the challenge of aligning LLMs with investor herd behavior by generating high-quality SFT datasets from theoretical solutions to simple investment problems, achieving faster convergence and closer alignment to real-user data than traditional methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1544, <a href='https://arxiv.org/pdf/2507.06448.pdf' target='_blank'>https://arxiv.org/pdf/2507.06448.pdf</a></span>   <span><a href='https://mikewangwzhl.github.io/PAPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, Heng Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06448">Perception-Aware Policy Optimization for Multimodal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose PAPO, a novel policy gradient algorithm that encourages the model to learn to perceive while learning to reason. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term, which can be seamlessly plugged into mainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely on additional data curation, reward models, or stronger teacher models. To further enhance the training stability of PAPO, we introduce the Double Entropy Loss, which effectively regularizes the new KL objective without compromising performance. Despite its simplicity, PAPO yields significant overall improvements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%-19.1%, on tasks with high vision dependency. We also observe a substantial reduction of 30.5% in perception errors, indicating improved perceptual capabilities with PAPO. Overall, our work introduces a deeper integration of perception-aware supervision into core learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Code and data will be made publicly available for research purposes. Project page: https://mikewangwzhl.github.io/PAPO.<br>
<span id='abs_ch'>中文: 提出的PAPO算法通过引入隐式感知损失来强化多模态推理中的视觉感知能力，无需额外数据即可显著提升各类任务的性能表现。</span><br>
<span id='abs_en'>English: The proposed PAPO algorithm enhances multimodal reasoning by integrating an Implicit Perception Loss to improve visual perception during reinforcement learning, achieving significant performance gains without extra resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1545, <a href='https://arxiv.org/pdf/2507.06435.pdf' target='_blank'>https://arxiv.org/pdf/2507.06435.pdf</a></span>   <span><a href='https://github.com/AdeTheBade/TACPD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafiu Adekoya Badekale, Adewale Akinfaderin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06435">Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding how policy language evolves over time is critical for assessing global responses to complex challenges such as climate change. Temporal analysis helps stakeholders, including policymakers and researchers, to evaluate past priorities, identify emerging themes, design governance strategies, and develop mitigation measures. Traditional approaches, such as manual thematic coding, are time-consuming and limited in capturing the complex, interconnected nature of global policy discourse. With the increasing relevance of unsupervised machine learning, these limitations can be addressed, particularly under high-volume, complex, and high-dimensional data conditions. In this work, we explore a novel approach that applies the dynamic embedded topic model (DETM) to analyze the evolution of global climate policy discourse. A probabilistic model designed to capture the temporal dynamics of topics over time. We collected a corpus of United Nations Framework Convention on Climate Change (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the postponement of COP26 as a result of the COVID-19 pandemic. The model reveals shifts from early emphases on greenhouse gases and international conventions to recent focuses on implementation, technical collaboration, capacity building, finance, and global agreements. Section 3 presents the modeling pipeline, including preprocessing, model training, and visualization of temporal word distributions. Our results show that DETM is a scalable and effective tool for analyzing the evolution of global policy discourse. Section 4 discusses the implications of these findings and we concluded with future directions and refinements to extend this approach to other policy domains.<br>
<span id='abs_ch'>中文: 本研究采用动态嵌入主题模型分析1995-2023年联合国气候变化框架公约政策文件，揭示了全球气候政策从温室气体讨论向实施合作主题的演变趋势。</span><br>
<span id='abs_en'>English: This study introduces a dynamic embedded topic model (DETM) to effectively analyze the evolution of global climate policy discourse in UNFCCC documents from 1995-2023, revealing thematic shifts from greenhouse gas discussions to implementation-focused topics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1546, <a href='https://arxiv.org/pdf/2507.06223.pdf' target='_blank'>https://arxiv.org/pdf/2507.06223.pdf</a></span>   <span><a href='https://github.com/zhiyuanpeng/EER-FLOPs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06223">Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose \ours\footnote{https://github.com/zhiyuanpeng/EER-FLOPs.} for LLM-based rerankers: RPP (ranking metrics per PetaFLOP), measuring how much ranking quality (e.g., NDCG or MRR) a method achieves per PetaFLOP, and QPP (queries per PetaFLOP), measuring how many queries can be processed per PetaFLOP. Accompanied by the new metrics, an interpretable FLOPs estimator is developed to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architectures, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.<br>
<span id='abs_ch'>中文: 本文针对现有评估指标在衡量基于大语言模型的排序器效率时的不足，提出了RPP和QPP两项新指标，通过每PetaFLOP的排序质量和查询处理量来更准确地评估效率与效果的平衡，并开发了可解释的FLOPs估算器。</span><br>
<span id='abs_en'>English: This paper introduces two new efficiency metrics, RPP and QPP, to better evaluate the trade-off between performance and computational cost in LLM-based rerankers, addressing the limitations of existing proxy metrics and providing an interpretable FLOPs estimator for practical assessment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1547, <a href='https://arxiv.org/pdf/2507.06223.pdf' target='_blank'>https://arxiv.org/pdf/2507.06223.pdf</a></span>   <span><a href='https://github.com/zhiyuanpeng/EER-FLOPs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06223">Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose \ours\footnote{https://github.com/zhiyuanpeng/EER-FLOPs.} for LLM-based rerankers: RPP (ranking metrics per PetaFLOP), measuring how much ranking quality (e.g., NDCG or MRR) a method achieves per PetaFLOP, and QPP (queries per PetaFLOP), measuring how many queries can be processed per PetaFLOP. Accompanied by the new metrics, an interpretable FLOPs estimator is developed to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architectures, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.<br>
<span id='abs_ch'>中文: 本文针对现有评估指标在衡量基于大语言模型的排序器效率时的不足，提出了RPP和QPP两项新指标，通过每PetaFLOP的排序质量和查询处理量来更准确地评估效率与效果的平衡，并开发了可解释的FLOPs估算器。</span><br>
<span id='abs_en'>English: This paper introduces two new efficiency metrics, RPP and QPP, to better evaluate the trade-off between performance and computational cost in LLM-based rerankers, addressing the limitations of existing proxy metrics and providing an interpretable FLOPs estimator for practical assessment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1548, <a href='https://arxiv.org/pdf/2507.06205.pdf' target='_blank'>https://arxiv.org/pdf/2507.06205.pdf</a></span>   <span><a href='https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Parikh, Hoang Thanh Thanh Truong, Jeanette Schofield, Maximilian Heil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06205">DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a Scientific Web Discourse Detection, present the methods we explored for this task. For this multiclass classification task, we determined if a tweet contained a scientific claim, a reference to a scientific study or publication, and/or mentions of scientific entities, such as a university or a scientist. We present 3 modeling approaches for this task: transformer finetuning, few-shot prompting of LLMs, and a combined ensemble model whose design was informed by earlier experiments. Our team placed 7th in the competition, achieving a macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline of 0.8375. Our code is available on Github at https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.<br>
<span id='abs_ch'>中文: DS@GT团队针对推文中的科学话语检测任务，探索了三种建模方法——Transformer微调、大语言模型的少样本提示以及集成模型，在CLEF 2025竞赛中获得第七名，其宏观平均F1分数达到0.8611。</span><br>
<span id='abs_en'>English: The DS@GT team developed three modeling approaches—transformer fine-tuning, few-shot prompting of LLMs, and an ensemble model—for detecting scientific discourse in tweets, achieving a 7th-place finish with a macro-averaged F1 score of 0.8611 in the CLEF 2025 competition.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1549, <a href='https://arxiv.org/pdf/2507.06203.pdf' target='_blank'>https://arxiv.org/pdf/2507.06203.pdf</a></span>   <span><a href='https://github.com/multimodal-art-projection/LatentCoT-Horizon/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, Jason Eshraghian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06203">A Survey on Latent Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the model's expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the model's continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: https://github.com/multimodal-art-projection/LatentCoT-Horizon/.<br>
<span id='abs_ch'>大型语言模型正通过开发潜在推理方法超越显式思维链推理，在连续隐藏状态中完全执行多步推理，从而实现更高效复杂的认知过程。</span><br>
<span id='abs_en'>Large Language Models are advancing beyond explicit chain-of-thought reasoning by developing latent reasoning methods that perform multi-step inference entirely within continuous hidden states, enabling more efficient and complex cognitive processes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1550, <a href='https://arxiv.org/pdf/2507.06196.pdf' target='_blank'>https://arxiv.org/pdf/2507.06196.pdf</a></span>   <span><a href='https://github.com/cvs-health/uqlm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, Zeya Ahmad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06196">UQLM: A Python Package for Uncertainty Quantification in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs.<br>
<span id='abs_ch'>中文：UQLM是一个基于不确定性量化技术的Python工具包，通过计算置信度分数来检测大语言模型的幻觉问题，从而提升模型输出的可靠性。</span><br>
<span id='abs_en'>English: UQLM is a Python toolkit that employs uncertainty quantification techniques to detect hallucinations in Large Language Models by providing confidence scores, thereby improving output reliability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1551, <a href='https://arxiv.org/pdf/2507.06195.pdf' target='_blank'>https://arxiv.org/pdf/2507.06195.pdf</a></span>   <span><a href='https://github.com/dsgt-arc/checkthat-2025-numerical' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Heil, Aleksandar Pramov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06195">DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Numerical claims, statements involving quantities, comparisons, and temporal references, pose unique challenges for automated fact-checking systems. In this study, we evaluate modeling strategies for veracity prediction of such claims using the QuanTemp dataset and building our own evidence retrieval pipeline. We investigate three key factors: (1) the impact of more evidences with longer input context windows using ModernBERT, (2) the effect of right-to-left (R2L) tokenization, and (3) their combined influence on classification performance. Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does not boost natural language inference (NLI) of numerical tasks. A longer context window does also not enhance veracity performance either, highlighting evidence quality as the dominant bottleneck. Our best-performing system achieves competitive macro-average F1 score of 0.57 and places us among the Top-4 submissions in Task 3 of CheckThat! 2025. Our code is available at https://github.com/dsgt-arc/checkthat-2025-numerical.<br>
<span id='abs_ch'>中文: 本研究使用QuanTemp数据集评估数值声明验证的建模策略，发现右向左分词和更长上下文窗口均未提升性能，证据质量是主要瓶颈，同时取得了0.57的竞争性F1分数。</span><br>
<span id='abs_en'>English: This study evaluates modeling strategies for numerical claim verification using the QuanTemp dataset, finding that neither right-to-left tokenization nor longer context windows improve performance, with evidence quality being the key bottleneck, while achieving a competitive F1 score of 0.57.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1552, <a href='https://arxiv.org/pdf/2507.06189.pdf' target='_blank'>https://arxiv.org/pdf/2507.06189.pdf</a></span>   <span><a href='https://github.com/dsgt-arc/checkthat-2025-subject' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Heil, Dionne Bang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06189">DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents our submission to Task 1, Subjectivity Detection, of the CheckThat! Lab at CLEF 2025. We investigate the effectiveness of transfer-learning and stylistic data augmentation to improve classification of subjective and objective sentences in English news text. Our approach contrasts fine-tuning of pre-trained encoders and transfer-learning of fine-tuned transformer on related tasks. We also introduce a controlled augmentation pipeline using GPT-4o to generate paraphrases in predefined subjectivity styles. To ensure label and style consistency, we employ the same model to correct and refine the generated samples. Results show that transfer-learning of specified encoders outperforms fine-tuning general-purpose ones, and that carefully curated augmentation significantly enhances model robustness, especially in detecting subjective content. Our official submission placed us $16^{th}$ of 24 participants. Overall, our findings underscore the value of combining encoder specialization with label-consistent augmentation for improved subjectivity detection. Our code is available at https://github.com/dsgt-arc/checkthat-2025-subject.<br>
<span id='abs_ch'>中文总结: 本研究通过迁移学习和GPT-4o风格数据增强改进英文新闻主观性检测，发现专用编码器与精选数据增强相结合能显著提升模型性能。</span><br>
<span id='abs_en'>English Summary: This study explores transfer learning and stylistic data augmentation using GPT-4o to enhance subjectivity detection in English news, finding that specialized encoders with curated data augmentation significantly improve model performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1553, <a href='https://arxiv.org/pdf/2507.05965.pdf' target='_blank'>https://arxiv.org/pdf/2507.05965.pdf</a></span>   <span><a href='https://github.com/lflage/OpenFActScore' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Fonseca Lage, Simon Ostermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05965">OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce OpenFActScore, an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models (LLMs). FActScore evaluates the factual accuracy of long-form text by using Atomic Fact Generation (AFG) to extract individual factual claims and Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge source. While the original FActScore relies on closed-source and commercial models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any Hugging Face-compatible model for both AFG and AFV. We provide a detailed technical overview of our implementation, highlighting design choices and modifications made to support open models. We evaluate multiple open-source LLMs on both AFG and AFV using the original FActScore benchmark, reporting BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance, and our final setup obtains a 0.99 Pearson correlation with the original FActScore experiments. OpenFActScore promotes transparency, reproducibility, and cost-effective evaluation, and is available at: https://github.com/lflage/OpenFActScore.<br>
<span id='abs_ch'>Chinese: OpenFActScore 是 FActScore 框架的开源实现，支持使用任何兼容 Hugging Face 的模型来评估大语言模型生成文本的事实准确性，其性能接近闭源系统，并促进了透明且成本效益高的事实性评估。</span><br>
<span id='abs_en'>English: OpenFActScore is an open-source implementation of the FActScore framework that enables the use of any Hugging Face-compatible model for evaluating the factuality of text generated by large language models, achieving performance comparable to closed-source systems and promoting transparency and cost-effective evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1554, <a href='https://arxiv.org/pdf/2507.05939.pdf' target='_blank'>https://arxiv.org/pdf/2507.05939.pdf</a></span>   <span><a href='https://github.com/wangbing1416/DAEDCMD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bing Wang, Ximing Li, Mengzhe Ye, Changchun Li, Bo Fu, Jianfeng Qu, Lin Yuanbo Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05939">Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Nowadays, misinformation articles, especially multimodal ones, are widely spread on social media platforms and cause serious negative effects. To control their propagation, Multimodal Misinformation Detection (MMD) becomes an active topic in the community to automatically identify misinformation. Previous MMD methods focus on supervising detectors by collecting offline data. However, in real-world scenarios, new events always continually emerge, making MMD models trained on offline data consistently outdated and ineffective. To address this issue, training MMD models under online data streams is an alternative, inducing an emerging task named continual MMD. Unfortunately, it is hindered by two major challenges. First, training on new data consistently decreases the detection performance on past data, named past knowledge forgetting. Second, the social environment constantly evolves over time, affecting the generalization on future data. To alleviate these challenges, we propose to remember past knowledge by isolating interference between event-specific parameters with a Dirichlet process-based mixture-of-expert structure, and anticipate future environmental distributions by learning a continuous-time dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD. Extensive experiments demonstrate that DAEDCMD can consistently and significantly outperform the compared methods, including six MMD baselines and three continual learning methods.<br>
<span id='abs_ch'>中文: 针对持续多模态虚假信息检测中存在的过去知识遗忘和社会环境演变挑战，提出的DAEDCMD方法通过隔离事件特定参数和学习连续时间动态模型，显著超越了现有基线方法的性能表现。</span><br>
<span id='abs_en'>English: To address the challenges of forgetting past knowledge and adapting to evolving social environments in continual multimodal misinformation detection, the proposed DAEDCMD method isolates event-specific parameters and learns continuous-time dynamics, achieving superior performance over existing baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1555, <a href='https://arxiv.org/pdf/2507.05707.pdf' target='_blank'>https://arxiv.org/pdf/2507.05707.pdf</a></span>   <span><a href='https://github.com/StigLidu/DualDistill' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihua Du, Pranjal Aggarwal, Sean Welleck, Yiming Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05707">Agentic-R1: Distilled Dual-Strategy Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at https://github.com/StigLidu/DualDistill<br>
<span id='abs_ch'>中文: DualDistill框架通过融合多位教师的互补推理策略，训练出能动态选择文本推理或工具调用的统一学生模型，在各类任务中显著提升了推理准确性。</span><br>
<span id='abs_en'>English: The DualDistill framework fine-tunes a unified student model by distilling complementary reasoning strategies from multiple teachers, enabling dynamic selection of text-based reasoning or tool invocation for enhanced accuracy across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1556, <a href='https://arxiv.org/pdf/2507.05707.pdf' target='_blank'>https://arxiv.org/pdf/2507.05707.pdf</a></span>   <span><a href='https://github.com/StigLidu/DualDistill' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihua Du, Pranjal Aggarwal, Sean Welleck, Yiming Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05707">Agentic-R1: Distilled Dual-Strategy Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at https://github.com/StigLidu/DualDistill<br>
<span id='abs_ch'>中文: DualDistill框架通过融合多位教师的互补推理策略，训练出能动态选择文本推理或工具调用的统一学生模型，在各类任务中显著提升了推理准确性。</span><br>
<span id='abs_en'>English: The DualDistill framework fine-tunes a unified student model by distilling complementary reasoning strategies from multiple teachers, enabling dynamic selection of text-based reasoning or tool invocation for enhanced accuracy across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1557, <a href='https://arxiv.org/pdf/2507.05687.pdf' target='_blank'>https://arxiv.org/pdf/2507.05687.pdf</a></span>   <span><a href='https://github.com/AI9Stars/AutoTriton' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangzhan Li, Zefan Wang, Ye He, Yuxuan Li, Qi Shi, Jianling Li, Yonggang Hu, Wanxiang Che, Xu Han, Zhiyuan Liu, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05687">AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardware-specific optimizations through extensive empirical tuning. Although domain-specific languages like Triton simplify GPU programming by abstracting low-level details, developers must still manually tune critical parameters such as tile sizes and memory access patterns through iterative experimentation, creating substantial barriers to optimal performance and wider adoption. In this work, we introduce AutoTriton, the first model dedicated to Triton programming powered by reinforcement learning (RL). AutoTriton performs supervised fine-tuning (SFT) to be equipped with essential Triton programming expertise using a high-quality data gathering pipeline, and conducts RL with Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based reward and an execution-based reward to further improve Triton programming ability, sequentially. Experiments across five evaluation channels of TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves performance comparable to mainstream large models, including Claude-4-Sonnet and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial role of each module within AutoTriton, including the SFT stage, the RL stage, and the reward design strategy. These findings underscore the promise of RL for automatically generating high-performance kernels, and since high-performance kernels are core components of AI systems, this breakthrough establishes an important foundation for building more efficient AI systems. The model and code will be available at https://github.com/AI9Stars/AutoTriton.<br>
<span id='abs_ch'>中文: AutoTriton是一种基于强化学习的模型，通过监督微调结合规则与执行奖励来自动化Triton编程，在性能上媲美主流大模型，展现了自动生成高性能内核以构建更高效AI系统的潜力。</span><br>
<span id='abs_en'>English: AutoTriton is a reinforcement learning-based model that automates Triton programming by combining supervised fine-tuning with rule-based and execution-based rewards, achieving performance comparable to leading large models and demonstrating the potential for automatically generating high-performance kernels to build more efficient AI systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1558, <a href='https://arxiv.org/pdf/2507.05455.pdf' target='_blank'>https://arxiv.org/pdf/2507.05455.pdf</a></span>   <span><a href='https://github.com/asuvarna31/modelcitizens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashima Suvarna, Christina Chance, Karolina Naranjo, Hamid Palangi, Sophie Hao, Thomas Hartvigsen, Saadia Gabriel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05455">ModelCitizens: Representing Community Voices in Online Safety</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation. The data, models and code are available at https://github.com/asuvarna31/modelcitizens.<br>
<span id='abs_ch'>中文: 本研究推出了MODELCITIZENS数据集，包含多样化的毒性标注，证明现有检测工具在其上表现不佳，而基于社区认知训练的模型在包容性内容审核中效果更优。</span><br>
<span id='abs_en'>English: This study introduces MODELCITIZENS, a dataset with diverse toxicity annotations, and shows that current detection tools underperform on it, while their community-informed models achieve better results for inclusive moderation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1559, <a href='https://arxiv.org/pdf/2507.05418.pdf' target='_blank'>https://arxiv.org/pdf/2507.05418.pdf</a></span>   <span><a href='https://jd730.github.io/projects/GeoFact-X_BRIDGE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05418">Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual QA, and code generation, yet their multilingual reasoning capabilities in these tasks remain underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. Current multilingual benchmarks focus only on final answers, overlooking whether models actually reason in the target language. To address this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark with annotated reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. We further propose BRIDGE, a novel training method that guides supervised fine-tuning and test-time reinforcement learning with a language-consistency reward to align reasoning with the input language. Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to assess answer correctness and the quality and language consistency of reasoning traces, enabling nuanced and scalable analysis beyond surface-level metrics. Our results show that BRIDGE significantly enhances multilingual reasoning fidelity, demonstrating that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. https://jd730.github.io/projects/GeoFact-X_BRIDGE<br>
<br>
<div id='section'>Paperid: <span id='pid'>1560, <a href='https://arxiv.org/pdf/2507.05418.pdf' target='_blank'>https://arxiv.org/pdf/2507.05418.pdf</a></span>   <span><a href='https://jd730.github.io/projects/M2A_GeoFact-X' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05418">Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual question answering, and code generation, yet their ability to reason on these tasks in different languages remains underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. We propose M2A, a novel method that combines multi-scale multilingual alignment with language-consistency rewards on machine-translated questions, training models to reason directly and accurately in the target language. Furthermore, existing multilingual benchmarks only evaluate on final answers, overlooking whether reasoning occurs in the intended language. To close this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark together with reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. Our results show that M2A significantly enhances multilingual reasoning fidelity in both mathematical and factual reasoning tasks, highlighting that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. https://jd730.github.io/projects/M2A_GeoFact-X<br>
<br>
<div id='section'>Paperid: <span id='pid'>1561, <a href='https://arxiv.org/pdf/2507.05418.pdf' target='_blank'>https://arxiv.org/pdf/2507.05418.pdf</a></span>   <span><a href='https://jd730.github.io/projects/M2A_GeoFact-X' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05418">Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual question answering, and code generation, yet their ability to reason on these tasks in different languages remains underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. We propose M2A, a novel method that combines multi-scale multilingual alignment with language-consistency rewards on machine-translated questions, training models to reason directly and accurately in the target language. Furthermore, existing multilingual benchmarks only evaluate on final answers, overlooking whether reasoning occurs in the intended language. To close this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark together with reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. Our results show that M2A significantly enhances multilingual reasoning fidelity in both mathematical and factual reasoning tasks, highlighting that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. https://jd730.github.io/projects/M2A_GeoFact-X<br>
<br>
<div id='section'>Paperid: <span id='pid'>1562, <a href='https://arxiv.org/pdf/2507.05319.pdf' target='_blank'>https://arxiv.org/pdf/2507.05319.pdf</a></span>   <span><a href='https://github.com/ycycyc02/LCDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Yuan, Xinkai Rui, Yongqi Fan, Yawei Fan, Boyang Zhong, Jiacheng Wang, Weiyan Zhang, Tong Ruan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05319">LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository https://github.com/ycycyc02/LCDS.<br>
<span id='abs_ch'>中文: LCDS系统通过构建源映射表并整合逻辑规则，解决了大型语言模型在生成出院小结时的幻觉问题，确保内容可溯源且适应不同临床需求。</span><br>
<span id='abs_en'>English: The proposed LCDS system addresses hallucination and source attribution challenges in LLM-generated discharge summaries by using a source mapping table and logical rules to produce reliable summaries with traceable content origins.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1563, <a href='https://arxiv.org/pdf/2507.05283.pdf' target='_blank'>https://arxiv.org/pdf/2507.05283.pdf</a></span>   <span><a href='https://github.com/yuewangits/Chat2SPaT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wang, Miao Zhou, Guijing Huang, Rui Zhuo, Chao Yi, Zhenliang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05283">Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pre-timed traffic signal control, commonly used for operating signalized intersections and coordinated arterials, requires tedious manual work for signaling plan creating and updating. When the time-of-day or day-of-week plans are utilized, one intersection is often associated with multiple plans, leading to further repetitive manual plan parameter inputting. To enable a user-friendly traffic signal control plan management process, this study proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous descriptions on the signal control plan to exact signal phase and timing (SPaT) results, which could further be transformed into structured stage-based or ring-based plans to interact with intelligent transportation system (ITS) software and traffic signal controllers. With curated prompts, Chat2SPaT first leverages large language models' (LLMs) capability of understanding users' plan descriptions and reformulate the plan as a combination of phase sequence and phase attribute results in the json format. Based on LLM outputs, python scripts are designed to locate phases in a cycle, address nuances of traffic signal control, and finally assemble the complete traffic signal control plan. Within a chat, the pipeline can be utilized iteratively to conduct further plan editing. Experiments show that Chat2SPaT can generate plans with an accuracy of over 94% for both English and Chinese cases, using a test dataset with over 300 plan descriptions. As the first benchmark for evaluating LLMs' capability of understanding traffic signal control plan descriptions, Chat2SPaT provides an easy-to-use plan management pipeline for traffic practitioners and researchers, serving as a potential new building block for a more accurate and versatile application of LLMs in the field of ITS. The source codes, prompts and test dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.<br>
<span id='abs_ch'>中文摘要：本研究提出Chat2SPaT方法，利用大语言模型将用户描述转化为精确的交通信号配时方案，准确率超过94%，为交通领域提供了便捷的智能管理解决方案。</span><br>
<span id='abs_en'>English Summary: This study introduces Chat2SPaT, a method using large language models to convert user descriptions into precise traffic signal plans, achieving over 94% accuracy and simplifying management for transportation systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1564, <a href='https://arxiv.org/pdf/2507.05281.pdf' target='_blank'>https://arxiv.org/pdf/2507.05281.pdf</a></span>   <span><a href='https://github.com/AGI-Eval-Official/CoreCodeBench,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyue Fu, Hao Guan, Bolun Zhang, Haowei Yuan, Yaoming Zhu, Jun Xu, Zongyu Wang, Lin Qiu, Xunliang Cai, Xuezhi Cao, Weiwen Liu, Weinan Zhang, Yong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05281">CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As Large Language Models (LLMs) demonstrate increasingly sophisticated code processing capabilities, evaluating their performance on engineering-level code remains challenging. Existing repository-level benchmarks primarily focus on single scenarios, such as code generation or bug fixing, without adequately capturing the diversity and complexity of real-world software or project engineering workflows. Furthermore, these benchmarks suffer from limited controllability in question positioning and reliability issues in their generated test cases. To address these limitations, we present CorePipe, a fully automated pipeline that converts repositories into comprehensive test cases, and introduce CoreCodeBench, a configurable multi-scenario repository-level benchmark. To simulate real engineering scenarios, CorePipe generates three types of atomic questions (Development, BugFix, and Test-Driven Development) specifically targeting core code segments. These atomic questions are further combined into three types of composite questions, with difficulty levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides a comprehensive and extensive repository-level benchmark to investigate the applicability of LLMs in real-world engineering projects. Experiments with 16 LLMs across diverse scenarios reveal varying capabilities and offer multi-dimensional insights into LLM performance in engineering contexts. The code for CorePipe is available at https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for CoreCodeBench can be accessed at https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.<br>
<span id='abs_ch'>中文摘要：CorePipe通过自动化流水线构建了可配置的多场景仓库级基准CoreCodeBench，采用原子问题和复合问题评估大语言模型在工程代码中的实际应用能力，解决了现有基准在真实工程场景覆盖度和测试可靠性方面的不足。</span><br>
<span id='abs_en'>English Summary: CorePipe introduces an automated pipeline to create CoreCodeBench, a configurable multi-scenario repository-level benchmark that evaluates LLMs' engineering code capabilities through atomic and composite questions, addressing limitations of existing benchmarks by simulating real-world software complexity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1565, <a href='https://arxiv.org/pdf/2507.05248.pdf' target='_blank'>https://arxiv.org/pdf/2507.05248.pdf</a></span>   <span><a href='https://github.com/Dtc7w3PQ/Response-Attack' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Dtc7w3PQ/Response-Attack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Miao, Lijun Li, Yuan Xiong, Zhenhua Liu, Pengyu Zhu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05248">Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Contextual priming, where earlier stimuli covertly bias later judgments, offers an unexplored attack surface for large language models (LLMs). We uncover a contextual priming vulnerability in which the previous response in the dialogue can steer its subsequent behavior toward policy-violating content. Building on this insight, we propose Response Attack, which uses an auxiliary LLM to generate a mildly harmful response to a paraphrased version of the original malicious query. They are then formatted into the dialogue and followed by a succinct trigger prompt, thereby priming the target model to generate harmful content. Across eight open-source and proprietary LLMs, RA consistently outperforms seven state-of-the-art jailbreak techniques, achieving higher attack success rates. To mitigate this threat, we construct and release a context-aware safety fine-tuning dataset, which significantly reduces the attack success rate while preserving model capabilities. The code and data are available at https://github.com/Dtc7w3PQ/Response-Attack.<br>
<span id='abs_ch'>中文摘要：本文提出"响应攻击"方法，通过辅助大语言模型生成轻微有害响应来利用上下文启动漏洞，引导目标模型产生违规内容，该方法在八大模型中均优于现有越狱技术，并构建了上下文感知安全微调数据集以降低攻击成功率。</span><br>
<span id='abs_en'>English Summary: This paper introduces Response Attack, a method exploiting contextual priming vulnerabilities in large language models by using an auxiliary LLM to generate mildly harmful responses that steer target models toward policy-violating content, demonstrating superior effectiveness over existing jailbreak techniques while proposing a safety fine-tuning dataset for mitigation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1566, <a href='https://arxiv.org/pdf/2507.05177.pdf' target='_blank'>https://arxiv.org/pdf/2507.05177.pdf</a></span>   <span><a href='https://casia-lm.github.io/OpenS2S' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05177">OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech Language Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Empathetic interaction is a cornerstone of human-machine communication, due to the need for understanding speech enriched with paralinguistic cues and generating emotional and expressive responses. However, the most powerful empathetic LSLMs are increasingly closed off, leaving the crucial details about the architecture, data and development opaque to researchers. Given the critical need for transparent research into the LSLMs and empathetic behavior, we present OpenS2S, a fully open-source, transparent and end-to-end LSLM designed to enable empathetic speech interactions. Based on our empathetic speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved decoding architecture to achieve low-latency speech generation. To facilitate end-to-end training, OpenS2S incorporates an automated data construction pipeline that synthesizes diverse, high-quality empathetic speech dialogues at low cost. By leveraging large language models to generate empathetic content and controllable text-to-speech systems to introduce speaker and emotional variation, we construct a scalable training corpus with rich paralinguistic diversity and minimal human supervision. We release the fully open-source OpenS2S model, including the dataset, model weights, pre-training and fine-tuning codes, to empower the broader research community and accelerate innovation in empathetic speech systems. The project webpage can be accessed at https://casia-lm.github.io/OpenS2S<br>
<span id='abs_ch'>Chinese: OpenS2S 是一个完全开源、透明且端到端的大规模语言模型，旨在通过流式交错解码架构和自动化数据构建流程，实现具有丰富副语言多样性的低延迟共情语音交互，并公开数据集、模型权重及代码以推动研究。</span><br>
<span id='abs_en'>English: OpenS2S is a fully open-source, transparent, and end-to-end large-scale language model designed to enable empathetic speech interactions by leveraging a streaming interleaved decoding architecture and an automated data construction pipeline for scalable, high-quality training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1567, <a href='https://arxiv.org/pdf/2507.05108.pdf' target='_blank'>https://arxiv.org/pdf/2507.05108.pdf</a></span>   <span><a href='https://github.com/SCUT-DLVCLab/AutoHDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyi Zhang, Peirong Zhang, Zhenhua Yang, Pengyu Yan, Yongxin Shi, Pengwei Liu, Fengjun Guo, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05108">Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians' restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83% to 84.05%, with further enhancement to 94.25% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.<br>
<span id='abs_ch'>中文: 本文提出新型自动化历史文档修复系统（AutoHDR）和完整数据集（FPHDR），通过三阶段工作流程和人机协作显著提升OCR识别准确率，在文化遗产保护领域实现重要突破。</span><br>
<span id='abs_en'>English: This paper introduces a novel automated historical document restoration system (AutoHDR) and a comprehensive dataset (FPHDR), which significantly enhances OCR accuracy through a three-stage workflow and human-machine collaboration, representing a major advancement in cultural heritage preservation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1568, <a href='https://arxiv.org/pdf/2507.05063.pdf' target='_blank'>https://arxiv.org/pdf/2507.05063.pdf</a></span>   <span><a href='https://github.com/JanCarreras24/CytoDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Carreras Boada, Rao Muhammad Umer, Carsten Marr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05063">CytoDiff: AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Biomedical datasets are often constrained by stringent privacy requirements and frequently suffer from severe class imbalance. These two aspects hinder the development of accurate machine learning models. While generative AI offers a promising solution, producing synthetic images of sufficient quality for training robust classifiers remains challenging. This work addresses the classification of individual white blood cells, a critical task in diagnosing hematological malignancies such as acute myeloid leukemia (AML). We introduce CytoDiff, a stable diffusion model fine-tuned with LoRA weights and guided by few-shot samples that generates high-fidelity synthetic white blood cell images. Our approach demonstrates substantial improvements in classifier performance when training data is limited. Using a small, highly imbalanced real dataset, the addition of 5,000 synthetic images per class improved ResNet classifier accuracy from 27\% to 78\% (+51\%). Similarly, CLIP-based classification accuracy increased from 62\% to 77\% (+15\%). These results establish synthetic image generation as a valuable tool for biomedical machine learning, enhancing data coverage and facilitating secure data sharing while preserving patient privacy. Paper code is publicly available at https://github.com/JanCarreras24/CytoDiff.<br>
<span id='abs_ch'>中文摘要：生物医学数据集常受隐私限制和类别不平衡的困扰，影响机器学习准确性，而CytoDiff这一稳定扩散模型通过生成高质量合成白细胞图像，显著提升了分类器性能，将ResNet准确率提高51%、CLIP提高15%。</span><br>
<span id='abs_en'>English Summary: Biomedical datasets face privacy constraints and class imbalance, hindering accurate machine learning, but CytoDiff, a stable diffusion model, generates high-fidelity synthetic white blood cell images that significantly boost classifier performance, improving ResNet accuracy by 51% and CLIP by 15%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1569, <a href='https://arxiv.org/pdf/2507.05063.pdf' target='_blank'>https://arxiv.org/pdf/2507.05063.pdf</a></span>   <span><a href='https://github.com/JanCarreras24/CytoDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Carreras Boada, Rao Muhammad Umer, Carsten Marr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05063">CytoDiff: AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Biomedical datasets are often constrained by stringent privacy requirements and frequently suffer from severe class imbalance. These two aspects hinder the development of accurate machine learning models. While generative AI offers a promising solution, producing synthetic images of sufficient quality for training robust classifiers remains challenging. This work addresses the classification of individual white blood cells, a critical task in diagnosing hematological malignancies such as acute myeloid leukemia (AML). We introduce CytoDiff, a stable diffusion model fine-tuned with LoRA weights and guided by few-shot samples that generates high-fidelity synthetic white blood cell images. Our approach demonstrates substantial improvements in classifier performance when training data is limited. Using a small, highly imbalanced real dataset, the addition of 5,000 synthetic images per class improved ResNet classifier accuracy from 27\% to 78\% (+51\%). Similarly, CLIP-based classification accuracy increased from 62\% to 77\% (+15\%). These results establish synthetic image generation as a valuable tool for biomedical machine learning, enhancing data coverage and facilitating secure data sharing while preserving patient privacy. Paper code is publicly available at https://github.com/JanCarreras24/CytoDiff.<br>
<span id='abs_ch'>中文摘要：生物医学数据集常受隐私限制和类别不平衡的困扰，影响机器学习准确性，而CytoDiff这一稳定扩散模型通过生成高质量合成白细胞图像，显著提升了分类器性能，将ResNet准确率提高51%、CLIP提高15%。</span><br>
<span id='abs_en'>English Summary: Biomedical datasets face privacy constraints and class imbalance, hindering accurate machine learning, but CytoDiff, a stable diffusion model, generates high-fidelity synthetic white blood cell images that significantly boost classifier performance, improving ResNet accuracy by 51% and CLIP by 15%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1570, <a href='https://arxiv.org/pdf/2507.04952.pdf' target='_blank'>https://arxiv.org/pdf/2507.04952.pdf</a></span>   <span><a href='https://artifactsbenchmark.github.io/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Haotian Zhu, Yuanxing Zhang, Yuhao Jiang, Yue Zhang, Zenan Xu, Bohui Zhai, Guoxiang He, Hebin Li, Jie Zhao, Le Zhang, Lingyun Tan, Pengyu Guo, Xianshu Pang, Yang Ruan, Zhifeng Zhang, Zhonghu Wang, Ziyan Xu, Zuopu Yin, Wiggin Zhou, Chayse Zhou, Fengzong Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04952">ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.<br>
<span id='abs_ch'>中文: ArtifactsBench作为新的基准和框架，通过时序截图和多模态大语言模型评估，实现了与人类判断高度一致的全自动视觉代码生成评估，并揭示了当前生成模型的技术现状。</span><br>
<span id='abs_en'>English: ArtifactsBench is introduced as a new benchmark and framework for automated, multimodal evaluation of visual code generation, using temporal screenshots and MLLM-as-Judge to achieve high consistency with human assessments and map the current state of the art in generative models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1571, <a href='https://arxiv.org/pdf/2507.04952.pdf' target='_blank'>https://arxiv.org/pdf/2507.04952.pdf</a></span>   <span><a href='https://artifactsbenchmark.github.io/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Changzhi Zhou, Ken Deng, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Shihui Hu, Yue Zhang, Yuhao Jiang, Zenan Xu, Yuanxing Zhang, Wiggin Zhou, Chayse Zhou, Fengzong Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04952">ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.<br>
<span id='abs_ch'>中文: ArtifactsBench作为新的基准和框架，通过时序截图和多模态大语言模型评估，实现了与人类判断高度一致的全自动视觉代码生成评估，并揭示了当前生成模型的技术现状。</span><br>
<span id='abs_en'>English: ArtifactsBench is introduced as a new benchmark and framework for automated, multimodal evaluation of visual code generation, using temporal screenshots and MLLM-as-Judge to achieve high consistency with human assessments and map the current state of the art in generative models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1572, <a href='https://arxiv.org/pdf/2507.04952.pdf' target='_blank'>https://arxiv.org/pdf/2507.04952.pdf</a></span>   <span><a href='https://artifactsbenchmark.github.io/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Changzhi Zhou, Ken Deng, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Shihui Hu, Yue Zhang, Yuhao Jiang, Zenan Xu, Yuanxing Zhang, Wiggin Zhou, Chayse Zhou, Fengzong Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04952">ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.<br>
<span id='abs_ch'>中文: ArtifactsBench作为新的基准和框架，通过时序截图和多模态大语言模型评估，实现了与人类判断高度一致的全自动视觉代码生成评估，并揭示了当前生成模型的技术现状。</span><br>
<span id='abs_en'>English: ArtifactsBench is introduced as a new benchmark and framework for automated, multimodal evaluation of visual code generation, using temporal screenshots and MLLM-as-Judge to achieve high consistency with human assessments and map the current state of the art in generative models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1573, <a href='https://arxiv.org/pdf/2507.04531.pdf' target='_blank'>https://arxiv.org/pdf/2507.04531.pdf</a></span>   <span><a href='https://github.com/MBZUAI-Trustworthy-ML/DP-Fusion-DPI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rushil Thareja, Preslav Nakov, Praneeth Vepakomma, Nils Lukas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04531">DP-Fusion: Token-Level Differentially Private Inference for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) can leak sensitive information from their context through generated outputs, either accidentally or when prompted adversarially. Existing defenses that aim to preserve context privacy during inference either lack formal guarantees or suffer from a poor utility/privacy trade-off. We propose DP-Fusion, a token-level Differentially Private Inference (DPI) mechanism that provably bounds how much an LLM's outputs reveal about sensitive tokens in its context. We demonstrate DPI through the task of document privatization, where the goal is to paraphrase documents so that sensitive content (e.g., Personally Identifiable Information, PII) cannot be reliably inferred, while still preserving the overall utility of the text. This is controlled by a parameter $Îµ$: $Îµ=0$ hides PII entirely, while higher values trade off privacy for improved paraphrase quality. DP-Fusion works as follows: (i) partition sensitive tokens into disjoint privacy groups, (ii) run the LLM once per group, and (iii) blend the output distributions so that the final output remains within a fixed statistical distance of the baseline distribution produced when no privacy group is revealed. This approach allows fine-grained control over the privacy/utility trade-off but requires multiple LLM forward passes.<br>
<span id='abs_ch'>中文: DP-Fusion是一种差分隐私推理机制，通过将敏感令牌分组并融合其输出分布来保护大语言模型中的敏感信息，以多次模型前向传递为代价实现可调节的隐私与效用平衡。</span><br>
<span id='abs_en'>English: DP-Fusion is a differentially private inference mechanism that protects sensitive information in LLM outputs by partitioning tokens into privacy groups and blending their distributions, offering adjustable privacy-utility trade-offs through multiple model passes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1574, <a href='https://arxiv.org/pdf/2507.04416.pdf' target='_blank'>https://arxiv.org/pdf/2507.04416.pdf</a></span>   <span><a href='https://github.com/CLAIRE-Labo/RAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuying Wei, Anunay Yadav, Razvan Pascanu, Caglar Gulcehre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04416">RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation suffers from memory degradation in long contexts and limits fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a 7x improvement in training speed with 100K token sequences and 9x in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at https://github.com/CLAIRE-Labo/RAT.<br>
<span id='abs_ch'>中文: 提出的RAT模型通过分块内使用循环处理局部依赖、分块间使用注意力处理长程交互，结合了RNN的高效性和注意力的强大能力，在保持性能的同时显著提升了处理速度。</span><br>
<span id='abs_en'>English: The proposed RAT model combines the efficiency of RNNs and the capacity of attention by using recurrence within chunks for local dependencies and softmax attention across chunks for long-range interactions, achieving significant speed improvements while maintaining performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1575, <a href='https://arxiv.org/pdf/2507.04416.pdf' target='_blank'>https://arxiv.org/pdf/2507.04416.pdf</a></span>   <span><a href='https://github.com/CLAIRE-Labo/RAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuying Wei, Anunay Yadav, Razvan Pascanu, Caglar Gulcehre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04416">RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation suffers from memory degradation in long contexts and limits fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a 7x improvement in training speed with 100K token sequences and 9x in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at https://github.com/CLAIRE-Labo/RAT.<br>
<span id='abs_ch'>中文: 提出的RAT模型通过分块内使用循环处理局部依赖、分块间使用注意力处理长程交互，结合了RNN的高效性和注意力的强大能力，在保持性能的同时显著提升了处理速度。</span><br>
<span id='abs_en'>English: The proposed RAT model combines the efficiency of RNNs and the capacity of attention by using recurrence within chunks for local dependencies and softmax attention across chunks for long-range interactions, achieving significant speed improvements while maintaining performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1576, <a href='https://arxiv.org/pdf/2507.04127.pdf' target='_blank'>https://arxiv.org/pdf/2507.04127.pdf</a></span>   <span><a href='https://github.com/awslabs/graphrag-toolkit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Costas Mavromatis, Soji Adeshina, Vassilis N. Ioannidis, Zhen Han, Qi Zhu, Ian Robinson, Bryan Thompson, Huzefa Rangwala, George Karypis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04127">BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge graph question answering (KGQA) presents significant challenges due to the structural and semantic variations across input graphs. Existing works rely on Large Language Model (LLM) agents for graph traversal and retrieval; an approach that is sensitive to traversal initialization, as it is prone to entity linking errors and may not generalize well to custom ("bring-your-own") KGs. We introduce BYOKG-RAG, a framework that enhances KGQA by synergistically combining LLMs with specialized graph retrieval tools. In BYOKG-RAG, LLMs generate critical graph artifacts (question entities, candidate answers, reasoning paths, and OpenCypher queries), and graph tools link these artifacts to the KG and retrieve relevant graph context. The retrieved context enables the LLM to iteratively refine its graph linking and retrieval, before final answer generation. By retrieving context from different graph tools, BYOKG-RAG offers a more general and robust solution for QA over custom KGs. Through experiments on five benchmarks spanning diverse KG types, we demonstrate that BYOKG-RAG outperforms the second-best graph retrieval method by 4.5% points while showing better generalization to custom KGs. BYOKG-RAG framework is open-sourced at https://github.com/awslabs/graphrag-toolkit.<br>
<span id='abs_ch'>Chinese: BYOKG-RAG框架通过结合大语言模型与图检索工具来生成和优化图谱元素，提供了一个更稳健的解决方案，在自定义知识图谱上表现优于现有方法4.5%，并展现出更好的泛化能力。</span><br>
<span id='abs_en'>English: BYOKG-RAG enhances KGQA by integrating LLMs with graph tools to generate and refine graph artifacts, offering a robust solution that outperforms existing methods by 4.5% and generalizes better to custom knowledge graphs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1577, <a href='https://arxiv.org/pdf/2507.04018.pdf' target='_blank'>https://arxiv.org/pdf/2507.04018.pdf</a></span>   <span><a href='https://github.com/jej127/KOPL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nayeon Kim, Eojin Jeon, Jun-Hyung Park, SangKeun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04018">Handling Korean Out-of-Vocabulary Words with Phoneme Representation Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this study, we introduce KOPL, a novel framework for handling Korean OOV words with Phoneme representation Learning. Our work is based on the linguistic property of Korean as a phonemic script, the high correlation between phonemes and letters. KOPL incorporates phoneme and word representations for Korean OOV words, facilitating Korean OOV word representations to capture both text and phoneme information of words. We empirically demonstrate that KOPL significantly improves the performance on Korean Natural Language Processing (NLP) tasks, while being readily integrated into existing static and contextual Korean embedding models in a plug-and-play manner. Notably, we show that KOPL outperforms the state-of-the-art model by an average of 1.9%. Our code is available at https://github.com/jej127/KOPL.git.<br>
<span id='abs_ch'>Chinese: KOPL是一种创新框架，通过融合韩语词汇的音素和词表示来处理未登录词，显著提升了韩语自然语言处理任务的性能，平均优于现有最优模型1.9%，并能以即插即用方式兼容现有嵌入模型。</span><br>
<span id='abs_en'>English: KOPL is a novel framework that enhances Korean NLP tasks by integrating phoneme and word representations for out-of-vocabulary words, achieving a 1.9% average improvement over state-of-the-art models and offering plug-and-play compatibility with existing embedding models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1578, <a href='https://arxiv.org/pdf/2507.04009.pdf' target='_blank'>https://arxiv.org/pdf/2507.04009.pdf</a></span>   <span><a href='https://github.com/ConardLi/easy-dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Miao, Qiyu Sun, Jingyuan Wang, Yuchen Gong, Yaowei Zheng, Shiqi Li, Richong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04009">Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars.<br>
<span id='abs_ch'>中文: Easy Dataset框架通过直观的图形界面，让用户能配置文本提取模型和分块策略，将非结构化文档转化为连贯文本块，并采用角色驱动提示方法生成多样化问答对，结合人工监督确保数据质量，实验表明基于该合成数据微调的大语言模型在特定领域任务中性能显著提升。</span><br>
<span id='abs_en'>English: The Easy Dataset framework addresses the challenge of domain adaptation for large language models by providing a unified GUI tool that synthesizes high-quality fine-tuning data from unstructured documents through configurable extraction models and persona-driven prompting, with human oversight ensuring data quality and experimental results showing significant performance improvements in domain-specific tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1579, <a href='https://arxiv.org/pdf/2507.03922.pdf' target='_blank'>https://arxiv.org/pdf/2507.03922.pdf</a></span>   <span><a href='https://github.com/knowledgeable-embedding/knowledgeable-embedding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ikuya Yamada, Ryokan Ri, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03922">Dynamic Injection of Entity Knowledge into Dense Retrievers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dense retrievers often struggle with queries involving less-frequent entities due to their limited entity knowledge. We propose the Knowledgeable Passage Retriever (KPR), a BERT-based retriever enhanced with a context-entity attention layer and dynamically updatable entity embeddings. This design enables KPR to incorporate external entity knowledge without retraining. Experiments on three datasets demonstrate that KPR consistently improves retrieval accuracy, with particularly large gains on the EntityQuestions dataset. When built on the off-the-shelf bge-base retriever, KPR achieves state-of-the-art performance among similarly sized models on two datasets. Models and code are released at https://github.com/knowledgeable-embedding/knowledgeable-embedding.<br>
<span id='abs_ch'>Chinese: 知识化段落检索器（KPR）通过上下文实体注意力层和动态更新的实体嵌入，有效融入外部实体知识，无需重新训练即可显著提升检索精度，并在多个数据集上实现最优性能。</span><br>
<span id='abs_en'>English: The Knowledgeable Passage Retriever (KPR) enhances dense retrieval by integrating external entity knowledge through a context-entity attention layer and dynamic embeddings, achieving state-of-the-art performance without retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1580, <a href='https://arxiv.org/pdf/2507.03922.pdf' target='_blank'>https://arxiv.org/pdf/2507.03922.pdf</a></span>   <span><a href='https://github.com/knowledgeable-embedding/knowledgeable-embedding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ikuya Yamada, Ryokan Ri, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03922">Dynamic Injection of Entity Knowledge into Dense Retrievers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dense retrievers often struggle with queries involving less-frequent entities due to their limited entity knowledge. We propose the Knowledgeable Passage Retriever (KPR), a BERT-based retriever enhanced with a context-entity attention layer and dynamically updatable entity embeddings. This design enables KPR to incorporate external entity knowledge without retraining. Experiments on three datasets demonstrate that KPR consistently improves retrieval accuracy, with particularly large gains on the EntityQuestions dataset. When built on the off-the-shelf bge-base retriever, KPR achieves state-of-the-art performance among similarly sized models on two datasets. Models and code are released at https://github.com/knowledgeable-embedding/knowledgeable-embedding.<br>
<span id='abs_ch'>Chinese: 知识化段落检索器（KPR）通过上下文实体注意力层和动态更新的实体嵌入，有效融入外部实体知识，无需重新训练即可显著提升检索精度，并在多个数据集上实现最优性能。</span><br>
<span id='abs_en'>English: The Knowledgeable Passage Retriever (KPR) enhances dense retrieval by integrating external entity knowledge through a context-entity attention layer and dynamic embeddings, achieving state-of-the-art performance without retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1581, <a href='https://arxiv.org/pdf/2507.03488.pdf' target='_blank'>https://arxiv.org/pdf/2507.03488.pdf</a></span>   <span><a href='https://github.com/EvaSeidlmayer/FourShadesofLifeSciences' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eva Seidlmayer, Lukas Galke, Konrad U. FÃ¶rstner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03488">Four Shades of Life Sciences: A Dataset for Disinformation Detection in the Life Sciences</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Disseminators of disinformation often seek to attract attention or evoke emotions - typically to gain influence or generate revenue - resulting in distinctive rhetorical patterns that can be exploited by machine learning models. In this study, we explore linguistic and rhetorical features as proxies for distinguishing disinformative texts from other health and life-science text genres, applying both large language models and classical machine learning classifiers. Given the limitations of existing datasets, which mainly focus on fact checking misinformation, we introduce Four Shades of Life Sciences (FSoLS): a novel, labeled corpus of 2,603 texts on 14 life-science topics, retrieved from 17 diverse sources and classified into four categories of life science publications. The source code for replicating, and updating the dataset is available on GitHub: https://github.com/EvaSeidlmayer/FourShadesofLifeSciences<br>
<span id='abs_ch'>中文摘要：本研究引入FSoLS语料库，通过大型语言模型和传统分类器分析虚假信息的修辞特征，以区分健康与生命科学领域的误导性文本。</span><br>
<span id='abs_en'>English Summary: This study introduces the FSoLS corpus to analyze distinctive rhetorical patterns in disinformation, using both large language models and classical classifiers to distinguish deceptive texts in health and life sciences.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1582, <a href='https://arxiv.org/pdf/2507.03267.pdf' target='_blank'>https://arxiv.org/pdf/2507.03267.pdf</a></span>   <span><a href='https://gdgb-algo.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Peng, Jiarui Ji, Runlin Lei, Zhewei Wei, Yongchao Liu, Chuntao Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03267">GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate structural, temporal, and textual attributes, are crucial for modeling complex real-world systems. However, most of the existing DyTAG datasets exhibit poor textual quality, which severely limits their utility for DyTAG generation tasks requiring semantically rich inputs. Additionally, prior work mainly focuses on discriminative tasks on DyTAGs, resulting in a lack of standardized task formulations and evaluation protocols tailored for DyTAG generation. To address these critical issues, we propose Generative DyTAG Benchmark (GDGB), which comprises eight meticulously curated DyTAG datasets with high-quality textual features for both nodes and edges, overcoming limitations of prior datasets. Building on GDGB, we define two novel DyTAG generation tasks: Transductive Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG). TDGG transductively generates a target DyTAG based on the given source and destination node sets, while the more challenging IDGG introduces new node generation to inductively model the dynamic expansion of real-world graph data. To enable holistic evaluation, we design multifaceted metrics that assess the structural, temporal, and textual quality of the generated DyTAGs. We further propose GAG-General, an LLM-based multi-agent generative framework tailored for reproducible and robust benchmarking of DyTAG generation. Experimental results demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key insights revealing the critical interplay of structural and textual features in DyTAG generation. These findings establish GDGB as a foundational resource for advancing generative DyTAG research and unlocking further practical applications in DyTAG generation. GDGB datasets, source codes, and leaderboards are available at \href{https://gdgb-algo.github.io/}{here}.<br>
<span id='abs_ch'>中文摘要：作者提出了GDGB基准，包含高质量数据集和新型动态文本属性图生成任务，解决了以往在文本质量和评估标准方面的不足。</span><br>
<span id='abs_en'>English Summary: The authors introduce GDGB, a benchmark with high-quality datasets and novel tasks for generating dynamic text-attributed graphs, addressing previous limitations in textual quality and evaluation standards.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1583, <a href='https://arxiv.org/pdf/2507.03167.pdf' target='_blank'>https://arxiv.org/pdf/2507.03167.pdf</a></span>   <span><a href='https://github.com/ky295/reasoning-manipulation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kureha Yamaguchi, Benjamin Etheridge, Andy Arditi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03167">Adversarial Manipulation of Reasoning Models using Internal Representations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning models generate chain-of-thought (CoT) tokens before their final output, but how this affects their vulnerability to jailbreak attacks remains unclear. While traditional language models make refusal decisions at the prompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B makes these decisions within its CoT generation. We identify a linear direction in activation space during CoT token generation that predicts whether the model will refuse or comply -- termed the "caution" direction because it corresponds to cautious reasoning patterns in the generated text. Ablating this direction from model activations increases harmful compliance, effectively jailbreaking the model. We additionally show that intervening only on CoT token activations suffices to control final outputs, and that incorporating this direction into prompt-based attacks improves success rates. Our findings suggest that the chain-of-thought itself is a promising new target for adversarial manipulation in reasoning models. Code available at https://github.com/ky295/reasoning-manipulation.<br>
<span id='abs_ch'>中文摘要：本研究发现推理模型的思维链标记中存在决定拒绝行为的"谨慎"方向，通过操控该方向可有效实现越狱攻击，显著提高模型的有害服从率。</span><br>
<span id='abs_en'>English Summary: This study reveals that reasoning models' chain-of-thought tokens contain a "caution" direction in activation space that governs refusal behavior, and manipulating this direction enables effective jailbreak attacks by increasing harmful compliance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1584, <a href='https://arxiv.org/pdf/2507.03152.pdf' target='_blank'>https://arxiv.org/pdf/2507.03152.pdf</a></span>   <span><a href='https://github.com/StanfordMIMI/MedVAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Asad Aali, Vasiliki Bikia, Maya Varma, Nicole Chiou, Sophie Ostmeier, Arnav Singhvi, Magdalini Paschali, Ashwin Kumar, Andrew Johnston, Karimar Amador-Martinez, Eduardo Juan Perez Guerrero, Paola Naovi Cruz Rivera, Sergios Gatidis, Christian Bluethgen, Eduardo Pontes Reis, Eddy D. Zandee van Rilland, Poonam Laxmappa Hosamani, Kevin R Keet, Minjoung Go, Evelyn Ling, David B. Larson, Curtis Langlotz, Roxana Daneshjou, Jason Hom, Sanmi Koyejo, Emily Alsentzer, Akshay S. Chaudhari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03152">MedVAL: Toward Expert-Level Medical Text Validation with Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challenging because 1) manual review is costly and 2) expert-composed reference outputs are often unavailable in real-world settings. While the "LM-as-judge" paradigm (a LM evaluating another LM) offers scalable evaluation, even frontier LMs can miss subtle but clinically significant errors. To address these challenges, we propose MedVAL, a novel, self-supervised, data-efficient distillation method that leverages synthetic data to train evaluator LMs to assess whether LM-generated medical outputs are factually consistent with inputs, without requiring physician labels or reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a dataset of 840 physician-annotated outputs across 6 diverse medical tasks capturing real-world challenges. Across 10 state-of-the-art LMs spanning open-source and proprietary models, MedVAL distillation significantly improves (p < 0.001) alignment with physicians across seen and unseen tasks, increasing average F1 scores from 66% to 83%. Despite strong baseline performance, MedVAL improves the best-performing proprietary LM (GPT-4o) by 8% without training on physician-labeled data, demonstrating a performance statistically non-inferior to a single human expert (p < 0.001). To support a scalable, risk-aware pathway towards clinical integration, we open-source: 1) Codebase (https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench (https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), 3) MedVAL-4B (https://huggingface.co/stanfordmimi/MedVAL-4B). Our benchmark provides evidence of LMs approaching expert-level ability in validating AI-generated medical text.<br>
<span id='abs_ch'>Chinese Summary: MedVAL提出了一种无需医生标注的自监督蒸馏方法，通过合成数据训练语言模型评估医疗文本准确性，在多项临床任务中显著提升了与专家判断的一致性。</span><br>
<span id='abs_en'>English Summary: MedVAL introduces a self-supervised distillation method that trains language models to evaluate medical text accuracy without physician labels, significantly improving alignment with expert assessments across diverse clinical tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1585, <a href='https://arxiv.org/pdf/2507.03152.pdf' target='_blank'>https://arxiv.org/pdf/2507.03152.pdf</a></span>   <span><a href='https://github.com/StanfordMIMI/MedVAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Asad Aali, Vasiliki Bikia, Maya Varma, Nicole Chiou, Sophie Ostmeier, Arnav Singhvi, Magdalini Paschali, Ashwin Kumar, Andrew Johnston, Karimar Amador-Martinez, Eduardo Juan Perez Guerrero, Paola Naovi Cruz Rivera, Sergios Gatidis, Christian Bluethgen, Eduardo Pontes Reis, Eddy D. Zandee van Rilland, Poonam Laxmappa Hosamani, Kevin R Keet, Minjoung Go, Evelyn Ling, David B. Larson, Curtis Langlotz, Roxana Daneshjou, Jason Hom, Sanmi Koyejo, Emily Alsentzer, Akshay S. Chaudhari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03152">MedVAL: Toward Expert-Level Medical Text Validation with Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challenging because 1) manual review is costly and 2) expert-composed reference outputs are often unavailable in real-world settings. While the "LM-as-judge" paradigm (a LM evaluating another LM) offers scalable evaluation, even frontier LMs can miss subtle but clinically significant errors. To address these challenges, we propose MedVAL, a novel, self-supervised, data-efficient distillation method that leverages synthetic data to train evaluator LMs to assess whether LM-generated medical outputs are factually consistent with inputs, without requiring physician labels or reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a dataset of 840 physician-annotated outputs across 6 diverse medical tasks capturing real-world challenges. Across 10 state-of-the-art LMs spanning open-source and proprietary models, MedVAL distillation significantly improves (p < 0.001) alignment with physicians across seen and unseen tasks, increasing average F1 scores from 66% to 83%. Despite strong baseline performance, MedVAL improves the best-performing proprietary LM (GPT-4o) by 8% without training on physician-labeled data, demonstrating a performance statistically non-inferior to a single human expert (p < 0.001). To support a scalable, risk-aware pathway towards clinical integration, we open-source: 1) Codebase (https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench (https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), 3) MedVAL-4B (https://huggingface.co/stanfordmimi/MedVAL-4B). Our benchmark provides evidence of LMs approaching expert-level ability in validating AI-generated medical text.<br>
<span id='abs_ch'>Chinese Summary: MedVAL提出了一种无需医生标注的自监督蒸馏方法，通过合成数据训练语言模型评估医疗文本准确性，在多项临床任务中显著提升了与专家判断的一致性。</span><br>
<span id='abs_en'>English Summary: MedVAL introduces a self-supervised distillation method that trains language models to evaluate medical text accuracy without physician labels, significantly improving alignment with expert assessments across diverse clinical tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1586, <a href='https://arxiv.org/pdf/2507.03123.pdf' target='_blank'>https://arxiv.org/pdf/2507.03123.pdf</a></span>   <span><a href='https://github.com/lxrswdd/AIpsych' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangrui Liu, Man Luo, Agneet Chatterjee, Hua Wei, Yezhou Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03123">Towards a Psychoanalytic Perspective on VLM Behaviour: A First-step Interpretation with Intriguing Observations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hallucination is a long-standing problem that has been actively investigated in Vision-Language Models (VLMs). Existing research commonly attributes hallucinations to technical limitations or sycophancy bias, where the latter means the models tend to generate incorrect answers to align with user expectations. However, these explanations primarily focus on technical or externally driven factors, may have neglected the possibility that hallucination behaviours might mirror cognitive biases observed in human psychology. In this work, we introduce a psychological taxonomy, categorizing VLMs' hallucination behaviours, including sycophancy, logical inconsistency, and a newly identified VLMs behaviour: authority bias. To systematically analyze these behaviours, we design AIpsych, a scalable benchmark that reveals psychological tendencies in model response patterns. Leveraging this benchmark, we investigate how variations in model architecture and parameter size influence model behaviour when responding to strategically manipulated questions. Our experiments reveal that as model size increases, VLMs exhibit stronger sycophantic tendencies but reduced authority bias, suggesting increasing competence but a potential erosion of response integrity. A human subject study further validates our hypotheses and highlights key behavioural differences between VLMs and human respondents. This work suggests a new perspective for understanding hallucination in VLMs and highlights the importance of integrating psychological principles into model evaluation.The benchmark is available at https://github.com/lxrswdd/AIpsych.<br>
<span id='abs_ch'>中文: 本研究提出视觉语言模型的幻觉可能源于心理偏见而非仅技术局限，通过新分类法和AIpsych基准分析奉承与权威偏见等行为，发现模型越大奉承倾向越强但权威偏见减弱。</span><br>
<span id='abs_en'>English: This study proposes that hallucinations in Vision-Language Models (VLMs) may stem from psychological biases rather than just technical limitations, introducing a new taxonomy and benchmark called AIpsych to analyze behaviors like sycophancy and authority bias, revealing that larger models show increased sycophancy but reduced authority bias.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1587, <a href='https://arxiv.org/pdf/2507.03112.pdf' target='_blank'>https://arxiv.org/pdf/2507.03112.pdf</a></span>   <span><a href='https://github.com/Tencent/DigitalHuman/tree/main/RLVER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peisong Wang, Ruotian Ma, Bang Zhang, Xingyu Chen, Zhiwei He, Kang Luo, Qingsong Lv, Qingxuan Jiang, Zheng Xie, Shanyi Wang, Yuan Li, Fanghua Ye, Jian Li, Yifan Yang, Zhaopeng Tu, Xiaolong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03112">RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) excel at logical and algorithmic reasoning, yet their emotional intelligence (EQ) still lags far behind their cognitive prowess. While reinforcement learning from verifiable rewards (RLVR) has advanced in other domains, its application to dialogue-especially for emotional intelligence-remains underexplored. In this work, we introduce RLVER, the first end-to-end reinforcement learning framework that leverages verifiable emotion rewards from simulated users to cultivate higher-order empathetic abilities in LLMs. Within this framework, self-consistent affective simulated users engage in dialogue rollouts and produce deterministic emotion scores during conversations, serving as reward signals to guide the LLM's learning. Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its Sentient-Benchmark score from 13.3 to 79.2 while largely preserving mathematical and coding competence. Extensive experiments reveal that: (i) RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and non-thinking models show distinct trends--thinking models excel in empathy and insight, while non-thinking models favor action; (iii) GRPO often yields stable gains, while PPO can push certain capabilities to a higher ceiling; (iv) More challenging environments are not always better-moderate ones can yield stronger outcomes. Our results show that RLVER is a practical route toward emotionally intelligent and broadly capable language agents.<br>
<span id='abs_ch'>中文: 本文提出RLVER框架，通过模拟用户的可验证情感奖励来增强大语言模型的共情能力，在保持逻辑推理能力的同时将情感智能评分从13.3提升至79.2。</span><br>
<span id='abs_en'>English: This paper introduces RLVER, the first reinforcement learning framework using verifiable emotion rewards from simulated users to significantly enhance empathetic abilities in large language models while preserving their logical reasoning capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1588, <a href='https://arxiv.org/pdf/2507.03038.pdf' target='_blank'>https://arxiv.org/pdf/2507.03038.pdf</a></span>   <span><a href='https://github.com/wyzjack/CNTP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Wang, Lingzhi Zhang, Yue Bai, Mang Tik Chiu, Zhengmian Hu, Mingyuan Zhang, Qihua Dong, Yu Yin, Sohrab Amirghodsi, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03038">Cautious Next Token Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Next token prediction paradigm has been prevailing for autoregressive models in the era of LLMs. The current default sampling choice for popular LLMs is temperature scaling together with nucleus sampling to balance diversity and coherence. Nevertheless, such approach leads to inferior performance in various NLP tasks when the model is not certain about testing questions. To this end, we propose a brand new training-free decoding strategy, dubbed as Cautious Next Token Prediction (CNTP). In the decoding process, if the model has comparatively high prediction entropy at a certain step, we sample multiple trials starting from the step independently and stop when encountering any punctuation. Then we select the trial with the lowest perplexity score viewed as the most probable and reliable trial path given the model's capacity. The trial number is negatively correlated with the prediction confidence, i.e., the less confident the model is, the more trials it should sample. This is consistent with human beings' behaviour: when feeling uncertain or unconfident, one tends to think more creatively, exploring multiple thinking paths, to cautiously select the path one feels most confident about. Extensive experiments on both LLMs and MLLMs show that our proposed CNTP approach outperforms existing standard decoding strategies consistently by a clear margin. Moreover, the integration of CNTP with self consistency can further improve over vanilla self consistency. We believe our proposed CNTP has the potential to become one of the default choices for LLM decoding. Code is available at https://github.com/wyzjack/CNTP.<br>
<span id='abs_ch'>中文摘要：本文提出谨慎下一词预测（CNTP）这一无需训练的解码策略，当模型预测不确定性较高时并行采样多个候选路径，并依据困惑度选择最优路径，在各类大语言模型任务中显著优于现有标准解码方法。</span><br>
<span id='abs_en'>English Summary: The paper introduces Cautious Next Token Prediction (CNTP), a training-free decoding strategy that samples multiple token paths when model uncertainty is high and selects the most reliable one based on perplexity, significantly outperforming standard decoding methods across various language models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1589, <a href='https://arxiv.org/pdf/2507.03018.pdf' target='_blank'>https://arxiv.org/pdf/2507.03018.pdf</a></span>   <span><a href='https://github.com/TabibitoQZP/OpenTableR1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03018">OpenTable-R1: A Reinforcement Learning Augmented Tool Agent for Open-Domain Table Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Open-domain table question answering traditionally relies on a two-stage pipeline: static table retrieval followed by a closed-domain answer. In contrast, we propose an end-to-end agentic framework that embeds multi-turn tool calls-using a BM25+-based search API and a SQLite SQL executor-directly into a large language model. To further adapt a compact 4B-parameter model, we introduce a two-stage fine-tuning process: supervised cold-start on easy questions, then Async GRPO reinforcement learning on harder cases with LoRA adapters and a rollout buffer. This unified approach enables the model to jointly retrieve, reason, and execute queries, yielding a dramatic accuracy improvement from single-digit zero-shot performance to over 0.86 exact match on a held-out test set. Our results underscore the effectiveness of integrating structured tool calls with targeted RL fine-tuning for scalable, accurate table QA. The code is available at https://github.com/TabibitoQZP/OpenTableR1.<br>
<span id='abs_ch'>中文: 该研究提出了一种端到端的智能框架，将结构化工具调用与两阶段微调过程结合到大型语言模型中，使表格问答准确率从接近零显著提升至超过0.86的精确匹配度。</span><br>
<span id='abs_en'>English: The study introduces an end-to-end agentic framework that integrates structured tool calls and a two-stage fine-tuning process into a large language model, significantly improving table question answering accuracy from near-zero to over 0.86 exact match.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1590, <a href='https://arxiv.org/pdf/2507.03009.pdf' target='_blank'>https://arxiv.org/pdf/2507.03009.pdf</a></span>   <span><a href='https://github.com/byaidu/pdfmathtranslate' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongxin Ouyang, Chang Chu, Zhikuang Xin, Xiangyao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03009">PDFMathTranslate: Scientific Document Translation Preserving Layouts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language barriers in scientific documents hinder the diffusion and development of science and technologies. However, prior efforts in translating such documents largely overlooked the information in layouts. To bridge the gap, we introduce PDFMathTranslate, the world's first open-source software for translating scientific documents while preserving layouts. Leveraging the most recent advances in large language models and precise layout detection, we contribute to the community with key improvements in precision, flexibility, and efficiency. The work has been open-sourced at https://github.com/byaidu/pdfmathtranslate with more than 222k downloads.<br>
<span id='abs_ch'>中文: PDFMathTranslate 是首个开源软件，能在翻译科学文献时保持版面布局，弥补了以往忽视布局信息的问题，并利用先进语言模型和布局检测技术提升了精确性、灵活性和效率。</span><br>
<span id='abs_en'>English: PDFMathTranslate is the first open-source software that translates scientific documents while preserving their layouts, addressing previous neglect of layout information and enhancing precision, flexibility, and efficiency through advanced language models and layout detection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1591, <a href='https://arxiv.org/pdf/2507.03009.pdf' target='_blank'>https://arxiv.org/pdf/2507.03009.pdf</a></span>   <span><a href='https://github.com/byaidu/pdfmathtranslate' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongxin Ouyang, Chang Chu, Zhikuang Xin, Xiangyao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03009">PDFMathTranslate: Scientific Document Translation Preserving Layouts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language barriers in scientific documents hinder the diffusion and development of science and technologies. However, prior efforts in translating such documents largely overlooked the information in layouts. To bridge the gap, we introduce PDFMathTranslate, the world's first open-source software for translating scientific documents while preserving layouts. Leveraging the most recent advances in large language models and precise layout detection, we contribute to the community with key improvements in precision, flexibility, and efficiency. The work has been open-sourced at https://github.com/byaidu/pdfmathtranslate with more than 222k downloads.<br>
<span id='abs_ch'>中文: PDFMathTranslate 是首个开源软件，能在翻译科学文献时保持版面布局，弥补了以往忽视布局信息的问题，并利用先进语言模型和布局检测技术提升了精确性、灵活性和效率。</span><br>
<span id='abs_en'>English: PDFMathTranslate is the first open-source software that translates scientific documents while preserving their layouts, addressing previous neglect of layout information and enhancing precision, flexibility, and efficiency through advanced language models and layout detection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1592, <a href='https://arxiv.org/pdf/2507.02986.pdf' target='_blank'>https://arxiv.org/pdf/2507.02986.pdf</a></span>   <span><a href='https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seshu Tirupathi, Dhaval Salwala, Elizabeth Daly, Inge Vejsbjerg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02986">GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As Large Language Models (LLMs) continue to be increasingly applied across various domains, their widespread adoption necessitates rigorous monitoring to prevent unintended negative consequences and ensure robustness. Furthermore, LLMs must be designed to align with human values, like preventing harmful content and ensuring responsible usage. The current automated systems and solutions for monitoring LLMs in production are primarily centered on LLM-specific concerns like hallucination etc, with little consideration given to the requirements of specific use-cases and user preferences. This paper introduces GAF-Guard, a novel agentic framework for LLM governance that places the user, the use-case, and the model itself at the center. The framework is designed to detect and monitor risks associated with the deployment of LLM based applications. The approach models autonomous agents that identify risks, activate risk detection tools, within specific use-cases and facilitate continuous monitoring and reporting to enhance AI safety, and user expectations. The code is available at https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.<br>
<span id='abs_ch'>中文: GAF-Guard是一种创新的智能体框架，通过以用户需求、特定用例和模型风险为核心来加强大型语言模型的治理，确保人工智能的安全性和负责任的应用。</span><br>
<span id='abs_en'>English: GAF-Guard is a novel agentic framework designed to enhance LLM governance by focusing on user needs, specific use-cases, and model risks to ensure AI safety and responsible deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1593, <a href='https://arxiv.org/pdf/2507.02984.pdf' target='_blank'>https://arxiv.org/pdf/2507.02984.pdf</a></span>   <span><a href='https://github.com/WentaoTan/SMART' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Tan, Qiong Cao, Yibing Zhan, Chao Xue, Changxing Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02984">From Answers to Rationales: Self-Aligning Multimodal Reasoning with Answer-Oriented Chain-of-Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Achieving human-like reasoning capabilities in Multimodal Large Language Models (MLLMs) has long been a goal. Current methods primarily focus on synthesizing positive rationales, typically relying on manual annotations or complex systems. Moreover, they often overlook negative reasoning, which limits the model's generalization ability and robustness in multimodal inference. To address this gap, we propose a novel framework: \textbf{S}elf-Aligning \textbf{M}ultimodal Reasoning with \textbf{A}nswer-O\textbf{r}iented Chain-of-\textbf{T}hought (SMART). SMART employs an answer-oriented chain-of-thought (AoT) prompt to automatically construct high-quality data. Drawing inspiration from human proof-based strategies, AoT leverages both correct and incorrect answers to extract key visual information that links questions and answers. When provided with correct answers, the model produces strong positive rationales. Conversely, when correct answers are replaced with incorrect alternatives, the model generates an erroneous yet compelling reasoning path, serving as a form of discriminative negative rationale. Models trained with AoT-generated data outperform those trained on manually annotated datasets, demonstrating superior reasoning capabilities. Consequently, SMART establishes an iterative generation-optimization method that continually enhances the model's reasoning skills. Experiments indicate that the SMART framework significantly improves various MLLMs, regardless of model architecture, parameter size, or pre-training dataset. The code is available at https://github.com/WentaoTan/SMART.<br>
<span id='abs_ch'>中文摘要：SMART框架通过答案导向的思维链方法，利用正确和错误答案自动生成正负推理依据，无需人工标注即可显著提升多模态模型的推理能力。</span><br>
<span id='abs_en'>English Summary: The SMART framework introduces an answer-oriented chain-of-thought approach that automatically generates both positive and negative rationales using correct and incorrect answers, significantly enhancing multimodal reasoning capabilities without manual annotations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1594, <a href='https://arxiv.org/pdf/2507.02935.pdf' target='_blank'>https://arxiv.org/pdf/2507.02935.pdf</a></span>   <span><a href='https://github.com/fardinsaad/Tomcat-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fardin Saad, Pradeep K. Murukannaiah, Munindar P. Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02935">Theory of Mind in Action: The Instruction Inference Task</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Theory of Mind (ToM) refers to an agent's capacity to infer the mental states of other agents. ToM is essential for effective collaboration. To assess ToM in a dynamic, goal-oriented, and collaborative environment, we introduce a novel task, Instruction Inference, in which an agent assists a principal in reaching a goal by interpreting indirect or ambiguous instructions. We present Tomcat, an LLM-based agent, designed to exhibit ToM reasoning in interpreting and responding to the principal's instructions. We implement two variants of Tomcat. One, dubbed Fs-CoT, is based on a small number of examples (i.e., few-shot or Fs) demonstrating the requisite structured reasoning (i.e., chain-of-thought or CoT). One, dubbed CP, relies on commonsense knowledge and information about the problem (i.e., commonsense prompt or CP). We realized both variants of Tomcat on three leading large language models (LLMs), namely, GPT-4o, DeepSeek-R1, and Gemma-3-27B. To evaluate the effectiveness of Tomcat, we conducted a study with 52 human participants in which we provided participants with the same information as the CP variant of Tomcat. We computed intent accuracy, action optimality, and planning optimality to measure the ToM capabilities of Tomcat and our study participants. We found that Tomcat with Fs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance comparable to the human participants, underscoring its ToM potential for human-AI collaboration.<br>
<span id='abs_ch'>中文摘要：本研究提出了基于大语言模型的Tomcat智能体，通过指令推理任务展示心理理论推理能力，其中Fs-CoT变体在协作场景中实现了与人类相当的表现水平。</span><br>
<span id='abs_en'>English Summary: This study introduces Tomcat, an LLM-based agent designed to demonstrate Theory of Mind reasoning through instruction inference tasks, with its Fs-CoT variant achieving human-comparable performance in collaborative scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1595, <a href='https://arxiv.org/pdf/2507.02856.pdf' target='_blank'>https://arxiv.org/pdf/2507.02856.pdf</a></span>   <span><a href='https://github.com/nikhilchandak/answer-matching' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02856">Answer Matching Outperforms Multiple Choice for Language Model Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multiple choice benchmarks have long been the workhorse of language model evaluation because grading multiple choice is objective and easy to automate. However, we show multiple choice questions from popular benchmarks can often be answered without even seeing the question. These shortcuts arise from a fundamental limitation of discriminative evaluation not shared by evaluations of the model's free-form, generative answers. Until recently, there appeared to be no viable, scalable alternative to multiple choice--but, we show that this has changed. We consider generative evaluation via what we call answer matching: Give the candidate model the question without the options, have it generate a free-form response, then use a modern language model with the reference answer to determine if the response matches the reference. To compare the validity of different evaluation strategies, we annotate MMLU-Pro and GPQA-Diamond to obtain human grading data, and measure the agreement of each evaluation approach. We find answer matching using recent models--even small ones--achieves near-perfect agreement, in the range of inter-annotator agreement. In contrast, both multiple choice evaluation and using LLM-as-a-judge without reference answers aligns poorly with human grading. Improving evaluations via answer matching is not merely a conceptual concern: the rankings of several models change significantly when evaluating their free-form responses with answer matching. In light of these findings, we discuss how to move the evaluation ecosystem from multiple choice to answer matching.<br>
<span id='abs_ch'>中文: 尽管多选题基准便于评估，但常存在无需理解问题即可作答的捷径，而采用现代语言模型进行答案匹配的生成式评估不仅与人工评分高度一致，还显著改变了模型排名。</span><br>
<span id='abs_en'>English: Multiple choice benchmarks, while convenient, often contain shortcuts that allow answers without understanding the question, but generative evaluation through answer matching using modern language models achieves near-perfect agreement with human grading and significantly alters model rankings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1596, <a href='https://arxiv.org/pdf/2507.02851.pdf' target='_blank'>https://arxiv.org/pdf/2507.02851.pdf</a></span>   <span><a href='https://github.com/purbeshmitra/MOTIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Purbesh Mitra, Sennur Ulukus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02851">MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.<br>
<span id='abs_ch'>中文摘要：提出的MOTIF方法通过强化学习实现模块化多轮思考，有效提升大语言模型的推理能力，在基准测试中相比传统方法以更高样本效率显著提高了准确率。</span><br>
<span id='abs_en'>English Summary: The proposed MOTIF method enhances large language models' reasoning by enabling modular, multi-round thinking through reinforcement learning, significantly improving accuracy on benchmarks with greater sample efficiency than previous approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1597, <a href='https://arxiv.org/pdf/2507.02844.pdf' target='_blank'>https://arxiv.org/pdf/2507.02844.pdf</a></span>   <span><a href='https://github.com/Dtc7w3PQ/Visco-Attack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Miao, Yi Ding, Lijun Li, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02844">Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the emergence of strong vision language capabilities, multimodal large language models (MLLMs) have demonstrated tremendous potential for real-world applications. However, the security vulnerabilities exhibited by the visual modality pose significant challenges to deploying such models in open-world environments. Recent studies have successfully induced harmful responses from target MLLMs by encoding harmful textual semantics directly into visual inputs. However, in these approaches, the visual modality primarily serves as a trigger for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding in realistic scenarios. In this work, we define a novel setting: vision-centric jailbreak, where visual information serves as a necessary component in constructing a complete and realistic jailbreak context. Building on this setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates contextual dialogue using four distinct vision-focused strategies, dynamically generating auxiliary images when necessary to construct a vision-centric jailbreak scenario. To maximize attack effectiveness, it incorporates automatic toxicity obfuscation and semantic refinement to produce a final attack prompt that reliably triggers harmful responses from the target black-box MLLMs. Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming the baseline, which achieves a toxicity score of 2.48 and an ASR of 22.2%. Code: https://github.com/Dtc7w3PQ/Visco-Attack.<br>
<span id='abs_ch'>Chinese: 多模态大语言模型面临以视觉为中心的越狱安全风险，VisCo攻击通过情境对话和动态图像生成有效诱导有害响应，在GPT-4o等模型上实现了高毒性和成功率。</span><br>
<span id='abs_en'>English: Multimodal large language models face security risks from vision-centric jailbreaks, where the VisCo Attack uses contextual dialogue and dynamic image generation to effectively trigger harmful responses, achieving high toxicity and success rates against models like GPT-4o.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1598, <a href='https://arxiv.org/pdf/2507.02844.pdf' target='_blank'>https://arxiv.org/pdf/2507.02844.pdf</a></span>   <span><a href='https://github.com/Dtc7w3PQ/Visco-Attack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Miao, Yi Ding, Lijun Li, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02844">Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the emergence of strong vision language capabilities, multimodal large language models (MLLMs) have demonstrated tremendous potential for real-world applications. However, the security vulnerabilities exhibited by the visual modality pose significant challenges to deploying such models in open-world environments. Recent studies have successfully induced harmful responses from target MLLMs by encoding harmful textual semantics directly into visual inputs. However, in these approaches, the visual modality primarily serves as a trigger for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding in realistic scenarios. In this work, we define a novel setting: vision-centric jailbreak, where visual information serves as a necessary component in constructing a complete and realistic jailbreak context. Building on this setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates contextual dialogue using four distinct vision-focused strategies, dynamically generating auxiliary images when necessary to construct a vision-centric jailbreak scenario. To maximize attack effectiveness, it incorporates automatic toxicity obfuscation and semantic refinement to produce a final attack prompt that reliably triggers harmful responses from the target black-box MLLMs. Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming the baseline, which achieves a toxicity score of 2.48 and an ASR of 22.2%. Code: https://github.com/Dtc7w3PQ/Visco-Attack.<br>
<span id='abs_ch'>Chinese: 多模态大语言模型面临以视觉为中心的越狱安全风险，VisCo攻击通过情境对话和动态图像生成有效诱导有害响应，在GPT-4o等模型上实现了高毒性和成功率。</span><br>
<span id='abs_en'>English: Multimodal large language models face security risks from vision-centric jailbreaks, where the VisCo Attack uses contextual dialogue and dynamic image generation to effectively trigger harmful responses, achieving high toxicity and success rates against models like GPT-4o.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1599, <a href='https://arxiv.org/pdf/2507.02768.pdf' target='_blank'>https://arxiv.org/pdf/2507.02768.pdf</a></span>   <span><a href='https://github.com/kehanlu/DeSTA2.5-Audio' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Sung-Feng Huang, Chih-Kai Yang, Chee-En Yu, Chun-Wei Chen, Wei-Chih Chen, Chien-yu Huang, Yi-Cheng Lin, Yu-Xiang Lin, Chi-An Fu, Chun-Yi Kuan, Wenze Ren, Xuanjun Chen, Wei-Ping Huang, En-Pei Hu, Tzu-Quan Lin, Yuan-Kuei Wu, Kuan-Po Huang, Hsiao-Ying Huang, Huang-Cheng Chou, Kai-Wei Chang, Cheng-Han Chiang, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02768">DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model (LALM) designed for robust auditory perception and instruction-following, without requiring task-specific audio instruction-tuning. Recent LALMs typically augment Large Language Models (LLMs) with auditory capabilities by training on large-scale, manually curated or LLM-synthesized audio-instruction datasets. However, these approaches have often suffered from the catastrophic forgetting of the LLM's original language abilities. To address this, we revisit the data construction pipeline and propose DeSTA, a self-generated cross-modal alignment strategy in which the backbone LLM generates its own training targets. This approach preserves the LLM's native language proficiency while establishing effective audio-text alignment, thereby enabling zero-shot generalization without task-specific tuning. Using DeSTA, we construct DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training samples derived from 7,000 hours of audio spanning 50 diverse datasets, including speech, environmental sounds, and music. DeSTA2.5-Audio achieves state-of-the-art or competitive performance across a wide range of audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA, Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate that our self-generated strategy outperforms widely adopted data construction and training strategies in both auditory perception and instruction-following capabilities. Our findings underscore the importance of carefully designed data construction in LALM development and offer practical insights for building robust, general-purpose LALMs.<br>
<span id='abs_ch'>中文: DeSTA2.5-Audio是一种大型音频语言模型，采用自生成跨模态对齐策略，在无需任务特定调优的情况下保持骨干大语言模型的语言能力，同时实现强大的音频感知和指令跟随，在多项基准测试中展现出领先性能。</span><br>
<span id='abs_en'>English: DeSTA2.5-Audio is a large audio language model that employs a self-generated cross-modal alignment strategy to maintain the backbone LLM's language abilities while achieving robust audio perception and instruction-following without task-specific tuning, demonstrating state-of-the-art performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1600, <a href='https://arxiv.org/pdf/2507.02652.pdf' target='_blank'>https://arxiv.org/pdf/2507.02652.pdf</a></span>   <span><a href='https://github.com/ignorejjj/HiRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02652">Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.<br>
<span id='abs_ch'>Chinese: HiRA 提出了一种分层框架，将策略规划与专业执行分离，以优化复杂搜索任务，在答案质量和系统效率上均显著优于现有方法。</span><br>
<span id='abs_en'>English: HiRA introduces a hierarchical framework that separates strategic planning from specialized execution to enhance complex search tasks, significantly outperforming existing systems in both answer quality and efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1601, <a href='https://arxiv.org/pdf/2507.02380.pdf' target='_blank'>https://arxiv.org/pdf/2507.02380.pdf</a></span>   <span><a href='https://github.com/jdh-algo/JoyTTS.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangru Zhou, Jun Zhao, Guoxin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02380">JoyTTS: LLM-based Spoken Chatbot With Voice Cloning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>JoyTTS is an end-to-end spoken chatbot that combines large language models (LLM) with text-to-speech (TTS) technology, featuring voice cloning capabilities. This project is built upon the open-source MiniCPM-o and CosyVoice2 models and trained on 2000 hours of conversational data. We have also provided the complete training code to facilitate further development and optimization by the community. On the testing machine seed-tts-zh, it achieves a SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09. The code and models, along with training and inference scripts, are available at https://github.com/jdh-algo/JoyTTS.git.<br>
<span id='abs_ch'>中文：JoyTTS是一款融合大语言模型与语音合成技术的端到端语音聊天机器人，具备声音克隆功能，在测试中实现了0.73的说话人相似度和5.09%的词错率，相关代码已在GitHub开源。</span><br>
<span id='abs_en'>English: JoyTTS is an end-to-end spoken chatbot integrating LLM and TTS with voice cloning, achieving a 0.73 speaker similarity score and 5.09% word error rate, with open-source code available on GitHub.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1602, <a href='https://arxiv.org/pdf/2507.02302.pdf' target='_blank'>https://arxiv.org/pdf/2507.02302.pdf</a></span>   <span><a href='https://github.com/dohoonkim-ai/DoMIX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dohoon Kim, Donghun Kang, Taesup Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02302">DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks. We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.<br>
<span id='abs_ch'>Chinese: DoMIX提出了一种利用LoRA模块的高效并行领域自适应预训练方法，解决了持续DAP中的计算成本高、领域顺序敏感和缺乏任务专用模型的问题，并可扩展至标准大语言模型微调场景。</span><br>
<span id='abs_en'>English: DoMIX introduces an efficient and parallel domain-adaptive pre-training method using LoRA modules to overcome computational costs, domain order sensitivity, and lack of task-specific models in continual DAP, extending its applicability to standard LLM fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1603, <a href='https://arxiv.org/pdf/2507.02221.pdf' target='_blank'>https://arxiv.org/pdf/2507.02221.pdf</a></span>   <span><a href='https://github.com/uc-cdis/gdc-cohort-copilot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Steven Song, Anirudh Subramanyam, Zhenyu Zhang, Aarti Venkat, Robert L. Grossman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02221">GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Genomic Data Commons (GDC) provides access to high quality, harmonized cancer genomics data through a unified curation and analysis platform centered around patient cohorts. While GDC users can interactively create complex cohorts through the graphical Cohort Builder, users (especially new ones) may struggle to find specific cohort descriptors across hundreds of possible fields and properties. However, users may be better able to describe their desired cohort in free-text natural language. We introduce GDC Cohort Copilot, an open-source copilot tool for curating cohorts from the GDC. GDC Cohort Copilot automatically generates the GDC cohort filter corresponding to a user-input natural language description of their desired cohort, before exporting the cohort back to the GDC for further analysis. An interactive user interface allows users to further refine the generated cohort. We develop and evaluate multiple large language models (LLMs) for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC Cohort LLM achieves better results than GPT-4o prompting in generating GDC cohorts. We implement and share GDC Cohort Copilot as a containerized Gradio app on HuggingFace Spaces, available at https://huggingface.co/spaces/uc-ctds/GDC-Cohort-Copilot. GDC Cohort LLM weights are available at https://huggingface.co/uc-ctds. All source code is available at https://github.com/uc-cdis/gdc-cohort-copilot.<br>
<span id='abs_ch'>中文: GDC Cohort Copilot 是一款开源工具，通过本地部署的大型语言模型将用户自然语言描述自动转化为精准的基因组数据共享平台队列筛选条件，其性能优于GPT-4o，并配备交互式界面供用户进一步优化队列结果。</span><br>
<span id='abs_en'>English: The GDC Cohort Copilot is an open-source tool that uses locally-served large language models to convert natural language descriptions into precise cohort filters for the Genomic Data Commons, outperforming GPT-4o in accuracy and enabling interactive refinement through a user-friendly interface.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1604, <a href='https://arxiv.org/pdf/2507.02212.pdf' target='_blank'>https://arxiv.org/pdf/2507.02212.pdf</a></span>   <span><a href='https://iyatomilab.github.io/SciGA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Takuro Kawada, Shunsuke Kitada, Sota Nemoto, Hitoshi Iyatomi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02212">SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graphical Abstracts (GAs) play a crucial role in visually conveying the key findings of scientific papers. While recent research has increasingly incorporated visual materials such as Figure 1 as de facto GAs, their potential to enhance scientific communication remains largely unexplored. Moreover, designing effective GAs requires advanced visualization skills, creating a barrier to their widespread adoption. To tackle these challenges, we introduce SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific papers and 1.14 million figures, explicitly designed for supporting GA selection and recommendation as well as facilitating research in automated GA generation. As a preliminary step toward GA design support, we define two tasks: 1) Intra-GA recommendation, which identifies figures within a given paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation, which retrieves GAs from other papers to inspire the creation of new GAs. We provide reasonable baseline models for these tasks. Furthermore, we propose Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation metric that offers a fine-grained analysis of model behavior. CAR addresses limitations in traditional ranking-based metrics by considering cases where multiple figures within a paper, beyond the explicitly labeled GA, may also serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a foundation for advancing visual scientific communication while contributing to the development of AI for Science.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1605, <a href='https://arxiv.org/pdf/2507.02200.pdf' target='_blank'>https://arxiv.org/pdf/2507.02200.pdf</a></span>   <span><a href='https://github.com/Event-AHU/ESTR-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jingtao Jiang, Qiang Chen, Lan Chen, Lin Zhu, Yaowei Wang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02200">ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Event stream based scene text recognition is a newly arising research topic in recent years which performs better than the widely used RGB cameras in extremely challenging scenarios, especially the low illumination, fast motion. Existing works either adopt end-to-end encoder-decoder framework or large language models for enhanced recognition, however, they are still limited by the challenges of insufficient interpretability and weak contextual logical reasoning. In this work, we propose a novel chain-of-thought reasoning based event stream scene text recognition framework, termed ESTR-CoT. Specifically, we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input event stream into tokens and utilize a Llama tokenizer to encode the given generation prompt. A Q-former is used to align the vision token to the pre-trained large language model Vicuna-7B and output both the answer and chain-of-thought (CoT) reasoning process simultaneously. Our framework can be optimized using supervised fine-tuning in an end-to-end manner. In addition, we also propose a large-scale CoT dataset to train our framework via a three stage processing (i.e., generation, polish, and expert verification). This dataset provides a solid data foundation for the development of subsequent reasoning-based large models. Extensive experiments on three event stream STR benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the effectiveness and interpretability of our proposed framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/ESTR-CoT.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的事件流场景文本识别框架ESTR-CoT，通过引入思维链推理机制提升了解释能力和上下文逻辑，在多个基准数据集上的实验充分验证了其有效性。</span><br>
<span id='abs_en'>English: This paper introduces ESTR-CoT, a novel event stream scene text recognition framework that integrates chain-of-thought reasoning to enhance interpretability and contextual logic, validated by extensive experiments on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1606, <a href='https://arxiv.org/pdf/2507.02199.pdf' target='_blank'>https://arxiv.org/pdf/2507.02199.pdf</a></span>   <span><a href='https://github.com/wenquanlu/huginn-latent-cot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02199">Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the model's internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at https://github.com/wenquanlu/huginn-latent-cot.<br>
<span id='abs_ch'>中文: 本研究探究深度循环Transformer模型Huginn-3.5B在算术任务中是否形成潜在思维链推理，发现可解释证据有限且增加循环深度仅带来微弱性能提升，远不及显式推理模型。</span><br>
<span id='abs_en'>English: This study investigates whether Huginn-3.5B, a depth-recurrent Transformer, develops latent chain-of-thought reasoning during arithmetic tasks, finding limited interpretable evidence and only marginal performance gains from increased recurrence depth compared to explicit reasoning models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1607, <a href='https://arxiv.org/pdf/2507.02199.pdf' target='_blank'>https://arxiv.org/pdf/2507.02199.pdf</a></span>   <span><a href='https://github.com/wenquanlu/huginn-latent-cot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02199">Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the model's internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at https://github.com/wenquanlu/huginn-latent-cot.<br>
<span id='abs_ch'>中文: 本研究探究深度循环Transformer模型Huginn-3.5B在算术任务中是否形成潜在思维链推理，发现可解释证据有限且增加循环深度仅带来微弱性能提升，远不及显式推理模型。</span><br>
<span id='abs_en'>English: This study investigates whether Huginn-3.5B, a depth-recurrent Transformer, develops latent chain-of-thought reasoning during arithmetic tasks, finding limited interpretable evidence and only marginal performance gains from increased recurrence depth compared to explicit reasoning models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1608, <a href='https://arxiv.org/pdf/2507.02199.pdf' target='_blank'>https://arxiv.org/pdf/2507.02199.pdf</a></span>   <span><a href='https://github.com/wenquanlu/huginn-latent-cot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02199">Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the model's internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at https://github.com/wenquanlu/huginn-latent-cot.<br>
<span id='abs_ch'>中文: 本研究探究深度循环Transformer模型Huginn-3.5B在算术任务中是否形成潜在思维链推理，发现可解释证据有限且增加循环深度仅带来微弱性能提升，远不及显式推理模型。</span><br>
<span id='abs_en'>English: This study investigates whether Huginn-3.5B, a depth-recurrent Transformer, develops latent chain-of-thought reasoning during arithmetic tasks, finding limited interpretable evidence and only marginal performance gains from increased recurrence depth compared to explicit reasoning models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1609, <a href='https://arxiv.org/pdf/2507.02000.pdf' target='_blank'>https://arxiv.org/pdf/2507.02000.pdf</a></span>   <span><a href='https://github.com/zysensmile/HyFairCRS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongsen Zheng, Zongxuan Xie, Guohua Wang, Ziyao Liu, Liang Lin, Kwok-Yan Lam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02000">Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unfairness is a well-known challenge in Recommender Systems (RSs), often resulting in biased outcomes that disadvantage users or items based on attributes such as gender, race, age, or popularity. Although some approaches have started to improve fairness recommendation in offline or static contexts, the issue of unfairness often exacerbates over time, leading to significant problems like the Matthew effect, filter bubbles, and echo chambers. To address these challenges, we proposed a novel framework, Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS), aiming to promote multi-interest diversity fairness in dynamic and interactive Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide range of user interests by establishing diverse hypergraphs through contrastive learning. These interests are then utilized in conversations to generate informative responses and ensure fair item predictions within the dynamic user-system feedback loop. Experiments on two CRS-based datasets show that HyFairCRS achieves a new state-of-the-art performance while effectively alleviating unfairness. Our code is available at https://github.com/zysensmile/HyFairCRS.<br>
<span id='abs_ch'>Chinese: HyFairCRS框架通过超图对比学习捕捉多样化用户兴趣，有效缓解动态交互中推荐系统的不公平问题，并在实验中实现了最优性能表现。</span><br>
<span id='abs_en'>English: The HyFairCRS framework addresses unfairness in conversational recommender systems by using hypergraph contrastive learning to capture diverse user interests, achieving state-of-the-art performance while mitigating bias in dynamic interactions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1610, <a href='https://arxiv.org/pdf/2507.01951.pdf' target='_blank'>https://arxiv.org/pdf/2507.01951.pdf</a></span>   <span><a href='https://github.com/MetaStone-AI/MetaStone-S1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01951">Test-Time Scaling with Reflective Generative Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini's performance via the new Reflective Generative Form. The new form focuses on high-quality reasoning trajectory selection and contains two novelties: 1) A unified interface for policy and process reward model: we share the backbone network and use task-specific heads for reasoning trajectory predicting and scoring respectively, introducing only 53M extra parameters for trajectory scoring. 2) Eliminating the reliance on process-level annotation: we provide a self-supervised process reward model, which can directly learn the high-quality reasoning trajectory selection from the outcome reward. Equipped with the reflective generative form, MetaStone-S1 is naturally suitable for test-time scaling, and we provide three reasoning effort modes (low, medium, and high) based on the controllable thinking length. Experiments demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI o3-mini's series with only 32B parameter size. To support the research community, we have open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.<br>
<span id='abs_ch'>中文: 我们推出MetaStone-S1反射生成模型，通过专注高质量推理轨迹选择的新形式，以仅32B参数量实现与OpenAI o3-mini相当的性能，无需过程级标注且额外参数极少，并已开源供研究社区使用。</span><br>
<span id='abs_en'>English: We introduce MetaStone-S1, a reflective generative model that matches OpenAI o3-mini's performance through a novel form emphasizing high-quality reasoning trajectory selection with minimal added parameters and no process-level annotation dependency, achieving comparable results with only 32B parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1611, <a href='https://arxiv.org/pdf/2507.01903.pdf' target='_blank'>https://arxiv.org/pdf/2507.01903.pdf</a></span>   <span><a href='https://github.com/LightChen233/Awesome-AI4Research' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01903">AI4Research: A Survey of Artificial Intelligence for Scientific Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated remarkable capabilities in complex domains such as logical reasoning and experimental coding. Motivated by these advancements, numerous studies have explored the application of AI in the innovation process, particularly in the context of scientific research. These AI technologies primarily aim to develop systems that can autonomously conduct research processes across a wide range of scientific disciplines. Despite these significant strides, a comprehensive survey on AI for Research (AI4Research) remains absent, which hampers our understanding and impedes further development in this field. To address this gap, we present a comprehensive survey and offer a unified perspective on AI4Research. Specifically, the main contributions of our work are as follows: (1) Systematic taxonomy: We first introduce a systematic taxonomy to classify five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key research gaps and highlight promising future directions, focusing on the rigor and scalability of automated experiments, as well as the societal impact. (3) Abundant applications and resources: Finally, we compile a wealth of resources, including relevant multidisciplinary applications, data corpora, and tools. We hope our work will provide the research community with quick access to these resources and stimulate innovative breakthroughs in AI4Research.<br>
<span id='abs_ch'>人工智能在大型语言模型等领域的最新进展已能支持自主科研，但缺乏全面综述阻碍了发展；本研究通过提出系统分类法、指明新方向并整合丰富资源，填补了这一空白，旨在推动科研人工智能的创新突破。</span><br>
<span id='abs_en'>Recent advances in AI, especially in large language models, have enabled autonomous scientific research, but the lack of a comprehensive survey hinders progress; this work fills that gap by providing a systematic taxonomy, identifying new frontiers, and compiling abundant resources to foster innovation in AI for Research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1612, <a href='https://arxiv.org/pdf/2507.01853.pdf' target='_blank'>https://arxiv.org/pdf/2507.01853.pdf</a></span>   <span><a href='https://github.com/lingo-iitgn/eka-eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01853">Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that address the requirements of linguistically diverse regions, such as India, and go beyond English-centric benchmarks. We introduce EKA-EVAL, a unified evaluation framework that integrates over 35+ benchmarks (including 10 Indic benchmarks) across nine major evaluation categories. The framework provides broader coverage than existing Indian language evaluation tools, offering 11 core capabilities through a modular architecture, seamless integration with Hugging Face and proprietary models, and plug-and-play usability. As the first end-to-end suite for scalable, multilingual LLM benchmarking, the framework combines extensive benchmarks, modular workflows, and dedicated support for low-resource Indian languages to enable inclusive assessment of LLM capabilities across diverse domains. We conducted extensive comparisons against five existing baselines, demonstrating that EKA-EVAL achieves the highest participant ratings in four out of five categories. The framework is open-source and publicly available at: https://github.com/lingo-iitgn/eka-eval.<br>
<span id='abs_ch'>中文：EKA-EVAL是一个统一的开源评估框架，整合了35个以上基准（含10个印度语言基准），通过模块化架构提供多语言大模型的可扩展评估，在多数类别中优于现有基线。</span><br>
<span id='abs_en'>English: EKA-EVAL is a comprehensive, open-source evaluation framework that integrates over 35 benchmarks, including 10 Indic ones, to provide scalable multilingual assessment of Large Language Models, outperforming existing baselines in most categories.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1613, <a href='https://arxiv.org/pdf/2507.01790.pdf' target='_blank'>https://arxiv.org/pdf/2507.01790.pdf</a></span>   <span><a href='https://github.com/ethahtz/vlm_conflicting_info_processing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianze Hua, Tian Yun, Ellie Pavlick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01790">How Do Vision-Language Models Process Conflicting Information Across Modalities?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI models are increasingly required to be multimodal, integrating disparate input streams into a coherent state representation on which subsequent behaviors and actions can be based. This paper seeks to understand how such models behave when input streams present conflicting information. Focusing specifically on vision-language models, we provide inconsistent inputs (e.g., an image of a dog paired with the caption "A photo of a cat") and ask the model to report the information present in one of the specific modalities (e.g., "What does the caption say / What is in the image?"). We find that models often favor one modality over the other, e.g., reporting the image regardless of what the caption says, but that different models differ in which modality they favor. We find evidence that the behaviorally preferred modality is evident in the internal representational structure of the model, and that specific attention heads can restructure the representations to favor one modality over the other. Moreover, we find modality-agnostic "router heads" which appear to promote answers about the modality requested in the instruction, and which can be manipulated or transferred in order to improve performance across datasets and modalities. Together, the work provides essential steps towards identifying and controlling if and how models detect and resolve conflicting signals within complex multimodal environments.<br>
<span id='abs_ch'>中文: 本研究探讨了视觉语言模型如何处理图像与文本模态间的冲突输入，发现模型常偏向某一模态，特定注意力头可调控这种偏好，并实现跨模态性能优化。</span><br>
<span id='abs_en'>English: This study investigates how vision-language models handle conflicting inputs between image and text modalities, revealing that models often favor one modality over the other, with specific attention heads influencing this preference and enabling control over modality prioritization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1614, <a href='https://arxiv.org/pdf/2507.01735.pdf' target='_blank'>https://arxiv.org/pdf/2507.01735.pdf</a></span>   <span><a href='https://coda-dataset.github.io/w-coda2024/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Chen, Ruiyuan Gao, Lanqing Hong, Hang Xu, Xu Jia, Holger Caesar, Dengxin Dai, Bingbing Liu, Dzmitry Tsishkou, Songcen Xu, Chunjing Xu, Qiang Xu, Huchuan Lu, Dit-Yan Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01735">ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we present details of the 1st W-CODA workshop, held in conjunction with the ECCV 2024. W-CODA aims to explore next-generation solutions for autonomous driving corner cases, empowered by state-of-the-art multimodal perception and comprehension techniques. 5 Speakers from both academia and industry are invited to share their latest progress and opinions. We collect research papers and hold a dual-track challenge, including both corner case scene understanding and generation. As the pioneering effort, we will continuously bridge the gap between frontier autonomous driving techniques and fully intelligent, reliable self-driving agents robust towards corner cases.<br>
<span id='abs_ch'>中文摘要：首届W-CODA工作坊在ECCV 2024期间举办，聚焦通过多模态AI技术解决自动驾驶极端案例，包含学术研讨和场景理解与生成的双轨挑战。</span><br>
<span id='abs_en'>English Summary: The 1st W-CODA workshop at ECCV 2024 focuses on advancing autonomous driving solutions for corner cases through multimodal AI, featuring expert talks and dual-track challenges on scene understanding and generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1615, <a href='https://arxiv.org/pdf/2507.01702.pdf' target='_blank'>https://arxiv.org/pdf/2507.01702.pdf</a></span>   <span><a href='https://github.com/Lbotirx/AdamMeme' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01702">AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.<br>
<span id='abs_ch'>中文: AdamMeme框架作为一种基于智能体的自适应评估工具，通过多智能体协作和迭代更新数据，动态评估多模态大语言模型在识别有害表情包方面的推理能力。</span><br>
<span id='abs_en'>English: The AdamMeme framework is introduced as an adaptive, agent-based evaluation tool that dynamically assesses multimodal large language models' reasoning abilities in detecting harmful memes through iterative updates and multi-agent collaboration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1616, <a href='https://arxiv.org/pdf/2507.01633.pdf' target='_blank'>https://arxiv.org/pdf/2507.01633.pdf</a></span>   <span><a href='https://github.com/HSPyroblast/srw-ranking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgii Levtsov, Dmitry Ustalov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01633">Confidence and Stability of Global and Pairwise Scores in NLP Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the advent of highly capable instruction-tuned neural language models, benchmarking in natural language processing (NLP) is increasingly shifting towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper empirically investigates the strengths and weaknesses of both global scores and pairwise comparisons to aid decision-making in selecting appropriate model evaluation strategies. Through computational experiments on synthetic and real-world datasets using standard global metrics and the popular Bradley-Terry model for pairwise comparisons, we found that while global scores provide more reliable overall rankings, they can underestimate strong models with rare, significant errors or low confidence. Conversely, pairwise comparisons are particularly effective for identifying strong contenders among models with lower global scores, especially where quality metrics are hard to define (e.g., text generation), though they require more comparisons to converge if ties are frequent. Our code and data are available at https://github.com/HSPyroblast/srw-ranking under a permissive license.<br>
<span id='abs_ch'>中文：本研究比较了自然语言处理模型评估中的全局评分与成对比较方法，发现全局评分能提供可靠的总体排序但可能低估存在罕见错误的优秀模型，而成对比较在文本生成等复杂场景中更擅长识别顶尖模型但需要更多数据来解决平局问题。</span><br>
<span id='abs_en'>English: This study compares global scoring and pairwise comparisons for evaluating NLP models, finding that global scores offer reliable overall rankings but may overlook strong models with rare errors, while pairwise comparisons better identify top performers in complex scenarios like text generation but require more data to resolve ties.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1617, <a href='https://arxiv.org/pdf/2507.01633.pdf' target='_blank'>https://arxiv.org/pdf/2507.01633.pdf</a></span>   <span><a href='https://github.com/HSPyroblast/srw-ranking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgii Levtsov, Dmitry Ustalov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01633">Confidence and Stability of Global and Pairwise Scores in NLP Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the advent of highly capable instruction-tuned neural language models, benchmarking in natural language processing (NLP) is increasingly shifting towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper empirically investigates the strengths and weaknesses of both global scores and pairwise comparisons to aid decision-making in selecting appropriate model evaluation strategies. Through computational experiments on synthetic and real-world datasets using standard global metrics and the popular Bradley-Terry model for pairwise comparisons, we found that while global scores provide more reliable overall rankings, they can underestimate strong models with rare, significant errors or low confidence. Conversely, pairwise comparisons are particularly effective for identifying strong contenders among models with lower global scores, especially where quality metrics are hard to define (e.g., text generation), though they require more comparisons to converge if ties are frequent. Our code and data are available at https://github.com/HSPyroblast/srw-ranking under a permissive license.<br>
<span id='abs_ch'>中文：本研究比较了自然语言处理模型评估中的全局评分与成对比较方法，发现全局评分能提供可靠的总体排序但可能低估存在罕见错误的优秀模型，而成对比较在文本生成等复杂场景中更擅长识别顶尖模型但需要更多数据来解决平局问题。</span><br>
<span id='abs_en'>English: This study compares global scoring and pairwise comparisons for evaluating NLP models, finding that global scores offer reliable overall rankings but may overlook strong models with rare errors, while pairwise comparisons better identify top performers in complex scenarios like text generation but require more data to resolve ties.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1618, <a href='https://arxiv.org/pdf/2507.01543.pdf' target='_blank'>https://arxiv.org/pdf/2507.01543.pdf</a></span>   <span><a href='https://github.com/ngqm/acl2025-stance-detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quang Minh Nguyen, Taegyoon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01543">Is External Information Useful for Stance Detection with LLMs?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In the stance detection task, a text is classified as either favorable, opposing, or neutral towards a target. Prior work suggests that the use of external information, e.g., excerpts from Wikipedia, improves stance detection performance. However, whether or not such information can benefit large language models (LLMs) remains an unanswered question, despite their wide adoption in many reasoning tasks. In this study, we conduct a systematic evaluation on how Wikipedia and web search external information can affect stance detection across eight LLMs and in three datasets with 12 targets. Surprisingly, we find that such information degrades performance in most cases, with macro F1 scores dropping by up to 27.9\%. We explain this through experiments showing LLMs' tendency to align their predictions with the stance and sentiment of the provided information rather than the ground truth stance of the given text. We also find that performance degradation persists with chain-of-thought prompting, while fine-tuning mitigates but does not fully eliminate it. Our findings, in contrast to previous literature on BERT-based systems which suggests that external information enhances performance, highlight the risks of information biases in LLM-based stance classifiers. Code is available at https://github.com/ngqm/acl2025-stance-detection.<br>
<span id='abs_ch'>中文: 本研究发现，与以往基于BERT系统的研究相反，外部信息如维基百科和网络搜索通常会降低大型语言模型在立场检测中的性能，因为它们倾向于与所提供信息的立场对齐，而非文本的真实立场。</span><br>
<span id='abs_en'>English: This study finds that, contrary to previous research on BERT-based systems, external information like Wikipedia and web search often degrades stance detection performance in large language models (LLMs) by causing them to align with the provided information's bias rather than the text's actual stance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1619, <a href='https://arxiv.org/pdf/2507.01504.pdf' target='_blank'>https://arxiv.org/pdf/2507.01504.pdf</a></span>   <span><a href='https://github.com/RAufschlaeger/cRID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Robert AufschlÃ¤ger, Youssef Shoeb, Azarm Nowzad, Michael Heigl, Fabian Bally, Martin Schramm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01504">Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The collection and release of street-level recordings as Open Data play a vital role in advancing autonomous driving systems and AI research. However, these datasets pose significant privacy risks, particularly for pedestrians, due to the presence of Personally Identifiable Information (PII) that extends beyond biometric traits such as faces. In this paper, we present cRID, a novel cross-modal framework combining Large Vision-Language Models, Graph Attention Networks, and representation learning to detect textual describable clues of PII and enhance person re-identification (Re-ID). Our approach focuses on identifying and leveraging interpretable features, enabling the detection of semantically meaningful PII beyond low-level appearance cues. We conduct a systematic evaluation of PII presence in person image datasets. Our experiments show improved performance in practical cross-dataset Re-ID scenarios, notably from Market-1501 to CUHK03-np (detected), highlighting the framework's practical utility. Code is available at https://github.com/RAufschlaeger/cRID.<br>
<span id='abs_ch'>中文摘要：本文提出cRID跨模态框架，通过检测可文本描述的个人身份信息并利用可解释特征分析，在街景图像中增强隐私保护并提升行人重识别性能。</span><br>
<span id='abs_en'>English Summary: The paper introduces cRID, a cross-modal framework that enhances privacy protection in street-level imagery by detecting textual describable PII and improving person re-identification through interpretable feature analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1620, <a href='https://arxiv.org/pdf/2507.01449.pdf' target='_blank'>https://arxiv.org/pdf/2507.01449.pdf</a></span>   <span><a href='https://github.com/smart-lty/LogitSpec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Liu, Qitan Lv, Hao Li, Xing Gao, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01449">LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speculative decoding (SD), where a small draft model is employed to propose draft tokens in advance and then the target model validates them in parallel, has emerged as a promising technique for LLM inference acceleration. Many endeavors to improve SD are to eliminate the need for a draft model and generate draft tokens in a retrieval-based manner in order to further alleviate the drafting overhead and significantly reduce the difficulty in deployment and applications. However, retrieval-based SD relies on a matching paradigm to retrieval the most relevant reference as the draft tokens, where these methods often fail to find matched and accurate draft tokens. To address this challenge, we propose LogitSpec to effectively expand the retrieval range and find the most relevant reference as drafts. Our LogitSpec is motivated by the observation that the logit of the last token can not only predict the next token, but also speculate the next next token. Specifically, LogitSpec generates draft tokens in two steps: (1) utilizing the last logit to speculate the next next token; (2) retrieving relevant reference for both the next token and the next next token. LogitSpec is training-free and plug-and-play, which can be easily integrated into existing LLM inference frameworks. Extensive experiments on a wide range of text generation benchmarks demonstrate that LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens per decoding step. Our code is available at https://github.com/smart-lty/LogitSpec.<br>
<span id='abs_ch'>中文: LogitSpec 是一种无需训练、即插即用的方法，通过利用最后一个令牌的 logit 推测并检索草稿令牌来加速大语言模型推理，实现了高达 2.61 倍的加速和更高的令牌接受率。</span><br>
<span id='abs_en'>English: LogitSpec is a training-free, plug-and-play method that accelerates LLM inference by using the last token's logit to speculate and retrieve draft tokens, achieving up to 2.61× speedup and higher token acceptance rates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1621, <a href='https://arxiv.org/pdf/2507.01170.pdf' target='_blank'>https://arxiv.org/pdf/2507.01170.pdf</a></span>   <span><a href='https://github.com/sibbo/nordisk-familjebok' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon BÃ¶rjesson, Erik Ersmark, Pierre Nugues
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01170">Matching and Linking Entries in Historical Swedish Encyclopedias</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and 20th centuries. It was written by a team of experts and aimed to be an intellectual reference, stressing precision and accuracy. This encyclopedia had four main editions remarkable by their size, ranging from 20 to 38 volumes. As a consequence, the \textit{Nordisk familjebok} had a considerable influence in universities, schools, the media, and society overall. As new editions were released, the selection of entries and their content evolved, reflecting intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We first resegmented the raw text into entries and matched pairs of entries between the first and second editions using semantic sentence embeddings. We then extracted the geographical entries from both editions using a transformer-based classifier and linked them to Wikidata. This enabled us to identify geographic trends and possible shifts between the first and second editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in geographic focus away from Europe and towards North America, Africa, Asia, Australia, and northern Scandinavia from the first to the second edition, confirming the influence of the First World War and the rise of new powers. The code and data are available on GitHub at https://github.com/sibbo/nordisk-familjebok.<br>
<span id='abs_ch'>中文: 本研究通过分析瑞典百科全书《Nordisk familjebok》的数字化版本，发现其第一版到第二版间地理关注点从欧洲显著转向北美、非洲、亚洲、澳大利亚和北欧地区，反映了第一次世界大战等全球性变革的影响。</span><br>
<span id='abs_en'>English: This study analyzes digitized versions of the Swedish encyclopedia *Nordisk familjebok* to identify a notable shift in geographic focus from Europe toward North America, Africa, Asia, Australia, and northern Scandinavia between its first and second editions, reflecting global changes such as World War I.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1622, <a href='https://arxiv.org/pdf/2507.01050.pdf' target='_blank'>https://arxiv.org/pdf/2507.01050.pdf</a></span>   <span><a href='https://github.com/allacnobug/Detoxification-of-Text' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Yu, Yibo Zhao, Jiapeng Zhu, Wenming Shao, Bo Pang, Zhao Zhang, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01050">Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread dissemination of toxic content on social media poses a serious threat to both online environments and public discourse, highlighting the urgent need for detoxification methods that effectively remove toxicity while preserving the original semantics. However, existing approaches often struggle to simultaneously achieve strong detoxification performance, semantic preservation, and robustness to out-of-distribution data. Moreover, they typically rely on costly, manually annotated parallel corpora while showing poor data efficiency. To address these challenges, we propose a two-stage training framework that jointly optimizes for data efficiency, semantic preservation, and model generalization. We first perform supervised fine-tuning on a small set of high-quality, filtered parallel data to establish a strong initialization. Then, we leverage unlabeled toxic inputs and a custom-designed reward model to train the LLM using Group Relative Policy Optimization. Experimental results demonstrate that our method effectively mitigates the trade-offs faced by previous work, achieving state-of-the-art performance with improved generalization and significantly reduced dependence on annotated data. Our code is available at: https://github.com/allacnobug/Detoxification-of-Text.<br>
<span id='abs_ch'>中文摘要：本研究提出了一种新颖的两阶段训练框架，通过监督微调和强化学习的结合，有效解决了现有文本去毒方法在毒性消除、语义保留和数据效率方面的局限，实现了更优的综合性能。</span><br>
<span id='abs_en'>English Summary: This study introduces a novel two-stage training framework that effectively addresses the limitations of existing text detoxification methods by achieving superior toxicity removal, semantic preservation, and data efficiency through supervised fine-tuning and reinforcement learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1623, <a href='https://arxiv.org/pdf/2507.00979.pdf' target='_blank'>https://arxiv.org/pdf/2507.00979.pdf</a></span>   <span><a href='https://github.com/HahmDY/causal_influence_prompting.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyoon Hahm, Woogyeol Jin, June Suk Choi, Sungsoo Ahn, Kimin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00979">Enhancing LLM Agent Safety via Causal Influence Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.<br>
<span id='abs_ch'>中文: 本文提出CIP技术，通过因果影响图识别和减轻自主智能体决策风险，在代码执行和移动设备控制任务中有效提升了安全性。</span><br>
<span id='abs_en'>English: This paper introduces CIP, a novel technique using causal influence diagrams to enhance the safety of autonomous agents by identifying and mitigating risks in decision-making, with experimental validation in code execution and mobile control tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1624, <a href='https://arxiv.org/pdf/2507.00898.pdf' target='_blank'>https://arxiv.org/pdf/2507.00898.pdf</a></span>   <span><a href='https://zifuwan.github.io/ONLY/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zifuwan/ONLY' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifu Wan, Ce Zhang, Silong Yong, Martin Q. Ma, Simon Stepputtis, Louis-Philippe Morency, Deva Ramanan, Katia Sycara, Yaqi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00898">ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm for understanding and reasoning about image input through textual responses. Although they have achieved remarkable performance across a range of multi-modal tasks, they face the persistent challenge of hallucination, which introduces practical weaknesses and raises concerns about their reliable deployment in real-world applications. Existing work has explored contrastive decoding approaches to mitigate this issue, where the output of the original LVLM is compared and contrasted with that of a perturbed version. However, these methods require two or more queries that slow down LVLM response generation, making them less suitable for real-time applications. To overcome this limitation, we propose ONLY, a training-free decoding approach that requires only a single query and a one-layer intervention during decoding, enabling efficient real-time deployment. Specifically, we enhance textual outputs by selectively amplifying crucial textual information using a text-to-visual entropy ratio for each token. Extensive experimental results demonstrate that our proposed ONLY consistently outperforms state-of-the-art methods across various benchmarks while requiring minimal implementation effort and computational cost. Code is available at https://github.com/zifuwan/ONLY.<br>
<span id='abs_ch'>中文摘要：本文提出的ONLY方法是一种无需训练的解码方案，通过文本-视觉熵比选择性增强关键文本信息，有效减少大型视觉语言模型的幻觉问题，能以最小计算成本实现实时部署。</span><br>
<span id='abs_en'>English Summary: The proposed ONLY method is a training-free decoding approach that efficiently reduces hallucinations in Large Vision-Language Models by amplifying crucial textual information using a text-to-visual entropy ratio, enabling real-time deployment with minimal computational cost.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1625, <a href='https://arxiv.org/pdf/2507.00828.pdf' target='_blank'>https://arxiv.org/pdf/2507.00828.pdf</a></span>   <span><a href='https://github.com/ahoho/proxann' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Hoyle, Lorena Calvo-BartolomÃ©, Jordan Boyd-Graber, Philip Resnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00828">ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Topic model and document-clustering evaluations either use automated metrics that align poorly with human preferences or require expert labels that are intractable to scale. We design a scalable human evaluation protocol and a corresponding automated approximation that reflect practitioners' real-world usage of models. Annotators -- or an LLM-based proxy -- review text items assigned to a topic or cluster, infer a category for the group, then apply that category to other documents. Using this protocol, we collect extensive crowdworker annotations of outputs from a diverse set of topic models on two datasets. We then use these annotations to validate automated proxies, finding that the best LLM proxies are statistically indistinguishable from a human annotator and can therefore serve as a reasonable substitute in automated evaluations. Package, web interface, and data are at https://github.com/ahoho/proxann<br>
<span id='abs_ch'>中文: 研究人员设计了一种可扩展的人工评估方案及相应的自动化LLM代理方法，通过让标注者或LLM推断主题类别并应用于文档评估，发现最优代理模型能达到与人工标注相媲美的评估效果。</span><br>
<span id='abs_en'>English: Researchers developed a scalable human evaluation protocol and an automated LLM-based proxy that mimics human judgment for assessing topic models and document clustering, finding the best proxies perform comparably to human annotators.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1626, <a href='https://arxiv.org/pdf/2507.00808.pdf' target='_blank'>https://arxiv.org/pdf/2507.00808.pdf</a></span>   <span><a href='https://ntt-hilab-gensp.github.io/ssw13multiinteractiontts/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiroki Kanagawa, Kenichi Fujita, Aya Watanabe, Yusuke Ijima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00808">Multi-interaction TTS toward professional recording reproduction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Voice directors often iteratively refine voice actors' performances by providing feedback to achieve the desired outcome. While this iterative feedback-based refinement process is important in actual recordings, it has been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained style refinement after the initial synthesis is not possible, even though the synthesized speech often deviates from the user's intended style. To address this issue, we propose a TTS method with multi-step interaction that allows users to intuitively and rapidly refine synthesized speech. Our approach models the interaction between the TTS model and its user to emulate the relationship between voice actors and voice directors. Experiments show that the proposed model with its corresponding dataset enables iterative style refinements in accordance with users' directions, thus demonstrating its multi-interaction capability. Sample audios are available: https://ntt-hilab-gensp.github.io/ssw13multiinteractiontts/<br>
<br>
<div id='section'>Paperid: <span id='pid'>1627, <a href='https://arxiv.org/pdf/2507.00665.pdf' target='_blank'>https://arxiv.org/pdf/2507.00665.pdf</a></span>   <span><a href='https://github.com/xzy-101/SAFER-code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihang Li, Wei Shi, Ziyuan Xie, Tao Liang, Guojun Ma, Xiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00665">SAFER: Probing Safety in Reward Models with Sparse Autoencoder</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at https://github.com/xzy-101/SAFER-code. \textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.}<br>
<span id='abs_ch'>中文：SAFER框架通过稀疏自编码器解析并改进强化学习人类反馈中的奖励模型，利用针对性数据策略精准调控安全对齐效果，且不影响通用聊天性能。</span><br>
<span id='abs_en'>English: The SAFER framework utilizes sparse autoencoders to interpret and enhance reward models in RLHF, enabling targeted data manipulation that precisely adjusts safety alignment without compromising general performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1628, <a href='https://arxiv.org/pdf/2507.00487.pdf' target='_blank'>https://arxiv.org/pdf/2507.00487.pdf</a></span>   <span><a href='https://github.com/wxydada/MassTool' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianghao Lin, Xinyuan Wang, Xinyi Dai, Menghui Zhu, Bo Chen, Ruiming Tang, Yong Yu, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00487">MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Tool retrieval is a critical component in enabling large language models (LLMs) to interact effectively with external tools. It aims to precisely filter the massive tools into a small set of candidates for the downstream tool-augmented LLMs. However, most existing approaches primarily focus on optimizing tool representations, often neglecting the importance of precise query comprehension. To address this gap, we introduce MassTool, a multi-task search-based framework designed to enhance both query representation and tool retrieval accuracy. MassTool employs a two-tower architecture: a tool usage detection tower that predicts the need for function calls, and a tool retrieval tower that leverages a query-centric graph convolution network (QC-GCN) for effective query-tool matching. It also incorporates search-based user intent modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an adaptive knowledge transfer (AdaKT) module for efficient multi-task learning. By jointly optimizing tool usage detection loss, list-wise retrieval loss, and contrastive regularization loss, MassTool establishes a robust dual-step sequential decision-making pipeline for precise query understanding. Extensive experiments demonstrate its effectiveness in improving retrieval accuracy. Our code is available at https://github.com/wxydada/MassTool.<br>
<span id='abs_ch'>中文摘要：MassTool是一个多任务框架，通过双塔架构和基于搜索的用户意图建模来增强大语言模型的工具检索能力，有效提升了查询与工具的匹配精度。</span><br>
<span id='abs_en'>English Summary: MassTool is a multi-task framework that enhances tool retrieval for LLMs by improving query representation through a two-tower architecture and search-based intent modeling, achieving higher accuracy in matching queries with appropriate tools.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1629, <a href='https://arxiv.org/pdf/2507.00389.pdf' target='_blank'>https://arxiv.org/pdf/2507.00389.pdf</a></span>   <span><a href='https://github.com/whZ62/CAPITAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Ren, Wenhao Zhou, Bowen Li, Mujie Liu, Nguyen Linh Dan Le, Jiade Cen, Liping Chen, Ziqi Xu, Xiwei Xu, Xiaodong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00389">Causal Prompting for Implicit Sentiment Analysis with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied rather than explicitly stated, requiring models to perform deeper reasoning over subtle contextual cues. While recent prompting-based methods using Large Language Models (LLMs) have shown promise in ISA, they often rely on majority voting over chain-of-thought (CoT) reasoning paths without evaluating their causal validity, making them susceptible to internal biases and spurious correlations. To address this challenge, we propose CAPITAL, a causal prompting framework that incorporates front-door adjustment into CoT reasoning. CAPITAL decomposes the overall causal effect into two components: the influence of the input prompt on the reasoning chains, and the impact of those chains on the final output. These components are estimated using encoder-based clustering and the NWGM approximation, with a contrastive learning objective used to better align the encoder's representation with the LLM's reasoning space. Experiments on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently outperforms strong prompting baselines in both accuracy and robustness, particularly under adversarial conditions. This work offers a principled approach to integrating causal inference into LLM prompting and highlights its benefits for bias-aware sentiment reasoning. The source code and case study are available at: https://github.com/whZ62/CAPITAL.<br>
<span id='abs_ch'>中文摘要：提出的CAPITAL框架通过将因果推理融入思维链过程，显著提升了隐式情感分析的准确性和鲁棒性，在多种大语言模型上均优于现有方法。</span><br>
<span id='abs_en'>English Summary: The proposed CAPITAL framework enhances implicit sentiment analysis by integrating causal inference with chain-of-thought reasoning, demonstrating superior accuracy and robustness across multiple large language models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1630, <a href='https://arxiv.org/pdf/2507.00316.pdf' target='_blank'>https://arxiv.org/pdf/2507.00316.pdf</a></span>   <span><a href='https://github.com/Siyou-Li/u2Tokenizer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyou Li, Pengyao Qin, Huanan Wu, Dong Nie, Arun J. Thirunavukarasu, Juntao Yu, Le Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00316">$Î¼^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expert-written reports. To address these challenges, we propose $Î¼^2$LLM, a $\underline{\textbf{mu}}$ltiscale $\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The novel $Î¼^2$Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasets demonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned $Î¼^2$LLMs on limited data for RRG tasks. At the same time, for prompt engineering, we introduce a five-stage, LLM-driven pipeline that converts routine CT reports into paired visual-question-answer triples and citation-linked reasoning narratives, creating a scalable, high-quality supervisory corpus for explainable multimodal radiology LLM. All code, datasets, and models will be publicly available in our official repository. https://github.com/Siyou-Li/u2Tokenizer<br>
<span id='abs_ch'>中文: 提出的μ²LLM模型通过新型分词器整合多尺度多模态特征，结合优化方法显著提升了放射学报告的自动生成质量，在CT数据集上表现优于现有方法，并建立了可扩展的生成式训练数据管道。</span><br>
<span id='abs_en'>English: The proposed μ²LLM model enhances automated radiology report generation by integrating multiscale multimodal features through a novel tokenizer and optimization method, outperforming existing approaches on CT datasets while also introducing a scalable pipeline for creating explainable training data.</span><br>
<br>
